{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TEAM LOS GALACTICOS PROJECT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "require(data.table, quietly = TRUE)\n",
    "require(glue, quietly = TRUE)\n",
    "require(ggplot2, quietly = TRUE)\n",
    "require(scatterplot3d, quietly = TRUE)\n",
    "require(gridExtra, quietly = TRUE)\n",
    "require(tidyr, quietly = TRUE)\n",
    "require(tseries, quietly = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "library(pROC, quietly = TRUE)\n",
    "library(caret, quietly = TRUE)\n",
    "library(rpart, quietly = TRUE)\n",
    "library(rpart.plot, quietly = TRUE)\n",
    "library(e1071, quietly = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: tiger\n",
      "Warning message:\n",
      "\"package 'tiger' was built under R version 3.6.3\"Loading required package: hexbin\n",
      "Loading required package: qualV\n",
      "Warning message:\n",
      "\"package 'qualV' was built under R version 3.6.3\"Loading required package: KernSmooth\n",
      "KernSmooth 2.23 loaded\n",
      "Copyright M. P. Wand 1997-2009\n",
      "\n",
      "Attaching package: 'qualV'\n",
      "\n",
      "The following objects are masked from 'package:caret':\n",
      "\n",
      "    MAE, RMSE\n",
      "\n",
      "Loading required package: klaR\n",
      "Warning message:\n",
      "\"package 'klaR' was built under R version 3.6.3\"Loading required package: MASS\n",
      "Loading required package: som\n"
     ]
    }
   ],
   "source": [
    "# Required packages for submission\n",
    "require(jsonlite)\n",
    "require(httr)\n",
    "require(data.table)\n",
    "require(tiger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Dataload\n",
    "allpath <- \"Data/IE582_Fall20_ProjectTrain.csv\"\n",
    "alldata <- read.csv(allpath)\n",
    "\n",
    "submitdatapath <- \"Data/IE582_Fall20_ProjectTest.csv\"\n",
    "submitdata <- read.csv(submitdatapath)\n",
    "\n",
    "submitdata <- submitdata[,-61] # Empty y column of the submission data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Submission Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_token <- function(username, password, url_site){\n",
    "    \n",
    "    post_body = list(username=username,password=password)\n",
    "    post_url_string = paste0(url_site,'/token/')\n",
    "    result = POST(post_url_string, body = post_body)\n",
    "\n",
    "    # error handling (wrong credentials)\n",
    "    if(result$status_code==400){\n",
    "        print('Check your credentials')\n",
    "        return(0)\n",
    "    }\n",
    "    else if (result$status_code==201){\n",
    "        output = content(result)\n",
    "        token = output$key\n",
    "    }\n",
    "\n",
    "    return(token)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "send_submission <- function(predictions, token, url_site, submit_now=F){\n",
    "    \n",
    "    format_check=check_format(predictions)\n",
    "    if(!format_check){\n",
    "        return(FALSE)\n",
    "    }\n",
    "    \n",
    "    post_string=\"list(\"\n",
    "    for(i in 1:length(predictions)){\n",
    "        if(i<length(predictions)){\n",
    "            post_string=sprintf(\"%s%s,\",post_string,predictions[i])\n",
    "        } else {\n",
    "            post_string=sprintf(\"%s%s)\",post_string,predictions[i])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    submission = eval(parse(text=post_string))\n",
    "    json_body = jsonlite::toJSON(submission, auto_unbox = TRUE)\n",
    "    submission=list(submission=json_body)\n",
    "    print(submission)\n",
    "\n",
    "    if(!submit_now){\n",
    "        print(\"You did not submit.\")\n",
    "        return(FALSE)      \n",
    "    }\n",
    "    \n",
    "\n",
    "    header = add_headers(c(Authorization=paste('Token',token,sep=' ')))\n",
    "    post_url_string = paste0(url_site,'/submission/')\n",
    "    result = POST(post_url_string, header, body=submission)\n",
    "    \n",
    "    if (result$status_code==201){\n",
    "        print(\"Successfully submitted. Below you can see the details of your submission\")\n",
    "    } else {\n",
    "        print(\"Could not submit. Please check the error message below, contact the assistant if needed.\")\n",
    "    }\n",
    "    \n",
    "    print(content(result))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "check_format <- function(predictions){\n",
    "    \n",
    "    if(all(is.numeric(predictions)) & all(predictions<=1)){\n",
    "        print(\"Format OK\")\n",
    "        return(TRUE)\n",
    "    } else {\n",
    "        print(\"Wrong format\")\n",
    "        return(FALSE)\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'59900fa65e181d65c4c676e8904fbdf065448577'"
      ],
      "text/latex": [
       "'59900fa65e181d65c4c676e8904fbdf065448577'"
      ],
      "text/markdown": [
       "'59900fa65e181d65c4c676e8904fbdf065448577'"
      ],
      "text/plain": [
       "[1] \"59900fa65e181d65c4c676e8904fbdf065448577\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this part is main code\n",
    "subm_url ='http://46.101.121.83'\n",
    "\n",
    "u_name = \"Los Galacticos\"\n",
    "p_word = \"E6lOux9kirvumsWW\"\n",
    "submit_now = FALSE\n",
    "\n",
    "username = u_name\n",
    "password = p_word\n",
    "\n",
    "token = get_token(username=u_name, password=p_word, url=subm_url)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       x1              x2               x3              x4        \n",
       " Min.   :13.00   Min.   :0.0000   Min.   :0.000   Min.   :0.0000  \n",
       " 1st Qu.:27.00   1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:0.0000  \n",
       " Median :30.00   Median :1.0000   Median :1.000   Median :1.0000  \n",
       " Mean   :30.09   Mean   :0.6712   Mean   :0.662   Mean   :0.6905  \n",
       " 3rd Qu.:33.00   3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000  \n",
       " Max.   :50.00   Max.   :1.0000   Max.   :1.000   Max.   :1.0000  \n",
       "       x5               x6              x7               x8       \n",
       " Min.   : 0.000   Min.   : 0.00   Min.   : 0.000   Min.   :13.00  \n",
       " 1st Qu.: 4.000   1st Qu.: 4.00   1st Qu.: 4.000   1st Qu.:26.00  \n",
       " Median : 9.000   Median : 9.00   Median : 9.000   Median :30.00  \n",
       " Mean   : 9.083   Mean   : 8.99   Mean   : 9.109   Mean   :30.17  \n",
       " 3rd Qu.:14.000   3rd Qu.:14.00   3rd Qu.:14.000   3rd Qu.:34.00  \n",
       " Max.   :18.000   Max.   :18.00   Max.   :18.000   Max.   :49.00  \n",
       "       x9              x10              x11              x12        \n",
       " Min.   :  0.10   Min.   :  0.00   Min.   :  0.10   Min.   :0.0000  \n",
       " 1st Qu.: 49.52   1st Qu.: 49.20   1st Qu.: 52.42   1st Qu.:0.0000  \n",
       " Median :101.45   Median : 99.65   Median : 97.50   Median :0.0000  \n",
       " Mean   :101.25   Mean   : 99.72   Mean   : 99.66   Mean   :0.3428  \n",
       " 3rd Qu.:153.30   3rd Qu.:150.25   3rd Qu.:147.75   3rd Qu.:1.0000  \n",
       " Max.   :200.00   Max.   :199.90   Max.   :199.90   Max.   :1.0000  \n",
       "      x13               x14             x15              x16        \n",
       " Min.   :0.00000   Min.   : 20.0   Min.   :0.0000   Min.   :0.0000  \n",
       " 1st Qu.:0.00000   1st Qu.:404.0   1st Qu.:1.0000   1st Qu.:0.0000  \n",
       " Median :0.00000   Median :404.0   Median :1.0000   Median :0.0000  \n",
       " Mean   :0.03327   Mean   :405.8   Mean   :0.8496   Mean   :0.1128  \n",
       " 3rd Qu.:0.00000   3rd Qu.:454.0   3rd Qu.:1.0000   3rd Qu.:0.0000  \n",
       " Max.   :1.00000   Max.   :999.0   Max.   :1.0000   Max.   :1.0000  \n",
       "      x17              x18               x19               x20         \n",
       " Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n",
       " 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n",
       " Median :0.0000   Median :0.00000   Median :0.00000   Median :0.00000  \n",
       " Mean   :0.2387   Mean   :0.02025   Mean   :0.04436   Mean   :0.04581  \n",
       " 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n",
       " Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n",
       "      x21               x22               x23              x24        \n",
       " Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n",
       " 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n",
       " Median :0.00000   Median :0.00000   Median :0.0000   Median :0.0000  \n",
       " Mean   :0.02845   Mean   :0.04725   Mean   :0.4836   Mean   :0.1037  \n",
       " 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n",
       " Max.   :1.00000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n",
       "      x25              x26                x27             x28        \n",
       " Min.   :0.0000   Min.   :0.000000   Min.   : 14.0   Min.   :0.0000  \n",
       " 1st Qu.:0.0000   1st Qu.:0.000000   1st Qu.: 79.0   1st Qu.:0.0000  \n",
       " Median :0.0000   Median :0.000000   Median :120.0   Median :0.0000  \n",
       " Mean   :0.1196   Mean   :0.008197   Mean   :127.9   Mean   :0.1013  \n",
       " 3rd Qu.:0.0000   3rd Qu.:0.000000   3rd Qu.:159.0   3rd Qu.:0.0000  \n",
       " Max.   :1.0000   Max.   :1.000000   Max.   :570.0   Max.   :1.0000  \n",
       "      x29               x30             x31               x32       \n",
       " Min.   :0.00000   Min.   : 62.0   Min.   :0.00000   Min.   :189.0  \n",
       " 1st Qu.:0.00000   1st Qu.:562.0   1st Qu.:0.00000   1st Qu.:311.0  \n",
       " Median :0.00000   Median :624.0   Median :0.00000   Median :411.0  \n",
       " Mean   :0.03423   Mean   :635.8   Mean   :0.03375   Mean   :424.8  \n",
       " 3rd Qu.:0.00000   3rd Qu.:812.0   3rd Qu.:0.00000   3rd Qu.:522.0  \n",
       " Max.   :1.00000   Max.   :999.0   Max.   :1.00000   Max.   :999.0  \n",
       "      x33               x34               x35               x36        \n",
       " Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :  0.00  \n",
       " 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:  0.00  \n",
       " Median :0.00000   Median :0.00000   Median :0.00000   Median :  0.00  \n",
       " Mean   :0.03327   Mean   :0.05979   Mean   :0.06606   Mean   : 20.05  \n",
       " 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:  0.00  \n",
       " Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :845.00  \n",
       "      x37                 x38              x39              x40        \n",
       " Min.   :0.0000000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n",
       " 1st Qu.:0.0000000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n",
       " Median :0.0000000   Median :0.0000   Median :0.0000   Median :0.0000  \n",
       " Mean   :0.0004822   Mean   :0.1418   Mean   :0.1244   Mean   :0.1239  \n",
       " 3rd Qu.:0.0000000   3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n",
       " Max.   :1.0000000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n",
       "      x41              x42              x43             x44        \n",
       " Min.   :0.0000   Min.   :  0.00   Min.   :0.000   Min.   :0.0000  \n",
       " 1st Qu.:0.0000   1st Qu.:  0.00   1st Qu.:0.000   1st Qu.:0.0000  \n",
       " Median :1.0000   Median :  0.00   Median :0.000   Median :1.0000  \n",
       " Mean   :0.7367   Mean   : 10.46   Mean   :0.027   Mean   :0.6924  \n",
       " 3rd Qu.:1.0000   3rd Qu.:  0.00   3rd Qu.:0.000   3rd Qu.:1.0000  \n",
       " Max.   :1.0000   Max.   :999.00   Max.   :1.000   Max.   :1.0000  \n",
       "      x45               x46                x47              x48        \n",
       " Min.   :0.00000   Min.   :0.000000   Min.   :0.0000   Min.   :0.0000  \n",
       " 1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:0.0000  \n",
       " Median :0.00000   Median :0.000000   Median :0.0000   Median :0.0000  \n",
       " Mean   :0.06027   Mean   :0.007232   Mean   :0.1268   Mean   :0.1437  \n",
       " 3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n",
       " Max.   :1.00000   Max.   :1.000000   Max.   :1.0000   Max.   :1.0000  \n",
       "      x49               x50         x51              x52         x53         \n",
       " Min.   :0.00000   Min.   :0   Min.   :0.0000   Min.   :0   Min.   :0.00000  \n",
       " 1st Qu.:0.00000   1st Qu.:0   1st Qu.:0.0000   1st Qu.:0   1st Qu.:0.00000  \n",
       " Median :0.00000   Median :0   Median :0.0000   Median :0   Median :0.00000  \n",
       " Mean   :0.01013   Mean   :0   Mean   :0.1051   Mean   :0   Mean   :0.08631  \n",
       " 3rd Qu.:0.00000   3rd Qu.:0   3rd Qu.:0.0000   3rd Qu.:0   3rd Qu.:0.00000  \n",
       " Max.   :1.00000   Max.   :0   Max.   :1.0000   Max.   :0   Max.   :1.00000  \n",
       "      x54              x55               x56              x57           \n",
       " Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000000  \n",
       " 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000000  \n",
       " Median :0.0000   Median :0.00000   Median :0.0000   Median :0.0000000  \n",
       " Mean   :0.3202   Mean   :0.01736   Mean   :0.4291   Mean   :0.0009643  \n",
       " 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000000  \n",
       " Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000000  \n",
       "      x58              x59                x60          y       \n",
       " Min.   :0.0000   Min.   :0.000000   Min.   :0.00000   a:1565  \n",
       " 1st Qu.:0.0000   1st Qu.:0.000000   1st Qu.:0.00000   b: 509  \n",
       " Median :0.0000   Median :0.000000   Median :0.00000           \n",
       " Mean   :0.1422   Mean   :0.006268   Mean   :0.03134           \n",
       " 3rd Qu.:0.0000   3rd Qu.:0.000000   3rd Qu.:0.00000           \n",
       " Max.   :1.0000   Max.   :1.000000   Max.   :1.00000           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$x1\n",
       "\n",
       " 13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32 \n",
       "  1   1   2   1   3   3  12  16  28  30  50  77 100 126 148 180 182 177 166 172 \n",
       " 33  34  35  36  37  38  39  40  41  42  43  44  45  47  49  50 \n",
       "133 108 106  66  63  38  28  23  13   9   4   4   1   1   1   1 \n",
       "\n",
       "$x2\n",
       "\n",
       "   0    1 \n",
       " 682 1392 \n",
       "\n",
       "$x3\n",
       "\n",
       "   0    1 \n",
       " 701 1373 \n",
       "\n",
       "$x4\n",
       "\n",
       "   0    1 \n",
       " 642 1432 \n",
       "\n",
       "$x5\n",
       "\n",
       "  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 \n",
       "105 118 113 105 107 102  91 122 118 112 107  92  94 117 121  90 123 120 117 \n",
       "\n",
       "$x6\n",
       "\n",
       "  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 \n",
       "113 113 126 103 128  97 111  93 101 126  93  90  97 113  89 120 136 109 116 \n",
       "\n",
       "$x7\n",
       "\n",
       "  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 \n",
       "127 105  98  90 103 100 102 125 112 120 112 101 111  98 122 106 112 121 109 \n",
       "\n",
       "$x8\n",
       "\n",
       " 13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32 \n",
       "  1   1   1   5   7  14  14  35  35  55  76  81 102 110 118 160 146 152 143 145 \n",
       " 33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49 \n",
       "130  91  96  80  67  58  43  36  24  16   7   4   6   4   5   3   3 \n",
       "\n",
       "$x9\n",
       "\n",
       "  0.1   0.2   0.3   0.4   0.6   0.9   1.3   1.4   1.5   1.8   1.9     2   2.1 \n",
       "    2     2     1     2     2     1     1     1     1     1     1     2     1 \n",
       "  2.2   2.3   2.4   2.5   2.6   2.8   3.1   3.2   3.4   3.5   3.7   3.8     4 \n",
       "    1     6     3     2     3     2     1     1     1     1     3     1     1 \n",
       "  4.1   4.4   4.5   4.9   5.2   5.4   5.6   6.3   6.4   6.5   6.6   6.8     7 \n",
       "    1     1     3     1     3     1     1     2     1     1     1     1     1 \n",
       "  7.3   7.4   7.7   8.1   8.2   8.5   8.6   8.7   8.8   9.1   9.2   9.3   9.4 \n",
       "    4     2     2     2     2     1     1     4     3     1     1     1     1 \n",
       "  9.5   9.6   9.7   9.9  10.2  10.3  10.5  10.7  10.8  10.9    11  11.1  11.2 \n",
       "    1     2     2     2     1     2     1     1     2     1     3     1     2 \n",
       " 11.5  11.6  11.8  12.1  12.3  12.5  12.6  12.7  12.8    13  13.2  13.4  13.6 \n",
       "    1     2     1     1     2     2     1     2     2     1     3     2     1 \n",
       " 13.8  13.9    14  14.1  14.3  14.5  14.7  14.9    15  15.1  15.2  15.4  15.7 \n",
       "    1     1     2     3     1     1     1     1     2     2     4     2     1 \n",
       " 15.8    16  16.1  16.2  16.3  16.4  16.5  16.7  16.8    17  17.1  17.2  17.3 \n",
       "    1     2     1     1     1     1     1     3     3     1     1     1     1 \n",
       " 17.4  17.5  17.6  17.8  17.9    18  18.2  18.3  18.5  18.6  18.8    19  19.2 \n",
       "    4     3     1     2     2     1     1     2     1     1     2     3     1 \n",
       " 19.3  19.5  19.9    20  20.1  20.2  20.3  20.5  20.6  20.7  20.8    21  21.2 \n",
       "    1     1     1     1     1     1     2     3     2     4     2     1     1 \n",
       " 21.3  21.4  21.5  21.6  21.7  21.8  21.9    22  22.5  22.6  22.7  22.8  22.9 \n",
       "    1     1     1     2     1     1     1     1     1     1     1     1     2 \n",
       "   23  23.2  23.3  23.6  23.7  23.9    24  24.3  24.4  24.5  24.7  24.8  24.9 \n",
       "    3     2     1     1     1     1     1     2     2     1     2     1     1 \n",
       "   25  25.4  25.7  25.8    26  26.1  26.2  26.3  26.5  26.6  26.7  26.8  26.9 \n",
       "    1     1     2     1     1     2     2     1     1     1     1     2     2 \n",
       "   27  27.1  27.2  27.3  27.5  27.6  27.7  27.8  27.9    28  28.1  28.2  28.5 \n",
       "    3     3     1     2     1     1     2     2     1     3     2     2     1 \n",
       " 28.6  28.7    29  29.1  29.2  29.5  29.6  29.7  29.9    30  30.2  30.4  30.5 \n",
       "    2     2     1     1     1     1     2     2     1     1     2     3     1 \n",
       " 30.6  30.7  30.8    31  31.2  31.3  31.5  31.8  31.9    32  32.1  32.2  32.3 \n",
       "    1     1     1     1     2     2     1     1     1     1     1     3     1 \n",
       " 32.5  32.6  32.7  32.8  32.9    33  33.1  33.2  33.3  33.5  33.7  33.8  33.9 \n",
       "    1     2     1     2     1     1     3     3     1     3     1     1     1 \n",
       "   34  34.1  34.3  34.4  34.5  34.6  34.7  34.9    35  35.1  35.2  35.3  35.4 \n",
       "    1     1     2     3     2     1     1     3     4     1     1     2     1 \n",
       " 35.5  35.6  35.7  35.8  35.9  36.1  36.2  36.4  36.5  36.6  36.7  36.9    37 \n",
       "    2     1     1     1     1     2     1     2     2     1     1     1     1 \n",
       " 37.2  37.3  37.6  37.7    38  38.1  38.3  38.4  38.8  39.1  39.2  39.3  39.4 \n",
       "    1     1     2     2     2     1     2     1     1     1     1     1     1 \n",
       " 39.5  39.6  39.9    40  40.2  40.5  40.7  40.9    41  41.1  41.2  41.3  41.4 \n",
       "    1     1     1     1     3     2     2     1     1     1     1     1     1 \n",
       " 41.6  41.7  41.8  42.1  42.2  42.4  42.5  42.6  42.7  42.8  42.9  43.1  43.3 \n",
       "    1     1     1     1     3     2     1     2     1     4     1     3     1 \n",
       " 43.5  43.6  43.8  43.9    44  44.1  44.2  44.3  44.4  44.5  44.6  44.8  44.9 \n",
       "    2     1     1     1     2     2     1     1     2     2     3     3     1 \n",
       "   45  45.2  45.5  45.7  45.8  45.9  46.2  46.3  46.5  46.7  46.8  46.9    47 \n",
       "    1     2     3     3     1     2     4     2     2     1     1     1     1 \n",
       " 47.1  47.2  47.3  47.4  47.5  47.6  47.8  48.1  48.2  48.5  48.6  48.7  48.8 \n",
       "    1     2     1     3     1     1     4     1     1     1     2     1     3 \n",
       " 49.3  49.4  49.5  49.6  49.7  49.9  50.1  50.2  50.3  50.4  50.7  50.8    51 \n",
       "    3     2     2     3     4     1     1     2     1     1     1     2     1 \n",
       " 51.1  51.2  51.4  51.5  51.6    52  52.2  52.3  52.5  52.6  52.7  52.8  52.9 \n",
       "    3     1     1     1     3     2     1     2     2     2     1     1     1 \n",
       " 53.2  53.4  53.5  53.8  54.2  54.4  54.5  54.6  54.7  54.8  54.9    55  55.1 \n",
       "    3     2     1     3     2     2     2     1     2     1     1     2     3 \n",
       " 55.2  55.5  55.6  55.8  56.1  56.2  56.8  56.9    57  57.1  57.2  57.5  57.7 \n",
       "    1     1     1     1     3     1     3     2     2     1     1     1     1 \n",
       " 57.8  57.9    58  58.1  58.3  58.4  58.5  58.9  59.1  59.2  59.3  59.4  59.6 \n",
       "    1     3     1     2     2     2     1     1     2     2     2     2     2 \n",
       " 59.7  60.1  60.2  60.3  60.5  60.6  60.7  60.8  60.9    61  61.1  61.2  61.4 \n",
       "    1     3     1     1     1     1     1     1     1     1     2     1     1 \n",
       " 61.5  61.6  61.7  61.8    62  62.2  62.3  62.4  62.5  62.6  62.7  62.9    63 \n",
       "    1     1     1     2     1     1     2     1     1     1     3     1     1 \n",
       " 63.3  63.4  63.5  63.6  63.8    64  64.2  64.3  64.6  64.7  64.9  65.1  65.4 \n",
       "    1     2     2     2     1     1     3     1     1     2     2     1     1 \n",
       " 65.6  65.9    66  66.3  66.5  66.6  66.7  66.8  66.9    67  67.1  67.4  67.6 \n",
       "    2     2     1     2     1     1     2     1     1     1     2     2     1 \n",
       " 67.9  68.1  68.2  68.4  68.5  68.6  68.7  68.8  69.1  69.2  69.4  69.5  69.6 \n",
       "    1     2     1     1     1     4     1     1     1     2     1     1     1 \n",
       " 69.7  69.9  70.2  70.3  70.4  70.5  70.6  70.7  70.9    71  71.1  71.3  71.4 \n",
       "    1     1     1     1     2     1     1     1     1     1     1     2     1 \n",
       " 71.5  71.8  71.9    72  72.1  72.2  72.5  72.6  72.7    73  73.2  73.3  73.4 \n",
       "    2     1     4     2     1     1     2     2     1     4     2     1     2 \n",
       " 73.5  73.6  73.7  73.8    74  74.2  74.4  74.6  74.8  74.9  75.1  75.3  75.4 \n",
       "    2     2     1     2     1     1     2     1     2     1     3     1     1 \n",
       " 75.5  75.6  75.9  76.1  76.3  76.7    77  77.3  77.5  77.6  77.8  77.9  78.1 \n",
       "    3     2     1     1     2     2     2     1     2     1     1     4     2 \n",
       " 78.2  78.5  78.8    79  79.1  79.2  79.3  79.4  79.5  79.7  79.8  79.9  80.1 \n",
       "    1     1     1     1     1     2     4     1     1     1     1     1     1 \n",
       " 80.4  80.6  80.8  80.9  81.1  81.2  81.3  81.4  81.6  81.7  81.8    82  82.1 \n",
       "    1     2     1     1     1     1     1     3     1     1     2     1     1 \n",
       " 82.3  82.6  82.7  82.8  82.9    83  83.4  83.5  83.9    84  84.1  84.3  84.4 \n",
       "    1     1     1     1     3     2     1     2     2     1     1     3     1 \n",
       " 84.5  84.7  84.8  84.9    85  85.1  85.2  85.3  85.4  85.5  85.6  85.7  86.3 \n",
       "    1     2     1     1     1     1     3     2     1     1     1     4     1 \n",
       " 86.5  86.6  86.7  86.9    87  87.2  87.3  87.4  87.5  87.6  87.7  87.8    88 \n",
       "    1     1     2     1     2     1     1     1     3     1     3     2     1 \n",
       " 88.1  88.3  88.4  88.5  88.7  88.8  88.9    89  89.1  89.2  89.4  89.6  89.7 \n",
       "    4     2     1     2     3     1     1     1     1     2     1     1     1 \n",
       " 89.8  89.9    90  90.2  90.4  90.6    91  91.1  91.3  91.4  91.5  91.7  91.8 \n",
       "    1     1     1     1     1     1     1     1     2     1     2     1     2 \n",
       " 91.9    92  92.2  92.3  92.4  92.5  92.7  92.8    93  93.2  93.3  93.4  93.5 \n",
       "    1     3     1     4     1     2     2     1     2     2     1     1     1 \n",
       " 93.6  93.7  93.8  94.1  94.2  94.3  94.4  94.5  94.7  95.1  95.2  95.4  95.5 \n",
       "    2     3     2     4     1     1     2     2     2     1     1     2     2 \n",
       " 95.7  95.8    96  96.1  96.3  96.4  96.5  96.8  96.9    97  97.1  97.3  97.4 \n",
       "    4     2     1     1     1     1     1     1     1     2     4     2     2 \n",
       " 97.7    98  98.4  98.6  98.7  98.8  98.9    99  99.1  99.2  99.3  99.5  99.6 \n",
       "    1     1     1     2     2     1     2     2     1     2     2     1     2 \n",
       " 99.7  99.8 100.2 100.5 100.6 100.7 100.8 100.9 101.1 101.4 101.5 101.6 102.1 \n",
       "    2     1     1     1     4     1     3     3     1     1     1     1     2 \n",
       "102.2 102.3 102.4 102.5 102.7 102.8   103 103.2 103.3 103.5 103.7 103.8 103.9 \n",
       "    2     2     2     1     1     3     2     2     2     2     2     1     1 \n",
       "104.4 104.8 104.9 105.4 105.8 105.9 106.2 106.6 106.8 106.9 107.1 107.2 107.3 \n",
       "    5     3     2     1     2     1     1     1     2     1     3     4     3 \n",
       "107.5 107.7 107.8 107.9   108 108.1 108.2 108.3 108.4 108.6 108.7 108.8 109.1 \n",
       "    3     1     2     1     2     3     2     2     1     1     2     3     1 \n",
       "109.2 109.3 109.4 109.8 109.9   110 110.1 110.2 110.3 110.5 110.6 110.7 110.8 \n",
       "    1     1     1     1     2     1     1     2     2     1     3     1     1 \n",
       "  111 111.1 111.2 111.3 111.5 111.6 111.8 111.9   112 112.2 112.3 112.4 112.5 \n",
       "    3     1     2     3     1     1     1     2     1     1     2     1     1 \n",
       "112.6 112.7 112.9 113.1 113.3 113.5 113.8 113.9   114 114.1 114.2 114.3 114.5 \n",
       "    2     1     2     1     2     3     1     1     1     1     1     2     1 \n",
       "114.6 114.7 114.8 114.9 115.1 115.2 115.3 115.5 115.6 115.7 115.8 116.2 116.6 \n",
       "    1     2     1     2     1     2     1     1     3     1     2     1     2 \n",
       "116.7 116.9 117.1 117.2 117.3 117.6 117.7 117.8 117.9   118 118.4 118.5 118.6 \n",
       "    3     1     2     2     1     1     1     1     2     2     3     1     3 \n",
       "118.8 118.9 119.1 119.2 119.3 119.5 119.6 119.7 119.8 119.9 120.1 120.2 120.3 \n",
       "    1     1     1     1     1     1     1     2     1     1     1     2     2 \n",
       "120.6 120.7 120.8 120.9   121 121.1 121.2 121.3 121.4 121.8 121.9 122.1 122.2 \n",
       "    1     2     1     1     1     1     2     2     1     2     1     2     2 \n",
       "122.3 122.5 122.6 122.7 122.8 122.9   123 123.3 123.4 123.5 123.6 123.7 123.8 \n",
       "    1     1     2     1     1     1     1     3     1     2     1     2     2 \n",
       "123.9 124.2 124.3 124.4 124.5 124.6 124.7 124.9   125 125.1 125.2 125.4 125.8 \n",
       "    1     2     2     1     2     2     3     1     2     1     1     1     2 \n",
       "125.9   126 126.1 126.3 126.5 126.7 126.8 126.9   127 127.4 127.5 127.6 127.7 \n",
       "    2     1     3     1     1     1     2     1     1     1     2     1     5 \n",
       "127.9   128 128.1 128.5 128.6 128.7 128.8 129.1 129.2 129.3 129.4 129.6 129.7 \n",
       "    1     1     1     1     1     1     2     1     2     2     2     1     2 \n",
       "129.8 129.9   130 130.3 130.6 130.8 130.9 131.1 131.3 131.4 131.6 131.9 132.1 \n",
       "    1     2     1     1     3     1     1     3     2     2     1     2     1 \n",
       "132.2 132.3 132.5 132.7 132.8 132.9   133 133.1 133.2 133.4 133.5 133.6 133.7 \n",
       "    1     1     1     1     1     1     1     1     1     2     3     1     1 \n",
       "134.1 134.2 134.4 134.5 134.6 134.7 134.8 134.9 135.2 135.3 135.5 135.8   136 \n",
       "    1     1     1     2     1     3     2     2     1     2     1     2     1 \n",
       "136.5 136.6 136.9   137 137.2 137.4 137.5 137.7 137.9 138.3 138.4 138.8 138.9 \n",
       "    2     2     1     2     1     2     3     1     1     1     1     2     4 \n",
       "139.1 139.3 139.4 139.6 139.7 139.9   140 140.1 140.3 140.5 140.6 140.7 140.8 \n",
       "    1     2     1     2     2     2     1     1     2     2     2     2     1 \n",
       "  141 141.1 141.2 141.3 141.4 141.5 141.7 141.8 141.9   142 142.1 142.3 142.5 \n",
       "    1     1     1     3     2     1     1     2     1     1     1     1     3 \n",
       "142.6 142.7 142.8 143.1 143.2 143.4 143.6 143.9   144 144.4 144.5 144.7 144.9 \n",
       "    1     3     3     2     1     1     3     1     4     2     3     2     2 \n",
       "  145 145.1 145.2 145.7 145.9   146 146.4 146.6 146.7   147 147.1 147.2 147.5 \n",
       "    3     1     1     1     2     1     1     1     1     2     1     1     1 \n",
       "147.6 147.7 147.8 147.9   148 148.1 148.2 148.3 148.4   149 149.2 149.3 149.4 \n",
       "    2     3     2     2     2     1     2     4     1     2     1     3     1 \n",
       "149.5 149.9   150 150.3 150.5 150.6 150.8 151.1 151.2 151.3 151.4 151.5 151.8 \n",
       "    1     1     1     1     3     3     1     1     1     3     1     2     1 \n",
       "151.9 152.1 152.2 152.3 152.5 152.6 152.8 153.1 153.2 153.3 153.5 153.6 153.7 \n",
       "    1     1     1     1     1     1     1     2     2     2     1     2     1 \n",
       "153.9   154 154.1 154.2 154.3 154.6 154.7 154.9   155 155.3 155.5 155.6 155.8 \n",
       "    2     2     4     1     1     2     1     5     2     3     2     2     3 \n",
       "  156 156.2 156.5 156.6 156.7 156.8 156.9   157 157.1 157.2 157.6 157.7 157.9 \n",
       "    1     1     2     1     1     2     1     3     1     5     2     1     1 \n",
       "  158 158.1 158.2 158.3 158.4 158.5 158.6 158.9 159.1 159.2 159.3 159.4 159.5 \n",
       "    1     2     1     2     1     1     1     3     2     1     1     2     2 \n",
       "159.6 159.7 159.9   160 160.1 160.2 160.3 160.6 160.7 160.8   161 161.1 161.2 \n",
       "    2     1     2     1     1     3     1     1     2     1     1     1     2 \n",
       "161.3 161.4 161.5 161.6 161.7 161.8 161.9 162.4 162.5 162.6 162.7 162.8 162.9 \n",
       "    3     2     1     1     2     1     1     3     2     1     1     1     3 \n",
       "  163 163.1 163.2 163.3 163.6 163.7 163.8 163.9   164 164.2 164.3 164.4 164.7 \n",
       "    2     3     2     1     2     2     2     1     1     1     1     1     3 \n",
       "164.8 164.9 165.1 165.2 165.3 165.4 165.6 165.8 165.9   166 166.1 166.2 166.3 \n",
       "    1     1     1     2     1     2     2     2     2     2     1     2     1 \n",
       "166.4 166.6 166.7 166.8 166.9   167 167.1 167.2 167.3 167.4 167.6 167.7 167.9 \n",
       "    1     1     1     1     1     1     2     2     2     1     1     1     1 \n",
       "  168 168.1 168.3 168.4 168.6 168.7 168.9 169.7 169.8 170.1 170.2 170.5 170.7 \n",
       "    1     1     2     2     1     2     1     2     1     5     1     1     3 \n",
       "170.9 171.1 171.3 171.4 171.5 171.6 171.8 172.1 172.2 172.3 172.4 172.6 172.8 \n",
       "    1     1     4     1     3     1     1     2     1     1     2     1     1 \n",
       "172.9 173.1 173.3 173.4 173.5 173.6 173.7 173.9   174 174.1 174.2 174.3 174.4 \n",
       "    2     1     3     1     2     2     2     1     1     1     1     1     3 \n",
       "174.5 174.6 174.7 174.9   175 175.1 175.2 175.4 175.5   176 176.1 176.2 176.3 \n",
       "    2     1     1     2     1     2     1     1     1     1     1     2     3 \n",
       "176.4 176.5 176.9   177 177.2 177.3 177.4 177.5 177.7 177.8 177.9   178 178.1 \n",
       "    3     2     2     2     3     1     1     1     1     2     1     2     1 \n",
       "178.4 178.5 178.6 178.7 178.8 179.1 179.2 179.4 179.7 179.9 180.1 180.3 180.4 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     2     3 \n",
       "180.5 180.6 180.7 180.9 181.1 181.3 181.5 181.6 181.7 181.8 181.9   182 182.1 \n",
       "    2     1     1     2     2     4     1     2     1     1     1     1     2 \n",
       "182.2 182.4 182.5 182.6 182.9   183 183.1 183.2 183.5 183.7 183.8 183.9 184.1 \n",
       "    1     1     2     1     4     2     1     1     1     1     1     1     1 \n",
       "184.4 184.5 184.8 184.9   185 185.1 185.2 185.5 185.6 185.7 185.8 185.9 186.1 \n",
       "    2     2     1     3     3     2     1     1     1     2     2     2     1 \n",
       "186.2 186.6 186.7 186.8 186.9 187.2 187.3 187.4 187.6 187.7 187.9   188 188.1 \n",
       "    1     1     2     2     1     2     1     1     2     1     1     1     2 \n",
       "188.2 188.3 188.4 188.8 188.9   189 189.1 189.3 189.5 189.6 189.7 189.8 189.9 \n",
       "    2     2     2     2     1     1     5     2     3     3     2     2     1 \n",
       "  190 190.2 190.3 190.4 190.6 190.8   191 191.4 191.8 192.1 192.3 192.4 192.5 \n",
       "    1     2     2     2     3     1     1     1     1     1     1     1     1 \n",
       "192.6 192.7 192.8 192.9   193 193.2 193.6 193.7 193.9 194.1 194.3 194.4 194.5 \n",
       "    2     1     4     1     1     1     3     1     1     1     1     2     2 \n",
       "194.6 194.8   195 195.3 195.4 195.5 195.6 195.9   196 196.1 196.2 196.3 196.4 \n",
       "    2     1     1     2     3     2     1     2     1     1     1     4     1 \n",
       "196.5 196.6 196.8   197 197.1 197.3 197.4 197.5 197.6 197.8 197.9   198 198.1 \n",
       "    3     4     1     1     2     2     5     1     1     2     1     1     1 \n",
       "198.2 198.5 198.6 198.8 198.9   199 199.1 199.2 199.3 199.4 199.5 199.6 199.8 \n",
       "    3     2     3     2     1     1     1     1     1     2     1     3     3 \n",
       "199.9   200 \n",
       "    3     1 \n",
       "\n",
       "$x10\n",
       "\n",
       "    0   0.1   0.2   0.4   0.6   0.8   0.9     1   1.4   1.5   1.7   1.8   1.9 \n",
       "    2     1     2     2     1     2     3     1     2     1     1     2     1 \n",
       "  2.2   2.3   2.4   2.5   2.6   2.7   2.8   2.9   3.1   3.2   3.3   3.5   3.6 \n",
       "    2     3     2     1     2     1     1     1     2     2     1     1     2 \n",
       "  3.8   3.9   4.1   4.3   4.4   4.5   4.6   4.8   4.9     5   5.1   5.2   5.4 \n",
       "    1     1     1     2     1     1     1     2     1     1     2     1     1 \n",
       "  5.6   5.7   5.8   5.9     6   6.1   6.2   6.6   6.7   6.8     7   7.1   7.3 \n",
       "    2     2     2     2     1     2     1     2     3     2     1     1     1 \n",
       "  7.5   7.6   7.7   7.8   7.9   8.2   8.3   8.4   8.6   8.7   8.8   9.1   9.3 \n",
       "    1     2     1     1     1     2     1     1     2     2     1     1     2 \n",
       "  9.5   9.6   9.7   9.8    10  10.1  10.2  10.3  10.5  10.7  10.8  10.9    11 \n",
       "    3     2     1     2     1     2     1     4     2     1     1     1     2 \n",
       " 11.1  11.2  11.3  11.4  11.5  11.6  11.9    12  12.1  12.2  12.4  12.5  12.6 \n",
       "    1     1     1     2     1     2     1     1     1     2     2     2     3 \n",
       " 12.9  13.2  13.3  13.4  13.5  13.6  13.7  13.8  13.9    14  14.1  14.2  14.4 \n",
       "    2     1     1     1     2     3     1     1     1     1     2     1     4 \n",
       " 14.5  14.8    15  15.3  15.4  15.5  15.7  15.9    16  16.1  16.2  16.3  16.4 \n",
       "    1     1     1     1     1     1     1     2     1     1     1     1     2 \n",
       " 16.5  16.6  16.8  16.9  17.1  17.2  17.3  17.4  17.5  17.6  17.7  17.8  17.9 \n",
       "    1     2     2     2     1     2     3     1     1     1     2     1     1 \n",
       "   18  18.1  18.3  18.4  18.5  18.6  18.7  18.8    19  19.2  19.3  19.4  19.7 \n",
       "    1     1     1     1     2     2     1     1     1     1     2     1     2 \n",
       " 19.8  19.9    20  20.2  20.3  20.4  20.5  20.7  20.8  21.1  21.2  21.3  21.4 \n",
       "    2     1     1     1     1     1     3     1     1     2     1     2     3 \n",
       " 21.6  21.8  21.9  22.1  22.6  22.8  23.1  23.5  23.6  23.8  23.9    24  24.1 \n",
       "    1     1     1     1     1     3     2     1     1     1     1     2     2 \n",
       " 24.2  24.3  24.5  24.6  24.7  24.8  24.9    25  25.1  25.2  25.5  25.7  25.8 \n",
       "    2     2     1     2     3     1     2     1     2     1     2     3     1 \n",
       " 25.9    26  26.1  26.2  26.3  26.5  26.6  26.8  27.1  27.2  27.3  27.4  27.5 \n",
       "    1     1     3     2     1     1     2     1     1     1     3     2     2 \n",
       " 27.6  27.7    28  28.1  28.2  28.4  28.6  28.8  28.9    29  29.1  29.3  29.6 \n",
       "    1     1     1     1     3     1     2     2     2     1     2     1     2 \n",
       " 29.7  30.1  30.4  30.5    31  31.2  31.3  31.5  31.7  31.8  31.9    32  32.1 \n",
       "    2     2     1     2     1     2     2     1     2     2     3     1     1 \n",
       " 32.3  32.4  32.5  32.8  32.9    33  33.2  33.3  33.6  33.7  33.9  34.1  34.2 \n",
       "    1     3     2     1     1     2     1     1     2     1     1     1     1 \n",
       " 34.3  34.5  34.7  34.9    35  35.1  35.4  35.5  35.7  35.8    36  36.1  36.3 \n",
       "    1     4     3     3     1     2     1     1     2     1     2     1     1 \n",
       " 36.5  36.6  36.7  36.8  37.1  37.6  37.7  37.8  37.9    38  38.1  38.2  38.6 \n",
       "    1     1     2     2     1     1     2     2     1     2     1     1     1 \n",
       " 38.7  38.8  38.9    39  39.3  39.4  39.5  39.7  39.9  40.2  40.3  40.4  40.5 \n",
       "    3     1     1     2     1     1     1     2     2     1     2     1     1 \n",
       " 40.7  40.8  40.9  41.1  41.3  41.4  41.5  41.6  41.7  42.1  42.2  42.3  42.5 \n",
       "    1     1     1     1     2     2     2     2     1     1     1     1     3 \n",
       " 42.7  42.8  43.1  43.2  43.3  43.5  43.7  43.9    44  44.1  44.2  44.3  44.5 \n",
       "    1     1     2     2     1     1     2     1     1     2     1     2     1 \n",
       " 44.6  44.7  44.8  44.9    45  45.1  45.2  45.3  45.4  45.5  45.6  45.8  46.1 \n",
       "    2     1     1     1     2     3     2     1     1     3     2     2     1 \n",
       " 46.2  46.4  46.6  46.7  46.8  46.9    47  47.1  47.2  47.3  47.4  47.5  47.6 \n",
       "    1     1     4     2     3     1     2     1     2     1     3     1     2 \n",
       " 47.7  47.8    48  48.1  48.2  48.3  48.4  48.5  48.6  48.7  48.9  49.2  49.3 \n",
       "    3     1     1     1     3     2     1     1     1     3     2     3     2 \n",
       " 49.5  49.6  49.7  50.1  50.3  50.4  50.5  50.6  50.7  50.8    51  51.1  51.3 \n",
       "    4     1     1     3     1     2     1     2     2     1     3     1     1 \n",
       " 51.4  51.6  51.8  51.9    52  52.1  52.7    53  53.1  53.8    54  54.2  54.4 \n",
       "    1     2     1     1     1     1     2     1     1     1     1     2     2 \n",
       " 54.7  54.8  55.4  55.5  56.1  56.3  56.4  56.5  56.6  56.7  56.8  56.9  57.1 \n",
       "    1     3     3     1     4     1     1     4     3     3     2     1     2 \n",
       " 57.2  57.4  57.7  57.8  57.9    58  58.2  58.4  58.5  58.6  58.7  58.8    59 \n",
       "    1     1     1     1     3     1     2     1     1     1     2     2     1 \n",
       " 59.1  59.2  59.5  59.6  59.7  59.8    60  60.1  60.2  60.3  60.4  60.6  60.7 \n",
       "    1     3     3     1     1     1     1     1     3     1     1     1     1 \n",
       " 60.8  60.9    61  61.2  61.3  61.5  61.7  61.8  61.9  62.2  62.3  62.4  62.6 \n",
       "    3     4     1     1     2     1     1     2     1     2     1     1     2 \n",
       " 62.8  62.9  63.1  63.3  63.4  63.6  63.9    64  64.1  64.2  64.5  64.6  64.8 \n",
       "    1     1     1     1     1     2     2     1     1     2     4     1     1 \n",
       " 65.3  65.4  65.5  65.7  65.8  65.9    66  66.1  66.2  66.3  66.4  66.6  66.9 \n",
       "    2     1     3     2     1     3     2     1     3     1     2     2     1 \n",
       " 67.1  67.2  67.3  67.4  67.5  67.8  67.9    68  68.2  68.4  68.5  68.8  68.9 \n",
       "    1     2     1     3     1     4     1     1     2     3     2     4     2 \n",
       "   69  69.1  69.2  69.3  69.4  69.6  69.7  69.8  69.9  70.3  70.4  70.5  70.7 \n",
       "    1     2     2     1     1     2     3     2     2     1     1     3     1 \n",
       " 70.8  70.9    71  71.1  71.5  71.7  71.8    72  72.1  72.2  72.3  72.6  72.8 \n",
       "    1     1     2     3     1     1     2     1     2     2     2     1     2 \n",
       "   73  73.3  73.5  73.9  74.2  74.4  74.5  74.6  74.7  74.8  74.9    75  75.1 \n",
       "    1     3     1     1     1     1     1     2     2     1     2     1     4 \n",
       " 75.3  75.5  75.6  75.8    76  76.1  76.2  76.3  76.4  76.5  76.6    77  77.1 \n",
       "    2     1     2     2     3     2     2     2     1     1     1     1     2 \n",
       " 77.4  77.6  77.8    78  78.1  78.3  78.6  78.7  78.9  79.3  79.4  79.5  79.6 \n",
       "    3     1     3     2     1     1     1     2     1     2     2     2     3 \n",
       " 79.8    80  80.1  80.2  80.3  80.4  80.5  80.6  80.7  80.8  80.9  81.1  81.3 \n",
       "    2     2     1     2     3     1     1     1     1     2     1     1     1 \n",
       " 81.6  81.9    82  82.1  82.2  82.3  82.4  82.5  82.8  82.9    83  83.3  83.4 \n",
       "    1     3     1     2     1     4     1     2     1     1     3     1     4 \n",
       " 83.6    84  84.1  84.3  84.4  84.5  84.6  84.8  84.9    85  85.2  85.3  85.4 \n",
       "    1     1     1     1     1     2     1     2     4     1     1     2     1 \n",
       " 85.5  85.6  85.7  85.8  85.9    86  86.1  86.2  86.3  86.4  86.7    87  87.2 \n",
       "    1     3     2     1     2     1     2     2     1     2     1     3     2 \n",
       " 87.3  87.7  87.8  87.9    88  88.1  88.2  88.4  88.7  88.8  88.9  89.2  89.3 \n",
       "    2     1     1     4     3     1     2     2     1     1     1     2     1 \n",
       " 89.5  89.7  89.8  89.9    90  90.3  90.7  90.8    91  91.3  91.4  91.5  91.7 \n",
       "    1     1     1     1     1     2     1     1     3     1     2     2     2 \n",
       " 92.1  92.2  92.3  92.4  92.6  92.7  92.8    93  93.1  93.2  93.4  93.6  93.7 \n",
       "    1     1     1     2     1     1     3     1     2     1     1     2     3 \n",
       " 93.8  93.9    94  94.3  94.4  94.6  94.7  94.8  94.9  95.1  95.2  95.3  95.4 \n",
       "    2     2     1     1     2     2     1     1     1     2     2     1     1 \n",
       " 95.9  96.1  96.2  96.3  96.4  96.5  96.8  96.9  97.2  97.3  97.4  97.5  97.8 \n",
       "    1     1     2     1     1     2     2     1     2     3     1     1     2 \n",
       " 97.9    98  98.3  98.6  98.7  98.9    99  99.1  99.3  99.6  99.7   100 100.1 \n",
       "    3     1     2     1     2     5     1     2     2     1     2     2     2 \n",
       "100.2 100.3 100.4 100.5 100.7 100.8 101.1 101.4 101.6 101.8 101.9 102.2 102.4 \n",
       "    1     2     1     1     3     1     1     2     1     1     2     3     2 \n",
       "102.5 102.6 102.7 102.8   103 103.1 103.2 103.3 103.4 103.6 103.9 104.1 104.2 \n",
       "    2     2     1     2     1     1     1     1     2     1     1     1     1 \n",
       "104.3 104.6 104.7 104.8 105.2 105.6 105.7 105.9   106 106.2 106.3 106.6 106.7 \n",
       "    1     1     1     1     1     3     1     1     2     2     1     2     2 \n",
       "106.8 106.9 107.1 107.2 107.3 107.6 107.7 107.8 107.9 108.2 108.3 108.4 108.5 \n",
       "    2     3     1     1     1     4     2     1     1     1     2     2     1 \n",
       "108.6 108.7 108.8 108.9   109 109.1 109.3 109.4 109.5 109.8   110 110.1 110.3 \n",
       "    1     1     1     2     1     2     2     1     1     1     1     2     2 \n",
       "110.4 110.5 110.7 110.8 110.9   111 111.1 111.3 111.6 111.8 111.9 112.1 112.2 \n",
       "    1     1     1     1     2     1     2     1     4     1     1     2     2 \n",
       "112.3 112.5 112.6 113.2 113.4 113.5 113.7 113.9   114 114.1 114.2 114.3 114.4 \n",
       "    2     3     1     2     1     1     1     2     3     3     1     2     2 \n",
       "114.5 114.7 114.8 115.1 115.4 115.6 115.8 116.1 116.3 116.5 116.8 116.9   117 \n",
       "    1     3     2     3     2     1     1     3     1     1     2     1     3 \n",
       "117.6 117.8 117.9   118 118.1 118.2 118.4 118.6 118.7 118.9   119 119.5 119.7 \n",
       "    3     1     4     1     1     2     3     1     2     1     2     1     2 \n",
       "119.8 119.9 120.2 120.4 120.5 120.6 120.7 120.8 121.1 121.3 121.4 121.7 121.8 \n",
       "    2     1     3     1     1     2     2     1     3     2     1     1     2 \n",
       "121.9 122.3 122.4 122.7 122.8 122.9 123.1 123.3 123.5 123.6 123.7 123.8 123.9 \n",
       "    2     1     3     1     1     4     3     1     1     1     1     1     2 \n",
       "124.1 124.3 124.4 124.5 124.6 125.1 125.2 125.3 125.4 125.6 125.7   126 126.1 \n",
       "    3     1     1     1     2     2     4     1     1     4     1     2     2 \n",
       "126.2 126.3 126.5 126.6 126.7 126.8 126.9   127 127.1 127.4 127.5 127.7 127.9 \n",
       "    2     1     1     1     1     4     4     1     1     1     1     1     1 \n",
       "  128 128.1 128.2 128.4 128.5   129 129.1 129.2 129.3 129.4 129.5 129.6 129.7 \n",
       "    1     1     1     1     1     1     1     1     2     1     3     2     2 \n",
       "129.8 129.9   130 130.1 130.2 130.5 130.6 130.7 130.8 130.9   131 131.1 131.3 \n",
       "    1     1     1     1     1     1     1     1     1     2     2     1     1 \n",
       "131.4 131.9   132 132.1 132.2 132.3 132.4 132.5 132.6 132.7 132.8 132.9   133 \n",
       "    1     1     1     1     1     1     2     2     1     1     2     1     4 \n",
       "133.1 133.2 133.3 133.4 133.5 133.6 134.1 134.2 134.3 134.4 134.5 134.6 134.7 \n",
       "    2     2     2     2     2     1     1     2     1     1     1     1     4 \n",
       "135.2 135.4 135.5 135.6 135.7 135.8 135.9   136 136.1 136.2 136.3 136.4 136.7 \n",
       "    1     2     1     1     1     2     1     2     1     1     2     1     1 \n",
       "136.8 137.1 137.2 137.3 137.7 137.8 137.9   138 138.1 138.2 138.3 138.5 138.6 \n",
       "    1     1     1     2     2     3     1     1     2     2     2     1     2 \n",
       "138.7 138.9 139.1 139.2 139.4 139.5 139.6 139.7   140 140.1 140.2 140.3 140.4 \n",
       "    2     2     1     2     2     2     1     3     2     1     1     1     1 \n",
       "140.5 140.6 140.7 140.9 141.1 141.3 141.4 141.6 141.7 141.8 141.9   142 142.1 \n",
       "    2     1     1     2     2     1     1     2     2     2     1     1     4 \n",
       "142.2 142.4 142.5 142.6 142.7 142.9   143 143.2 143.5 143.6 143.7 143.9   144 \n",
       "    1     1     1     1     2     2     1     1     1     1     1     2     1 \n",
       "144.1 144.2 144.3 144.4 144.7 144.8 144.9   145 145.3 145.4 145.5 145.9   146 \n",
       "    4     3     1     2     2     1     1     1     1     2     1     1     2 \n",
       "146.1 146.4 146.5 146.7 146.9 147.1 147.3 147.4 147.5 147.6 147.7 148.1 148.2 \n",
       "    3     2     1     3     1     3     2     1     1     1     1     2     1 \n",
       "148.4 148.5 148.6 148.7 148.9   149 149.1 149.2 149.3 149.7 149.8   150 150.1 \n",
       "    1     1     2     1     1     2     2     2     1     1     1     3     1 \n",
       "150.3 150.5 150.6 150.7 150.9 151.2 151.4 151.5 151.6 151.9   152 152.1 152.2 \n",
       "    1     1     1     1     3     1     1     2     1     1     1     2     1 \n",
       "152.3 152.4 152.5 152.6 152.7 152.8 152.9   153 153.1 153.3 153.4 153.5 153.7 \n",
       "    2     1     1     1     1     1     3     2     1     1     2     1     1 \n",
       "153.8 153.9   154 154.1 154.2 154.3 154.4 154.5 154.7 154.8 154.9   155 155.1 \n",
       "    1     1     2     1     2     2     2     1     2     1     2     4     1 \n",
       "155.2 155.3 155.6 155.7 155.9   156 156.2 156.3 156.4 156.6 156.7 156.8   157 \n",
       "    1     1     2     3     1     1     4     1     2     2     1     1     3 \n",
       "157.1 157.6 157.7 157.9   158 158.2 158.4 158.5 158.6 158.8   159 159.1 159.2 \n",
       "    1     1     2     2     2     1     2     1     2     1     1     1     1 \n",
       "159.3 159.4 159.5 159.6 159.7 159.8 159.9   160 160.1 160.2 160.3 160.4 160.5 \n",
       "    3     1     1     1     1     1     2     2     2     1     1     1     3 \n",
       "160.6 160.7 160.8 160.9   161 161.1 161.3 161.4 161.8   162 162.1 162.2 162.4 \n",
       "    1     1     1     1     1     1     2     2     2     1     2     2     1 \n",
       "162.6 162.7 162.8 162.9   163 163.1 163.5 163.6 163.8 163.9 164.1 164.2 164.4 \n",
       "    1     2     2     1     1     1     3     1     2     2     1     2     1 \n",
       "164.5 164.9   165 165.1 165.2 165.4 165.5 165.6 165.8   166 166.4 166.6 166.8 \n",
       "    1     1     3     1     2     2     3     1     1     1     1     1     2 \n",
       "  167 167.1 167.2 167.3 167.4 167.5 167.6 167.9   168 168.2 168.3 168.4 168.6 \n",
       "    1     1     2     6     1     3     1     1     1     1     4     1     2 \n",
       "  169 169.1 169.2 169.4 169.5 169.8   170 170.1 170.2 170.3 170.5 170.6 170.7 \n",
       "    3     2     1     1     1     1     3     5     1     1     1     1     1 \n",
       "170.9   171 171.1 171.2 171.5 171.6 171.7 171.9 172.1 172.2 172.3 172.4 172.5 \n",
       "    1     1     1     2     3     1     2     1     2     3     1     3     1 \n",
       "172.6 172.7 172.8 172.9   173 173.1 173.2 173.4 173.5 173.6 173.7 173.9 174.1 \n",
       "    2     1     1     2     1     2     4     1     1     2     1     2     2 \n",
       "174.2 174.3 174.4 174.6 174.8 174.9   175 175.2 175.3 175.6 176.1 176.2 176.8 \n",
       "    1     1     2     2     1     2     1     1     2     1     1     1     2 \n",
       "176.9   177 177.1 177.2 177.3 177.4 177.8 177.9   178 178.2 178.5 178.7 178.9 \n",
       "    2     2     2     1     2     2     2     1     1     1     2     1     3 \n",
       "  179 179.4 179.5 179.6 179.7 179.8 179.9   180 180.1 180.2 180.4 180.5 180.6 \n",
       "    1     2     1     2     2     1     2     1     2     2     2     4     3 \n",
       "180.7 180.8 180.9 181.2 181.4 181.5 181.6 181.7 181.8   182 182.2 182.3 182.4 \n",
       "    3     2     2     1     3     1     1     1     3     1     1     1     3 \n",
       "182.6 182.8 182.9 183.2 183.3 183.4 183.5 183.6 183.8 183.9 184.1 184.3 184.5 \n",
       "    3     1     2     1     2     1     1     1     1     1     1     3     1 \n",
       "184.8 184.9 185.1 185.2 185.3 185.5 185.6 185.7 185.8 185.9 186.2 186.3 186.4 \n",
       "    2     2     1     2     2     1     1     1     2     2     2     1     1 \n",
       "186.5 186.7 186.8 186.9   187 187.1 187.3 187.4 187.5 187.6 187.7 187.8 188.2 \n",
       "    1     2     1     1     1     1     2     3     2     2     1     1     1 \n",
       "188.3 188.4 188.5 188.7 188.8 189.1 189.2 189.3 189.4 189.5 189.7 189.8 189.9 \n",
       "    3     3     1     4     1     1     2     3     2     3     1     1     1 \n",
       "190.1 190.4 190.5 190.6 190.9 191.2 191.3 191.5 191.9   192 192.1 192.3 192.4 \n",
       "    1     3     1     1     1     1     1     3     2     1     2     3     1 \n",
       "192.8   193 193.1 193.2 193.3 193.5 193.7 193.8 193.9 194.2 194.3 194.5 194.6 \n",
       "    2     1     2     1     1     1     2     1     1     1     1     1     1 \n",
       "194.7   195 195.1 195.3 195.4 195.7 195.9 196.1 196.3 196.4 196.7 197.2 197.4 \n",
       "    2     1     1     1     1     3     1     1     2     1     2     1     1 \n",
       "197.5 197.6 197.7 197.9   198 198.4 198.8 198.9   199 199.1 199.2 199.3 199.5 \n",
       "    2     2     2     2     2     2     1     1     1     1     1     1     3 \n",
       "199.7 199.9 \n",
       "    1     3 \n",
       "\n",
       "$x11\n",
       "\n",
       "  0.1   0.2   0.3   0.7   0.8   0.9     1   1.1   1.2   1.3   1.4   1.5   1.6 \n",
       "    2     1     2     2     2     1     1     1     2     1     2     2     3 \n",
       "  1.7   1.8   2.1   2.3   2.4   2.5   2.9   3.1   3.3   3.6   3.7   3.8   3.9 \n",
       "    1     1     4     1     1     1     1     1     1     1     1     2     1 \n",
       "    4   4.3   4.4   4.5   4.6   4.7   4.8   4.9     5   5.2   5.3   5.7   5.8 \n",
       "    2     2     2     1     2     1     1     1     1     1     1     1     2 \n",
       "  5.9     6   6.1   6.2   6.3   6.4   6.6   6.7   6.8     7   7.1   7.4   7.6 \n",
       "    2     1     2     2     1     1     2     1     2     3     1     2     1 \n",
       "  7.8     8   8.1   8.2   8.3   8.5   8.7   8.8   8.9     9   9.1   9.3   9.9 \n",
       "    1     1     1     3     3     1     2     1     1     2     3     2     1 \n",
       "   10  10.1  10.2  10.4  10.5  10.9  11.2  11.3  11.4  11.5  11.6  11.9    12 \n",
       "    1     1     2     2     2     4     1     1     2     2     1     1     1 \n",
       " 12.1  12.2  12.3  12.4  12.6  12.7  12.8  13.1  13.2  13.5  13.6  13.7  13.8 \n",
       "    1     2     2     1     1     1     2     1     2     1     1     1     1 \n",
       " 13.9  14.1  14.3  14.5  14.6  14.7  14.9  15.3  15.4  15.5  15.6  15.8  15.9 \n",
       "    2     1     1     1     2     2     1     2     1     1     1     1     2 \n",
       " 16.3  16.7  16.8  16.9    17  17.2  17.3  17.7  17.8  17.9    18  18.1  18.3 \n",
       "    1     3     1     1     1     3     1     1     1     1     1     2     1 \n",
       " 18.4  18.5  18.6  18.7  18.8    19  19.1  19.2  19.3  19.4  19.5  19.6  19.9 \n",
       "    2     3     1     1     1     2     1     3     2     1     1     2     1 \n",
       "   20  20.2  20.3  20.4  20.6  20.7    21  21.1  21.3  21.5  21.8  21.9  22.1 \n",
       "    1     1     3     2     3     2     2     1     1     1     2     1     1 \n",
       " 22.6  22.7  22.9  23.3  23.6  23.8  23.9    24  24.1  24.2  24.3  24.4  24.5 \n",
       "    2     1     3     2     1     1     2     1     1     2     1     2     1 \n",
       " 24.6  24.7  24.8    25  25.2  25.3  25.4  25.5  25.6  25.7  25.9  26.2  26.6 \n",
       "    1     1     1     2     2     2     1     1     2     2     1     1     2 \n",
       " 26.9    27  27.1  27.2  27.3  27.4  27.6  27.9    28  28.1  28.2  28.3  28.4 \n",
       "    1     1     3     2     2     1     3     1     1     1     1     4     1 \n",
       " 28.5  28.7  28.8  28.9    29  29.1  29.2  29.3  29.4  29.5    30  30.4  30.5 \n",
       "    4     1     1     2     1     1     1     1     1     6     1     1     3 \n",
       " 30.6  30.7  30.8  30.9    31  31.1  31.2  31.4  31.5  31.7  31.8  31.9  32.1 \n",
       "    1     2     1     1     1     1     2     1     2     1     2     2     2 \n",
       " 32.3  32.4  32.5  32.6  32.7  32.9    33  33.1  33.3  33.7  33.8  33.9    34 \n",
       "    1     1     1     2     1     3     1     3     1     2     1     1     2 \n",
       " 34.1  34.2  34.4  34.8  34.9  35.1  35.2  35.4  35.7  35.8    36  36.1  36.2 \n",
       "    1     1     4     1     1     1     2     2     4     1     1     1     1 \n",
       " 36.7  36.9    37  37.2  37.5  37.8  37.9    38  38.3  38.4  38.5  38.6  38.7 \n",
       "    1     1     1     1     2     1     2     1     2     1     4     1     2 \n",
       " 38.8  39.2  39.4  39.5  39.6  39.8  39.9    40  40.1  40.4  40.5  40.6  40.7 \n",
       "    1     2     1     1     2     2     1     2     1     1     3     1     2 \n",
       " 40.9    41  41.3  41.4  41.5  41.6  41.7  41.8  41.9    42  42.1  42.3  42.4 \n",
       "    2     2     1     2     2     3     4     1     3     1     1     1     2 \n",
       " 42.5  42.7  42.8  42.9  43.1  43.2  43.4  43.6  43.7  44.2  44.3  44.5  44.6 \n",
       "    2     1     1     2     2     1     2     1     2     1     1     2     2 \n",
       " 44.7  44.9    45  45.3  45.5  45.8  45.9  46.2  46.3  46.7  47.2  47.4  47.5 \n",
       "    1     1     2     1     1     1     2     1     1     1     1     1     3 \n",
       " 47.6  47.7  47.9    48  48.1  48.2  48.4  48.6  48.9    49  49.1  49.2  49.3 \n",
       "    2     1     3     1     1     1     1     1     1     2     2     4     1 \n",
       " 49.4  49.5  49.6  49.7  49.9    50  50.2  50.3  50.5  50.6  50.7  50.8  50.9 \n",
       "    1     1     1     1     1     2     3     1     1     1     2     2     1 \n",
       " 51.1  51.2  51.5  51.8  51.9  52.1  52.3  52.4  52.5  52.7  52.8  52.9    53 \n",
       "    5     1     1     2     3     3     1     1     3     1     1     1     1 \n",
       " 53.1  53.2  53.3  53.4  53.5  53.7    54  54.2  54.3  54.4  54.6  54.7  54.9 \n",
       "    2     1     2     1     2     1     1     2     1     3     5     2     3 \n",
       "   55  55.1  55.2  55.3  55.4  55.5  55.7  55.9    56  56.2  56.4  56.5  56.6 \n",
       "    3     2     4     3     1     4     1     2     2     1     2     3     1 \n",
       " 56.7  56.8  56.9  57.1  57.5  57.6  57.7  57.8    58  58.1  58.2  58.3  58.5 \n",
       "    2     1     2     2     2     1     4     1     3     1     1     1     1 \n",
       " 58.6  58.7  58.9    59  59.1  59.3  59.4  59.5  59.6  59.7  59.9  60.2  60.3 \n",
       "    2     2     1     1     1     2     1     2     1     1     2     2     1 \n",
       " 60.4  60.5  60.6  60.8    61  61.1  61.4  61.5  61.7    62  62.2  62.5  62.6 \n",
       "    4     1     2     1     3     2     3     2     2     1     1     1     1 \n",
       "   63  63.3  63.9  64.2  64.3  64.4  64.5  64.6  64.7  64.8    65  65.1  65.3 \n",
       "    2     1     3     4     1     2     1     1     3     1     1     1     1 \n",
       " 65.4  65.6  65.9    66  66.1  66.2  66.3  66.4  66.6  66.7  66.8    67  67.1 \n",
       "    2     1     1     1     2     1     1     3     1     1     1     1     2 \n",
       " 67.3  67.5  67.6  67.9    68  68.3  68.4  68.5  68.6  68.7  68.8  68.9  69.1 \n",
       "    3     3     1     1     1     3     3     2     1     2     1     2     1 \n",
       " 69.3  69.6  69.7  69.9    70  70.1  70.2  70.3  70.4  70.5  70.7    71  71.1 \n",
       "    2     1     1     1     2     1     1     2     3     2     1     1     1 \n",
       " 71.3  71.5  71.7  71.8  71.9  72.1  72.2  72.3  72.4  72.5  72.6  72.7  72.8 \n",
       "    3     1     1     1     1     1     2     1     3     2     2     1     2 \n",
       " 72.9    73  73.2  73.3  73.5  73.6  73.8  74.2  74.6  74.7  74.8    75  75.1 \n",
       "    2     1     2     1     1     1     1     5     1     4     2     2     3 \n",
       " 75.2  75.8    76  76.1  76.3  76.5  76.6  76.7  76.8    77  77.1  77.2  77.3 \n",
       "    4     2     3     1     1     2     1     1     1     4     1     4     2 \n",
       " 77.4  77.6  77.8  77.9    78  78.1  78.2  78.3  78.4  78.5  78.6  78.7  78.8 \n",
       "    2     1     3     4     4     1     1     1     1     1     1     2     1 \n",
       " 78.9    79  79.2  79.3  79.4  79.6  79.7  79.8  80.1  80.2  80.3  80.4  80.5 \n",
       "    1     1     5     2     2     1     2     1     2     1     2     2     2 \n",
       " 80.6  80.8  80.9  81.1  81.2  81.3  81.4  81.5  81.6  81.7  81.8  81.9  82.1 \n",
       "    1     2     2     2     1     1     1     1     1     3     1     1     3 \n",
       " 82.2  82.3  82.4  82.5  82.6  82.8  82.9    83  83.1  83.3  83.5  83.6  83.8 \n",
       "    1     1     1     5     2     3     1     2     4     1     1     2     1 \n",
       "   84  84.1  84.2  84.3  84.4  84.7  84.9    85  85.2  85.3  85.4  85.7  85.8 \n",
       "    3     1     1     2     1     1     1     2     1     1     1     1     3 \n",
       " 85.9    86  86.1  86.2  86.4  86.5  86.9    87  87.2  87.3  87.5  87.6  87.8 \n",
       "    1     1     4     1     2     1     1     3     1     3     1     1     1 \n",
       " 87.9  88.2  88.3  88.4  88.5  88.7  88.8  88.9    89  89.1  89.4  89.8  89.9 \n",
       "    1     1     2     1     1     1     1     1     1     1     3     2     1 \n",
       "   90  90.1  90.3  90.4  90.5  90.6  90.7  90.8  90.9    91  91.4  91.5  91.6 \n",
       "    1     1     1     1     1     2     3     2     1     3     1     2     1 \n",
       " 91.9    92  92.1  92.5  92.6  92.7  92.9    93  93.1  93.2  93.4  93.5  93.6 \n",
       "    2     1     1     1     1     3     1     1     2     1     2     1     1 \n",
       " 93.7  93.9  94.2  94.3  94.4  94.6  94.7  94.9  95.1  95.2  95.3  95.6  95.8 \n",
       "    1     1     1     4     2     1     2     4     2     2     3     2     1 \n",
       " 95.9    96  96.2  96.4  96.5  96.6  96.8  96.9    97  97.2  97.3  97.4  97.5 \n",
       "    1     1     1     3     1     3     2     1     2     1     2     1     4 \n",
       " 97.7  97.9  98.1  98.2  98.3  98.4  98.5  98.6  98.7  98.8  98.9    99  99.1 \n",
       "    1     1     2     2     1     3     1     1     1     2     1     6     1 \n",
       " 99.2  99.3  99.4  99.6  99.8   100 100.1 100.2 100.3 100.5 100.8   101 101.1 \n",
       "    1     1     1     2     1     2     3     2     1     2     1     2     2 \n",
       "101.2 101.4 101.6 101.8   102 102.1 102.2 102.5 102.7 102.9 103.3 103.5 103.8 \n",
       "    4     2     1     1     1     2     1     2     1     1     2     1     1 \n",
       "103.9   104 104.1 104.4 104.5 104.6 104.8 104.9 105.1 105.2 105.5 105.6 105.8 \n",
       "    1     2     1     1     1     1     1     1     2     1     3     1     1 \n",
       "105.9   106 106.1 106.2 106.4 106.5 106.6   107 107.1 107.2 107.5 107.9   108 \n",
       "    1     3     2     3     1     1     1     1     3     1     1     1     1 \n",
       "108.4 108.6 108.7 108.8 109.1 109.3 109.4 109.5 109.7 109.8 110.1 110.7 110.8 \n",
       "    1     1     1     2     3     1     2     1     1     3     1     1     1 \n",
       "  111 111.2 111.3 111.5 111.6 111.8 111.9   112 112.1 112.4 112.5   113 113.1 \n",
       "    2     1     1     1     1     2     1     1     1     2     2     1     1 \n",
       "113.2 113.3 113.4 113.5 113.6 113.8 113.9 114.1 114.5 114.6   115 115.4 115.5 \n",
       "    1     2     1     1     1     1     2     1     2     1     3     1     1 \n",
       "115.6 115.7 115.9   116 116.2 116.3 116.4 116.5 116.6 116.7 116.8 116.9   117 \n",
       "    1     2     2     1     1     1     1     3     1     1     1     1     2 \n",
       "117.1 117.2 117.3 117.6 117.7 117.8 118.1 118.2 118.3 118.4 118.5 118.6 118.7 \n",
       "    2     2     1     1     1     3     2     1     2     2     1     1     2 \n",
       "118.9 119.1 119.2 119.3 119.4 119.5 119.6 119.7 119.8 119.9   120 120.2 120.4 \n",
       "    1     2     1     1     1     1     2     1     1     3     1     3     1 \n",
       "120.5 120.6 120.9 121.1 121.2 121.4 121.5 121.6 121.7 121.8 121.9   122 122.1 \n",
       "    3     2     1     1     2     2     1     1     1     2     1     1     1 \n",
       "122.3 122.4 122.5 123.1 123.3 123.5 123.7 123.8 123.9 124.1 124.2 124.3 124.5 \n",
       "    4     1     1     1     1     3     3     3     2     1     3     2     1 \n",
       "124.6 124.7 124.8 124.9   125 125.1 125.5 125.6 125.7 125.8   126 126.1 126.2 \n",
       "    1     1     2     2     2     1     1     2     3     2     2     1     1 \n",
       "126.4 126.5 126.6 126.7 126.8 126.9 127.1 127.2 127.4 127.5 127.7 127.8 127.9 \n",
       "    1     2     1     1     1     1     2     2     3     2     1     1     2 \n",
       "128.1 128.2 128.3 128.4 128.5 128.6 128.7 128.8 128.9   129 129.1 129.2 129.3 \n",
       "    2     4     2     1     1     2     1     1     1     3     1     1     1 \n",
       "129.7 130.1 130.2 130.4 130.7 130.8 130.9   131 131.2 131.3 131.6 131.7 131.9 \n",
       "    1     2     1     1     1     3     1     2     1     1     1     1     2 \n",
       "  132 132.1 132.2 132.5 132.6 132.7 132.9   133 133.1 133.2 133.4 133.5 133.7 \n",
       "    1     3     1     2     1     2     1     2     3     3     2     2     2 \n",
       "133.8 133.9 134.1 134.2 134.5 134.6 134.7   135 135.1 135.2 135.3 135.4 135.5 \n",
       "    1     1     6     1     1     2     2     2     2     2     1     2     2 \n",
       "135.6 135.7 135.8 135.9 136.3 136.4 136.6 136.9 137.6 137.8   138 138.2 138.4 \n",
       "    3     1     2     2     2     1     1     1     1     3     1     2     1 \n",
       "138.5 138.6 138.7 138.8 138.9   139 139.2 139.3 139.6 139.7 139.8   140 140.1 \n",
       "    3     1     1     4     1     1     3     1     1     2     2     1     2 \n",
       "140.2 140.3 140.4 140.5 140.7 140.9 141.2 141.3 141.4 141.5 141.6 141.8   142 \n",
       "    2     1     1     1     1     2     3     2     1     2     1     2     2 \n",
       "142.2 142.3 142.4 142.5 142.6 142.7 142.8   143 143.1 143.3 143.4 143.5 143.6 \n",
       "    1     2     3     2     2     4     1     2     1     1     2     2     3 \n",
       "143.8   144 144.1 144.2 144.5 144.6 144.7 144.8 145.1 145.3 145.5 145.7 145.8 \n",
       "    1     3     1     1     1     1     4     1     1     1     1     2     2 \n",
       "145.9   146 146.1 146.2 146.3 146.6 146.7 146.8   147 147.3 147.4 147.6 147.8 \n",
       "    1     3     1     2     1     1     1     1     1     3     1     1     1 \n",
       "148.2 148.5 148.6 148.7 148.8 148.9   149 149.1 149.5 149.6 149.7 149.8 150.1 \n",
       "    2     1     2     2     1     1     2     2     1     2     1     1     2 \n",
       "150.4 150.5 150.6 150.7 150.9 151.2 151.3 151.5 151.6 151.7 151.9   152 152.1 \n",
       "    1     2     1     1     1     1     1     1     1     1     1     1     1 \n",
       "152.2 152.7 152.9 153.1 153.2 153.3 153.6 153.7 153.8 153.9   154 154.1 154.3 \n",
       "    1     3     2     1     2     1     3     3     1     2     1     2     2 \n",
       "154.4 154.6 154.7   155 155.1 155.2 155.4 155.6 155.7   156 156.3 156.5 156.6 \n",
       "    1     4     1     1     1     1     1     1     3     2     1     2     3 \n",
       "156.7 156.8 157.3 157.5 157.9 158.1 158.2 158.3 158.4 158.5 158.6 158.8 158.9 \n",
       "    4     2     1     1     2     1     3     3     2     1     2     1     1 \n",
       "  159 159.1 159.2 159.3 159.5 159.9   160 160.1 160.2 160.3 160.5 160.7 160.8 \n",
       "    1     2     2     2     1     3     1     1     1     1     1     1     2 \n",
       "160.9 161.2 161.3 161.4 161.7 161.9 162.3 162.4 162.6 162.7 162.8   163 163.2 \n",
       "    1     2     4     2     7     1     1     2     1     1     1     2     2 \n",
       "163.3 163.4 163.5 163.6 163.7 163.8 163.9 164.1 164.2 164.3 164.4 164.8 165.1 \n",
       "    1     2     1     1     1     2     1     2     2     1     3     2     1 \n",
       "165.2 165.4 165.6 165.7 165.9   166 166.1 166.2 166.3 166.4 166.6 166.7 166.8 \n",
       "    3     1     1     1     1     3     2     3     1     2     3     1     1 \n",
       "166.9   167 167.1 167.2 167.4 167.6 167.7 167.8 167.9 168.3 168.5 168.6 168.9 \n",
       "    1     2     3     1     1     2     1     2     3     2     2     1     1 \n",
       "  169 169.2 169.3 169.4 169.5 169.7   170 170.2 170.4 170.5 170.6 170.8 170.9 \n",
       "    2     2     2     1     2     1     2     1     1     1     1     2     2 \n",
       "  171 171.4 171.5 171.8 171.9 172.2 172.3 172.6 173.2 173.3 173.4 173.6 173.8 \n",
       "    1     2     4     4     1     1     1     2     1     1     3     2     1 \n",
       "173.9   174 174.1 174.2 174.3 174.4 174.5 174.7 174.9   175 175.1 175.2 175.3 \n",
       "    2     1     2     1     1     1     1     2     1     2     6     2     2 \n",
       "175.5 175.8 175.9 176.1 176.2 176.4 176.5 176.6 176.7 176.9   177 177.1 177.2 \n",
       "    1     1     1     1     2     2     1     1     1     2     1     2     2 \n",
       "177.3 177.5 177.8 177.9 178.6 178.8 179.1 179.5 179.6 179.7 179.8 179.9   180 \n",
       "    2     1     1     2     1     1     1     1     1     3     3     1     1 \n",
       "180.2 180.3 180.6 180.7 180.8   181 181.3 181.6 181.8 181.9   182 182.1 182.3 \n",
       "    2     2     2     1     1     1     3     2     2     3     1     2     2 \n",
       "182.6 182.8 183.1 183.2 183.3 183.7 183.8 183.9 184.5 184.8 184.9   185 185.1 \n",
       "    3     1     3     2     1     2     2     1     2     1     1     1     1 \n",
       "185.2 185.3 185.5 185.6 185.7 185.9 186.1 186.7 186.8 186.9   187 187.1 187.2 \n",
       "    2     1     1     1     2     1     1     1     1     1     2     1     1 \n",
       "187.4 187.5 187.6 187.7 187.9 188.1 188.3 188.4 188.7 188.9 189.1 189.2 189.4 \n",
       "    1     3     3     2     1     1     1     1     3     1     1     3     1 \n",
       "189.5 189.8 189.9   190 190.1 190.3 190.5 190.6 190.7 190.8   191 191.1 191.2 \n",
       "    1     2     1     1     1     2     1     3     1     2     2     5     4 \n",
       "191.4 191.5 191.6 191.9   192 192.2 192.3 192.4 192.9 193.2 193.4 193.6 193.8 \n",
       "    1     2     3     1     4     1     1     1     1     2     2     2     1 \n",
       "193.9   194 194.1 194.2 194.4 194.5 194.6 194.9   195 195.1 195.4 195.6 195.7 \n",
       "    1     3     1     1     1     2     1     2     5     3     1     1     1 \n",
       "195.8   196 196.1 196.3 196.4 196.5 196.6   197 197.2 197.3 197.4 197.6 197.8 \n",
       "    2     2     1     1     2     4     2     2     1     1     1     2     1 \n",
       "197.9 198.2 198.3 198.4 198.5 198.6 198.8 198.9   199 199.1 199.2 199.4 199.7 \n",
       "    4     2     1     1     1     2     1     1     1     1     1     2     1 \n",
       "199.8 199.9 \n",
       "    1     2 \n",
       "\n",
       "$x12\n",
       "\n",
       "   0    1 \n",
       "1363  711 \n",
       "\n",
       "$x13\n",
       "\n",
       "   0    1 \n",
       "2005   69 \n",
       "\n",
       "$x14\n",
       "\n",
       " 20  30  40  50  61  71  81  91 101 121 131 141 151 161 172 182 202 212 222 232 \n",
       "  1   3   6   2   1   2   6   1  17  14   2   2  26  12   3   7  75   2   2   4 \n",
       "242 252 272 283 293 303 313 323 333 343 353 363 373 383 394 404 414 424 434 444 \n",
       " 13  39   2   5   2  85   1  15   1   2 104  13  11  28   5 941   1  16   9  12 \n",
       "454 464 474 484 505 515 525 535 545 555 565 595 605 626 646 656 686 706 727 737 \n",
       "138   6   4  44 188   1  12   1   3  42   1   1  83   1   1  14   1  18   1   1 \n",
       "757 807 848 858 868 908 969 999 \n",
       "  2   9   2   1   1   1   1   3 \n",
       "\n",
       "$x15\n",
       "\n",
       "   0    1 \n",
       " 312 1762 \n",
       "\n",
       "$x16\n",
       "\n",
       "   0    1 \n",
       "1840  234 \n",
       "\n",
       "$x17\n",
       "\n",
       "   0    1 \n",
       "1579  495 \n",
       "\n",
       "$x18\n",
       "\n",
       "   0    1 \n",
       "2032   42 \n",
       "\n",
       "$x19\n",
       "\n",
       "   0    1 \n",
       "1982   92 \n",
       "\n",
       "$x20\n",
       "\n",
       "   0    1 \n",
       "1979   95 \n",
       "\n",
       "$x21\n",
       "\n",
       "   0    1 \n",
       "2015   59 \n",
       "\n",
       "$x22\n",
       "\n",
       "   0    1 \n",
       "1976   98 \n",
       "\n",
       "$x23\n",
       "\n",
       "   0    1 \n",
       "1071 1003 \n",
       "\n",
       "$x24\n",
       "\n",
       "   0    1 \n",
       "1859  215 \n",
       "\n",
       "$x25\n",
       "\n",
       "   0    1 \n",
       "1826  248 \n",
       "\n",
       "$x26\n",
       "\n",
       "   0    1 \n",
       "2057   17 \n",
       "\n",
       "$x27\n",
       "\n",
       " 14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33 \n",
       "  3   3   9   4   2   6  11   9  14  18  12   7   9   9   3   7   5   5   8   4 \n",
       " 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53 \n",
       "  5   7   6   8   6   2   5   6   2   3   7  10   3   3   3   7   4   6   6   5 \n",
       " 54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73 \n",
       "  5   9   6   6   8   1  10   9   6  13   8   6  12  14   7  25   7  11   8  13 \n",
       " 74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93 \n",
       " 13  10  21  13  14  18   9  11  13   3  13  14   7   8   3   6  12  16   9  10 \n",
       " 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 \n",
       "  6   9  15  10   5  10  14  15  10  15  18  12  15  14  18  23  12  13  16  17 \n",
       "114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 \n",
       " 11  10  16  26  18  18  14  10  20  10  14  17  23  18  18  33  14  19  17  10 \n",
       "134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 \n",
       " 14   9  15  11  18  18  15  13   5   9   6  13   9  14  15   9   5  14  11  13 \n",
       "154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 \n",
       "  9  11   9   9  11  13   5   8  11   9  10   3   8   5   7   6   4   6   4   9 \n",
       "174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 \n",
       "  4   9   5   9   8   6   9   5   3   2   4   6   2   4   5   8   7   8   7   3 \n",
       "194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 \n",
       "  6   3   3   2   7   4   2   4   4   4   1   3   2   8   5   8   4   4   4   4 \n",
       "214 215 216 217 218 219 220 221 222 224 225 226 227 228 229 230 231 232 233 234 \n",
       "  1   4   3   7   1   3   2   4   5   6   3   5   3   4   2   2   1   4   1   3 \n",
       "235 236 237 238 239 240 241 242 243 244 245 246 247 248 250 251 252 254 255 256 \n",
       "  1   2   1   1   5   2   6   6   2   1   2   1   1   2   3   1   1   1   3   3 \n",
       "257 258 259 260 261 262 263 265 266 267 268 269 270 271 272 274 275 277 278 279 \n",
       "  2   2   3   1   1   4   2   2   2   1   4   4   4   1   1   4   2   2   2   2 \n",
       "280 281 282 283 284 288 291 292 294 295 296 300 302 303 304 305 306 307 310 313 \n",
       "  1   5   4   2   2   1   1   1   3   2   2   1   1   3   1   1   1   1   2   2 \n",
       "315 316 319 324 329 331 335 340 342 344 347 349 355 368 373 383 387 391 392 414 \n",
       "  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   2   1   1   1 \n",
       "420 434 438 440 442 461 468 476 503 570 \n",
       "  1   1   1   1   1   1   1   1   1   1 \n",
       "\n",
       "$x28\n",
       "\n",
       "   0    1 \n",
       "1864  210 \n",
       "\n",
       "$x29\n",
       "\n",
       "   0    1 \n",
       "2003   71 \n",
       "\n",
       "$x30\n",
       "\n",
       " 62 125 187 250 312 375 437 500 562 624 687 749 812 874 937 999 \n",
       "  1  11  20  39  29  49  72  27 659 470  87  62 372 119  26  31 \n",
       "\n",
       "$x31\n",
       "\n",
       "   0    1 \n",
       "2004   70 \n",
       "\n",
       "$x32\n",
       "\n",
       "189 200 211 222 233 244 255 266 278 289 300 311 322 333 344 355 366 377 389 400 \n",
       " 29  27  51  34  40  47  61  60  61  38  49  64  41  55  55  56  74  50  65  69 \n",
       "411 422 433 444 455 466 477 488 500 511 522 533 544 555 566 577 588 599 611 622 \n",
       " 74  62  55  39  43  57  55  27  54  47  37  51  33  28  37  29  26  17  26  28 \n",
       "633 644 655 666 677 688 699 710 722 733 744 755 766 777 788 799 810 821 833 855 \n",
       " 25  23  29  17  22  15  12  16   7   7   7   6   4   3   1   5   3   2   1   3 \n",
       "866 877 888 899 910 932 944 977 999 \n",
       "  3   1   3   1   3   1   1   1   1 \n",
       "\n",
       "$x33\n",
       "\n",
       "   0    1 \n",
       "2005   69 \n",
       "\n",
       "$x34\n",
       "\n",
       "   0    1 \n",
       "1950  124 \n",
       "\n",
       "$x35\n",
       "\n",
       "   0    1 \n",
       "1937  137 \n",
       "\n",
       "$x36\n",
       "\n",
       "   0   49   74  202  307  341  359  361  362  365  367  373  379  383  395  396 \n",
       "1978    1    1    1    3    2    2    1    2    4    1    1    1    4    1    1 \n",
       " 399  404  424  430  433  436  453  454  468  472  500  506  512  515  518  545 \n",
       "   3    1    2    4   12   11   10    7    1    1    1    1    1    1    4    1 \n",
       " 554  561  587  632  648  845 \n",
       "   4    1    1    1    1    1 \n",
       "\n",
       "$x37\n",
       "\n",
       "   0    1 \n",
       "2073    1 \n",
       "\n",
       "$x38\n",
       "\n",
       "   0    1 \n",
       "1780  294 \n",
       "\n",
       "$x39\n",
       "\n",
       "   0    1 \n",
       "1816  258 \n",
       "\n",
       "$x40\n",
       "\n",
       "   0    1 \n",
       "1817  257 \n",
       "\n",
       "$x41\n",
       "\n",
       "   0    1 \n",
       " 546 1528 \n",
       "\n",
       "$x42\n",
       "\n",
       "   0    6   11   14   15   18   22   24   26   27   28   29   30   31   33   35 \n",
       "1914    1    2    1    1    1    4    2    1    1    5    2    1   10    2    2 \n",
       "  39   41   44   47   48   49   50   52   54   64   68   73   74   77   79   86 \n",
       "   1    2   11    1    3    2    8    5    2    1    3   15    1   19    1    2 \n",
       " 105  106  135  141  143  150  200  278  341  999 \n",
       "   2    3    2    1    1   22    2    3    1   10 \n",
       "\n",
       "$x43\n",
       "\n",
       "   0    1 \n",
       "2018   56 \n",
       "\n",
       "$x44\n",
       "\n",
       "   0    1 \n",
       " 638 1436 \n",
       "\n",
       "$x45\n",
       "\n",
       "   0    1 \n",
       "1949  125 \n",
       "\n",
       "$x46\n",
       "\n",
       "   0    1 \n",
       "2059   15 \n",
       "\n",
       "$x47\n",
       "\n",
       "   0    1 \n",
       "1811  263 \n",
       "\n",
       "$x48\n",
       "\n",
       "   0    1 \n",
       "1776  298 \n",
       "\n",
       "$x49\n",
       "\n",
       "   0    1 \n",
       "2053   21 \n",
       "\n",
       "$x50\n",
       "\n",
       "   0 \n",
       "2074 \n",
       "\n",
       "$x51\n",
       "\n",
       "   0    1 \n",
       "1856  218 \n",
       "\n",
       "$x52\n",
       "\n",
       "   0 \n",
       "2074 \n",
       "\n",
       "$x53\n",
       "\n",
       "   0    1 \n",
       "1895  179 \n",
       "\n",
       "$x54\n",
       "\n",
       "   0    1 \n",
       "1410  664 \n",
       "\n",
       "$x55\n",
       "\n",
       "   0    1 \n",
       "2038   36 \n",
       "\n",
       "$x56\n",
       "\n",
       "   0    1 \n",
       "1184  890 \n",
       "\n",
       "$x57\n",
       "\n",
       "   0    1 \n",
       "2072    2 \n",
       "\n",
       "$x58\n",
       "\n",
       "   0    1 \n",
       "1779  295 \n",
       "\n",
       "$x59\n",
       "\n",
       "   0    1 \n",
       "2061   13 \n",
       "\n",
       "$x60\n",
       "\n",
       "   0    1 \n",
       "2009   65 \n",
       "\n",
       "$y\n",
       "\n",
       "   a    b \n",
       "1565  509 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sapply(alldata, table)  #col 50 and 52 gerekli değil (Tamamen 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nwT88PaOjH4IFoQAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nACFBACFBACFBgP8AJC+ejXFxrNYAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nCRAgSIDAP1fb4twU7kEcAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nGPgH1wUPQdZTEx4AAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nCRq28BovZP8AAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nDVz3H2kAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nSEAAQgICEBIQgJCAAIQEBPgP/GCDalxavu4AAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nBAFCggAhQYCQIEBIECAkCPgPQ8jqyDvmFvEAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nggAhQcA/jYfvS1Mg/tUAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nBAH/AV343j4ErFCLAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\n4UtUs05kAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nEBIECAkChAQBQoIAIUHAPw2g3qilVTPfAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nkCBASBAgJAgQEgQICQL+AcRM4GoI5g2JAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nCAkChAQBQoIAIUGAkCBASBDwDzGV3nsHUrxbAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\ngg==",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nIQEChAQIEBIgQEiAACEBAoQECBASIPAPPi6IIzMOceYAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAWt0lEQVR4nO3d7UKiQACG0UFNzdTu/25X0fK7CF8R3HN+bK4k07LzhAJZ+QTu\nVp79BcArEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBDSA5RSTm8d7jj21skXM6tK+R5ptflKltsby82N1e4rq7+4o4+Xd/I7\nm+kBGoX0UXWy7WfbFt6O/zrefhyXMtt/ZUJKsJkeoFFIHU3R0X4XdPT3+efnvJTR11chpASb\n6QEuQvr5kzr6WnY+SqnW683TvY+Txeefdut+brCZHuDWHmk92zyjKpP3z69v9rtPW7xtn30t\n9g9Zbf42nh89cjUq082t98nm9mi6+lrffFRGmxjmVRl/nA5/sr6LEjYLJ5PvZ3vnwdz6yC9s\npge4EdKq2uczPglpvL89qR/xsf+UwyNH9QO+Pqvek+w/YRPZ9Pu+b8frO+51b11/FdX69EsV\n0p1spge4EdJmX7DZGa3H21cphxk++SpkV1L1/devR5btwzYvasabuT896WPTw3GDOyfruxLS\n5/tujadfqpDuZDM9QDm2v2P35/aJ2Xr3Qn+/aLH5OF9vnvVtPi7qWV5tP1SHR24D2h4jWJ2s\naXPvfLu7WtYfDmOfre9aCaOvIw2fQoqxmR7gRkjbOL5fCn1N0bf6KNpnvbN5q/cn9We8Hx65\nOFv17s+Pkw+HTzhb35UStueSdlV+CinGZnqAGyHNdnfsWzos2r1cWdV3VF8T93zx5hPep+Py\nHdLnxYfvxx2v70oJ9UrGh09v9JFf2EwPcJh9p1N9+vXKZnWx6OtWuQxp9/f30VGZP4d0cuui\nhO2Tx+r7RZKQQmymB7gV0uf6fXdIbXyy6HsPUl3dI9V/3T7VG73Nl3/aI1XnCz93B+0+Pr4P\n2wkpxGZ6gJshbdVneQ73TX59jVQvHe3v/zWkyS+vkd7qo3nfJ5KEFGIzPcCNkEb7ncVhV7G+\nedSunEWy//j7HumXo3b7fdH3pQ1CCrGZHuBGSJs5Pl7Vxxy2Vypsj+FtP36fad3tIy7PI9Ur\nGtefvKh+Del8fWcljPY7rK+L7YQUYjM9wK2ndl8HG+pDZm/lcCH2Yd7Xe5RycmVDfff+goey\n25P8FNLZ+k4Xzr6P1+0v/xZSiM30ADdfI9Wvj8a71zCH1ymLt+roBNNye63d4iKS7d3V23L1\ndcHClbV/OVnfycLvH0f6/oEkIYXYTP203r2QerhbwQjpj2ymfim7MzzL8ekFdI8c7+Tjb/dz\ng83UL4dDBaeXBj3K12GNw+GNW3fyI5upX75/1KI+ovd4QgqxmXpmPdv+HET11sn+SEgxNhME\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKC\nACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQ8\nM6Ryryd+7XDiqSE9+fEQIyQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAh\nQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAg\nJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkC\nhAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFB\ngJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAk\nCGg/GT9mk7I1mX50Pnbm8RDTdjKuR+Vg3O3YqcdDTNvJOC3V+7K+tVpUZdrp2KnHQ0zbyViV\n5fftZak6HTv1eIhpOxlLufWXx4+dejzE2CNBwB2vkRar+pbXSNB+Mo6PjtqN1t2OHXo8xNxx\nHmlan0eqJjPnkfjvubIBAoQEAS4RggCXCEGAS4QgwAlZCHCJEATYI0GAS4QgwCVCEOASIQhw\nZQMEPGgylmOPGltI9Madk3E+KmWyaDmEkHgZ951H2h9x+PmgnZB4fXeFNC3T9efnalrmrYYQ\nEi/jrpCqUh/3XpdRqyGExMu4K6Sv4wg/XyIkJF7fXSG9fYX04yVCQuL1tQ9pMpsvyvvm5nr6\n89EGIfH62of0fY6olOrHS4SExOtrPRmXy/l8MqkPOUx/vtROSLy+DiajkHh9QoIAIUGAkCBA\nSBBw7+HvX35S4schhMTLaDsZ50KCg/bnkaqm768qJF5f+8m4bPreQULi9d0xGedHb23Xaggh\n8TIctYMAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFB\ngJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAk\nCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKE\nBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGA\nkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQI\nEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIOB4Mo5mq0cP0WzBvSuG\nrh1PxlLKI1oSEq/veDKu398e0ZKQeH3nk/FjNkq3JCRe35XJuKw2+6X5Q4f4ZcG9K4auXU7G\nxbhsjR84xG8L7l0xdO1sMq5nm93RaLHe1DT55ZEfs0ld3GT68achGixoSEj0xslk/NgebJgu\ndwt+nqbrUTn4ee8lJF7fyXmkzc5ovv5aUP34uGmp3nfFrRZVmTYdotmChoREb5ycR5osGj+u\nKsvv28ufoxMSr+/kPNJfHldu/eXHIZotaPo13Pl4iDmZjOvpdtdSTRsUZY8ER44n46qqdy2l\nVL+fj928RlrsPstrJDiZjOPytt0Xrae/HvquP/lg9OMuTEi8vqsvdX459L3zMa3PI1WTmfNI\n/PeOJ2NVdnuWdaOQWg3RbMG9K4auHU/GaRlvdy4f459f89wzRLMF964YunYyGceNrlTYc4kQ\nfDudjO/bNsZNrvx2iRAcaTsZXSIER9pORidk4UjbyegSIThyMhln3y98fn2cPRIcOZ6Ms8Ph\ng18f5xIhOHJ6QvYP79TgEiE4aP5S55xLhODb8WSclL/8RFKrIZotuHfF0LXTH6MY/7Jzab7a\nY03GbjXInY+HmLO3LG58sMElQnCkbUguEYIjLhGCAJcIQcDpZFxMts/qJg3eQt8lQnDk8ueR\ntu8N+XtJ9khw5Hgyzsu4/inzeXn79XEuEYIj5+/ZsH9Drt8f6BIhODh/qdM4JJcIwcHxZBzt\n90jLMnrUEM0W3Lti6NqV10iLP10F/rchmi24d8XQtZPJOPnLuwi1G6LRgntXDF27PI9UJu+P\nHKLJgntXDF3rYDIKidfX/s1PGv2kxI9DCImX0XYyzoUEB61/HmlZNT0kISReX/sf7Fs2fat9\nIfH6rkzGj3GD3zP2uX12t/z9k64P8cuChoREb1ybjOsGF63eOcTPC+5dMXTt6mT0i8bgb65N\nxvnPP1+UGOLnBfeuGLp2/WDD7FFDNFtw74qha9dCGkWvWRUS/wGXCEGAkCDgxgnZhidl/zxE\nswX3rhi6JiQIOJmMs2qx+fOj8VV0LYZotODeFUPXjifjbH/Nz7I0u0aoxRDNFty7Yuja1TdM\ndWUD/M3p+9p97ZG8ixD8yfFk3L576uaDdxGCvzqZjF/vntrwB43aDNFowb0rhq6dTsb3+l2E\nFo8cosmCe1cMXXNlAwQICQIu3yCy2S8aaz1EkwX3rhi6dnmw4bPRLxprO0SjBfeuGLp25U30\nG/2isZZDNFtw74qha6cnZP/wi8baDdFswb0rhq6dXyIkJGjheDL6RWPQ0pXXSC4Rgr86mYx+\n0Ri0c3keyS8agz9zZQMEHE/GSfaq72tDNFtw74qha1d/QvZxQzRbcO+KoWvnh78fPESzBfeu\nGLp2PBnXk/HHg4dotuDeFUPXbryv3aOGaLbg3hVD14QEAQ5/Q4CQIOAx7wl5dYjmC+5dMXTt\nNKSH5CQkXp+QIEBIECAkCBASBAgJAg4hPeTXXh4P0XzBvSuGrgkJAlzZAAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIA\nIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQ\nICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAHtJ+PHbFK2JtOPlkMIiZfRdjKu\nR+Vg3G4IIfEy2k7Gaanel/Wt1aIq01ZDCImX0XYyVmX5fXtZqlZDCImX0XYylnLrL82HEBIv\nwx4JAu54jbRY1be8RoL2k3F8dNRutG41hJB4GXecR5rW55Gqycx5JP57rmyAACFBgEuEIMAl\nQhDgEiEIcEIWAh50iVA5lh479XiIsUeCAJcIQYBLhCDAJUIQ4MoGCBASBNw5GeejUiaLlkMI\niZdx33mk/RGHHw/aCYn/wF0hTct0/fm5mpZ5qyGExMu4K6Sq1Me912XUaggh8TLuCunr6h/v\nIsT/7q6Q3r5CcokQ/7n2IU1m80V539xcT10ixP+ufUjfV3aXUrlEiP9c68m4XM7nk0l9yGH6\nY0dC4j/gygYIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAk\nCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKE\nBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGA\nkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQI\nEBIECImhKveKfjHJlf1xCCFxj17NHyExVL2aP0JiqHo1f4TEUPVq/giJoerV/BESQ9Wr+SMk\nhqpX80dIDFWv5o+QGKpezR8hMVS9mj9CYqh6NX+ExFD1av4IiaHq1fwREkPVq/kjJIaqV/NH\nSAxVr+aPkBiqXs0fITFUvZo/QmKoejV/hMRQ9Wr+CImh6tX8ERJD1av5IySGqlfzR0gMVa/m\nj5AYql7NHyExVL2aP0JiqHo1f4TEUPVq/giJoerV/BESQ9Wr+SMkhqpX80dIDFWv5o+QGKpe\nzR8hMVS9mj9CYqh6NX+ExFD1av4IiaHq1fwREkPVq/kjJIaqV/NHSAxVr+aPkBiqXs0fITFU\nvZo/QmKoejV/hMRQ9Wr+CImh6tX8ERJD1av5IySGqlfzR0gMVa/mj5AYql7NHyExVL2aP0Ji\nqHo1f4TEUPVq/giJoerV/BESQ9Wr+SMkhqpX80dIDFWv5o+QGKpezR8hMVS9mj9CYqh6NX/a\nr+xjNilbk+lHyyF6tSEYnF7Nn7YrW4/KwbjdEL3aEAxOr+ZP25VNS/W+rG+tFlWZthqiVxuC\nwenV/Gm7sqosv28vS9VqiF5tCAanV/On7cpKufWX/T1Hbq8Dnqjl3L8+mVs+7g97JHh9d7xG\nWqzqW7++RoLX13r3Nj7aRY7WyS8JhueO80jT+jxSNZn9ch4JXp8jXxAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIeGZIT3oTJtiJTubkygY0tvGN\nLyTjG79v4wvJ+Mbv28oGNLbxjS8k4xu/b+MLyfjG79vKBjS28Y0vJOMbv2/jC8n4xu/bygY0\ntvGNLyTjG79v4wvJ+Mbv28rgfyUkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUGAkCCg85CmVamm65/u6Hj8+ei54298dPi/cDH+8q2Ut9XTxl93/P+/+Q8/3dqh8bsO\naVz/GoDRD3d0PP60vqPq6n/y2j93XXX3v3Ax/uK5//5VtRu/u5KXp7+FIjX/Og7po1TLz2VV\nPm7e0fH4y/K23n6TenvS+FuT7C8Y+dv41eaO9aRMnzT+Wz3ytKvt/7kd/Hhrx+ZfxyFNy2Lz\n53uZ3byj4/Enuw3Q1VS+9s99D/+mnj+N/15P5HWpnjR+6Xb7b75ljk/Gis2/jkOalO0+fFkm\nN+/oePy9rv4jr4y/Ovuv7Xb8t7Lsauyr4++f1XYV8ufm+8bJ1o7Nv45DuvgG1PF3pBvDrcv4\naeOPy6q7kC7GH5XPWVU/vX3O+LP9U7uOnpF8Ls/+82PzT0hb83oH/5TxZ+W9uyc217b/pH6x\n/6zxP+fbow3VvKPxzwYXUmz82qrq6Jnl5fj1k4qnhrQ92PDW1R7h2jeSra52SGeDCyk2/ta6\n6uiJ3bWnVtsDz08NafsaadXV+YeL8efbp3abkDvcJb1ESNX5131xR8fjb407O4t1Mf5b/Zyy\nu5Au/v0dfyO7GH9Uti/P1t2dSDz7t8bm31OO2q3Oj9qtuj1qdzLcajTu7mzg+fiP+VX1zcfv\n+vD/xfhdH/4+Hys2/zoOaVZ/B14czv9d3NHx+JvbnT2vuzJ+1yHd2P6rrjbCxfi7PUJn57G2\nTrZ1bP7971c2dDaFboxfe+KVDZtXR+vta5T3J40/Ldvr3KZdfSPdeokrGzbPibfqybv7Bx3d\n8Yzx37rdI1z++09vdT/+7Lnbf3+tW5ffzb62dnb+dR3S7mLf3dDl7I5njN/xU6vLf//prSeM\nvxg/c/vvr77ubPzP85BS86/rkOAlCQkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAh\nQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAg\nJAgQEgQIaRje9r+dcVzedr9m8PDn6d95Dpt+IKoy3/w5r3/9t5D6x6YfiI9SVp/r3a/f3gVz\nnM3lPXTLph+K7ZO7yfaJnZD6yKYfjKrM6id2p9lc/skz2PSDsXlyVz+xE1If2fTD8bZ7Yiek\nPrLph6PaP7MTUg/Z9IPxVvbHGoTUQzb9UHxs9kf7F0lC6h+bfiiq8r4/HyukHrLpB2LzxO5z\nf4WQkHrIph+Gj1LWmw+r+smdkPrHph+G3aV2+4vthNQ/Nv0gfF38vXtyJ6T+sekHyLV2/WPT\nD5CQ+semH6Dznz/y80jPZ9MPkJD6x6aHACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAg4B/Ff/Cz/Oqd\nuAAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAexUlEQVR4nO3d22KiShBA0UYQb4D//7cDNBoIjrHpKmmovR5OnEyb4gh7vJHE\n3QFEc2tvALAHhAQIICRAACEBAggJEEBIgABCAgQQEiCAkAABhAQIICRAACEBAggJEEBIgABC\nAgQQEiCAkAABhAQIICRAACEBAggJEEBIgABCAgQQEiCAkAABhAQIICRAACEBAggJEEBIgABC\nAgQQEiCAkAABhAQIICRAACEBAggJEEBIgABCAgQQEiCAkAABhKTAOTe99POJseNXNuaUOfec\nVLdbUnUXqvZC7bes37jRx/kn8TduJgUfhXTLvnLbn7oWjuM/5t3H3LnTsGWEJIGbScFHIX3p\nED0Md0GjP5/v97Nzh8dWEJIEbiYFs5DeL/rStng357KmaR/u3SZ//XvZ/z6P/+BmUvC/e6Tm\n1D6icsXl/vjH3i+7HrtHX9fhKnX7p/w8umZ9cGV76VK0lw9l/fh654M7tDGcM5ffpuMnX29W\nQvuXRfF8tPc7mP99xB+4mRT8J6Q6G/LJJyHlw+Wiv8ZtWPJzzUN/hceq/p5kWNBGVj4/9zT+\neuNeB02/FVkz3VRCisTNpOA/IbX3Be2dUZN3z1J+jvDiUYgvKXv+8XFN112tfVKTt8d+Oemj\n7WHcoDf5ei9Cul/8V5xuKiFF4mZS4MaGT/j/dg/MGv9Ef/ira/vx3LSP+tqP1/4oz7oP2c81\nu4C61wjqyVdqP3vu7q6q/sPP7F9f71UJh8crDXdCEsPNpOA/IXVxPJ8KPQ7RY/8q2r2/szn2\n9yf9isvPNa+/vrT/723y4WfBr6/3ooTuvSRf5Z2QxHAzKfhPSCf/iaGln7/yT1fq/hPZ48D9\n/dftgkuZu2dI99mH5/XGX+9FCf0XyX+Wf/QRf+BmUvBz9E0P9fLxzKae/dXjkpuH5P98OYzK\nfB/S5NKshO7BY/Z8kkRIQriZFPwvpHtz8S+p5ZO/et6DZC/vkfo/dg/1DsdzFXSPlP3+y7t/\n0e52e75sR0hCuJkU/DekTv8uz8/nij+fI/V/exg+/2dIxR/PkY79q3nPN5IISQg3k4L/hHQY\n7ix+7iqa/75q535FMnz8+x7pj1fthvui56kNhCSEm0nBf0Jqj/G87l9z6M5U6F7D6z4+32n1\n9xHz95H6L5T3i6/ZnyH9/nq/SjgMd1iPk+0ISQg3k4L/PbR7vNjQv2R2dD8nYv8c9/09ipuc\n2dB/ejjhwfl7knch/fp60788PV+vG07/JiQh3EwK/vscqX9+lPvnMD/PU67HbPQGU9Wda3ed\nRdJ9OjtW9eOEhRdf/WHy9SZ/+fx2pOc3JBGSEG6mNDX+iZS6/wVDSIG4mdLi/Ds8VT49gU5z\n3uTjX5/Hf3AzpeXnpYLpqUFaHi9r/Ly88b9P4i1uprQ8v9Wif0VPHyEJ4WZKTHPqvg8iO37l\n/oiQxHAzAQIICRBASIAAQgIEEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQE\nCCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQE\nCCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQECCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQE\nCCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQECCAkQAAhAQIIacZ9Zu3NRFI4HmY+u0m44TDG\n8TBDSAjH8TBDSAjH8TBDSAjH8TBDSAjH8TBDSAjH8TBDSAjH8TBDSAjH8TBDSAi3/Hi4nYr+\nHf6ivAluTwIICeGWHg/NYXS2TC66SWsjJIRbejyULrtU/aX6mrlSboPWR0gIt/R4yFz1vFy5\nTGZj0kBICLf0eJic/byvU6EJCeG4R5ohJISLeI50rftLPEcClh8P+ehVu0MjuUlrIySEi3gf\nqezfR8qKE+8jwTyOhxlCQjiOhxlCQjhOEZohJITjFKEZQkI4ThGaISSE4w3ZGUJCOKVThLb8\noxQJCeG+cI+0tUOOkBDuC6cIbe2QIySE+8IpQls75AgJ4b5witDWDjlCQrgvHA9bO+QICeEI\naYaQEC72eDhn7nDWHfFthIRwi4+HqnDZ+X764BShrR1yhIRwS4+Hqi+odMfmXhfu7X3S1g45\nQkK4pcfDsXvvqPTvxDbuoDFiLYSEcHGnCLli9AfpEWshJISLC+niH9NxihCsW/7Q7vg4naE5\ncooQrFv8jX3Z8/Gc++O7KLZ2yBESwi0/HspHPtkf39a3tUOOkBCOMxtmCAnhCGmGkBCOkGYI\nCeEIaYaQEI6QZggJ4QhphpAQjpBmCAnhCGmGkBCOkGYICeEIaYaQEI6QZggJ4QhphpAQjpBm\nCAnhCGmGkBCOkGYICeEIaYaQEI6QZggJ4QhphpAQjpBmCAnhCGmGkBCOkGYICeEIaYaQEI6Q\nZggJ4QhphpAQjpBmCAnhCGmGkBCOkGYICeEIaYaQEI6QZggJ4QhphpAQjpBmCAnhCGmGkBCO\nkGYICeEIaYaQEI6QZggJ4QhphpAQjpBmCAnhCGmGkBCOkGYICeEIaYaQEI6QZggJ4fYSkvvI\nZ19KcBWs2E1IYosICQsQkuoqWEFIqqtgBSGproIVhKS6ClYQkuoqWEFIqqtgBSGproIVhKS6\nClYQkuoqWEFIqqtgBSGproIVhKS6ClYQkuoqWEFIqqtgBSGproIVhKS6ClYQkuoqWEFIqqtg\nxfLj4XYq+p+DUJQ3rREBCAmrWno8NIfRzxTJVUYEISSsaunxULrsUvWX6mvmSo0RQQgJq1p6\nPGSuel6uXKYxIgghYVVLj4fJz4h7/wPjCAn7xz2S6ipYEfEc6Vr3l3iOBCw/HvLRq3aHRmVE\nCELCqiLeRyr795Gy4sT7SDCPMxtUV8EKQlJdBSsij4fzwbniqjriM4SEVcW9jzS84vD2RTtC\nggFRIZWubO73unRnjRFh2yO2iJCwQFRImetf927cQWNE2PaILSIkLBAV0uPUoPkpQuG/cDIS\nIWFVUSEdHyFxihCMWx5ScTpf3aW92JScIgTrlof0fNjmXMYpQjBu8fFQVedzUfQvOZRvOyIk\nGMCZDaqrYAUhqa6CFYuPh+boXD6cHMR3yMK6pcdDk/mfxeW/CCHBuOXfIXtuazpn/U/iIiRY\nt/R4yPwV6+xQExIQd/Z3e6eU54QELD0eDu7x5tEhJySYt/R4OLvjcKl2OSHBusXHQ/ms5/rH\nCd6EhP1bfjxUxeNSfSQkGMeZDaqrYAUhqa6CFYSkugpWEJLqKlhBSKqrYAUhqa6CFYSkugpW\nEJLqKlhBSKqrYAUhaa5yn/loIJJGSJqruHMzg5A0VxGSGYSkuYqQzCAkzVWEZAYhaa4iJDMI\nSXMVIZlBSJqrCMkMQtJcRUhmEJLmKkIyg5A0VxGSGYS0dJXcyT+EtAOEpLmKkMwgJM1VhGQG\nIWmuIiQzCElzFSGZQUiaqwjJDELSXEVIZhCS5ipCMoOQNFcRkhmEpLmKkMwgJM1VhGQGIWmu\nIiQzCElzFSGZQUiaqwjJDELSXEVIZhCS5ipCMoOQNFcRkhmEpLmKkMwgJM1VhGQGIWmuIiQz\nCElzFSGZQUiaqwjJDELSXEVIZhCS5ipCMoOQNFcRkhmEpLmKkMwgJM1VhGQGIWmuIiQzbIX0\n7R/YTUhm2ApJ7ksREiYISXMVIZlBSJqrCMkMQtJcRUhmEJLmKkIyg5A0VxGSGYSkuYqQzCAk\nzVWEZAYhaa4iJDMISXMVIZlBSJqrCMmM5Tvxdir6M9OK8qY1IgAhYVVLd2JzGJ3lmauMCEJI\nWNXSnVi67FL1l+pr5kqNEUEICatauhMzVz0vVy7TGBGEkLCqpTtx8l0777+Fh5BEViFp3CNp\nriIkMyKeI13r/hLPkb4zEElbvBPz0at2h0ZlRAhCwqoi3kcq+/eRsuLE+0jfGIikcWaD5ipC\nMoOQNFcRkhmcIqS5ipDM4BQhzVWEZAanCGmuIiQzeENWcxUhmaF0ilD4jwCOREhYFfdImqsI\nyQxOEdJcRUhmcIqQ5ipCMoNThDRXEZIZnNmguYqQzCAkzVWEZMbindgcncuvwxd5+1UISWQV\nkrb4FKHMn2jnvwgh6Q9E0pa//H1uazpn/Wl2hPSFgUja8jdk+w91dqgJ6SsDkbTYU4SaPCek\nrwxE0pbuxIN7vAl7yAnpGwORtKU78eyOw6Xa5YT0hYFI2uKdWD7ruf5xgjchiaxC0pbvxKp4\nXKqPhKQ/EEnjzAbNVYRkBiFpriIkMwhJcxUhmUFImqsIyQxC0lxFSGYQkuYqQjKDkDRXEZIZ\nhKS5ipDMICTNVYRkBiFpriIkMwhJcxUhmUFImqsIyQxC0lxFSGYQkuYqQjKDkDRXEZIZ4514\nONXaI9QQElY13ondj8NXaImQRFYhaeOd2FyOGi0RksgqJO33TrydDtItEZLIKiTtxU6sup9G\nfFYdIY+QsKr5Trz6XyGWK45QQEhY1a+d2Jzau6PDtWlrKl5fIXqEDkLCqiY78da92FD637Is\n98vICUlkFZI2eR+pvTM6P34S8fvfVL50hBpCwqom7yMVV+0RaggJq5q8j6Q/Qg0hYVWTndiU\n3eO5rJQtipBEViFp451YZ/0rDM5louc2EJLIKiRtvBNzd+zui5pS7qXv3yPUEBJWNT1p9fcF\n8RFqCAmrGu/EbPgtfA0hJTgQSRvvxNLlt/bDLXel1gg1hIRVTXaiP8tO8jy72QgthIRVTXfi\npegyEjzzez5CCSFhVV/YiYQksgpJSz8k9xm5LSEkhNtASHKrCAlaJjux+zbzz/99XzRC6+qE\nhFWNd+Ip7IHSkhFqVyckrGr6hqzw63XzEWpXJySs6uUpQnoj1K5OSFjVeCcWTuU7kghJZBWS\nNt6JddafIqQ5Qu3qmw5J7gV+rGX60I4XGzY+EGshpF0NxFp4Q3ZXA7EWQtrVQKxlunuuRfeo\nrpD9dRSE9MVVWMlk9+T+6VFaP/xky8c1IZkx3j1nl/ffZX52R60RaldP87gmJDOmpwg19+EH\ncmmNULt6msc1IZnx+xQhQtr0QKxlvHsOwz1S5Q5aI9SunuZxTUhmvHiOdBU+C5yQvrgKK5ns\nniLFnyK05eOakMyYv4/kiovmCKWrp3lcE5IZnNmwq4FYCyHtaiDWQki7Goi1TN9H4tsoNj4Q\na1ke0u3kX+Qryj++rZaQvrgKK3mxe275B79nrDmMsnv/cjkhfXEVVvJq9zQfnLRauuxS9Zfq\na/b+18AQ0hdXYSUvd88HD+0yVz0vVy4LHvG5LR/XhGTGq91zfh+Gv5773x8+GhFgy8c1IZnx\n+sWG05/X4x4pyYFYy6uQDh+cs9o+R7r676PlOVJCA7GWxbsnH71qd3j7E1oJ6YursJLlu+dW\n9u8jZcWJ95GSGYi1/OcNWcmzGwjpi6uwEkLa1UCsZbJ7Ttm1/e8t+/wb+84H54prwIhwWz6u\nCcmM8e45DS9pV+7vc4T8HdbwisPbF+0I6ZursJKX76t+8LCuX1K6srnf6/L9z3ggpC+uwkrG\nuyd73iP9/VOE+pAy/5vJmvfrCemLq7CS8e7p3mRtP3z0U4QmP/9ufg8m+KrFlo9rQjJjsnse\nb7K+f87jr9dd8fgIiVOEEhmItUx3z6X/KUJ/vArnr+eK0/nquh841JScIpTKQKxl6e4ZPWxz\nLuMUoUQGYi2Ld09Vnc9F0b/kUL7/ZeiE9MVVWMl09/CLxjY+EGuZv9hw5xeNbXcg1jLePfyi\nsc0PxFqmb8jyi8Y2PhBr+X2KECFteiDWMt49Ib9oLOBbLgjpi6uwkhfPkT46RehMSCkOxFom\nuyfkF41VH3/XEiF9cRVWMn8f6dNfNFZ9ckrefESwLR/XhGRGxO45j360ndKIz6+e5nFNSGaM\nd0/x4V1MxAi1q6d5XBOSGb9f/lYeoXb1NI9rQjLj98vfyiPUrp7mcU1IZox3T1Pkf/ysx+gR\naldP87gmJDOmD+3Ef6bd7xFqV0/zuCYkMwhpVwOxli/sHkL64iqshJB2NRBreewepZe+xyNU\nr57mcU1IZkxDUsmJkGRWfeSjLwV5hGRwIOQRksGBkEdIBgdCHiEZHAh5hGRwIOT9hKT24g8h\npTYQ8gjJ4EDI48wGgwMhj5AMDoQ8QjI4EPIIyeBAyCMkgwMhj5AMDoQ8QjI4EPIIyeBAyCMk\ngwMhj5AMDoQ8QjI4EPIIyeBAyCMkgwMhj5AMDoQ8QjI4EPIIyeBAyCMkgwMhj5AMDoQ8QjI4\nEPIIyeBAyCMkgwMhj5AMDoQ8QjI4EPIIyeBAyCMkgwMhj5AMDoQ8QjI4EPIIyeBAyCMkgwMh\nj5AMDoQ8QjI4EPIIyeBAyCMkgwMhj5AMDoQ8QjI4EPIIyeBAyCMkgwMhj5AMDoQ8QjI4EPII\nyeBAyCMkgwMhj5AMDoQ8QjI4EPIIyeBAyCMkgwMhb/ktfzsVrlOUN60RAVff8nFNSDuw9JZv\nDu5HrjIi6OpbPq4JaQeW3vKlyy5Vf6m+Zq7UGBF09S0f14S0A0tv+cxVz8uVyzRGBF19y8c1\nIe3A0lveuf/9QWxE0NW3fFwT0g5wj2RwIORFPEe61v0lniNtbiDkLb7l89GrdodGZUTI1bd8\nXBPSDkS8j1T27yNlxYn3kTY2EPI4s8HgQMgjJIMDIY9ThPY18DMfDUQIThFiIARwihADIYA3\nZBkIAUqnCAk+Ik/0MNv/QITgHomBEMApQgyEgMW3KacI7X0gQkS8j8QpQvseiBCc2cBACCAk\nBkIAITEQAgiJgRBASBYHcmqruOVnNnx8mxPSjgdisPTWOhMSA/Fj8a1VZe+/eUJgRMjV0zzM\n9j8Qg+W3VvX+xCCJEQFXT/Mw2/9ADCJurfPovFWlEZ9fPc3DbP8DMeBVOwZGrYJHSAyMWgWP\nkBgYtQoeITEwahU8QmJg1Cp4hMTAqFXwCImBUavgERIDo1bBIyQGRq2CR0gMjFoFj5AYGLUK\nHiExMGoVPEJiYNQqeITEwKhV8AiJgVGr4BESA6NWwSMkBkatgkdIDIxaBY+QGBi1Ch4hMTBq\nFTxCYmDUKniExMCoVfAIiYFRq+AREgOjVsEjJAZGrYJHSAyMWgWPkBgYtQoeITEwahU8QmJg\n1Cp4hMTAqFXwCImBUavgERIDo1bBIyQGRq2CR0gMjFoFj5AYGLUKHiExMGoVPEJiYNQqeITE\nwKhV8AiJgVGr4BESA6NWwSMkBkatgkdIDIxaBY+QGBi1Ch4hMTBqFTxCYmDUKniExMCoVfAI\niYFRq+AREgOjVsEjJAZGrYJHSAyMWgWPkBgYtQoeITEwahU8QmJg1Cp4hMTAqFXwCImBUavg\nERIDo1bBIyQGRq2CR0gMjFoFj5AYGLUKHiExMGoVPEJiYNQqeITEwKhV8AiJgVGr4BESA6NW\nwSMkBkatgrdmSO4zcUOCF236uCaktSy/tW6noj/Oi/K2cMT+D7P9D8Rg6a3VHEb3GfmyEfs/\nzPY/EIOlt1bpskvVX6qvmSsXjdj/Ybb/gRgsvbUyVz0vVy5bNGL/h9n+B2Kw9NaavAbw/gUB\nQtrxQAy4R2Jg1Cp4Ec+RrnV/iedIlgdisPjWykev2h2aRSP2f5jtfyAGEe8jlf37SFlx4n0k\nuwMx+MKtRUg7HoiB0q310Tk++z/M9j8Qg8hb63xwrrguHLH/w2z/AzGIex9peMXh7Yt2hLTn\ngRhEhVS6srnf69KdF43Y/2G2/4EYRIWUuf5178YdFo3Y/2G2/4EYRIX0eB2BU4TMDsQgKqTj\nIyROEbI6EIPlIRWn89Vd2otNySlCZgdisDyk53tEzmWcImR1IAaLb62qOp+Lon/JoXzbESHt\neSAGX7i1CGnHAzEgJAZGrYJHSAyMWgWPkBgYtQoeITEwahU8QmJg1Cp4hMTAqFXwCImBUavg\nERIDo1bBIyQGRq2CR0gMjFoFj5AYGLUKHiExMGoVPEJiYNQqeITEwP+tkvvFpAYQEgP1BxpA\nSAzUH2gAITFQf6ABhMRA/YEGEBID9QcaQEgM1B9oACExUH+gAYTEQP2BBhASA/UHGkBIDNQf\naAAhMVB/oAGExED9gQYQEgP1BxpASAzUH2gAITFQf6ABhMRA/YEGEBID9QcaQEgM1B9oACEx\nUH+gAYTEQP2BBhASA/UHGkBIDNQfaAAhMVB/oAGExED9gQYQEgP1BxpASAzUH2gAITFQf6AB\nhMRA/YEGEBID9QcaQEgM1B9oACExUH+gAYTEwKiB/BYlj5AYmMrATSMkBqYycNMIiYGpDNw0\nQmJgKgM3jZAYmMrATSMkBqYycNMIiYGpDNw0QmJgKgM3jZAYmMrATSMkBqYycNMIiYGpDNw0\nQmJgKgM3jZAYmMrATSMkBqYycNMIiYHJDNzyd2QQEgN3OPD7CImBOxz4fYTEwB0O/D5CYuAO\nB34fITFwhwO/j5AYuMOB30dIDNzhwO8jJAbucOD3ERIDvzBQ7r1WQlo628BhxkDpgd9HSAzc\n1sBETyQiJAaaHSiJkBhodqAkQmKg2YGSCImBZgdKWj7xdir6p3VFeVs4YtM7gYHbHyhp6cTm\nMHqJJF82YtM7gYFJD/z6S3tLv1jpskvVX6qvmSsXjUh1JzDQxsAkQspc9bxcuWzRiC3vBAZu\nf2ASIU3uF+d3kh/dg3743hqgY+Gx//pgXni9gHskYP8iniNd6/7Sn8+RgP1bfPeWj+4iD43k\nJgHbE/E+Utm/j5QVpz/eRwL27/vvXAE7REiAAEICBBASIICQAAGEBAggJEAAIQECCAkQQEiA\nAEICBBASIICQAAGEBAggJEAAIQECCAkQQEiAgPRDWulHNWH/RA9TyS+mItEtTHOz0tyqRDeL\nkFKQ5maluVWJbhYhpSDNzUpzqxLdLEJKQZqbleZWJbpZhJSCNDcrza1KdLMIKQVpblaaW5Xo\nZhFSCtLcrDS3KtHNIqQUpLlZaW5VoptFSClIc7PS3KpEN4uQUpDmZqW5VYluFiGlIM3NSnOr\nEt0sQkpBmpuV5lYlulnGQgI2gJAAAYQECCAkQAAhAQIICRBASIAAQgIEEBIggJAAAYQECCAk\nQAAhAQIICRBASIAAQgIEpBjS+eCysukvltmri6u5DTdXQptVHZ071qltVfN6W1bdrPPjWFfZ\ntgRDKvvfFJB1/095f/Fwn15cTZP5myuhzbomeWPVmd+sOqHNqh6/feL1BsVuW3ohVe7YdP98\nHLs7gKy6V5m7TS6up/D7IqXNytr5TeHKtLbq2G1Q+y9iQvuwneuP9dcbFL1t6YVU+E3q/rdL\nd20vXdxpcnE1l+FX6iS0WZf+iG1cltRW3V1y+/Ds8mGjXm9Q9LalF9Kg+98uXPfYoHLF5OJa\n6se+SGizjq56XExoq+7DQ+Cu70Q2q/0HZwjp9QZFb1uqITUun/zDNrq4ltzVfnxCm3Vw91PW\nPxZOaavup+Gh3SmZzap+j/+1QdHblmpI5+6uNpGd4J3c5Z5cSM4V/bP6tLaq3X3dqw3ZOanN\nMhlSnXX3senshOFeP8GQuhcbjgn90++d+tfATveUNstiSE2Wdx/S2Qntg6juJeYEQ+qeI9Xd\ny7YJbVX7eKJ9aNf2fU5psyyGlPvX87Of/7ts5WPj2L+q48cntFnu5aasvVXtU7fuWVvT9Z3O\nZg1zX29Q9LalGFJ9yP1b9f6llPrnVZV6tReixr9TPqHNGr1XkNBWjftOZ7Mmr9r93qDobUsw\npKvLh0un/m7g2j1OGF1cxTikhDbLz6+7WyyhrRr+fe/f3kpns4aQXm9Q9LalF1L97CiZd8Wf\nkjuzoX121HRPRi5JbdW9dN1Za2VaJ1xYO7Ph+PNPf/tYu9OHNbq4nmFfJLRZp5ebsvZWDWeu\npbVZj+c/rzcodtvSC2n0GGo4ibj/9OjieoZ9kdJmXfMXm7L6Vt1fbsu6m/UI6fUGxW5beiEB\nG0RIgABCAgQQEiCAkAABhAQIICRAACEBAggJEEBIgABCAgQQEiCAkAABhAQIICRAACEBAggJ\nEEBIgABCAgQQEiCAkAABhAQIICRAACEBAggJEEBIgABCAgQQEiCAkAABhAQIICRAACEBAggJ\nEEBI23Acfitj7o7+lxr+/Hf6Z6yDm34jMndu/3vuflE4ISWIm34jbs7V98b/2m0fzDib+Wfw\nXdz0W9E9uCu6B3aElCJu+s3I3Kl/YDfNZv5frIGbfjPaB3f9AztCShE3/XYc/QM7QkoRN/12\nZMMjO0JKEDf9Zhzd8FoDISWIm34rbu390fAkiZDSw02/FZm7DO/HElKCuOk3on1gdx/OECKk\nBHHTb8PNuab9UPcP7ggpPdz02+BPtRtOtiOk9HDTb8Lj5G//4I6Q0sNNv0Gca5cebvoNIqT0\ncNNv0O/vP+L7kdbHTb9BhJQebnpAACEBAggJEEBIgABCAgQQEiCAkAABhAQIICRAACEBAggJ\nEEBIgABCAgQQEiCAkAABhAQIICRAACEBAggJEEBIgABCAgQQEiCAkAABhAQIICRAACEBAggJ\nEPAPenvws/Df52AAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAW5klEQVR4nO3d2ULaQACG0Qm7bL7/2xYCIotomvxEiOdcVAqasel8BpJRyzvQ\nWfntTwCGQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBID1BKubz1ece5aS+fzLwq5TTSZveZrPc31rsbm8NnVn9yZ29v7+Rn\ndtMDNAppVfWy7+f7Fqbnfx3v345LmR8/MyEl2E0P0Ciknqbo6HgIOvv74v19Ucro47MQUoLd\n9AA3IX3/Tj19LgerUqrtdvd0b3Xx8PW73bufO+ymB7h3RNrOd8+oyuTt/eOL/eHdltP9s6/l\n8UM2u7+NF2cfuRmV2e7W22R3ezTbfGxvMSqjXQyLqoxXl8NfbO+mhN2Dk8np2d51MPfe8gO7\n6QHuhLSpjvmML0IaH29P6o9YHd/l8yNH9Qd8vFd9JDm+wy6y2em+k/Ptnfd6tK0/i2p7+akK\nqSO76QHuhLQ7FuwORtvx/lXK5wyffBRyKKk6/fXjI8v+w3Yvasa7uT+76GPXw3mDBxfb+yKk\n97fDFi8/VSF1ZDc9QDl3vOPw5/6J2fbwQv/40HL3drHdPevbvV3Ws7zav6k+P3If0P4cweZi\nS7t7F/vD1bp+8zn21fa+KmH0cabhXUgxdtMD3AlpH8fppdDHFJ3WZ9He64PNtD6e1O/x9vmR\ny6tNH/5cXbz5fIer7X1Rwv5a0qHKdyHF2E0PcCek+eGOY0ufDx1ermzqO6qPiXv98O4d3mbj\ncgrp/ebN6ePOt/dFCfVGxp/v3ugtP7CbHuBz9l1O9dnHK5vNzUMft8ptSIe/v43Oyvw+pItb\nNyXsnzxWpxdJQgqxmx7gXkjv27fDKbXxxUOnI0j15RGp/uv+qd5oulj/1xGpun7w/XDSbrU6\nnbYTUojd9AB3Q9qrr/J83jf58TVS/ejoeP+PIU1+eI00rc/mnS4kCSnEbnqAOyGNjgeLz0PF\n9u5Zu3IVyfHtz0ekH87aHY9Fp6UNQgqxmx7gTki7OT7e1Occ9isV9ufw9m9PV1oPx4jb60j1\nhsb1Oy+rH0O63t5VCaPjAetjsZ2QQuymB7j31O7jZEN9ymxaPhdif877+ohSLlY21HcfFzyU\nw5Hku5Cutnf54Px0vu64/FtIIXbTA9x9jVS/PhofXsN8vk5ZTquzC0zr/Vq75U0k+7ur6Xrz\nsWDhi61/uNjexYOnb0c6fUOSkELspue0PbyQerh7wQjpP9lNz6UcrvCsx5cL6B453sXbn+7n\nDrvpuXyeKrhcGvQoH6c1Pk9v3LuTb9lNz+X0rRb1Gb3HE1KI3fRktvP990FU016OR0KKsZsg\nQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQLah7Sq\nf/12KZPZKvj5wEtqG9J2VD6No58SvJ62Ic1K9baub22WVZnlPiF4RW1Dqsr6dHtdqswnA6+q\nbUil3PsL/EGOSBDQ4TXSclPf8hoJ2p/+Hp+dtRttk58SvJ4O15Fm9XWkajJ3HYk/z2kCCBAS\nBFgiBAGWCEGAJUIQ4IIsBFgiBAGOSBBgiRAEWCIEAZYIQcBvniYoXf3i5w4XHjQZG833rmML\niafRwxIhITF8PSwREhLD18MSISExfD1ckBUSw9fDEiEhMXyOSBDQwxIhITF8PSwREhLD18MS\nISExfD1MRiExfEKCgNaTcTstZbw8bsTpb/641kuEqsNCu8NGhMQf1/7092JX06Kql9kJib+u\n/QXZ+s2mGm2EBF2XCG3HYyFB28k4Kh8XYUdjIfHntZ2MizI93tqUsZD461pPxtmpnuUPPz1B\nSAxf+8m4nnzc2kyFxB9nZQMECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQ\nEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAg\nQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUFA+8m4mk/K3mS2ajmEkBiMtpNx\nOyqfxu2GEBKD0XYyzkr1tq5vbZZVmbUaQkgMRtvJWJX16fa6VK2GEBKD0XYylnLvL82HEBKD\n4YgEAR1eIy039S2vkaD9ZByfnbUbbVsNISQGo8N1pFl9HamazF1H4s+zsgEChAQBlghBgCVC\nEGCJEAS4IAsBlghBgCMSBFgiBAGWCEGAJUIQYGUDBDxoMpZzjxpbSDyNjpNxMSplsmw5hJAY\njG7XkY5nHL49aSck/oBOIc3KbPv+vpmVRashhMRgdAqpKvV5720ZtRpCSAxGp5A+ziNYIsRf\n1ymk6UdIlgjxx7UPaTJfLMvb7uZ2ZokQf137kE7XiEqpLBHij2s9GdfrxWIyqU85zL7tSEj8\nAZYIQYCQIEBIECAkCBASBHQ9/f3Dd0p8O4SQGIy2k3EhJPjU/jpS9f3PV20whJAYjPaTcf3D\ntyH9PISQGIwOk3Fx9qPtWg0hJAbDWTsIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKE\nBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGA\nkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQI\nEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQE\nAUKCACFBgJAgQEgQICQIEBIECAkCzifjaL559BDNHui6Yejb+WQspTyiJSExfOeTcfs2fURL\nQmL4rifjaj5KtyQkhu+LybiudselxUOH+OGBrhuGvt1OxuW47I0fOMRPD3TdMPTtajJu57vD\n0Wi53dU0edAQDR7oumHo28VkXO1PNszWhwdi01RIDN/FdaTdwWix/XigesQQzR7oumHo28V1\npMny0UM0e6DrhqFvF9eRHj9Eswe6bhj6djEZt7P987lqli1KSAzf+WTcVPUZhlKq6NoGITF8\n55NxXKb7Y9F2ljv1fT1Eswe6bhj6drlo9fpGfIhmD3TdMPTtfDJW5fDiaCsk+D/nk3FWxqvd\nm9W4zB41RLMHum4Y+nYxGQ+r7JLr7G6GaPRA1w1D3y4n49tkn1Fw5fftEE0e6Lph6Juf2QAB\nQoIAIUHAxWTcf5v5wcOGaPRA1w1D384n47wUIUEblxdkw+frbodo9kDXDUPfvlwi9Lghmj3Q\ndcPQt/PJOCkP+Y4kITF8l99GUS8Ramg1n9QvpyazHz5ISAzf1Y8sbnyyYTs6e+/vlxQJieFr\nG9KsVG+HHze0WVbfL3IVEsPXdjJWZX26vf7+Jw4JieFrOxlL89N9QmL4LifjcrJvYtLgRzY4\nIsGZ2+9H2v9syJ9L2r1GWh7ey2skuJiMizKuv8t8UaY/f+D47NTE6NvrT0Ji+K5/ZsPxB3I1\n+MjVrL6OVE3mriPx512fM2geUqshmj3QdcPQt/PJODoekdZl9Kghmj3QdcPQty9eIy2brQK3\nRAhOLibjpPlPEbJECM7cXkcqk7cGH2eJEJyxRAgCLBGCAEckCOjwbRSWCMGHtiFZIgRnvpiM\nq3Gj3zNmiRCcfDUZt00WrXYb4vsHum4Y+vblZOy+1q6UBs8ThcRgfDUZF9+fhTuyRAhOvj7Z\nMP/x4ywRgjNfhTRqsGbVEiE444IsBFgiBAF3Lsj+eFHWEQnOtA3JEiE4czEZ59Vy9+eqavCN\nfZYIwZnzyTg/Pl1blyZrhCwRgpMvzxn4KULwfy5/rt3HEclPEYL/cj4Z9ycQdm8a/hShVkM0\ne6DrhqFvF5Px4wTCtyfhug3R6IGuG4a+XU7Gt/qnCC0fOUSTB7puGPrWfmVD42tOQmL42k7G\nhZDg0+0PiGz2i8be140u294O0eSBrhuGvt2ebHhv9IvG9ifJG56TEBLDdz4Z/+sXje3fbf3z\nO70Lib/g8oLs//yisVZDNHug64ahb9dLhIQELZxPRr9oDFr64jWSJULwvy4m43/8orG2QzR6\noOuGoW+315Ga/aKx1kM0eaDrhqFvPUxGITF855Nxkl31/dUQzR7oumHoW/OfqhUZotkDXTcM\nfbs+/f3gIZo90HXD0LfzybidjH/4OSadh2j2QNcNQ9/u/Fy7Rw3R7IGuG4a+CQkCnP6GACFB\nwGN+JuSXQzR/oOuGoW+XIT0kJyExfEKCACFBgJAgQEgQICQI+Ayp+a+9bDlE8we6bhj6JiQI\nsLIBAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKC\nACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUFA+8m4mk/K3mS2ajmEkBiMtpNxOyqfxu2GEBKD\n0XYyzkr1tq5vbZZVmbUaQkgMRtvJWJX16fa6VK2GEBKD0XYylnLvL82HEBKD4YgEAR1eIy03\n9S2vkaD9ZByfnbUbbVsNISQGo8N1pFl9HamazF1H4s+zsgEChAQBlghBgCVCEGCJEAS4IAsB\nD1oiVM6lx059PMQ4IkGAJUIQYIkQBFgiBAFWNkCAkCCg42RcjEqZLFsOISQGo9t1pOMZh29P\n2gmJP6BTSLMy276/b2Zl0WoIITEYnUKqSn3ee1tGrYYQEoPRKaSP1T9+ihB/XaeQph8hWSLE\nH9c+pMl8sSxvu5vbmSVC/HXtQzqt7C6lskSIP671ZFyvF4vJpD7lMPu2IyHxB1jZAAFCggAh\nQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAg\nJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkC\nhAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFB\ngJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAk\nCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKE\nBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJB4VaWr6CeT3Nh/DiEkuniq+SMkXtVT\nzR8h8aqeav4IiVf1VPNHSLyqp5o/QuJVPdX8ERKv6qnmj5B4VU81f4TEq3qq+SMkXtVTzR8h\n8aqeav4IiVf1VPNHSLyqp5o/QuJVPdX8ERKv6qnmT/uNreaT+ps6JrNVyyGeakfwcp5q/rTd\n2HZ09g1S43ZDPNWO4OU81fxpu7FZqd7W9a3NsiqzVkM81Y7g5TzV/Gm7saqsT7fXpWo1xFPt\nCF7OU82fthu7+Ib32+9+b/St8Z2/5x66aDn3v57MLT/uP45IMHwdXiMtN/WtH18jwfC1PryN\nzw6Ro23yU4LX0+E60qy+jlRN5j9cR4Lhc+YLAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUGAkCBASBAgJAgQEgQICQKEBAG/GdIv/RAmOIhO5uTGXmhs4xtfSMY3/rONLyTjG//Z\nNvZCYxvf+EIyvvGfbXwhGd/4z7axFxrb+MYXkvGN/2zjC8n4xn+2jb3Q2MY3vpCMb/xnG19I\nxjf+s20M/iohQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBvYc0\nq0o12353R8/jL0a/O/7Oqsf/hZvx19NSpptfG3/b8///7j/8cm+Hxu87pHH9awBG39zR8/iz\n+o6qr//Jr/6526q//4Wb8Ze/++/fVIfx+yt5fflbKFLzr+eQVqVav6+rsrp7R8/jr8t0u/8i\nNf2l8fcm2V8w8n/jV7s7tpMy+6Xxp/XIs772//t+8PO9HZt/PYc0K8vdn29lfveOnsefHHZA\nX1P5q3/uW/g39fzX+G/1RN6W6pfGL/3u/92XzPHFWLH513NIk7I/hq/L5O4dPY9/1Nd/5Bfj\nb67+a/sdf1rWfY395fjHZ7V9hfy++7pxsbdj86/nkG6+APX8FenOcNsy/rXxx2XTX0g344/K\n+7yqn97+zvjz41O7np6RvK+v/vNj809Ie4v6AP8r48/LW39PbL7a/5P6xf5vjf++2J9tqBY9\njX81uJBi49c2VU/PLG/Hr59U/GpI+5MN076OCF99Idnr64B0NbiQYuPvbauenth99dRqf+L5\nV0Pav0ba9HX94Wb8xf6p3S7kHg9Jgwipuv68b+7oefy9cW9XsW7Gn9bPKfsL6ebf3/MXspvx\nR2X/8mzb34XEq39rbP79ylm7zfVZu02/Z+0uhtuMxv1dDbwe/zG/qr75+H2f/r8Zv+/T39dj\nxeZfzyHN66/Ay8/rfzd39Dz+7nZvz+u+GL/vkO7s/01fO+Fm/MMRobfrWHsX+zo2//76yobe\nptCd8Wu/uLJh9+pou3+N8vZL48/Kfp3brK8vpHuDWNmwe068V0/ewz/o7I7fGH/a7xHh9t9/\neav/8ee/u/+Pa936/Gr2sbez86/vkA6LfQ9Dl6s7fmP8np9a3f77L2/9wvjL8W/u/+Pq697G\nf78OKTX/+g4JBklIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGA\nkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEivYXr8\n7YzjMj38msHPPy//zu+w619EVRa7Pxf1r/8W0vOx61/EqpTN+/bw67cPwZxnc3sP/bLrX8X+\nyd1k/8ROSM/Irn8ZVZnXT+wus7n9k99g17+M3ZO7+omdkJ6RXf86pocndkJ6Rnb966iOz+yE\n9ITs+pcxLcdzDUJ6Qnb9q1jtjkfHF0lCej52/auoytvxeqyQnpBd/yJ2T+zejyuEhPSE7PrX\nsCplu3uzqZ/cCen52PWv4bDU7rjYTkjPx65/CR+Lvw9P7oT0fOz6F2St3fOx61+QkJ6PXf+C\nrr//yPcj/T67/gUJ6fnY9RAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKC\nACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgT8A9I93+MGgRkfAAAAAElF\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nkCBASBAgJAj4B5lz3RNQRC83AAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nECAkCBASBAgJAoQEAUKCACFBgJAg4B/wmNxfWfgsNQAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAWsklEQVR4nO3d6VraQACG0Qm7rPd/tyUBNLjUGD5MwHN+VCpo5knnbSAZpRyA\nm5WhBwDPQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBId1BKub719om2+a8MZlmV8rql3XEk2/rG9nhjdxpZM7jWx4+f5Ht2\n0x10CmlT/cq+X9YtzNt/ndYfp6UszyMTUoLddAedQvqlKTo5H4Jaf18dDqtSJpdRCCnBbrqD\nDyH9/0G/NJaTTSnVfn98ure5uvv9w776PF+wm+7gqyPSfnl8RlVmL4fLf/anh63n9bOv9flL\ndse/TVetr9xNyuJ462V2vD1Z7C7fbzUpk2MMq6pMN9ebv/p+H0o43jmbvT7bex/MVx/5ht10\nB1+EtKvO+UyvQpqeb8+ar9icH/L2lZPmCy6Pao4k5wccI1u8fu5V+/u1ez3bN6Oo9tdDFdKN\n7KY7+CKk47HgeDDaT+tXKW8zfHYp5FRS9frXy1eW+suOL2qmx7m/uOrj2EO7wZOr7/dJSIeX\n03e8HqqQbmQ33UFpO3/i9Gf9xGx/eqF/vmt9/LjaH5/1HT+um1le1R+qt6+sA6rPEeyuvtPx\ns6v6cLVtPrxt+933+6yEyeVMw0FIMXbTHXwRUh3H60uhyxSdN2fRDs3BZt4cT5pHvLx95frd\ntz79ubn68PaAd9/vkxLqa0mnKg9CirGb7uCLkJanT5xbervr9HJl13yiukzc93cfH/CymJbX\nkA4fPrx+Xfv7fVJC802mbw/v9JFv2E138Db7rqf64vLKZvfhrsut8jGk099fJq0y/x/S1a0P\nJdRPHqvXF0lCCrGb7uCrkA77l9MptenVXa9HkOrTI1Lz1/qp3mS+2v7oiFS9v/NwOmm32bye\nthNSiN10B1+GVGuu8rx9bvbta6Tm3sn589+GNPvmNdK8OZv3eiFJSCF20x18EdLkfLB4O1Ts\nvzxrV95Fcv74/RHpm7N252PR69IGIYXYTXfwRUjHOT7dNecc6pUK9Tm8+uPrldbTMeLjdaTm\nG02bB6+rb0N6//3elTA5H7Aui+2EFGI33cFXT+0uJxuaU2bz8rYQ+23eN0eUcrWyofn0ecFD\nOR1J/hfSu+93fefy9Xzdefm3kELspjv48jVS8/poenoN8/Y6ZT2vWheYtvVau/WHSOpPV/Pt\n7rJg4ZPvfnH1/a7ufP1xpNcfSBJSiN00TvvTC6m7+yoYIf2Q3TQu5XSFZzu9XkB3z+1dffzu\n83zBbhqXt1MF10uD7uVyWuPt9MZXn+S/7KZxef1Ri+aM3v0JKcRuGpn9sv45iGr+K8cjIcXY\nTRAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCgoD+IW2a9x8pZbbYBMcDD6lvSPvJ21vL\nXd4oG/6sviEtSvVyen/s3br6pXeXg9HqG1J1eZv5Q/1O87/yBtwwXn1DunpHRG+PyF/niAQB\nN7xGWu+aW14jQf/T39PWWbvJPjkkeDw3XEdaNNeRqtnSdST+PKcJIEBIEGCJEARYIgQBlghB\ngAuyEGCJEAQ4IkGAJUIQYIkQBFgiBAFOE0DAnUIqbffZBIzILywREhLP7xeWCAmJ5/cLS4SE\nxPP7hQuyQuL5/cISISHx/IY8IpVueo4QftEvLBH6OqROWxISD+AXlggJief3C0uEhMTz+4Vp\nKiSen5AgoPc03c9Lma7P36Tf6W8h8TR6LxGqTgvtTt9ESPxx/U9/r441rapmmZ2Q+Ov6X5Bt\nPuyqyU5IcOsSof10KiToO00n5XIRdjIVEn9e32m6KvPzrV2ZCom/rvc0XbzWs/5mYamQeH79\np+l2drm1mwuJP87KBggQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAg\nQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIKD/NN0sZ6U2W2x6bkJIPI2+\n03Q/KW+m/TYhJJ5G32m6KNXLtrm1W1dl0WsTQuJp9J2mVdm+3t6WqtcmhMTT6DtNS/nqL903\nISSehiMSBNzwGmm9a255jQT9p+m0ddZusu+1CSHxNG64jrRoriNVs6XrSPx5VjZAgJAgwBIh\nCLBECAIsEYIAF2QhwBIhCHBEggBLhCDAEiEIsEQIAqxsgIA7TdPSdtu2hcQDuHGarialzNY9\nNyEknsZt15HOZxz+e9JOSPwBN4W0KIv94bBblFWvTQiJp3FTSFVpznvvy6TXJoTE07gppMt5\nBEuE+OtuCml+CckSIf64/iHNlqt1eTne3C8sEeKv6x/S6zWiUipLhPjjek/T7Xa1ms2aUw6L\n/3YkJP4AS4QgQEgQICQIEBIECAkCbj39/c1PSvx3E0LiafSdpishwZv+15Gq//9+1Q6bEBJP\no/803X7zY0jfb0JIPI0bpumq9avtem1CSDwNZ+0gQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKC\nACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUFAe5pOlrt7b6LbHT0e\nBYNqT9NSyj1aEhLPrz1N9y/ze7QkJJ7f+2m6WU7SLQmJ5/fJNN1Wx+PS6q6b+OaOHo+CQX2c\nputpqU3vuInv7ujxKBjUu2m6Xx4PR5P1/ljT7E6b6HBHj0fBoK6m6aY+2bDYnu6ITWAh8fyu\nriMdD0ar/eWO6h6b6HZHj0fBoK6uI83W995Etzt6PAoGdXUd6f6b6HZHj0fBoK6m6X5RP5+r\nFtmihMTza0/TXdWcYSiliq5tEBLPrz1Np2VeH4v2i9yp7/eb6HZHj0fBoK4Xrb6/Ed9Etzt6\nPAoG1Z6mVTm9ONoLCX6mPU0XZbo5fthMy+Jem+h2R49HwaCupulplV1ynd2HTXS6o8ejYFDX\n0/RlVmcUXPn9cRNd7ujxKBiU39kAAUKCACFBwNU0rX/M/ORum+h0R49HwaDa03RZyg9C2ixn\nzUNni033TXS7o8ejYFDXF2S7n6/bT96q++Z0uZB4fp8uEepgUaqX04/S7tbV/y/gConn156m\ns9L95yeqsn29vf3/T9MKied3/WMU029e7rS+rvuhTEg8v3e/srjzyQZHJGjpG9LxNdL69ON/\nXiNB/2k6bWU3+e9rKyHx/PpP082iuY5UzZauI/HnXU/T9ax+VjfLvh2FkHh+H38eqf7dkH75\nCfxIe5quyrT5KfNVmXf4SkuE4NX739lw/oVc336dJULQ8v66ateQLBGClvY0nZyPSNsy+fbr\nXJCFlk9eI627rAK3RAharqbprPtvEXJEgpaP15HK7KXD11kiBC2WCEGAJUIQ4LcIQUDfH6P4\n7tuWDt9MSDyN/iFZIgSvPpmmm2mH9xmzRAhaPpum+w6LVi0RgpZPp6nf2QA/89k0Xf0/jNPX\nWSIEbz4/2bD89usckaDls5AmHX5zsSVC0GKJEARYIgQBX1yQTaxu+GwT3e7o8SgYlJAg4Gqa\nLqv18c9N1eEH+/puotMdPR4Fg2pP0+X5lPa2dFgj1G8T3e7o8SgY1KfXVb2HLPzM9e+1uxyR\nvv8tQj94PSUknl97mtYXWY8fOv0WoZWQ4M3VNL1cZP3vQoWzbedTEkLi+V1P05fmtwitO33l\ntlNvHzbR5Y4ej4JB3TBNV611q702ISSehl9+AgEff0GkNxqDH/t4suHgjcbgp9rT9GdvNNZr\nE93u6PEoGNT1BdnubzTWcxPd7ujxKBjU+yVCQoIe2tP0J2801nMT3e7o8SgY1CevkTotEeq5\niW539HgUDOpqmv7gjcb6bqLTHT0eBYP6eB2p2xuN9d5Elzt6PAoGZWUDBLSn6azjKtQbNtHt\njh6PgkF1/83DkU10u6PHo2BQ709/33kT3e7o8SgYVHua7mfTb37X482b6HZHj0fBoL74vXb3\n2kS3O3o8CgYlJAhw+hsChAQB9/mdkJ9uovsdPR4Fg7oO6S45CYnnJyQIEBIECAkChAQBQoKA\nt5Du8raX7U10v6PHo2BQQoIAKxsgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIA\nIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQ\nICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJ\nAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBDQf5pulrNSmy02PTchJJ5G\n32m6n5Q3036bEBJPo+80XZTqZdvc2q2rsui1CSHxNPpO06psX29vS9VrE0LiafSdpqV89Zfu\nmxAST8MRCQJueI203jW3vEaC/tN02jprN9n32oSQeBo3XEdaNNeRqtnSdST+PCsbIEBIEGCJ\nEARYIgQBlghBgAuyEHCnJUKl7bZtC4kH4IgEAZYIQYAlQhBgiRAEWNkAAUKCgBun6WpSymzd\ncxNC4mncdh3pfMbhvyfthMQfcFNIi7LYHw67RVn12oSQeBo3hVSV5rz3vkx6bUJIPI2bQrqs\n/vFbhPjrbgppfgnJEiH+uP4hzZardXk53twvLBHir+sf0uvK7lIqS4T443pP0+12tZrNmlMO\ni/92JCT+ACsbIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGA\nkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQI\nEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQE\nAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQ\nEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQHkXpZOhR/llCehSd9oOdNRQhPQohjZqQ\nhtftSVu3kDwBHIiQhhfcD3bpUPrv081y1vzvNltsem7Cv/qJkJ5A3326n7SeKUz7bcK/+omQ\nnkDffboo1cu2ubVbV2XRaxP+1U9+P6SgThv8A/ruiKpsX29vS9VrE88fUnIy/voRyWnCn+i7\nI67+9T9OhU7zJPkfI/xYz7n/+WTu+XU/OCLB87vhNdJ619z69jUSPL/eh7dp6xA52SeHBI/n\nhutIi+Y6UjVbfnMdCZ6fsy4QICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBgypIF+CRN/wO9P5l/f4ji2/R+G9QPjHNUAwxLSB4b1A+MclZDGwLB+\nYJyjEtIYGNYPjHNUQhoDw/qBcY5KSGNgWD8wzlEJaQwM6wfGOSohjYFh/cA4RyWkMTCsHxjn\nqIQ0Bob1A+MclZDGwLB+YJyjEtIYGNYPjHNUfywkeBpCggAhQYCQIEBIECAkCBASBAgJAoQE\nAUKCACFBgJAgQEgQICQIEBIECAkCBgtpUZVqsR9q69dWk9extIY1hhFuzv8+IxrWdl7KfDe2\nUe0/H8vvDWuokKbNewZMBtr6tUUzlqre361hjWGE++r07zOiYa1HubN21WlYu8GGNVBIm1Jt\nD9uqbIbZ/JVtmR+nxarMr4Y1ihHOTu9PMqZhVcft72dlMa5RzesBHf9HHO7fcKCQFmV9/POl\nLIfZ/JXZaR/Uc7Y1rDGM8OX8Rj8jGtZLM2P3pRrVqA5l8H/DgUKalfogvC2zYTb/mfofoTWs\nEYxwV6anCTKiYc3L9nJzRKM6nJ8C130PNKyBQmr9DzIS+zK9GtYIRjgtu9PmRzSsSTksq+a5\n8JhGdVien9otBxuWkM5W9dOAMc2N4+R4OYwupFJmzav6cY3q+M9Xn22oVsMNS0gnu6o+/o9p\nbjTPSEYYUn2yYT7gf/2fWzbn55YHIQ1rX03rD2OaG5P6FPMIQ6pfI+3qU8ojGtXx+cTxqd2x\n79VfC6kaes+/Mz1da2gNa+gRzpszTqfNj2hY5dOhDD2q40u3+lXbvu57oGENetZuN5KzdrvJ\n9HSpvjWsoUfYfqf7EQ2rda1gRKNq9z3QsAYKadn8f7tuzrUMbl2m51utYQ09wnZIIxrWafu7\neo+NaFTnY09zeWugYVnZcJoVJ2O6WN8Y3cqG46ujff1i5GVUozosSr2ibjHggovhrkfUpt8/\n8P7mb//1t4c1ihGen7KMaFjLT4cy9KjOq+oGHNZQIZ1W6w608Wut51DtYY1ihOeQxjSs9fST\noQw+qsOnY/nFYY3ltBk8NCFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIA\nIT2G+fkNHKdlfnqPwbc/r//OMOz6B1GV1fHPVf2+3UIaIbv+QWxK2R32p3foPgXTzubjZ/hd\ndv2jqJ/czeondkIaI7v+YVRl2Tyxu87m458Mwa5/GMcnd80TOyGNkV3/OOanJ3ZCGiO7/nFU\n52d2Qhohu/5hzMv5XIOQRsiufxSb4/Ho/CJJSONj1z+Kqrycr8cKaYTs+gdxfGJ3OK8QEtII\n2fWPYVPK/vhh1zy5E9L42PWP4bTU7rzYTkjjY9c/hMvi79OTOyGNj13/gKy1Gx+7/gEJaXzs\n+gf0/ueP/DzS8Oz6BySk8bHrIUBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkC\nhAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCPgH6gPWpINqfZQAAAAA\nSUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nSUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nEgQICQKEBAH/AMHG68f7OHJtAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\n2jOT1I0dAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enw8PD////QFLu4AAAACXBIWXMAABJ0AAAS\ndAHeZh94AAAWzUlEQVR4nO3d2ULaQACG0Qm7bL7/2xbCIosoTX4CxHMuKgXN2HQ+A8mo5RNo\nrTz7E4A+EBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBDSA5RSzm993XFq3MknM61KOY602nwmy+2N5ebGaveZ1Z/cydvrO/md\n3fQAd4W0qDrZ99NtC+PTvw63b4elTPefmZAS7KYHuCukjqboYH8IOvn77PNzVsrg8FkIKcFu\neoCrkH5+p44+l51FKdV6vXm6tzh7+PLdbt3PDXbTA9w6Iq2nm2dUZfTxefhiv3u3+Xj77Gu+\n/5DV5m/D2clHrgZlsrn1MdrcHkxWh+3NBmWwiWFWleHifPiz7V2VsHlwNDo+27sM5tZbfmE3\nPcCNkFbVPp/hWUjD/e1R/RGL/bt8feSg/oDDe9VHkv07bCKbHO87Ot3eaa976/qzqNbnn6qQ\nWrKbHuBGSJtjweZgtB5uX6V8zfDRoZBdSdXxr4ePLNsP27yoGW7m/uSsj00Ppw3unG3vm5A+\nP3ZbPP9UhdSS3fQA5dT+jt2f2ydm690L/f1D883b2XrzrG/zdl7P8mr7pvr6yG1A23MEq7Mt\nbe6dbQ9Xy/rN19gX2/uuhMHhTMOnkGLspge4EdI2juNLocMUHddn0T7rg824Pp7U7/Hx9ZHz\ni03v/lycvfl6h4vtfVPC9lrSrspPIcXYTQ9wI6Tp7o59S18P7V6urOo7qsPEvXx48w4fk2E5\nhvR59eb4cafb+6aEeiPDr3e/6y2/sJse4Gv2nU/1yeGVzerqocOtch3S7u8fg5Myfw7p7NZV\nCdsnj9XxRZKQQuymB7gV0uf6Y3dKbXj20PEIUn17RKr/un2qNxjPlv91RKouH/zcnbRbLI6n\n7YQUYjc9wM2QtuqrPF/3jX59jVQ/Otjf/2tIo19eI43rs3nHC0lCCrGbHuBGSIP9weLrULG+\nedauXESyf/v7EemXs3b7Y9FxaYOQQuymB7gR0maOD1f1OYftSoXtObzt2+OV1t0x4vo6Ur2h\nYf3O8+rXkC63d1HCYH/AOiy2E1KI3fQAt57aHU421KfMxuVrIfbXvK+PKOVsZUN9937BQ9kd\nSX4K6WJ75w9Oj+fr9su/hRRiNz3AzddI9euj4e41zNfrlPm4OrnAtNyutZtfRbK9uxovV4cF\nC99s/eBse2cPHr8d6fgNSUIKsZte03r3QurhbgUjpP9kN72WsrvCsxyeL6B75Hhnb3+7nxvs\nptfydargfGnQoxxOa3yd3rh1Jz+ym17L8Vst6jN6jyekELvpxayn2++DqMadHI+EFGM3QYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQ\nEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUFA85AW01HZGk0Wwc8H3lLTkNaD8mUY/ZTg/TQNaVKqj2V9azWvyiT3CcE7ahpSVZbH\n28tSZT4ZeFdNQyrl1l/gD3JEgoAWr5Hmq/qW10jQ/PT38OSs3WCd/JTg/bS4jjSpryNVo6nr\nSPx5ThNAgJAgwBIhCLBECAIsEYIAF2QhwBIhCHjmEam01fBzh7hnLhFqG4KQeBnPXCIkJHrj\nmUuEhERvPHMyConeeNBkvOucgJDojQ6WCAmJ/utgiZCQ6L8OlggJif7r4IKskOi/DpYICYn+\nc0SCgA6WCAmJ/utgiZCQ6L8OlggJif7rYDIKif4TEgQ0nozrcSnD+X4jTn/zxzVeIlTtFtrt\nNiIk/rjmp79nm5pmVb3MTkj8dc0vyNZvVtVgJSRou0RoPRwKCZpOxkE5XIQdDIXEn9d0Ms7K\neH9rVYZC4q9rPBknx3rmv/yEOSHRf80n43J0uLUaC4k/zsoGCBASBAgJAoQEAUKCACFBgJAg\nQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAc0n\n42I6KlujyaLhEEKiN5pOxvWgfBk2G0JI9EbTyTgp1ceyvrWaV2XSaAgh0RtNJ2NVlsfby1I1\nGkJI9EbTyVjKrb/cP4SQ6A1HJAho8RppvqpveY0EzSfj8OSs3WDdaAgh0RstriNN6utI1Wjq\nOhJ/npUNECAkCLBECAIsEYIAS4QgwAVZCLBECAIckSDAEiEIsEQIAiwRggArGyDgQZOxnHrU\n2ELiZbScjLNBKaN5wyGERG+0u460P+Pw40k7IfEHtAppUibrz8/VpMwaDSEkeqNVSFWpz3uv\ny6DREEKiN1qFdDiPYIkQf12rkMaHkCwR4o9rHtJoOpuXj83N9cQSIf665iEdrxGVUlkixB/X\neDIul7PZaFSfcpj82JGQ+AMsEYIAIUGAkCBASBAgJAhoe/r7l++U+HEIIdEbTSfjTEjwpfl1\npOrnn696xxBCojeaT8blL9+G9PsQQqI3WkzG2cmPtms0hJDoDWftIEBIECAkCBASBAgJAoQE\nAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQ\nIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQ\nEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQB\nQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQcDpZBxMV48e4r4H2m4YunY6GUspj2hJSPTf\n6WRcf4wf0ZKQ6L/LybiYDtItCYn++2YyLqvNcWn20CF+eaDthqFr15NxPixbwwcO8dsDbTcM\nXbuYjOvp5nA0mK83NY0eNMQdD7TdMHTtbDIuticbJsvdA7FpKiT67+w60uZgNFsfHqgeMcR9\nD7TdMHTt7DrSaP7oIe57oO2GoWtn15EeP8R9D7TdMHTtbDKuJ9vnc9UkW5SQ6L/Tybiq6jMM\npVTRtQ1Cov9OJ+OwjLfHovUkd+r7coj7Hmi7Yeja+aLVyxvxIe57oO2GoWunk7EquxdHayHB\n/zmdjJMyXGzeLIZl8qgh7nug7Yaha2eTcbfKLrnO7mqIux5ou2Ho2vlk/BhtMwqu/L4e4p4H\n2m4YuuZnNkCAkCBASBBwNhm332a+87Ah7nqg7Yaha6eTcVqKkKCJ8wuy4fN110Pc90DbDUPX\nvl0i9Lgh7nug7Yaha6eTcVQe8h1JQqL/zr+Nol4i9Mgh7nug7Yahaxc/stjJBmiieUiL6ah+\n19Hkl8OYkOi/ppNxPTjJ7udFrkKi/5pOxkmpPnY/AG81r37+tgsh0X/nk3E+2j6rG93xIxuq\nsjzeXv78M/CERP9dfz/S9mdD/l5SKbf+8vMQdz1wJyHxMk4n46wM6+8yn5Xxrx/niAQnLn9m\nw/4Hcv36cZvXSPPdcctrJLhaInRvSMdvS98a/LgiQkj03+lkHOyPSMsyuOMjF5P6OlI1mrqO\nxJ/3zWukeXgVuJDov7PJOPJThKCR6+tIZfRx10daIgRHlghBgCVCENB0MrogCyeafhuFJUJw\nomlIjkhw4pvJuBje8XvGLBGCE99NxvUdi1YtEYIT307G+77V3BIhOPhuMs5+fs2TGOLnB9pu\nGLr2/cmGaevNljvOXAiJ3vgupMFda1YtEYIjS4QgwBIhCLhxQfbXi7IuyMKJpiFZIgQnzibj\ntJpv/lxUd3xjnyMSnDidjNN9HMvy+xohS4TgxLfP0PwUIfg/5z/X7nBE8lOE4L+cTsbt07XN\nGz9FCP7X2WQ8PF378SVPuyHueqDthqFr55Pxo/4pQvNHDnHPA203DF3rYDIKif4TEgRc/4DI\n+37R2H+sghAS/Xd9suHzrl80NhMSfDmdjP/zi8Y+l/csJLoa4r4H2m4YunZ+Qfb+XzS2vWx7\n51lyIdF/l0uE7g9pc+Ba/v5On0LiLzidjP/3i8YaDXHfA203DF375jWSJULwv84mo180Bs1c\nX0e69xeNNRzingfabhi6ZmUDBJxOxlF21fd3Q9z3QNsNQ9fu/xkmkSHue6DthqFrl6e/HzzE\nfQ+03TB07XQyrkfDX75rvPUQ9z3QdsPQtRs/1+5RQ9z3QNsNQ9eEBAFOf0OAkCDgf34mZMsh\n7n+g7Yaha+chPSQnIdF/QoIAIUGAkCBASBAgJAj4Cun+X3vZcIj7H2i7YeiakCDAygYIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKC\nACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBI\nECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQI\nCQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIA\nIUGAkCBASBAgJAgQEgQICQKEBAFCgoDmk3ExHZWt0WTRcAgh0RtNJ+N6UL4Mmw0hJHqj6WSc\nlOpjWd9azasyaTSEkOiNppOxKsvj7WWpGg0hJHqj6WQs5dZf7h9CSPSGIxIEtHiNNF/Vt7xG\nguaTcXhy1m6wbjSEkOiNFteRJvV1pGo0dR2JP8/KBggQEgRYIgQBlghBgCVCEOCCLAQ8aIlQ\nOZUeO/XxEOOIBAGWCEGAJUIQYIkQBFjZAAFCgoCWk3E2KGU0bziEkOiNdteR9mccfjxpJyT+\ngFYhTcpk/fm5mpRZoyGERG+0Cqkq9XnvdRk0GkJI9EarkA6rf/wUIf66ViGNDyFZIsQf1zyk\n0XQ2Lx+bm+uJJUL8dc1DOq7sLqWyRIg/rvFkXC5ns9GoPuUw+bEjIfEHWNkAAUKCACFBgJAg\nQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBAS\nBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFC\nggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBA\nSBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIE\nCAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAkCBASBAgJAoQEAUKC\nACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEu+qtBX9ZJIb+88hhEQbLzV/\nhMS7eqn5IyTe1UvNHyHxrl5q/giJd/VS80dIvKuXmj9C4l291PwREu/qpeaPkHhXLzV/hMS7\neqn5IyTe1UvNn+YbW0xH9YKl0WTRcIiX2hG8nZeaP003th6cLP4bNhvipXYEb+el5k/TjU1K\n9bGsb63mVZk0GuKldgRv56XmT9ONVWV5vL0sVaMhXmpH8HZeav403djZN3Ncf2fHXd/20fr7\nSaCNhnP/+8nc8OP+44gE/dfiNdJ8Vd/69TUS9F/jw9vw5BA5WCc/JXg/La4jTerrSNVo+st1\nJOg/Z74gQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBI\nEPDMkJ70Q5hgJzqZkxt7o7GNb3whGd/4rza+kIxv/Ffb2BuNbXzjC8n4xn+18YVkfOO/2sbe\naGzjG19Ixjf+q40vJOMb/9U29kZjG9/4QjK+8V9tfCEZ3/ivtjH4q4QEAUKCACFBgJAgQEgQ\nICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgR0HtKkKtVk/dMdHY8/Gzx3/I1Fh/8LV+Mv\nx6WMV08bf93x///mP/x8b4fG7zqkYf1rAAY/3NHx+JP6jqqr/8nv/rnrqrv/havx58/996+q\n3fjdlbw8/y0UqfnXcUiLUi0/l1VZ3Lyj4/GXZbzefpEaP2n8rVH2F4z83/jV5o71qEyeNP64\nHnnS1f7/3A5+urdj86/jkCZlvvnzo0xv3tHx+KPdDuhqKn/3z/0I/6ae/xr/o57I61I9afzS\n7f7ffMkcno0Vm38dhzQq22P4soxu3tHx+Htd/Ud+M/7q4r+22/HHZdnV2N+Ov39W21XIn5uv\nG2d7Ozb/Og7p6gtQx1+Rbgy3LsOnjT8sq+5Cuhp/UD6nVf309jnjT/dP7Tp6RvK5vPjPj80/\nIW3N6gP8U8aflo/unth8t/9H9Yv9Z43/OduebahmHY1/MbiQYuPXVlVHzyyvx6+fVDw1pO3J\nhnFXR4TvvpBsdXVAuhhcSLHxt9ZVR0/svntqtT3x/NSQtq+RVl1df7gaf7Z9arcJucNDUi9C\nqi4/76s7Oh5/a9jZVayr8cf1c8ruQrr693f8hexq/EHZvjxbd3ch8eLfGpt/Tzlrt7o8a7fq\n9qzd2XCrwbC7q4GX4z/mV9XfP37Xp/+vxu/69PflWLH513FI0/or8Pzr+t/VHR2Pv7nd2fO6\nb8bvOqQb+3/V1U64Gn93ROjsOtbW2b6Ozb+/vrKhsyl0Y/zaE1c2bF4drbevUT6eNP6kbNe5\nTbr6QrrVi5UNm+fEW/Xk3f2DTu54xvjjbo8I1//+81vdjz997v7fr3Xr8qvZYW9n51/XIe0W\n++6GLhd3PGP8jp9aXf/7z289Yfz58Jn7f7/6urPxPy9DSs2/rkOCXhISBAgJAoQEAUKCACFB\ngJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKEBAFCggAhQYCQIEBIECAk\nCBASBAgJAoQEAUKCACFBgJAgQEgQICQIENJ7GO9/O+OwjHe/ZvDrz/O/8xx2/Zuoymzz56z+\n9d9Cej12/ZtYlLL6XO9+/fYumNNsru+hW3b9u9g+uRttn9gJ6RXZ9W+jKtP6id15Ntd/8gx2\n/dvYPLmrn9gJ6RXZ9e9jvHtiJ6RXZNe/j2r/zE5IL8iufxvjsj/XIKQXZNe/i8XmeLR/kSSk\n12PXv4uqfOyvxwrpBdn1b2LzxO5zv0JISC/Irn8Pi1LWmzer+smdkF6PXf8edkvt9ovthPR6\n7Pq3cFj8vXtyJ6TXY9e/IWvtXo9d/4aE9Hrs+jd0+f1Hvh/p+ez6NySk12PXQ4CQIEBIECAk\nCBASBAgJAoQEAUKCACFBgJAgQEgQICQIEBIECAkChAQBQoIAIUGAkCBASBAgJAgQEgQICQKE\nBAFCggAhQYCQIEBIEPAPRdbgl3uXshMAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nhAQBQoIAIUGAkCDgH1Zy3PWWE0OFAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nIEBIECAkCBASBAgJAoQEAf8AitLipCpCoPkAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nSBAgJAj4D6yu65rnHRQqAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nxQGCADoAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nxQGCADoAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nEgQICQKEBAH/AVty8khp5KWBAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nPymXoEBkMxy4AAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nEgQICQKEBAFCgoB/w+rhpUeDziUAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nIkEARYIAigQB/gP9gZqoYBTFfgAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nEgQICQKEBAH/AMHG68f7OHJtAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ERROR",
     "evalue": "Error in hist.default(X[[i]], ...): 'x' must be numeric\n",
     "output_type": "error",
     "traceback": [
      "Error in hist.default(X[[i]], ...): 'x' must be numeric\nTraceback:\n",
      "1. sapply(traindata, hist)",
      "2. lapply(X = X, FUN = FUN, ...)",
      "3. FUN(X[[i]], ...)",
      "4. hist.default(X[[i]], ...)",
      "5. stop(\"'x' must be numeric\")"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nAoQEAUKCACFBwD+7WuLR6nB2/AAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "Plot with title \"Histogram of X[[i]]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sapply(alldata, hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sapply(alldata, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns 50 and 52 are empty columns. also output is omitted.\n",
    "alldata <- alldata[,-c(50,52)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitdata <- submitdata[,-c(50,52)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>2073</li>\n",
       "\t<li>60</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2073\n",
       "\\item 60\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2073\n",
       "2. 60\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2073   60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(submitdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "trainIndex = createDataPartition(alldata$y, p = 0.7, list = FALSE)\n",
    "traindata = alldata[trainIndex, ]\n",
    "testdata = alldata[-trainIndex, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1453</li>\n",
       "\t<li>61</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1453\n",
       "\\item 61\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1453\n",
       "2. 61\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1453   61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>621</li>\n",
       "\t<li>61</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 621\n",
       "\\item 61\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 621\n",
       "2. 61\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 621  61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(traindata)\n",
    "dim(testdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>x1</th><th scope=col>x2</th><th scope=col>x3</th><th scope=col>x4</th><th scope=col>x5</th><th scope=col>x6</th><th scope=col>x7</th><th scope=col>x8</th><th scope=col>x9</th><th scope=col>x10</th><th scope=col>...</th><th scope=col>x52</th><th scope=col>x53</th><th scope=col>x54</th><th scope=col>x55</th><th scope=col>x56</th><th scope=col>x57</th><th scope=col>x58</th><th scope=col>x59</th><th scope=col>x60</th><th scope=col>y</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2</th><td>30   </td><td>0    </td><td>1    </td><td>1    </td><td>18   </td><td>13   </td><td> 3   </td><td>19   </td><td> 86.7</td><td>132.9</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>33   </td><td>1    </td><td>1    </td><td>0    </td><td> 2   </td><td>15   </td><td>12   </td><td>39   </td><td> 55.0</td><td>187.6</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>33   </td><td>0    </td><td>0    </td><td>1    </td><td> 5   </td><td> 5   </td><td>12   </td><td>26   </td><td>144.7</td><td>150.9</td><td>...  </td><td>0    </td><td>1    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>b    </td></tr>\n",
       "\t<tr><th scope=row>9</th><td>28   </td><td>1    </td><td>1    </td><td>1    </td><td> 0   </td><td> 0   </td><td> 2   </td><td>40   </td><td>121.3</td><td> 90.0</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><th scope=row>13</th><td>25   </td><td>0    </td><td>1    </td><td>1    </td><td> 4   </td><td>18   </td><td>18   </td><td>33   </td><td> 72.2</td><td>178.9</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><th scope=row>14</th><td>29   </td><td>1    </td><td>1    </td><td>0    </td><td> 7   </td><td>12   </td><td> 0   </td><td>25   </td><td>184.9</td><td>102.5</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><th scope=row>15</th><td>36   </td><td>1    </td><td>0    </td><td>0    </td><td> 3   </td><td>16   </td><td>15   </td><td>26   </td><td>159.4</td><td>154.9</td><td>...  </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><th scope=row>20</th><td>30   </td><td>1    </td><td>1    </td><td>1    </td><td> 6   </td><td>17   </td><td>16   </td><td>30   </td><td> 92.4</td><td> 36.8</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><th scope=row>22</th><td>28   </td><td>0    </td><td>1    </td><td>1    </td><td> 4   </td><td> 4   </td><td> 1   </td><td>27   </td><td>142.7</td><td>  0.8</td><td>...  </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><th scope=row>28</th><td>27   </td><td>1    </td><td>0    </td><td>0    </td><td> 1   </td><td>17   </td><td>18   </td><td>30   </td><td>164.2</td><td>150.5</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>b    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x52 & x53 & x54 & x55 & x56 & x57 & x58 & x59 & x60 & y\\\\\n",
       "\\hline\n",
       "\t2 & 30    & 0     & 1     & 1     & 18    & 13    &  3    & 19    &  86.7 & 132.9 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t5 & 33    & 1     & 1     & 0     &  2    & 15    & 12    & 39    &  55.0 & 187.6 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t6 & 33    & 0     & 0     & 1     &  5    &  5    & 12    & 26    & 144.7 & 150.9 & ...   & 0     & 1     & 1     & 0     & 0     & 0     & 1     & 0     & 0     & b    \\\\\n",
       "\t9 & 28    & 1     & 1     & 1     &  0    &  0    &  2    & 40    & 121.3 &  90.0 & ...   & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t13 & 25    & 0     & 1     & 1     &  4    & 18    & 18    & 33    &  72.2 & 178.9 & ...   & 0     & 0     & 0     & 0     & 0     & 0     & 1     & 0     & 0     & a    \\\\\n",
       "\t14 & 29    & 1     & 1     & 0     &  7    & 12    &  0    & 25    & 184.9 & 102.5 & ...   & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t15 & 36    & 1     & 0     & 0     &  3    & 16    & 15    & 26    & 159.4 & 154.9 & ...   & 0     & 0     & 1     & 0     & 0     & 0     & 1     & 0     & 0     & a    \\\\\n",
       "\t20 & 30    & 1     & 1     & 1     &  6    & 17    & 16    & 30    &  92.4 &  36.8 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t22 & 28    & 0     & 1     & 1     &  4    &  4    &  1    & 27    & 142.7 &   0.8 & ...   & 0     & 0     & 1     & 0     & 0     & 0     & 1     & 0     & 0     & a    \\\\\n",
       "\t28 & 27    & 1     & 0     & 0     &  1    & 17    & 18    & 30    & 164.2 & 150.5 & ...   & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & b    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 | x9 | x10 | ... | x52 | x53 | x54 | x55 | x56 | x57 | x58 | x59 | x60 | y |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 2 | 30    | 0     | 1     | 1     | 18    | 13    |  3    | 19    |  86.7 | 132.9 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "| 5 | 33    | 1     | 1     | 0     |  2    | 15    | 12    | 39    |  55.0 | 187.6 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "| 6 | 33    | 0     | 0     | 1     |  5    |  5    | 12    | 26    | 144.7 | 150.9 | ...   | 0     | 1     | 1     | 0     | 0     | 0     | 1     | 0     | 0     | b     |\n",
       "| 9 | 28    | 1     | 1     | 1     |  0    |  0    |  2    | 40    | 121.3 |  90.0 | ...   | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | a     |\n",
       "| 13 | 25    | 0     | 1     | 1     |  4    | 18    | 18    | 33    |  72.2 | 178.9 | ...   | 0     | 0     | 0     | 0     | 0     | 0     | 1     | 0     | 0     | a     |\n",
       "| 14 | 29    | 1     | 1     | 0     |  7    | 12    |  0    | 25    | 184.9 | 102.5 | ...   | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | a     |\n",
       "| 15 | 36    | 1     | 0     | 0     |  3    | 16    | 15    | 26    | 159.4 | 154.9 | ...   | 0     | 0     | 1     | 0     | 0     | 0     | 1     | 0     | 0     | a     |\n",
       "| 20 | 30    | 1     | 1     | 1     |  6    | 17    | 16    | 30    |  92.4 |  36.8 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "| 22 | 28    | 0     | 1     | 1     |  4    |  4    |  1    | 27    | 142.7 |   0.8 | ...   | 0     | 0     | 1     | 0     | 0     | 0     | 1     | 0     | 0     | a     |\n",
       "| 28 | 27    | 1     | 0     | 0     |  1    | 17    | 18    | 30    | 164.2 | 150.5 | ...   | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | b     |\n",
       "\n"
      ],
      "text/plain": [
       "   x1 x2 x3 x4 x5 x6 x7 x8 x9    x10   ... x52 x53 x54 x55 x56 x57 x58 x59 x60\n",
       "2  30 0  1  1  18 13  3 19  86.7 132.9 ... 0   0   0   0   1   0   0   0   0  \n",
       "5  33 1  1  0   2 15 12 39  55.0 187.6 ... 0   0   0   0   1   0   0   0   0  \n",
       "6  33 0  0  1   5  5 12 26 144.7 150.9 ... 0   1   1   0   0   0   1   0   0  \n",
       "9  28 1  1  1   0  0  2 40 121.3  90.0 ... 0   0   0   0   0   0   0   0   0  \n",
       "13 25 0  1  1   4 18 18 33  72.2 178.9 ... 0   0   0   0   0   0   1   0   0  \n",
       "14 29 1  1  0   7 12  0 25 184.9 102.5 ... 0   0   0   0   0   0   0   0   0  \n",
       "15 36 1  0  0   3 16 15 26 159.4 154.9 ... 0   0   1   0   0   0   1   0   0  \n",
       "20 30 1  1  1   6 17 16 30  92.4  36.8 ... 0   0   0   0   1   0   0   0   0  \n",
       "22 28 0  1  1   4  4  1 27 142.7   0.8 ... 0   0   1   0   0   0   1   0   0  \n",
       "28 27 1  0  0   1 17 18 30 164.2 150.5 ... 0   0   0   0   0   0   0   0   0  \n",
       "   y\n",
       "2  a\n",
       "5  a\n",
       "6  b\n",
       "9  a\n",
       "13 a\n",
       "14 a\n",
       "15 a\n",
       "20 a\n",
       "22 a\n",
       "28 b"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(traindata, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lasso I - 0.8454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Control Settings and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = \"repeatedcv\", number = 10, verboseIter = TRUE, classProbs = TRUE, repeats = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold01.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold01.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold02.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold02.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold03.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold03.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold04.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold04.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold05.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold05.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold06.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold06.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold07.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold07.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold08.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold08.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold09.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold09.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.01, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.01, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.02, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.02, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.03, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.03, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.04, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.04, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.05, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.05, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.06, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.06, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.07, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.07, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.08, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.08, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.09, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.09, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.10, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.10, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.11, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.11, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.12, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.12, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.13, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.13, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.14, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.14, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.15, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.15, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.16, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.16, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.17, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.17, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.18, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.18, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.19, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.19, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.20, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.20, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.21, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.21, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.22, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.22, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.23, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.23, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.24, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.24, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.25, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.25, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.26, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.26, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.27, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.27, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.28, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.28, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.29, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.29, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.30, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.30, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.31, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.31, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.32, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.32, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.33, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.33, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.34, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.34, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.35, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.35, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.36, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.36, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.37, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.37, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.38, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.38, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.39, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.39, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.40, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.40, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.41, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.41, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.42, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.42, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.43, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.43, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.44, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.44, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.45, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.45, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.46, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.46, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.47, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.47, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.48, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.48, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.49, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.49, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.50, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.50, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.51, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.51, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.52, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.52, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.53, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.53, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.54, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.54, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.55, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.55, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.56, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.56, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.57, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.57, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.58, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.58, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.59, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.59, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.60, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.60, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.61, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.61, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.62, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.62, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.63, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.63, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.64, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.64, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.65, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.65, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.66, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.66, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.67, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.67, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.68, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.68, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.69, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.69, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.70, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.70, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.71, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.71, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.72, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.72, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.73, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.73, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.74, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.74, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.75, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.75, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.76, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.76, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.77, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.77, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.78, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.78, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.79, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.79, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.80, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.80, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.81, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.81, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.82, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.82, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.83, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.83, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.84, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.84, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.85, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.85, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.86, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.86, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.87, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.87, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.88, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.88, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.89, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.89, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.90, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.90, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.91, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.91, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.92, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.92, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.93, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.93, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.94, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.94, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.95, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.95, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.96, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.96, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.97, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.97, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.98, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.98, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=0.99, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=0.99, lambda=0.01 \n",
      "+ Fold10.Rep1: alpha=1.00, lambda=0.01 \n",
      "- Fold10.Rep1: alpha=1.00, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold01.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold01.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold02.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold02.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold03.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold03.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold04.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold04.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold05.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold05.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold06.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold06.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold07.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold07.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold08.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold08.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold09.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold09.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.01, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.01, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.02, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.02, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.03, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.03, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.04, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.04, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.05, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.05, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.06, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.06, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.07, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.07, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.08, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.08, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.09, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.09, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.10, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.10, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.11, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.11, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.12, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.12, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.13, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.13, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.14, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.14, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.15, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.15, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.16, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.16, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.17, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.17, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.18, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.18, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.19, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.19, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.20, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.20, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.21, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.21, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.22, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.22, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.23, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.23, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.24, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.24, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.25, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.25, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.26, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.26, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.27, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.27, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.28, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.28, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.29, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.29, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.30, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.30, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.31, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.31, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.32, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.32, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.33, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.33, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.34, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.34, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.35, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.35, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.36, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.36, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.37, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.37, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.38, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.38, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.39, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.39, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.40, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.40, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.41, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.41, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.42, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.42, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.43, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.43, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.44, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.44, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.45, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.45, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.46, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.46, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.47, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.47, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.48, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.48, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.49, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.49, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.50, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.50, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.51, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.51, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.52, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.52, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.53, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.53, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.54, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.54, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.55, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.55, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.56, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.56, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.57, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.57, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.58, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.58, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.59, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.59, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.60, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.60, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.61, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.61, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.62, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.62, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.63, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.63, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.64, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.64, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.65, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.65, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.66, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.66, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.67, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.67, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.68, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.68, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.69, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.69, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.70, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.70, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.71, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.71, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.72, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.72, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.73, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.73, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.74, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.74, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.75, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.75, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.76, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.76, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.77, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.77, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.78, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.78, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.79, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.79, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.80, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.80, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.81, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.81, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.82, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.82, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.83, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.83, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.84, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.84, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.85, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.85, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.86, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.86, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.87, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.87, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.88, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.88, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.89, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.89, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.90, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.90, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.91, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.91, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.92, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.92, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.93, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.93, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.94, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.94, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.95, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.95, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.96, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.96, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.97, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.97, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.98, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.98, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=0.99, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=0.99, lambda=0.01 \n",
      "+ Fold10.Rep2: alpha=1.00, lambda=0.01 \n",
      "- Fold10.Rep2: alpha=1.00, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold01.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold01.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold02.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold02.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold03.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold03.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold04.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold04.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold05.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold05.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold06.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold06.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold07.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold07.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold08.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold08.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold09.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold09.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.01, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.01, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.02, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.02, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.03, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.03, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.04, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.04, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.05, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.05, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.06, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.06, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.07, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.07, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.08, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.08, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.09, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.09, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.10, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.10, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.11, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.11, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.12, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.12, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.13, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.13, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.14, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.14, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.15, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.15, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.16, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.16, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.17, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.17, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.18, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.18, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.19, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.19, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.20, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.20, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.21, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.21, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.22, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.22, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.23, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.23, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.24, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.24, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.25, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.25, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.26, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.26, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.27, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.27, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.28, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.28, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.29, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.29, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.30, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.30, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.31, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.31, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.32, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.32, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.33, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.33, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.34, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.34, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.35, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.35, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.36, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.36, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.37, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.37, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.38, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.38, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.39, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.39, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.40, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.40, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.41, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.41, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.42, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.42, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.43, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.43, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.44, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.44, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.45, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.45, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.46, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.46, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.47, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.47, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.48, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.48, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.49, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.49, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.50, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.50, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.51, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.51, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.52, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.52, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.53, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.53, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.54, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.54, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.55, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.55, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.56, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.56, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.57, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.57, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.58, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.58, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.59, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.59, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.60, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.60, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.61, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.61, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.62, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.62, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.63, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.63, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.64, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.64, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.65, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.65, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.66, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.66, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.67, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.67, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.68, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.68, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.69, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.69, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.70, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.70, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.71, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.71, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.72, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.72, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.73, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.73, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.74, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.74, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.75, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.75, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.76, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.76, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.77, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.77, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.78, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.78, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.79, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.79, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.80, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.80, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.81, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.81, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.82, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.82, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.83, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.83, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.84, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.84, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.85, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.85, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.86, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.86, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.87, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.87, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.88, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.88, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.89, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.89, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.90, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.90, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.91, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.91, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.92, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.92, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.93, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.93, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.94, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.94, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.95, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.95, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.96, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.96, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.97, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.97, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.98, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.98, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=0.99, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=0.99, lambda=0.01 \n",
      "+ Fold10.Rep3: alpha=1.00, lambda=0.01 \n",
      "- Fold10.Rep3: alpha=1.00, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold01.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold01.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold02.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold02.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold03.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold03.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold04.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold04.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold05.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold05.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold06.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold06.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold07.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold07.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold08.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold08.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold09.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold09.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.01, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.01, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.02, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.02, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.03, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.03, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.04, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.04, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.05, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.05, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.06, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.06, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.07, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.07, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.08, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.08, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.09, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.09, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.10, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.10, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.11, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.11, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.12, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.12, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.13, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.13, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.14, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.14, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.15, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.15, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.16, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.16, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.17, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.17, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.18, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.18, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.19, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.19, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.20, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.20, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.21, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.21, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.22, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.22, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.23, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.23, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.24, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.24, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.25, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.25, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.26, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.26, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.27, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.27, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.28, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.28, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.29, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.29, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.30, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.30, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.31, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.31, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.32, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.32, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.33, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.33, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.34, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.34, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.35, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.35, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.36, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.36, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.37, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.37, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.38, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.38, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.39, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.39, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.40, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.40, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.41, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.41, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.42, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.42, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.43, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.43, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.44, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.44, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.45, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.45, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.46, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.46, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.47, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.47, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.48, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.48, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.49, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.49, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.50, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.50, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.51, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.51, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.52, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.52, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.53, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.53, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.54, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.54, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.55, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.55, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.56, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.56, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.57, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.57, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.58, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.58, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.59, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.59, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.60, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.60, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.61, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.61, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.62, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.62, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.63, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.63, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.64, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.64, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.65, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.65, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.66, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.66, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.67, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.67, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.68, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.68, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.69, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.69, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.70, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.70, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.71, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.71, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.72, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.72, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.73, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.73, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.74, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.74, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.75, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.75, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.76, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.76, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.77, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.77, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.78, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.78, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.79, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.79, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.80, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.80, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.81, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.81, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.82, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.82, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.83, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.83, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.84, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.84, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.85, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.85, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.86, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.86, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.87, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.87, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.88, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.88, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.89, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.89, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.90, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.90, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.91, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.91, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.92, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.92, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.93, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.93, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.94, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.94, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.95, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.95, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.96, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.96, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.97, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.97, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.98, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.98, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=0.99, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=0.99, lambda=0.01 \n",
      "+ Fold10.Rep4: alpha=1.00, lambda=0.01 \n",
      "- Fold10.Rep4: alpha=1.00, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold01.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold01.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold02.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold02.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold03.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold03.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold04.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold04.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold05.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold05.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold06.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold06.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold07.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold07.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold08.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold08.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold09.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold09.Rep5: alpha=1.00, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.01, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.01, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.02, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.02, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.03, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.03, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.04, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.04, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.05, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.05, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.06, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.06, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.07, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.07, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.08, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.08, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.09, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.09, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.10, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.10, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.11, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.11, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.12, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.12, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.13, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.13, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.14, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.14, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.15, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.15, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.16, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.16, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.17, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.17, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.18, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.18, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.19, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.19, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.20, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.20, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.21, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.21, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.22, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.22, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.23, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.23, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.24, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.24, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.25, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.25, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.26, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.26, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.27, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.27, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.28, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.28, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.29, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.29, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.30, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.30, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.31, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.31, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.32, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.32, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.33, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.33, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.34, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.34, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.35, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.35, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.36, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.36, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.37, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.37, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.38, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.38, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.39, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.39, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.40, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.40, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.41, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.41, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.42, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.42, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.43, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.43, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.44, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.44, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.45, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.45, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.46, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.46, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.47, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.47, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.48, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.48, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.49, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.49, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.50, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.50, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.51, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.51, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.52, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.52, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.53, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.53, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.54, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.54, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.55, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.55, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.56, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.56, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.57, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.57, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.58, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.58, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.59, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.59, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.60, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.60, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.61, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.61, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.62, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.62, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.63, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.63, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.64, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.64, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.65, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.65, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.66, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.66, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.67, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.67, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.68, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.68, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.69, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.69, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.70, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.70, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.71, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.71, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.72, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.72, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.73, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.73, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.74, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.74, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.75, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.75, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.76, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.76, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.77, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.77, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.78, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.78, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.79, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.79, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.80, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.80, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.81, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.81, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.82, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.82, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.83, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.83, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.84, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.84, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.85, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.85, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.86, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.86, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.87, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.87, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.88, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.88, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.89, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.89, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.90, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.90, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.91, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.91, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.92, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.92, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.93, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.93, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.94, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.94, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.95, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.95, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.96, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.96, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.97, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.97, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.98, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.98, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=0.99, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=0.99, lambda=0.01 \n",
      "+ Fold10.Rep5: alpha=1.00, lambda=0.01 \n",
      "- Fold10.Rep5: alpha=1.00, lambda=0.01 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting alpha = 0.86, lambda = 0.007 on full training set\n"
     ]
    }
   ],
   "source": [
    "tunegrid <- expand.grid(alpha = seq(0.01,1,by = 0.01),lambda = seq(0.001,0.01,by = 0.001))\n",
    "Lasso <- train(y~., data=traindata7, method=\"glmnet\", trControl = fitControl, tuneGrid = tunegrid)\n",
    "predLasso <- predict(Lasso,traindata3[,-59],type = 'prob')  #testdata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.160574950330444</li>\n",
       "\t<li>0.194614757214468</li>\n",
       "\t<li>0.0657642673096464</li>\n",
       "\t<li>0.0090848028351076</li>\n",
       "\t<li>0.0108183493696735</li>\n",
       "\t<li>0.328274188114084</li>\n",
       "\t<li>0.0128692228118427</li>\n",
       "\t<li>0.135185340992715</li>\n",
       "\t<li>0.0262270582348304</li>\n",
       "\t<li>0.736308993875852</li>\n",
       "\t<li>0.232280136046881</li>\n",
       "\t<li>0.180565932268704</li>\n",
       "\t<li>0.353884466798498</li>\n",
       "\t<li>0.141278124487713</li>\n",
       "\t<li>0.620304701153559</li>\n",
       "\t<li>0.646197537582489</li>\n",
       "\t<li>0.473042691032659</li>\n",
       "\t<li>0.00843101696370038</li>\n",
       "\t<li>0.0998384758039544</li>\n",
       "\t<li>0.86046324688653</li>\n",
       "\t<li>0.000952697013496243</li>\n",
       "\t<li>0.0491613547308922</li>\n",
       "\t<li>0.564323727160182</li>\n",
       "\t<li>0.00616664915214881</li>\n",
       "\t<li>0.0424192209665897</li>\n",
       "\t<li>0.0204973284053453</li>\n",
       "\t<li>0.277756181954646</li>\n",
       "\t<li>0.669862183309902</li>\n",
       "\t<li>0.29283164108411</li>\n",
       "\t<li>0.073742669913145</li>\n",
       "\t<li>0.956510691442855</li>\n",
       "\t<li>0.645195609645712</li>\n",
       "\t<li>0.00222944648840101</li>\n",
       "\t<li>0.0642951616935489</li>\n",
       "\t<li>0.15950039260962</li>\n",
       "\t<li>0.00361918265583433</li>\n",
       "\t<li>0.149162887069616</li>\n",
       "\t<li>0.0133544798177277</li>\n",
       "\t<li>0.0379488060425114</li>\n",
       "\t<li>0.692349209723638</li>\n",
       "\t<li>0.818859615955628</li>\n",
       "\t<li>0.0698489247122723</li>\n",
       "\t<li>0.0128890656898538</li>\n",
       "\t<li>0.637594046088248</li>\n",
       "\t<li>0.0358308500949368</li>\n",
       "\t<li>0.0163667569305153</li>\n",
       "\t<li>0.039050456872504</li>\n",
       "\t<li>0.207951969398532</li>\n",
       "\t<li>0.213026987635583</li>\n",
       "\t<li>0.0704242697118449</li>\n",
       "\t<li>0.709245282799869</li>\n",
       "\t<li>0.00192910792915866</li>\n",
       "\t<li>0.567849341915015</li>\n",
       "\t<li>0.02184291683155</li>\n",
       "\t<li>0.909870295637369</li>\n",
       "\t<li>0.190739918054592</li>\n",
       "\t<li>0.210933850776207</li>\n",
       "\t<li>0.00321592478724331</li>\n",
       "\t<li>0.027723725866694</li>\n",
       "\t<li>0.534321606679551</li>\n",
       "\t<li>0.196591374794026</li>\n",
       "\t<li>0.244281657991462</li>\n",
       "\t<li>0.111520445878349</li>\n",
       "\t<li>0.514158404335423</li>\n",
       "\t<li>0.169053320049634</li>\n",
       "\t<li>0.33898993986781</li>\n",
       "\t<li>0.353592033567074</li>\n",
       "\t<li>0.917337288114215</li>\n",
       "\t<li>0.0036081380640822</li>\n",
       "\t<li>0.0564426239842941</li>\n",
       "\t<li>0.0465747326350894</li>\n",
       "\t<li>0.165961509381525</li>\n",
       "\t<li>0.205367164160717</li>\n",
       "\t<li>0.0288348964901622</li>\n",
       "\t<li>0.548167173229679</li>\n",
       "\t<li>0.0312231270992339</li>\n",
       "\t<li>0.156158473529706</li>\n",
       "\t<li>0.251235257162444</li>\n",
       "\t<li>0.376636338394775</li>\n",
       "\t<li>0.886803578557474</li>\n",
       "\t<li>0.376827960787195</li>\n",
       "\t<li>0.226186941402099</li>\n",
       "\t<li>0.0169519742001024</li>\n",
       "\t<li>0.00639368479797662</li>\n",
       "\t<li>0.207655356381026</li>\n",
       "\t<li>0.0922712223563015</li>\n",
       "\t<li>0.00819751978744117</li>\n",
       "\t<li>0.325605630965992</li>\n",
       "\t<li>0.0118846373687264</li>\n",
       "\t<li>0.153025955271613</li>\n",
       "\t<li>0.0631177824266063</li>\n",
       "\t<li>0.405602744858094</li>\n",
       "\t<li>0.279217534904339</li>\n",
       "\t<li>0.0150225553051513</li>\n",
       "\t<li>0.289793474313642</li>\n",
       "\t<li>0.221088800577378</li>\n",
       "\t<li>0.0180259821139205</li>\n",
       "\t<li>0.116502449288295</li>\n",
       "\t<li>0.0585713260213269</li>\n",
       "\t<li>0.13769494400464</li>\n",
       "\t<li>0.329729761655856</li>\n",
       "\t<li>0.635589905840042</li>\n",
       "\t<li>0.0670963077693865</li>\n",
       "\t<li>0.259761139540847</li>\n",
       "\t<li>0.143894687291994</li>\n",
       "\t<li>0.333581539566896</li>\n",
       "\t<li>0.129856327370709</li>\n",
       "\t<li>0.352909466124156</li>\n",
       "\t<li>0.472984919368092</li>\n",
       "\t<li>0.792348629505367</li>\n",
       "\t<li>0.268478280433625</li>\n",
       "\t<li>0.0209414537979283</li>\n",
       "\t<li>0.305852532909438</li>\n",
       "\t<li>0.645961081707</li>\n",
       "\t<li>0.190763116557985</li>\n",
       "\t<li>0.14478694381518</li>\n",
       "\t<li>0.180726075401429</li>\n",
       "\t<li>0.031487793289834</li>\n",
       "\t<li>0.15030097981072</li>\n",
       "\t<li>0.0140095536641778</li>\n",
       "\t<li>0.0572649805371286</li>\n",
       "\t<li>0.00404497796074312</li>\n",
       "\t<li>0.323667762858189</li>\n",
       "\t<li>0.0119131549994086</li>\n",
       "\t<li>0.0544206447492262</li>\n",
       "\t<li>0.187598795381614</li>\n",
       "\t<li>0.927456808579371</li>\n",
       "\t<li>0.506844848743513</li>\n",
       "\t<li>0.0171043631369857</li>\n",
       "\t<li>0.534841063882283</li>\n",
       "\t<li>0.221174703736585</li>\n",
       "\t<li>0.492774320590721</li>\n",
       "\t<li>0.0214099054683917</li>\n",
       "\t<li>0.00163577792879356</li>\n",
       "\t<li>0.0801465586173491</li>\n",
       "\t<li>0.141055568155103</li>\n",
       "\t<li>0.00343244988227266</li>\n",
       "\t<li>0.017327335046576</li>\n",
       "\t<li>0.599409016844675</li>\n",
       "\t<li>0.0702024757526832</li>\n",
       "\t<li>0.415479125968076</li>\n",
       "\t<li>0.0237720420429561</li>\n",
       "\t<li>0.00361209321769246</li>\n",
       "\t<li>0.0388817791717024</li>\n",
       "\t<li>0.0153585652763299</li>\n",
       "\t<li>0.537855190384946</li>\n",
       "\t<li>0.0384715180519116</li>\n",
       "\t<li>0.0568751771484888</li>\n",
       "\t<li>0.129978254892003</li>\n",
       "\t<li>0.0793894981671511</li>\n",
       "\t<li>0.580686155637608</li>\n",
       "\t<li>0.0225047683567532</li>\n",
       "\t<li>0.222028172971665</li>\n",
       "\t<li>0.697623292731847</li>\n",
       "\t<li>0.197341291744287</li>\n",
       "\t<li>0.836629016481079</li>\n",
       "\t<li>0.130308905909367</li>\n",
       "\t<li>0.159181897198667</li>\n",
       "\t<li>0.141779225353498</li>\n",
       "\t<li>0.577697000323039</li>\n",
       "\t<li>0.304192467969838</li>\n",
       "\t<li>0.30486331365902</li>\n",
       "\t<li>0.872175309681557</li>\n",
       "\t<li>0.0335039851983496</li>\n",
       "\t<li>0.00679064672837951</li>\n",
       "\t<li>0.547561085586362</li>\n",
       "\t<li>0.0231764628494631</li>\n",
       "\t<li>0.11402805751797</li>\n",
       "\t<li>0.0298786128211531</li>\n",
       "\t<li>0.029866447661217</li>\n",
       "\t<li>0.983617036814556</li>\n",
       "\t<li>0.476926476451727</li>\n",
       "\t<li>0.258450841196137</li>\n",
       "\t<li>0.657174245420284</li>\n",
       "\t<li>0.167561296819024</li>\n",
       "\t<li>0.0052499231787139</li>\n",
       "\t<li>0.033547983059923</li>\n",
       "\t<li>0.94791421596923</li>\n",
       "\t<li>0.229634140464392</li>\n",
       "\t<li>0.120156867650525</li>\n",
       "\t<li>0.142599605916586</li>\n",
       "\t<li>0.510880482769551</li>\n",
       "\t<li>0.492502423220792</li>\n",
       "\t<li>0.126805546976777</li>\n",
       "\t<li>0.407427981575015</li>\n",
       "\t<li>0.581683597187791</li>\n",
       "\t<li>0.141220867702616</li>\n",
       "\t<li>0.491239753741101</li>\n",
       "\t<li>0.506108233450003</li>\n",
       "\t<li>0.554117473397666</li>\n",
       "\t<li>0.910908909002514</li>\n",
       "\t<li>0.0269220420130241</li>\n",
       "\t<li>0.754692853853804</li>\n",
       "\t<li>0.0038520850350016</li>\n",
       "\t<li>0.237275521978451</li>\n",
       "\t<li>0.0167363580961563</li>\n",
       "\t<li>0.0254319902118264</li>\n",
       "\t<li>0.0427567427091678</li>\n",
       "\t<li>0.0513634163501955</li>\n",
       "\t<li>0.0830602532777982</li>\n",
       "\t<li>0.127811558950524</li>\n",
       "\t<li>0.015304406307749</li>\n",
       "\t<li>0.157569965129412</li>\n",
       "\t<li>0.0867457828881429</li>\n",
       "\t<li>0.260415248246903</li>\n",
       "\t<li>0.536101435055817</li>\n",
       "\t<li>0.755638271041679</li>\n",
       "\t<li>0.111936309172342</li>\n",
       "\t<li>0.0946035535687098</li>\n",
       "\t<li>0.431373346209228</li>\n",
       "\t<li>0.586116091274582</li>\n",
       "\t<li>0.243447741927022</li>\n",
       "\t<li>0.00692472313037775</li>\n",
       "\t<li>0.780407222774983</li>\n",
       "\t<li>0.400909625570225</li>\n",
       "\t<li>0.371780967149011</li>\n",
       "\t<li>0.286448907973697</li>\n",
       "\t<li>0.860804894005965</li>\n",
       "\t<li>0.654651003628947</li>\n",
       "\t<li>0.703567200454583</li>\n",
       "\t<li>0.209465641522204</li>\n",
       "\t<li>0.568627477780791</li>\n",
       "\t<li>0.124654362884931</li>\n",
       "\t<li>0.277632053649806</li>\n",
       "\t<li>0.0561373748161562</li>\n",
       "\t<li>0.0236720068270888</li>\n",
       "\t<li>0.0146236434841262</li>\n",
       "\t<li>0.16074680015728</li>\n",
       "\t<li>0.0242615307597458</li>\n",
       "\t<li>0.00989702285626594</li>\n",
       "\t<li>0.734834096371288</li>\n",
       "\t<li>0.0146209771289039</li>\n",
       "\t<li>0.700805780866803</li>\n",
       "\t<li>0.0233939143347865</li>\n",
       "\t<li>0.0492909910083302</li>\n",
       "\t<li>0.278628757090079</li>\n",
       "\t<li>0.752031524506524</li>\n",
       "\t<li>0.039345529866763</li>\n",
       "\t<li>0.0186371720155162</li>\n",
       "\t<li>0.999999707540076</li>\n",
       "\t<li>0.0935536461554124</li>\n",
       "\t<li>0.623929138162292</li>\n",
       "\t<li>0.402888193365402</li>\n",
       "\t<li>0.12699870177318</li>\n",
       "\t<li>0.0243572791459279</li>\n",
       "\t<li>0.448752348025535</li>\n",
       "\t<li>0.0532387781770922</li>\n",
       "\t<li>0.113177776632805</li>\n",
       "\t<li>0.167676638215059</li>\n",
       "\t<li>0.00654841366788043</li>\n",
       "\t<li>0.466087119303045</li>\n",
       "\t<li>0.386747701586424</li>\n",
       "\t<li>0.0164965298549906</li>\n",
       "\t<li>0.0367778198550268</li>\n",
       "\t<li>0.00238258791252777</li>\n",
       "\t<li>0.00321796991081338</li>\n",
       "\t<li>0.283340407643654</li>\n",
       "\t<li>0.00896644039881044</li>\n",
       "\t<li>0.733352604736861</li>\n",
       "\t<li>0.00441750047533496</li>\n",
       "\t<li>0.678317988139266</li>\n",
       "\t<li>0.0720838090401987</li>\n",
       "\t<li>0.0107076242093414</li>\n",
       "\t<li>0.0955796655255012</li>\n",
       "\t<li>0.202076772008901</li>\n",
       "\t<li>0.00982938242376169</li>\n",
       "\t<li>0.234251785347733</li>\n",
       "\t<li>0.71547746468794</li>\n",
       "\t<li>0.819380809867352</li>\n",
       "\t<li>0.0340657785425034</li>\n",
       "\t<li>0.834428855419787</li>\n",
       "\t<li>0.325728125074179</li>\n",
       "\t<li>0.513442626958064</li>\n",
       "\t<li>0.145989697652764</li>\n",
       "\t<li>0.0416133829464936</li>\n",
       "\t<li>0.262299721726386</li>\n",
       "\t<li>0.136582835298833</li>\n",
       "\t<li>0.00863082094882987</li>\n",
       "\t<li>0.0588905460404537</li>\n",
       "\t<li>0.0792714499560307</li>\n",
       "\t<li>0.242801812274397</li>\n",
       "\t<li>0.864697526004389</li>\n",
       "\t<li>0.741507777870621</li>\n",
       "\t<li>0.373944969220621</li>\n",
       "\t<li>0.509642309332493</li>\n",
       "\t<li>0.264162647491831</li>\n",
       "\t<li>0.178624922783598</li>\n",
       "\t<li>0.00322296309834809</li>\n",
       "\t<li>0.0216674952111267</li>\n",
       "\t<li>0.0194932626481211</li>\n",
       "\t<li>0.596229293130115</li>\n",
       "\t<li>0.776172263874826</li>\n",
       "\t<li>0.399593539053717</li>\n",
       "\t<li>0.606594059081995</li>\n",
       "\t<li>0.113553882839064</li>\n",
       "\t<li>0.0425413674085636</li>\n",
       "\t<li>0.0136725792213377</li>\n",
       "\t<li>0.488215579682298</li>\n",
       "\t<li>0.0119836050925559</li>\n",
       "\t<li>0.0719986572858782</li>\n",
       "\t<li>0.435615924833523</li>\n",
       "\t<li>0.00345051004846592</li>\n",
       "\t<li>0.0378459210750139</li>\n",
       "\t<li>0.0114251791102418</li>\n",
       "\t<li>0.125273074910697</li>\n",
       "\t<li>0.1575690693746</li>\n",
       "\t<li>0.327168048466392</li>\n",
       "\t<li>0.295916572532276</li>\n",
       "\t<li>0.11104175531405</li>\n",
       "\t<li>0.014955934769429</li>\n",
       "\t<li>0.0106148444555317</li>\n",
       "\t<li>0.527001368360012</li>\n",
       "\t<li>0.00262895935429216</li>\n",
       "\t<li>0.395103978142983</li>\n",
       "\t<li>0.203728443261776</li>\n",
       "\t<li>0.658453967489166</li>\n",
       "\t<li>0.119776074616211</li>\n",
       "\t<li>0.00927084158055183</li>\n",
       "\t<li>0.413609505755124</li>\n",
       "\t<li>0.00729988264878958</li>\n",
       "\t<li>0.496842319672098</li>\n",
       "\t<li>0.0467516640068276</li>\n",
       "\t<li>0.488966714158882</li>\n",
       "\t<li>0.00292655310513345</li>\n",
       "\t<li>0.0978872634392816</li>\n",
       "\t<li>0.409159876673382</li>\n",
       "\t<li>0.00746559610308782</li>\n",
       "\t<li>0.290840580191103</li>\n",
       "\t<li>0.935538612345386</li>\n",
       "\t<li>0.166115983762243</li>\n",
       "\t<li>0.0894719921995294</li>\n",
       "\t<li>0.99999988312744</li>\n",
       "\t<li>0.0492194614333566</li>\n",
       "\t<li>0.725142014025603</li>\n",
       "\t<li>0.851325425051723</li>\n",
       "\t<li>0.0453861269966289</li>\n",
       "\t<li>0.234429222282555</li>\n",
       "\t<li>0.0679034997456688</li>\n",
       "\t<li>0.00311268362037586</li>\n",
       "\t<li>0.258186422389191</li>\n",
       "\t<li>0.673808286298615</li>\n",
       "\t<li>0.385916734010346</li>\n",
       "\t<li>0.0791191082169789</li>\n",
       "\t<li>0.112803626438731</li>\n",
       "\t<li>0.01390605375495</li>\n",
       "\t<li>0.640587881379281</li>\n",
       "\t<li>0.985543514263911</li>\n",
       "\t<li>0.0094569017637177</li>\n",
       "\t<li>0.647949182331332</li>\n",
       "\t<li>0.290692960922754</li>\n",
       "\t<li>0.920930206937619</li>\n",
       "\t<li>0.452534392915463</li>\n",
       "\t<li>0.761096085325328</li>\n",
       "\t<li>0.511185474939446</li>\n",
       "\t<li>0.623218459375131</li>\n",
       "\t<li>0.0162141541642301</li>\n",
       "\t<li>0.333177899990478</li>\n",
       "\t<li>0.140060743990682</li>\n",
       "\t<li>0.37238114621485</li>\n",
       "\t<li>0.495830561133254</li>\n",
       "\t<li>0.00702547324014362</li>\n",
       "\t<li>0.638281170071597</li>\n",
       "\t<li>0.642296764035918</li>\n",
       "\t<li>0.306906721143049</li>\n",
       "\t<li>0.604142271534431</li>\n",
       "\t<li>0.0780824149027675</li>\n",
       "\t<li>0.685527595941055</li>\n",
       "\t<li>0.141919931193779</li>\n",
       "\t<li>0.719426913329513</li>\n",
       "\t<li>0.980936895165277</li>\n",
       "\t<li>0.329592462343399</li>\n",
       "\t<li>0.225619594020078</li>\n",
       "\t<li>0.758030216545074</li>\n",
       "\t<li>0.061634311916085</li>\n",
       "\t<li>0.722338100370991</li>\n",
       "\t<li>0.597710954934034</li>\n",
       "\t<li>0.601819734804895</li>\n",
       "\t<li>0.999999689846865</li>\n",
       "\t<li>0.396457884824599</li>\n",
       "\t<li>0.425650269137989</li>\n",
       "\t<li>0.031663277291782</li>\n",
       "\t<li>0.0851040779306962</li>\n",
       "\t<li>0.231992734492738</li>\n",
       "\t<li>0.0114518373425591</li>\n",
       "\t<li>0.650776669436919</li>\n",
       "\t<li>0.4206232580536</li>\n",
       "\t<li>0.542682302635948</li>\n",
       "\t<li>0.18180295770518</li>\n",
       "\t<li>0.0149771399800867</li>\n",
       "\t<li>0.569748133350704</li>\n",
       "\t<li>0.0187928189967538</li>\n",
       "\t<li>0.023477504661038</li>\n",
       "\t<li>0.340417619684114</li>\n",
       "\t<li>0.0040098737160981</li>\n",
       "\t<li>0.329470585254571</li>\n",
       "\t<li>0.0574727537594718</li>\n",
       "\t<li>0.0107173991334299</li>\n",
       "\t<li>0.355640669111306</li>\n",
       "\t<li>0.0173829926611912</li>\n",
       "\t<li>0.0620256104346949</li>\n",
       "\t<li>0.219398174731547</li>\n",
       "\t<li>0.565174117284585</li>\n",
       "\t<li>0.261167232490392</li>\n",
       "\t<li>0.108724262356992</li>\n",
       "\t<li>0.794849142106868</li>\n",
       "\t<li>0.910915006518465</li>\n",
       "\t<li>0.0108097203243075</li>\n",
       "\t<li>0.199673287515818</li>\n",
       "\t<li>0.00445214468678075</li>\n",
       "\t<li>0.0056895418316378</li>\n",
       "\t<li>0.0156252741687175</li>\n",
       "\t<li>0.0434542838599873</li>\n",
       "\t<li>0.37981942585084</li>\n",
       "\t<li>0.936779543832573</li>\n",
       "\t<li>0.639230476961909</li>\n",
       "\t<li>0.285950396633674</li>\n",
       "\t<li>0.643706591276826</li>\n",
       "\t<li>0.24080420522909</li>\n",
       "\t<li>0.013317486135993</li>\n",
       "\t<li>0.248732745090195</li>\n",
       "\t<li>0.0150149809510639</li>\n",
       "\t<li>0.0864050181182037</li>\n",
       "\t<li>0.0219124775648427</li>\n",
       "\t<li>0.0477403674659473</li>\n",
       "\t<li>0.343106389140021</li>\n",
       "\t<li>0.0199018916237982</li>\n",
       "\t<li>0.0402270091673715</li>\n",
       "\t<li>0.11835109243603</li>\n",
       "\t<li>0.0215147382927905</li>\n",
       "\t<li>0.380268790736627</li>\n",
       "\t<li>0.0709091768165848</li>\n",
       "\t<li>0.248539095134567</li>\n",
       "\t<li>0.0458959571816002</li>\n",
       "\t<li>0.338157016619925</li>\n",
       "\t<li>0.0744694363872355</li>\n",
       "\t<li>0.0875756130497682</li>\n",
       "\t<li>0.234081539708774</li>\n",
       "\t<li>0.371581349556878</li>\n",
       "\t<li>0.0105443102676181</li>\n",
       "\t<li>0.0103120872564989</li>\n",
       "\t<li>0.0680291992736389</li>\n",
       "\t<li>0.0276318056830353</li>\n",
       "\t<li>0.0440082320571134</li>\n",
       "\t<li>0.00377136552089557</li>\n",
       "\t<li>0.00783041686685496</li>\n",
       "\t<li>0.0409763591288367</li>\n",
       "\t<li>0.0180951115376393</li>\n",
       "\t<li>0.420336652250682</li>\n",
       "\t<li>0.233790557430558</li>\n",
       "\t<li>0.829962285004331</li>\n",
       "\t<li>0.712566469894219</li>\n",
       "\t<li>0.999999911574732</li>\n",
       "\t<li>0.408515097379173</li>\n",
       "\t<li>0.125435192824696</li>\n",
       "\t<li>0.306269305842519</li>\n",
       "\t<li>0.00430628276523339</li>\n",
       "\t<li>0.0254059673455803</li>\n",
       "\t<li>0.0625322241782875</li>\n",
       "\t<li>0.775409684179629</li>\n",
       "\t<li>0.137930277138711</li>\n",
       "\t<li>0.050667847221433</li>\n",
       "\t<li>0.216099554213866</li>\n",
       "\t<li>0.62106315472439</li>\n",
       "\t<li>0.0202661870490799</li>\n",
       "\t<li>0.843686308468121</li>\n",
       "\t<li>0.213026158540125</li>\n",
       "\t<li>0.0774242687870972</li>\n",
       "\t<li>0.152747879915751</li>\n",
       "\t<li>0.503699642442566</li>\n",
       "\t<li>0.0445974342466566</li>\n",
       "\t<li>0.437533082839485</li>\n",
       "\t<li>0.0663400648135764</li>\n",
       "\t<li>0.403796667778126</li>\n",
       "\t<li>0.0325709092701417</li>\n",
       "\t<li>0.0411410965957106</li>\n",
       "\t<li>0.110801390149083</li>\n",
       "\t<li>0.284565334097029</li>\n",
       "\t<li>0.453694109004949</li>\n",
       "\t<li>0.545860894240575</li>\n",
       "\t<li>0.140119416640547</li>\n",
       "\t<li>0.0505452215169591</li>\n",
       "\t<li>0.985872533419196</li>\n",
       "\t<li>0.187786309170919</li>\n",
       "\t<li>0.026204627530121</li>\n",
       "\t<li>0.00395045193940631</li>\n",
       "\t<li>0.0139113041818318</li>\n",
       "\t<li>0.205325832609876</li>\n",
       "\t<li>0.420938987301914</li>\n",
       "\t<li>0.594037472314046</li>\n",
       "\t<li>0.0351771484343449</li>\n",
       "\t<li>0.0101241210170218</li>\n",
       "\t<li>0.655772066423433</li>\n",
       "\t<li>0.0414823421175045</li>\n",
       "\t<li>0.344306642777449</li>\n",
       "\t<li>0.151969111797743</li>\n",
       "\t<li>0.52747757995499</li>\n",
       "\t<li>0.305607768146873</li>\n",
       "\t<li>0.00486704556981821</li>\n",
       "\t<li>0.065909468992276</li>\n",
       "\t<li>0.652951925947531</li>\n",
       "\t<li>0.0864093277018086</li>\n",
       "\t<li>0.139178073137567</li>\n",
       "\t<li>0.341995924104932</li>\n",
       "\t<li>0.761104634238521</li>\n",
       "\t<li>0.00395302268326937</li>\n",
       "\t<li>0.328641035005631</li>\n",
       "\t<li>0.113700151809085</li>\n",
       "\t<li>0.0280373435296362</li>\n",
       "\t<li>0.441713650736839</li>\n",
       "\t<li>0.392843826669875</li>\n",
       "\t<li>0.0770264454821476</li>\n",
       "\t<li>0.217891120326254</li>\n",
       "\t<li>0.0963246833068634</li>\n",
       "\t<li>0.466468665526836</li>\n",
       "\t<li>0.0646993663244186</li>\n",
       "\t<li>0.115774308247929</li>\n",
       "\t<li>0.23165407120508</li>\n",
       "\t<li>0.0282893906831235</li>\n",
       "\t<li>0.224137905702612</li>\n",
       "\t<li>0.0117933497552779</li>\n",
       "\t<li>0.0639182021284336</li>\n",
       "\t<li>0.00820693900500471</li>\n",
       "\t<li>0.0497418898663526</li>\n",
       "\t<li>0.275763126242334</li>\n",
       "\t<li>0.820409225076895</li>\n",
       "\t<li>0.258760375424514</li>\n",
       "\t<li>0.224002737543071</li>\n",
       "\t<li>0.208821992665129</li>\n",
       "\t<li>0.150022186525851</li>\n",
       "\t<li>0.0106071775111602</li>\n",
       "\t<li>0.64788330615334</li>\n",
       "\t<li>0.379170960909486</li>\n",
       "\t<li>0.365998455060223</li>\n",
       "\t<li>0.878862922584708</li>\n",
       "\t<li>0.303307486561192</li>\n",
       "\t<li>0.0792783025873824</li>\n",
       "\t<li>0.112109861488785</li>\n",
       "\t<li>0.319550358098912</li>\n",
       "\t<li>0.315691131453868</li>\n",
       "\t<li>0.00136798946929624</li>\n",
       "\t<li>0.646882616879907</li>\n",
       "\t<li>0.55959575970945</li>\n",
       "\t<li>0.08124522881382</li>\n",
       "\t<li>0.0804990202762637</li>\n",
       "\t<li>0.697254993419886</li>\n",
       "\t<li>0.0990462042246573</li>\n",
       "\t<li>0.0194032930002707</li>\n",
       "\t<li>0.613112731840573</li>\n",
       "\t<li>0.163373409756571</li>\n",
       "\t<li>0.313347825812335</li>\n",
       "\t<li>0.240213171064493</li>\n",
       "\t<li>0.12787343129881</li>\n",
       "\t<li>0.0647466145010186</li>\n",
       "\t<li>0.212852338886759</li>\n",
       "\t<li>0.512703953314063</li>\n",
       "\t<li>0.867244624126617</li>\n",
       "\t<li>0.104466079104929</li>\n",
       "\t<li>0.00892226841767057</li>\n",
       "\t<li>0.411225023200145</li>\n",
       "\t<li>0.0293224813462951</li>\n",
       "\t<li>0.932105572313179</li>\n",
       "\t<li>0.0831694984213733</li>\n",
       "\t<li>0.999998900212684</li>\n",
       "\t<li>0.298514547875925</li>\n",
       "\t<li>0.0763507372120488</li>\n",
       "\t<li>0.0412518596764603</li>\n",
       "\t<li>0.172107091347054</li>\n",
       "\t<li>0.0153042645880929</li>\n",
       "\t<li>0.00879655926353673</li>\n",
       "\t<li>0.0406685755911528</li>\n",
       "\t<li>0.1487786573974</li>\n",
       "\t<li>0.24062725420514</li>\n",
       "\t<li>0.0194330890348856</li>\n",
       "\t<li>0.0371532255064101</li>\n",
       "\t<li>0.346003159331868</li>\n",
       "\t<li>0.0871983173015547</li>\n",
       "\t<li>0.0562310151731386</li>\n",
       "\t<li>0.196347894741266</li>\n",
       "\t<li>0.0482474898980965</li>\n",
       "\t<li>0.0781141639325009</li>\n",
       "\t<li>0.0133641923867073</li>\n",
       "\t<li>0.631510295925159</li>\n",
       "\t<li>0.824502258542784</li>\n",
       "\t<li>0.285033270491086</li>\n",
       "\t<li>0.00566167761326275</li>\n",
       "\t<li>0.19872513240031</li>\n",
       "\t<li>0.00977960629528194</li>\n",
       "\t<li>0.620715034602006</li>\n",
       "\t<li>0.127601720623136</li>\n",
       "\t<li>0.313203256005209</li>\n",
       "\t<li>0.00601927828501729</li>\n",
       "\t<li>0.114255686258546</li>\n",
       "\t<li>0.455293684443776</li>\n",
       "\t<li>0.552655530673489</li>\n",
       "\t<li>0.00385282658062091</li>\n",
       "\t<li>0.207449364940697</li>\n",
       "\t<li>0.0342211360252181</li>\n",
       "\t<li>0.137632114468045</li>\n",
       "\t<li>0.016001654991982</li>\n",
       "\t<li>0.128871773321818</li>\n",
       "\t<li>0.0847315951960205</li>\n",
       "\t<li>0.0270433195415661</li>\n",
       "\t<li>0.306044259868799</li>\n",
       "\t<li>0.31188208156991</li>\n",
       "\t<li>0.0564158804449249</li>\n",
       "\t<li>0.0641236894739447</li>\n",
       "\t<li>0.136150606443115</li>\n",
       "\t<li>0.214066230166767</li>\n",
       "\t<li>0.0843620307226987</li>\n",
       "\t<li>0.436706512599558</li>\n",
       "\t<li>0.696452017127779</li>\n",
       "\t<li>0.141759051590259</li>\n",
       "\t<li>0.358399928469222</li>\n",
       "\t<li>0.734331335406255</li>\n",
       "\t<li>0.0298883290837451</li>\n",
       "\t<li>0.100385832860669</li>\n",
       "\t<li>0.713072752989206</li>\n",
       "\t<li>0.081923908211841</li>\n",
       "\t<li>0.29947548880847</li>\n",
       "\t<li>0.428429381499062</li>\n",
       "\t<li>0.291351862991912</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.160574950330444\n",
       "\\item 0.194614757214468\n",
       "\\item 0.0657642673096464\n",
       "\\item 0.0090848028351076\n",
       "\\item 0.0108183493696735\n",
       "\\item 0.328274188114084\n",
       "\\item 0.0128692228118427\n",
       "\\item 0.135185340992715\n",
       "\\item 0.0262270582348304\n",
       "\\item 0.736308993875852\n",
       "\\item 0.232280136046881\n",
       "\\item 0.180565932268704\n",
       "\\item 0.353884466798498\n",
       "\\item 0.141278124487713\n",
       "\\item 0.620304701153559\n",
       "\\item 0.646197537582489\n",
       "\\item 0.473042691032659\n",
       "\\item 0.00843101696370038\n",
       "\\item 0.0998384758039544\n",
       "\\item 0.86046324688653\n",
       "\\item 0.000952697013496243\n",
       "\\item 0.0491613547308922\n",
       "\\item 0.564323727160182\n",
       "\\item 0.00616664915214881\n",
       "\\item 0.0424192209665897\n",
       "\\item 0.0204973284053453\n",
       "\\item 0.277756181954646\n",
       "\\item 0.669862183309902\n",
       "\\item 0.29283164108411\n",
       "\\item 0.073742669913145\n",
       "\\item 0.956510691442855\n",
       "\\item 0.645195609645712\n",
       "\\item 0.00222944648840101\n",
       "\\item 0.0642951616935489\n",
       "\\item 0.15950039260962\n",
       "\\item 0.00361918265583433\n",
       "\\item 0.149162887069616\n",
       "\\item 0.0133544798177277\n",
       "\\item 0.0379488060425114\n",
       "\\item 0.692349209723638\n",
       "\\item 0.818859615955628\n",
       "\\item 0.0698489247122723\n",
       "\\item 0.0128890656898538\n",
       "\\item 0.637594046088248\n",
       "\\item 0.0358308500949368\n",
       "\\item 0.0163667569305153\n",
       "\\item 0.039050456872504\n",
       "\\item 0.207951969398532\n",
       "\\item 0.213026987635583\n",
       "\\item 0.0704242697118449\n",
       "\\item 0.709245282799869\n",
       "\\item 0.00192910792915866\n",
       "\\item 0.567849341915015\n",
       "\\item 0.02184291683155\n",
       "\\item 0.909870295637369\n",
       "\\item 0.190739918054592\n",
       "\\item 0.210933850776207\n",
       "\\item 0.00321592478724331\n",
       "\\item 0.027723725866694\n",
       "\\item 0.534321606679551\n",
       "\\item 0.196591374794026\n",
       "\\item 0.244281657991462\n",
       "\\item 0.111520445878349\n",
       "\\item 0.514158404335423\n",
       "\\item 0.169053320049634\n",
       "\\item 0.33898993986781\n",
       "\\item 0.353592033567074\n",
       "\\item 0.917337288114215\n",
       "\\item 0.0036081380640822\n",
       "\\item 0.0564426239842941\n",
       "\\item 0.0465747326350894\n",
       "\\item 0.165961509381525\n",
       "\\item 0.205367164160717\n",
       "\\item 0.0288348964901622\n",
       "\\item 0.548167173229679\n",
       "\\item 0.0312231270992339\n",
       "\\item 0.156158473529706\n",
       "\\item 0.251235257162444\n",
       "\\item 0.376636338394775\n",
       "\\item 0.886803578557474\n",
       "\\item 0.376827960787195\n",
       "\\item 0.226186941402099\n",
       "\\item 0.0169519742001024\n",
       "\\item 0.00639368479797662\n",
       "\\item 0.207655356381026\n",
       "\\item 0.0922712223563015\n",
       "\\item 0.00819751978744117\n",
       "\\item 0.325605630965992\n",
       "\\item 0.0118846373687264\n",
       "\\item 0.153025955271613\n",
       "\\item 0.0631177824266063\n",
       "\\item 0.405602744858094\n",
       "\\item 0.279217534904339\n",
       "\\item 0.0150225553051513\n",
       "\\item 0.289793474313642\n",
       "\\item 0.221088800577378\n",
       "\\item 0.0180259821139205\n",
       "\\item 0.116502449288295\n",
       "\\item 0.0585713260213269\n",
       "\\item 0.13769494400464\n",
       "\\item 0.329729761655856\n",
       "\\item 0.635589905840042\n",
       "\\item 0.0670963077693865\n",
       "\\item 0.259761139540847\n",
       "\\item 0.143894687291994\n",
       "\\item 0.333581539566896\n",
       "\\item 0.129856327370709\n",
       "\\item 0.352909466124156\n",
       "\\item 0.472984919368092\n",
       "\\item 0.792348629505367\n",
       "\\item 0.268478280433625\n",
       "\\item 0.0209414537979283\n",
       "\\item 0.305852532909438\n",
       "\\item 0.645961081707\n",
       "\\item 0.190763116557985\n",
       "\\item 0.14478694381518\n",
       "\\item 0.180726075401429\n",
       "\\item 0.031487793289834\n",
       "\\item 0.15030097981072\n",
       "\\item 0.0140095536641778\n",
       "\\item 0.0572649805371286\n",
       "\\item 0.00404497796074312\n",
       "\\item 0.323667762858189\n",
       "\\item 0.0119131549994086\n",
       "\\item 0.0544206447492262\n",
       "\\item 0.187598795381614\n",
       "\\item 0.927456808579371\n",
       "\\item 0.506844848743513\n",
       "\\item 0.0171043631369857\n",
       "\\item 0.534841063882283\n",
       "\\item 0.221174703736585\n",
       "\\item 0.492774320590721\n",
       "\\item 0.0214099054683917\n",
       "\\item 0.00163577792879356\n",
       "\\item 0.0801465586173491\n",
       "\\item 0.141055568155103\n",
       "\\item 0.00343244988227266\n",
       "\\item 0.017327335046576\n",
       "\\item 0.599409016844675\n",
       "\\item 0.0702024757526832\n",
       "\\item 0.415479125968076\n",
       "\\item 0.0237720420429561\n",
       "\\item 0.00361209321769246\n",
       "\\item 0.0388817791717024\n",
       "\\item 0.0153585652763299\n",
       "\\item 0.537855190384946\n",
       "\\item 0.0384715180519116\n",
       "\\item 0.0568751771484888\n",
       "\\item 0.129978254892003\n",
       "\\item 0.0793894981671511\n",
       "\\item 0.580686155637608\n",
       "\\item 0.0225047683567532\n",
       "\\item 0.222028172971665\n",
       "\\item 0.697623292731847\n",
       "\\item 0.197341291744287\n",
       "\\item 0.836629016481079\n",
       "\\item 0.130308905909367\n",
       "\\item 0.159181897198667\n",
       "\\item 0.141779225353498\n",
       "\\item 0.577697000323039\n",
       "\\item 0.304192467969838\n",
       "\\item 0.30486331365902\n",
       "\\item 0.872175309681557\n",
       "\\item 0.0335039851983496\n",
       "\\item 0.00679064672837951\n",
       "\\item 0.547561085586362\n",
       "\\item 0.0231764628494631\n",
       "\\item 0.11402805751797\n",
       "\\item 0.0298786128211531\n",
       "\\item 0.029866447661217\n",
       "\\item 0.983617036814556\n",
       "\\item 0.476926476451727\n",
       "\\item 0.258450841196137\n",
       "\\item 0.657174245420284\n",
       "\\item 0.167561296819024\n",
       "\\item 0.0052499231787139\n",
       "\\item 0.033547983059923\n",
       "\\item 0.94791421596923\n",
       "\\item 0.229634140464392\n",
       "\\item 0.120156867650525\n",
       "\\item 0.142599605916586\n",
       "\\item 0.510880482769551\n",
       "\\item 0.492502423220792\n",
       "\\item 0.126805546976777\n",
       "\\item 0.407427981575015\n",
       "\\item 0.581683597187791\n",
       "\\item 0.141220867702616\n",
       "\\item 0.491239753741101\n",
       "\\item 0.506108233450003\n",
       "\\item 0.554117473397666\n",
       "\\item 0.910908909002514\n",
       "\\item 0.0269220420130241\n",
       "\\item 0.754692853853804\n",
       "\\item 0.0038520850350016\n",
       "\\item 0.237275521978451\n",
       "\\item 0.0167363580961563\n",
       "\\item 0.0254319902118264\n",
       "\\item 0.0427567427091678\n",
       "\\item 0.0513634163501955\n",
       "\\item 0.0830602532777982\n",
       "\\item 0.127811558950524\n",
       "\\item 0.015304406307749\n",
       "\\item 0.157569965129412\n",
       "\\item 0.0867457828881429\n",
       "\\item 0.260415248246903\n",
       "\\item 0.536101435055817\n",
       "\\item 0.755638271041679\n",
       "\\item 0.111936309172342\n",
       "\\item 0.0946035535687098\n",
       "\\item 0.431373346209228\n",
       "\\item 0.586116091274582\n",
       "\\item 0.243447741927022\n",
       "\\item 0.00692472313037775\n",
       "\\item 0.780407222774983\n",
       "\\item 0.400909625570225\n",
       "\\item 0.371780967149011\n",
       "\\item 0.286448907973697\n",
       "\\item 0.860804894005965\n",
       "\\item 0.654651003628947\n",
       "\\item 0.703567200454583\n",
       "\\item 0.209465641522204\n",
       "\\item 0.568627477780791\n",
       "\\item 0.124654362884931\n",
       "\\item 0.277632053649806\n",
       "\\item 0.0561373748161562\n",
       "\\item 0.0236720068270888\n",
       "\\item 0.0146236434841262\n",
       "\\item 0.16074680015728\n",
       "\\item 0.0242615307597458\n",
       "\\item 0.00989702285626594\n",
       "\\item 0.734834096371288\n",
       "\\item 0.0146209771289039\n",
       "\\item 0.700805780866803\n",
       "\\item 0.0233939143347865\n",
       "\\item 0.0492909910083302\n",
       "\\item 0.278628757090079\n",
       "\\item 0.752031524506524\n",
       "\\item 0.039345529866763\n",
       "\\item 0.0186371720155162\n",
       "\\item 0.999999707540076\n",
       "\\item 0.0935536461554124\n",
       "\\item 0.623929138162292\n",
       "\\item 0.402888193365402\n",
       "\\item 0.12699870177318\n",
       "\\item 0.0243572791459279\n",
       "\\item 0.448752348025535\n",
       "\\item 0.0532387781770922\n",
       "\\item 0.113177776632805\n",
       "\\item 0.167676638215059\n",
       "\\item 0.00654841366788043\n",
       "\\item 0.466087119303045\n",
       "\\item 0.386747701586424\n",
       "\\item 0.0164965298549906\n",
       "\\item 0.0367778198550268\n",
       "\\item 0.00238258791252777\n",
       "\\item 0.00321796991081338\n",
       "\\item 0.283340407643654\n",
       "\\item 0.00896644039881044\n",
       "\\item 0.733352604736861\n",
       "\\item 0.00441750047533496\n",
       "\\item 0.678317988139266\n",
       "\\item 0.0720838090401987\n",
       "\\item 0.0107076242093414\n",
       "\\item 0.0955796655255012\n",
       "\\item 0.202076772008901\n",
       "\\item 0.00982938242376169\n",
       "\\item 0.234251785347733\n",
       "\\item 0.71547746468794\n",
       "\\item 0.819380809867352\n",
       "\\item 0.0340657785425034\n",
       "\\item 0.834428855419787\n",
       "\\item 0.325728125074179\n",
       "\\item 0.513442626958064\n",
       "\\item 0.145989697652764\n",
       "\\item 0.0416133829464936\n",
       "\\item 0.262299721726386\n",
       "\\item 0.136582835298833\n",
       "\\item 0.00863082094882987\n",
       "\\item 0.0588905460404537\n",
       "\\item 0.0792714499560307\n",
       "\\item 0.242801812274397\n",
       "\\item 0.864697526004389\n",
       "\\item 0.741507777870621\n",
       "\\item 0.373944969220621\n",
       "\\item 0.509642309332493\n",
       "\\item 0.264162647491831\n",
       "\\item 0.178624922783598\n",
       "\\item 0.00322296309834809\n",
       "\\item 0.0216674952111267\n",
       "\\item 0.0194932626481211\n",
       "\\item 0.596229293130115\n",
       "\\item 0.776172263874826\n",
       "\\item 0.399593539053717\n",
       "\\item 0.606594059081995\n",
       "\\item 0.113553882839064\n",
       "\\item 0.0425413674085636\n",
       "\\item 0.0136725792213377\n",
       "\\item 0.488215579682298\n",
       "\\item 0.0119836050925559\n",
       "\\item 0.0719986572858782\n",
       "\\item 0.435615924833523\n",
       "\\item 0.00345051004846592\n",
       "\\item 0.0378459210750139\n",
       "\\item 0.0114251791102418\n",
       "\\item 0.125273074910697\n",
       "\\item 0.1575690693746\n",
       "\\item 0.327168048466392\n",
       "\\item 0.295916572532276\n",
       "\\item 0.11104175531405\n",
       "\\item 0.014955934769429\n",
       "\\item 0.0106148444555317\n",
       "\\item 0.527001368360012\n",
       "\\item 0.00262895935429216\n",
       "\\item 0.395103978142983\n",
       "\\item 0.203728443261776\n",
       "\\item 0.658453967489166\n",
       "\\item 0.119776074616211\n",
       "\\item 0.00927084158055183\n",
       "\\item 0.413609505755124\n",
       "\\item 0.00729988264878958\n",
       "\\item 0.496842319672098\n",
       "\\item 0.0467516640068276\n",
       "\\item 0.488966714158882\n",
       "\\item 0.00292655310513345\n",
       "\\item 0.0978872634392816\n",
       "\\item 0.409159876673382\n",
       "\\item 0.00746559610308782\n",
       "\\item 0.290840580191103\n",
       "\\item 0.935538612345386\n",
       "\\item 0.166115983762243\n",
       "\\item 0.0894719921995294\n",
       "\\item 0.99999988312744\n",
       "\\item 0.0492194614333566\n",
       "\\item 0.725142014025603\n",
       "\\item 0.851325425051723\n",
       "\\item 0.0453861269966289\n",
       "\\item 0.234429222282555\n",
       "\\item 0.0679034997456688\n",
       "\\item 0.00311268362037586\n",
       "\\item 0.258186422389191\n",
       "\\item 0.673808286298615\n",
       "\\item 0.385916734010346\n",
       "\\item 0.0791191082169789\n",
       "\\item 0.112803626438731\n",
       "\\item 0.01390605375495\n",
       "\\item 0.640587881379281\n",
       "\\item 0.985543514263911\n",
       "\\item 0.0094569017637177\n",
       "\\item 0.647949182331332\n",
       "\\item 0.290692960922754\n",
       "\\item 0.920930206937619\n",
       "\\item 0.452534392915463\n",
       "\\item 0.761096085325328\n",
       "\\item 0.511185474939446\n",
       "\\item 0.623218459375131\n",
       "\\item 0.0162141541642301\n",
       "\\item 0.333177899990478\n",
       "\\item 0.140060743990682\n",
       "\\item 0.37238114621485\n",
       "\\item 0.495830561133254\n",
       "\\item 0.00702547324014362\n",
       "\\item 0.638281170071597\n",
       "\\item 0.642296764035918\n",
       "\\item 0.306906721143049\n",
       "\\item 0.604142271534431\n",
       "\\item 0.0780824149027675\n",
       "\\item 0.685527595941055\n",
       "\\item 0.141919931193779\n",
       "\\item 0.719426913329513\n",
       "\\item 0.980936895165277\n",
       "\\item 0.329592462343399\n",
       "\\item 0.225619594020078\n",
       "\\item 0.758030216545074\n",
       "\\item 0.061634311916085\n",
       "\\item 0.722338100370991\n",
       "\\item 0.597710954934034\n",
       "\\item 0.601819734804895\n",
       "\\item 0.999999689846865\n",
       "\\item 0.396457884824599\n",
       "\\item 0.425650269137989\n",
       "\\item 0.031663277291782\n",
       "\\item 0.0851040779306962\n",
       "\\item 0.231992734492738\n",
       "\\item 0.0114518373425591\n",
       "\\item 0.650776669436919\n",
       "\\item 0.4206232580536\n",
       "\\item 0.542682302635948\n",
       "\\item 0.18180295770518\n",
       "\\item 0.0149771399800867\n",
       "\\item 0.569748133350704\n",
       "\\item 0.0187928189967538\n",
       "\\item 0.023477504661038\n",
       "\\item 0.340417619684114\n",
       "\\item 0.0040098737160981\n",
       "\\item 0.329470585254571\n",
       "\\item 0.0574727537594718\n",
       "\\item 0.0107173991334299\n",
       "\\item 0.355640669111306\n",
       "\\item 0.0173829926611912\n",
       "\\item 0.0620256104346949\n",
       "\\item 0.219398174731547\n",
       "\\item 0.565174117284585\n",
       "\\item 0.261167232490392\n",
       "\\item 0.108724262356992\n",
       "\\item 0.794849142106868\n",
       "\\item 0.910915006518465\n",
       "\\item 0.0108097203243075\n",
       "\\item 0.199673287515818\n",
       "\\item 0.00445214468678075\n",
       "\\item 0.0056895418316378\n",
       "\\item 0.0156252741687175\n",
       "\\item 0.0434542838599873\n",
       "\\item 0.37981942585084\n",
       "\\item 0.936779543832573\n",
       "\\item 0.639230476961909\n",
       "\\item 0.285950396633674\n",
       "\\item 0.643706591276826\n",
       "\\item 0.24080420522909\n",
       "\\item 0.013317486135993\n",
       "\\item 0.248732745090195\n",
       "\\item 0.0150149809510639\n",
       "\\item 0.0864050181182037\n",
       "\\item 0.0219124775648427\n",
       "\\item 0.0477403674659473\n",
       "\\item 0.343106389140021\n",
       "\\item 0.0199018916237982\n",
       "\\item 0.0402270091673715\n",
       "\\item 0.11835109243603\n",
       "\\item 0.0215147382927905\n",
       "\\item 0.380268790736627\n",
       "\\item 0.0709091768165848\n",
       "\\item 0.248539095134567\n",
       "\\item 0.0458959571816002\n",
       "\\item 0.338157016619925\n",
       "\\item 0.0744694363872355\n",
       "\\item 0.0875756130497682\n",
       "\\item 0.234081539708774\n",
       "\\item 0.371581349556878\n",
       "\\item 0.0105443102676181\n",
       "\\item 0.0103120872564989\n",
       "\\item 0.0680291992736389\n",
       "\\item 0.0276318056830353\n",
       "\\item 0.0440082320571134\n",
       "\\item 0.00377136552089557\n",
       "\\item 0.00783041686685496\n",
       "\\item 0.0409763591288367\n",
       "\\item 0.0180951115376393\n",
       "\\item 0.420336652250682\n",
       "\\item 0.233790557430558\n",
       "\\item 0.829962285004331\n",
       "\\item 0.712566469894219\n",
       "\\item 0.999999911574732\n",
       "\\item 0.408515097379173\n",
       "\\item 0.125435192824696\n",
       "\\item 0.306269305842519\n",
       "\\item 0.00430628276523339\n",
       "\\item 0.0254059673455803\n",
       "\\item 0.0625322241782875\n",
       "\\item 0.775409684179629\n",
       "\\item 0.137930277138711\n",
       "\\item 0.050667847221433\n",
       "\\item 0.216099554213866\n",
       "\\item 0.62106315472439\n",
       "\\item 0.0202661870490799\n",
       "\\item 0.843686308468121\n",
       "\\item 0.213026158540125\n",
       "\\item 0.0774242687870972\n",
       "\\item 0.152747879915751\n",
       "\\item 0.503699642442566\n",
       "\\item 0.0445974342466566\n",
       "\\item 0.437533082839485\n",
       "\\item 0.0663400648135764\n",
       "\\item 0.403796667778126\n",
       "\\item 0.0325709092701417\n",
       "\\item 0.0411410965957106\n",
       "\\item 0.110801390149083\n",
       "\\item 0.284565334097029\n",
       "\\item 0.453694109004949\n",
       "\\item 0.545860894240575\n",
       "\\item 0.140119416640547\n",
       "\\item 0.0505452215169591\n",
       "\\item 0.985872533419196\n",
       "\\item 0.187786309170919\n",
       "\\item 0.026204627530121\n",
       "\\item 0.00395045193940631\n",
       "\\item 0.0139113041818318\n",
       "\\item 0.205325832609876\n",
       "\\item 0.420938987301914\n",
       "\\item 0.594037472314046\n",
       "\\item 0.0351771484343449\n",
       "\\item 0.0101241210170218\n",
       "\\item 0.655772066423433\n",
       "\\item 0.0414823421175045\n",
       "\\item 0.344306642777449\n",
       "\\item 0.151969111797743\n",
       "\\item 0.52747757995499\n",
       "\\item 0.305607768146873\n",
       "\\item 0.00486704556981821\n",
       "\\item 0.065909468992276\n",
       "\\item 0.652951925947531\n",
       "\\item 0.0864093277018086\n",
       "\\item 0.139178073137567\n",
       "\\item 0.341995924104932\n",
       "\\item 0.761104634238521\n",
       "\\item 0.00395302268326937\n",
       "\\item 0.328641035005631\n",
       "\\item 0.113700151809085\n",
       "\\item 0.0280373435296362\n",
       "\\item 0.441713650736839\n",
       "\\item 0.392843826669875\n",
       "\\item 0.0770264454821476\n",
       "\\item 0.217891120326254\n",
       "\\item 0.0963246833068634\n",
       "\\item 0.466468665526836\n",
       "\\item 0.0646993663244186\n",
       "\\item 0.115774308247929\n",
       "\\item 0.23165407120508\n",
       "\\item 0.0282893906831235\n",
       "\\item 0.224137905702612\n",
       "\\item 0.0117933497552779\n",
       "\\item 0.0639182021284336\n",
       "\\item 0.00820693900500471\n",
       "\\item 0.0497418898663526\n",
       "\\item 0.275763126242334\n",
       "\\item 0.820409225076895\n",
       "\\item 0.258760375424514\n",
       "\\item 0.224002737543071\n",
       "\\item 0.208821992665129\n",
       "\\item 0.150022186525851\n",
       "\\item 0.0106071775111602\n",
       "\\item 0.64788330615334\n",
       "\\item 0.379170960909486\n",
       "\\item 0.365998455060223\n",
       "\\item 0.878862922584708\n",
       "\\item 0.303307486561192\n",
       "\\item 0.0792783025873824\n",
       "\\item 0.112109861488785\n",
       "\\item 0.319550358098912\n",
       "\\item 0.315691131453868\n",
       "\\item 0.00136798946929624\n",
       "\\item 0.646882616879907\n",
       "\\item 0.55959575970945\n",
       "\\item 0.08124522881382\n",
       "\\item 0.0804990202762637\n",
       "\\item 0.697254993419886\n",
       "\\item 0.0990462042246573\n",
       "\\item 0.0194032930002707\n",
       "\\item 0.613112731840573\n",
       "\\item 0.163373409756571\n",
       "\\item 0.313347825812335\n",
       "\\item 0.240213171064493\n",
       "\\item 0.12787343129881\n",
       "\\item 0.0647466145010186\n",
       "\\item 0.212852338886759\n",
       "\\item 0.512703953314063\n",
       "\\item 0.867244624126617\n",
       "\\item 0.104466079104929\n",
       "\\item 0.00892226841767057\n",
       "\\item 0.411225023200145\n",
       "\\item 0.0293224813462951\n",
       "\\item 0.932105572313179\n",
       "\\item 0.0831694984213733\n",
       "\\item 0.999998900212684\n",
       "\\item 0.298514547875925\n",
       "\\item 0.0763507372120488\n",
       "\\item 0.0412518596764603\n",
       "\\item 0.172107091347054\n",
       "\\item 0.0153042645880929\n",
       "\\item 0.00879655926353673\n",
       "\\item 0.0406685755911528\n",
       "\\item 0.1487786573974\n",
       "\\item 0.24062725420514\n",
       "\\item 0.0194330890348856\n",
       "\\item 0.0371532255064101\n",
       "\\item 0.346003159331868\n",
       "\\item 0.0871983173015547\n",
       "\\item 0.0562310151731386\n",
       "\\item 0.196347894741266\n",
       "\\item 0.0482474898980965\n",
       "\\item 0.0781141639325009\n",
       "\\item 0.0133641923867073\n",
       "\\item 0.631510295925159\n",
       "\\item 0.824502258542784\n",
       "\\item 0.285033270491086\n",
       "\\item 0.00566167761326275\n",
       "\\item 0.19872513240031\n",
       "\\item 0.00977960629528194\n",
       "\\item 0.620715034602006\n",
       "\\item 0.127601720623136\n",
       "\\item 0.313203256005209\n",
       "\\item 0.00601927828501729\n",
       "\\item 0.114255686258546\n",
       "\\item 0.455293684443776\n",
       "\\item 0.552655530673489\n",
       "\\item 0.00385282658062091\n",
       "\\item 0.207449364940697\n",
       "\\item 0.0342211360252181\n",
       "\\item 0.137632114468045\n",
       "\\item 0.016001654991982\n",
       "\\item 0.128871773321818\n",
       "\\item 0.0847315951960205\n",
       "\\item 0.0270433195415661\n",
       "\\item 0.306044259868799\n",
       "\\item 0.31188208156991\n",
       "\\item 0.0564158804449249\n",
       "\\item 0.0641236894739447\n",
       "\\item 0.136150606443115\n",
       "\\item 0.214066230166767\n",
       "\\item 0.0843620307226987\n",
       "\\item 0.436706512599558\n",
       "\\item 0.696452017127779\n",
       "\\item 0.141759051590259\n",
       "\\item 0.358399928469222\n",
       "\\item 0.734331335406255\n",
       "\\item 0.0298883290837451\n",
       "\\item 0.100385832860669\n",
       "\\item 0.713072752989206\n",
       "\\item 0.081923908211841\n",
       "\\item 0.29947548880847\n",
       "\\item 0.428429381499062\n",
       "\\item 0.291351862991912\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.160574950330444\n",
       "2. 0.194614757214468\n",
       "3. 0.0657642673096464\n",
       "4. 0.0090848028351076\n",
       "5. 0.0108183493696735\n",
       "6. 0.328274188114084\n",
       "7. 0.0128692228118427\n",
       "8. 0.135185340992715\n",
       "9. 0.0262270582348304\n",
       "10. 0.736308993875852\n",
       "11. 0.232280136046881\n",
       "12. 0.180565932268704\n",
       "13. 0.353884466798498\n",
       "14. 0.141278124487713\n",
       "15. 0.620304701153559\n",
       "16. 0.646197537582489\n",
       "17. 0.473042691032659\n",
       "18. 0.00843101696370038\n",
       "19. 0.0998384758039544\n",
       "20. 0.86046324688653\n",
       "21. 0.000952697013496243\n",
       "22. 0.0491613547308922\n",
       "23. 0.564323727160182\n",
       "24. 0.00616664915214881\n",
       "25. 0.0424192209665897\n",
       "26. 0.0204973284053453\n",
       "27. 0.277756181954646\n",
       "28. 0.669862183309902\n",
       "29. 0.29283164108411\n",
       "30. 0.073742669913145\n",
       "31. 0.956510691442855\n",
       "32. 0.645195609645712\n",
       "33. 0.00222944648840101\n",
       "34. 0.0642951616935489\n",
       "35. 0.15950039260962\n",
       "36. 0.00361918265583433\n",
       "37. 0.149162887069616\n",
       "38. 0.0133544798177277\n",
       "39. 0.0379488060425114\n",
       "40. 0.692349209723638\n",
       "41. 0.818859615955628\n",
       "42. 0.0698489247122723\n",
       "43. 0.0128890656898538\n",
       "44. 0.637594046088248\n",
       "45. 0.0358308500949368\n",
       "46. 0.0163667569305153\n",
       "47. 0.039050456872504\n",
       "48. 0.207951969398532\n",
       "49. 0.213026987635583\n",
       "50. 0.0704242697118449\n",
       "51. 0.709245282799869\n",
       "52. 0.00192910792915866\n",
       "53. 0.567849341915015\n",
       "54. 0.02184291683155\n",
       "55. 0.909870295637369\n",
       "56. 0.190739918054592\n",
       "57. 0.210933850776207\n",
       "58. 0.00321592478724331\n",
       "59. 0.027723725866694\n",
       "60. 0.534321606679551\n",
       "61. 0.196591374794026\n",
       "62. 0.244281657991462\n",
       "63. 0.111520445878349\n",
       "64. 0.514158404335423\n",
       "65. 0.169053320049634\n",
       "66. 0.33898993986781\n",
       "67. 0.353592033567074\n",
       "68. 0.917337288114215\n",
       "69. 0.0036081380640822\n",
       "70. 0.0564426239842941\n",
       "71. 0.0465747326350894\n",
       "72. 0.165961509381525\n",
       "73. 0.205367164160717\n",
       "74. 0.0288348964901622\n",
       "75. 0.548167173229679\n",
       "76. 0.0312231270992339\n",
       "77. 0.156158473529706\n",
       "78. 0.251235257162444\n",
       "79. 0.376636338394775\n",
       "80. 0.886803578557474\n",
       "81. 0.376827960787195\n",
       "82. 0.226186941402099\n",
       "83. 0.0169519742001024\n",
       "84. 0.00639368479797662\n",
       "85. 0.207655356381026\n",
       "86. 0.0922712223563015\n",
       "87. 0.00819751978744117\n",
       "88. 0.325605630965992\n",
       "89. 0.0118846373687264\n",
       "90. 0.153025955271613\n",
       "91. 0.0631177824266063\n",
       "92. 0.405602744858094\n",
       "93. 0.279217534904339\n",
       "94. 0.0150225553051513\n",
       "95. 0.289793474313642\n",
       "96. 0.221088800577378\n",
       "97. 0.0180259821139205\n",
       "98. 0.116502449288295\n",
       "99. 0.0585713260213269\n",
       "100. 0.13769494400464\n",
       "101. 0.329729761655856\n",
       "102. 0.635589905840042\n",
       "103. 0.0670963077693865\n",
       "104. 0.259761139540847\n",
       "105. 0.143894687291994\n",
       "106. 0.333581539566896\n",
       "107. 0.129856327370709\n",
       "108. 0.352909466124156\n",
       "109. 0.472984919368092\n",
       "110. 0.792348629505367\n",
       "111. 0.268478280433625\n",
       "112. 0.0209414537979283\n",
       "113. 0.305852532909438\n",
       "114. 0.645961081707\n",
       "115. 0.190763116557985\n",
       "116. 0.14478694381518\n",
       "117. 0.180726075401429\n",
       "118. 0.031487793289834\n",
       "119. 0.15030097981072\n",
       "120. 0.0140095536641778\n",
       "121. 0.0572649805371286\n",
       "122. 0.00404497796074312\n",
       "123. 0.323667762858189\n",
       "124. 0.0119131549994086\n",
       "125. 0.0544206447492262\n",
       "126. 0.187598795381614\n",
       "127. 0.927456808579371\n",
       "128. 0.506844848743513\n",
       "129. 0.0171043631369857\n",
       "130. 0.534841063882283\n",
       "131. 0.221174703736585\n",
       "132. 0.492774320590721\n",
       "133. 0.0214099054683917\n",
       "134. 0.00163577792879356\n",
       "135. 0.0801465586173491\n",
       "136. 0.141055568155103\n",
       "137. 0.00343244988227266\n",
       "138. 0.017327335046576\n",
       "139. 0.599409016844675\n",
       "140. 0.0702024757526832\n",
       "141. 0.415479125968076\n",
       "142. 0.0237720420429561\n",
       "143. 0.00361209321769246\n",
       "144. 0.0388817791717024\n",
       "145. 0.0153585652763299\n",
       "146. 0.537855190384946\n",
       "147. 0.0384715180519116\n",
       "148. 0.0568751771484888\n",
       "149. 0.129978254892003\n",
       "150. 0.0793894981671511\n",
       "151. 0.580686155637608\n",
       "152. 0.0225047683567532\n",
       "153. 0.222028172971665\n",
       "154. 0.697623292731847\n",
       "155. 0.197341291744287\n",
       "156. 0.836629016481079\n",
       "157. 0.130308905909367\n",
       "158. 0.159181897198667\n",
       "159. 0.141779225353498\n",
       "160. 0.577697000323039\n",
       "161. 0.304192467969838\n",
       "162. 0.30486331365902\n",
       "163. 0.872175309681557\n",
       "164. 0.0335039851983496\n",
       "165. 0.00679064672837951\n",
       "166. 0.547561085586362\n",
       "167. 0.0231764628494631\n",
       "168. 0.11402805751797\n",
       "169. 0.0298786128211531\n",
       "170. 0.029866447661217\n",
       "171. 0.983617036814556\n",
       "172. 0.476926476451727\n",
       "173. 0.258450841196137\n",
       "174. 0.657174245420284\n",
       "175. 0.167561296819024\n",
       "176. 0.0052499231787139\n",
       "177. 0.033547983059923\n",
       "178. 0.94791421596923\n",
       "179. 0.229634140464392\n",
       "180. 0.120156867650525\n",
       "181. 0.142599605916586\n",
       "182. 0.510880482769551\n",
       "183. 0.492502423220792\n",
       "184. 0.126805546976777\n",
       "185. 0.407427981575015\n",
       "186. 0.581683597187791\n",
       "187. 0.141220867702616\n",
       "188. 0.491239753741101\n",
       "189. 0.506108233450003\n",
       "190. 0.554117473397666\n",
       "191. 0.910908909002514\n",
       "192. 0.0269220420130241\n",
       "193. 0.754692853853804\n",
       "194. 0.0038520850350016\n",
       "195. 0.237275521978451\n",
       "196. 0.0167363580961563\n",
       "197. 0.0254319902118264\n",
       "198. 0.0427567427091678\n",
       "199. 0.0513634163501955\n",
       "200. 0.0830602532777982\n",
       "201. 0.127811558950524\n",
       "202. 0.015304406307749\n",
       "203. 0.157569965129412\n",
       "204. 0.0867457828881429\n",
       "205. 0.260415248246903\n",
       "206. 0.536101435055817\n",
       "207. 0.755638271041679\n",
       "208. 0.111936309172342\n",
       "209. 0.0946035535687098\n",
       "210. 0.431373346209228\n",
       "211. 0.586116091274582\n",
       "212. 0.243447741927022\n",
       "213. 0.00692472313037775\n",
       "214. 0.780407222774983\n",
       "215. 0.400909625570225\n",
       "216. 0.371780967149011\n",
       "217. 0.286448907973697\n",
       "218. 0.860804894005965\n",
       "219. 0.654651003628947\n",
       "220. 0.703567200454583\n",
       "221. 0.209465641522204\n",
       "222. 0.568627477780791\n",
       "223. 0.124654362884931\n",
       "224. 0.277632053649806\n",
       "225. 0.0561373748161562\n",
       "226. 0.0236720068270888\n",
       "227. 0.0146236434841262\n",
       "228. 0.16074680015728\n",
       "229. 0.0242615307597458\n",
       "230. 0.00989702285626594\n",
       "231. 0.734834096371288\n",
       "232. 0.0146209771289039\n",
       "233. 0.700805780866803\n",
       "234. 0.0233939143347865\n",
       "235. 0.0492909910083302\n",
       "236. 0.278628757090079\n",
       "237. 0.752031524506524\n",
       "238. 0.039345529866763\n",
       "239. 0.0186371720155162\n",
       "240. 0.999999707540076\n",
       "241. 0.0935536461554124\n",
       "242. 0.623929138162292\n",
       "243. 0.402888193365402\n",
       "244. 0.12699870177318\n",
       "245. 0.0243572791459279\n",
       "246. 0.448752348025535\n",
       "247. 0.0532387781770922\n",
       "248. 0.113177776632805\n",
       "249. 0.167676638215059\n",
       "250. 0.00654841366788043\n",
       "251. 0.466087119303045\n",
       "252. 0.386747701586424\n",
       "253. 0.0164965298549906\n",
       "254. 0.0367778198550268\n",
       "255. 0.00238258791252777\n",
       "256. 0.00321796991081338\n",
       "257. 0.283340407643654\n",
       "258. 0.00896644039881044\n",
       "259. 0.733352604736861\n",
       "260. 0.00441750047533496\n",
       "261. 0.678317988139266\n",
       "262. 0.0720838090401987\n",
       "263. 0.0107076242093414\n",
       "264. 0.0955796655255012\n",
       "265. 0.202076772008901\n",
       "266. 0.00982938242376169\n",
       "267. 0.234251785347733\n",
       "268. 0.71547746468794\n",
       "269. 0.819380809867352\n",
       "270. 0.0340657785425034\n",
       "271. 0.834428855419787\n",
       "272. 0.325728125074179\n",
       "273. 0.513442626958064\n",
       "274. 0.145989697652764\n",
       "275. 0.0416133829464936\n",
       "276. 0.262299721726386\n",
       "277. 0.136582835298833\n",
       "278. 0.00863082094882987\n",
       "279. 0.0588905460404537\n",
       "280. 0.0792714499560307\n",
       "281. 0.242801812274397\n",
       "282. 0.864697526004389\n",
       "283. 0.741507777870621\n",
       "284. 0.373944969220621\n",
       "285. 0.509642309332493\n",
       "286. 0.264162647491831\n",
       "287. 0.178624922783598\n",
       "288. 0.00322296309834809\n",
       "289. 0.0216674952111267\n",
       "290. 0.0194932626481211\n",
       "291. 0.596229293130115\n",
       "292. 0.776172263874826\n",
       "293. 0.399593539053717\n",
       "294. 0.606594059081995\n",
       "295. 0.113553882839064\n",
       "296. 0.0425413674085636\n",
       "297. 0.0136725792213377\n",
       "298. 0.488215579682298\n",
       "299. 0.0119836050925559\n",
       "300. 0.0719986572858782\n",
       "301. 0.435615924833523\n",
       "302. 0.00345051004846592\n",
       "303. 0.0378459210750139\n",
       "304. 0.0114251791102418\n",
       "305. 0.125273074910697\n",
       "306. 0.1575690693746\n",
       "307. 0.327168048466392\n",
       "308. 0.295916572532276\n",
       "309. 0.11104175531405\n",
       "310. 0.014955934769429\n",
       "311. 0.0106148444555317\n",
       "312. 0.527001368360012\n",
       "313. 0.00262895935429216\n",
       "314. 0.395103978142983\n",
       "315. 0.203728443261776\n",
       "316. 0.658453967489166\n",
       "317. 0.119776074616211\n",
       "318. 0.00927084158055183\n",
       "319. 0.413609505755124\n",
       "320. 0.00729988264878958\n",
       "321. 0.496842319672098\n",
       "322. 0.0467516640068276\n",
       "323. 0.488966714158882\n",
       "324. 0.00292655310513345\n",
       "325. 0.0978872634392816\n",
       "326. 0.409159876673382\n",
       "327. 0.00746559610308782\n",
       "328. 0.290840580191103\n",
       "329. 0.935538612345386\n",
       "330. 0.166115983762243\n",
       "331. 0.0894719921995294\n",
       "332. 0.99999988312744\n",
       "333. 0.0492194614333566\n",
       "334. 0.725142014025603\n",
       "335. 0.851325425051723\n",
       "336. 0.0453861269966289\n",
       "337. 0.234429222282555\n",
       "338. 0.0679034997456688\n",
       "339. 0.00311268362037586\n",
       "340. 0.258186422389191\n",
       "341. 0.673808286298615\n",
       "342. 0.385916734010346\n",
       "343. 0.0791191082169789\n",
       "344. 0.112803626438731\n",
       "345. 0.01390605375495\n",
       "346. 0.640587881379281\n",
       "347. 0.985543514263911\n",
       "348. 0.0094569017637177\n",
       "349. 0.647949182331332\n",
       "350. 0.290692960922754\n",
       "351. 0.920930206937619\n",
       "352. 0.452534392915463\n",
       "353. 0.761096085325328\n",
       "354. 0.511185474939446\n",
       "355. 0.623218459375131\n",
       "356. 0.0162141541642301\n",
       "357. 0.333177899990478\n",
       "358. 0.140060743990682\n",
       "359. 0.37238114621485\n",
       "360. 0.495830561133254\n",
       "361. 0.00702547324014362\n",
       "362. 0.638281170071597\n",
       "363. 0.642296764035918\n",
       "364. 0.306906721143049\n",
       "365. 0.604142271534431\n",
       "366. 0.0780824149027675\n",
       "367. 0.685527595941055\n",
       "368. 0.141919931193779\n",
       "369. 0.719426913329513\n",
       "370. 0.980936895165277\n",
       "371. 0.329592462343399\n",
       "372. 0.225619594020078\n",
       "373. 0.758030216545074\n",
       "374. 0.061634311916085\n",
       "375. 0.722338100370991\n",
       "376. 0.597710954934034\n",
       "377. 0.601819734804895\n",
       "378. 0.999999689846865\n",
       "379. 0.396457884824599\n",
       "380. 0.425650269137989\n",
       "381. 0.031663277291782\n",
       "382. 0.0851040779306962\n",
       "383. 0.231992734492738\n",
       "384. 0.0114518373425591\n",
       "385. 0.650776669436919\n",
       "386. 0.4206232580536\n",
       "387. 0.542682302635948\n",
       "388. 0.18180295770518\n",
       "389. 0.0149771399800867\n",
       "390. 0.569748133350704\n",
       "391. 0.0187928189967538\n",
       "392. 0.023477504661038\n",
       "393. 0.340417619684114\n",
       "394. 0.0040098737160981\n",
       "395. 0.329470585254571\n",
       "396. 0.0574727537594718\n",
       "397. 0.0107173991334299\n",
       "398. 0.355640669111306\n",
       "399. 0.0173829926611912\n",
       "400. 0.0620256104346949\n",
       "401. 0.219398174731547\n",
       "402. 0.565174117284585\n",
       "403. 0.261167232490392\n",
       "404. 0.108724262356992\n",
       "405. 0.794849142106868\n",
       "406. 0.910915006518465\n",
       "407. 0.0108097203243075\n",
       "408. 0.199673287515818\n",
       "409. 0.00445214468678075\n",
       "410. 0.0056895418316378\n",
       "411. 0.0156252741687175\n",
       "412. 0.0434542838599873\n",
       "413. 0.37981942585084\n",
       "414. 0.936779543832573\n",
       "415. 0.639230476961909\n",
       "416. 0.285950396633674\n",
       "417. 0.643706591276826\n",
       "418. 0.24080420522909\n",
       "419. 0.013317486135993\n",
       "420. 0.248732745090195\n",
       "421. 0.0150149809510639\n",
       "422. 0.0864050181182037\n",
       "423. 0.0219124775648427\n",
       "424. 0.0477403674659473\n",
       "425. 0.343106389140021\n",
       "426. 0.0199018916237982\n",
       "427. 0.0402270091673715\n",
       "428. 0.11835109243603\n",
       "429. 0.0215147382927905\n",
       "430. 0.380268790736627\n",
       "431. 0.0709091768165848\n",
       "432. 0.248539095134567\n",
       "433. 0.0458959571816002\n",
       "434. 0.338157016619925\n",
       "435. 0.0744694363872355\n",
       "436. 0.0875756130497682\n",
       "437. 0.234081539708774\n",
       "438. 0.371581349556878\n",
       "439. 0.0105443102676181\n",
       "440. 0.0103120872564989\n",
       "441. 0.0680291992736389\n",
       "442. 0.0276318056830353\n",
       "443. 0.0440082320571134\n",
       "444. 0.00377136552089557\n",
       "445. 0.00783041686685496\n",
       "446. 0.0409763591288367\n",
       "447. 0.0180951115376393\n",
       "448. 0.420336652250682\n",
       "449. 0.233790557430558\n",
       "450. 0.829962285004331\n",
       "451. 0.712566469894219\n",
       "452. 0.999999911574732\n",
       "453. 0.408515097379173\n",
       "454. 0.125435192824696\n",
       "455. 0.306269305842519\n",
       "456. 0.00430628276523339\n",
       "457. 0.0254059673455803\n",
       "458. 0.0625322241782875\n",
       "459. 0.775409684179629\n",
       "460. 0.137930277138711\n",
       "461. 0.050667847221433\n",
       "462. 0.216099554213866\n",
       "463. 0.62106315472439\n",
       "464. 0.0202661870490799\n",
       "465. 0.843686308468121\n",
       "466. 0.213026158540125\n",
       "467. 0.0774242687870972\n",
       "468. 0.152747879915751\n",
       "469. 0.503699642442566\n",
       "470. 0.0445974342466566\n",
       "471. 0.437533082839485\n",
       "472. 0.0663400648135764\n",
       "473. 0.403796667778126\n",
       "474. 0.0325709092701417\n",
       "475. 0.0411410965957106\n",
       "476. 0.110801390149083\n",
       "477. 0.284565334097029\n",
       "478. 0.453694109004949\n",
       "479. 0.545860894240575\n",
       "480. 0.140119416640547\n",
       "481. 0.0505452215169591\n",
       "482. 0.985872533419196\n",
       "483. 0.187786309170919\n",
       "484. 0.026204627530121\n",
       "485. 0.00395045193940631\n",
       "486. 0.0139113041818318\n",
       "487. 0.205325832609876\n",
       "488. 0.420938987301914\n",
       "489. 0.594037472314046\n",
       "490. 0.0351771484343449\n",
       "491. 0.0101241210170218\n",
       "492. 0.655772066423433\n",
       "493. 0.0414823421175045\n",
       "494. 0.344306642777449\n",
       "495. 0.151969111797743\n",
       "496. 0.52747757995499\n",
       "497. 0.305607768146873\n",
       "498. 0.00486704556981821\n",
       "499. 0.065909468992276\n",
       "500. 0.652951925947531\n",
       "501. 0.0864093277018086\n",
       "502. 0.139178073137567\n",
       "503. 0.341995924104932\n",
       "504. 0.761104634238521\n",
       "505. 0.00395302268326937\n",
       "506. 0.328641035005631\n",
       "507. 0.113700151809085\n",
       "508. 0.0280373435296362\n",
       "509. 0.441713650736839\n",
       "510. 0.392843826669875\n",
       "511. 0.0770264454821476\n",
       "512. 0.217891120326254\n",
       "513. 0.0963246833068634\n",
       "514. 0.466468665526836\n",
       "515. 0.0646993663244186\n",
       "516. 0.115774308247929\n",
       "517. 0.23165407120508\n",
       "518. 0.0282893906831235\n",
       "519. 0.224137905702612\n",
       "520. 0.0117933497552779\n",
       "521. 0.0639182021284336\n",
       "522. 0.00820693900500471\n",
       "523. 0.0497418898663526\n",
       "524. 0.275763126242334\n",
       "525. 0.820409225076895\n",
       "526. 0.258760375424514\n",
       "527. 0.224002737543071\n",
       "528. 0.208821992665129\n",
       "529. 0.150022186525851\n",
       "530. 0.0106071775111602\n",
       "531. 0.64788330615334\n",
       "532. 0.379170960909486\n",
       "533. 0.365998455060223\n",
       "534. 0.878862922584708\n",
       "535. 0.303307486561192\n",
       "536. 0.0792783025873824\n",
       "537. 0.112109861488785\n",
       "538. 0.319550358098912\n",
       "539. 0.315691131453868\n",
       "540. 0.00136798946929624\n",
       "541. 0.646882616879907\n",
       "542. 0.55959575970945\n",
       "543. 0.08124522881382\n",
       "544. 0.0804990202762637\n",
       "545. 0.697254993419886\n",
       "546. 0.0990462042246573\n",
       "547. 0.0194032930002707\n",
       "548. 0.613112731840573\n",
       "549. 0.163373409756571\n",
       "550. 0.313347825812335\n",
       "551. 0.240213171064493\n",
       "552. 0.12787343129881\n",
       "553. 0.0647466145010186\n",
       "554. 0.212852338886759\n",
       "555. 0.512703953314063\n",
       "556. 0.867244624126617\n",
       "557. 0.104466079104929\n",
       "558. 0.00892226841767057\n",
       "559. 0.411225023200145\n",
       "560. 0.0293224813462951\n",
       "561. 0.932105572313179\n",
       "562. 0.0831694984213733\n",
       "563. 0.999998900212684\n",
       "564. 0.298514547875925\n",
       "565. 0.0763507372120488\n",
       "566. 0.0412518596764603\n",
       "567. 0.172107091347054\n",
       "568. 0.0153042645880929\n",
       "569. 0.00879655926353673\n",
       "570. 0.0406685755911528\n",
       "571. 0.1487786573974\n",
       "572. 0.24062725420514\n",
       "573. 0.0194330890348856\n",
       "574. 0.0371532255064101\n",
       "575. 0.346003159331868\n",
       "576. 0.0871983173015547\n",
       "577. 0.0562310151731386\n",
       "578. 0.196347894741266\n",
       "579. 0.0482474898980965\n",
       "580. 0.0781141639325009\n",
       "581. 0.0133641923867073\n",
       "582. 0.631510295925159\n",
       "583. 0.824502258542784\n",
       "584. 0.285033270491086\n",
       "585. 0.00566167761326275\n",
       "586. 0.19872513240031\n",
       "587. 0.00977960629528194\n",
       "588. 0.620715034602006\n",
       "589. 0.127601720623136\n",
       "590. 0.313203256005209\n",
       "591. 0.00601927828501729\n",
       "592. 0.114255686258546\n",
       "593. 0.455293684443776\n",
       "594. 0.552655530673489\n",
       "595. 0.00385282658062091\n",
       "596. 0.207449364940697\n",
       "597. 0.0342211360252181\n",
       "598. 0.137632114468045\n",
       "599. 0.016001654991982\n",
       "600. 0.128871773321818\n",
       "601. 0.0847315951960205\n",
       "602. 0.0270433195415661\n",
       "603. 0.306044259868799\n",
       "604. 0.31188208156991\n",
       "605. 0.0564158804449249\n",
       "606. 0.0641236894739447\n",
       "607. 0.136150606443115\n",
       "608. 0.214066230166767\n",
       "609. 0.0843620307226987\n",
       "610. 0.436706512599558\n",
       "611. 0.696452017127779\n",
       "612. 0.141759051590259\n",
       "613. 0.358399928469222\n",
       "614. 0.734331335406255\n",
       "615. 0.0298883290837451\n",
       "616. 0.100385832860669\n",
       "617. 0.713072752989206\n",
       "618. 0.081923908211841\n",
       "619. 0.29947548880847\n",
       "620. 0.428429381499062\n",
       "621. 0.291351862991912\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 0.160574950 0.194614757 0.065764267 0.009084803 0.010818349 0.328274188\n",
       "  [7] 0.012869223 0.135185341 0.026227058 0.736308994 0.232280136 0.180565932\n",
       " [13] 0.353884467 0.141278124 0.620304701 0.646197538 0.473042691 0.008431017\n",
       " [19] 0.099838476 0.860463247 0.000952697 0.049161355 0.564323727 0.006166649\n",
       " [25] 0.042419221 0.020497328 0.277756182 0.669862183 0.292831641 0.073742670\n",
       " [31] 0.956510691 0.645195610 0.002229446 0.064295162 0.159500393 0.003619183\n",
       " [37] 0.149162887 0.013354480 0.037948806 0.692349210 0.818859616 0.069848925\n",
       " [43] 0.012889066 0.637594046 0.035830850 0.016366757 0.039050457 0.207951969\n",
       " [49] 0.213026988 0.070424270 0.709245283 0.001929108 0.567849342 0.021842917\n",
       " [55] 0.909870296 0.190739918 0.210933851 0.003215925 0.027723726 0.534321607\n",
       " [61] 0.196591375 0.244281658 0.111520446 0.514158404 0.169053320 0.338989940\n",
       " [67] 0.353592034 0.917337288 0.003608138 0.056442624 0.046574733 0.165961509\n",
       " [73] 0.205367164 0.028834896 0.548167173 0.031223127 0.156158474 0.251235257\n",
       " [79] 0.376636338 0.886803579 0.376827961 0.226186941 0.016951974 0.006393685\n",
       " [85] 0.207655356 0.092271222 0.008197520 0.325605631 0.011884637 0.153025955\n",
       " [91] 0.063117782 0.405602745 0.279217535 0.015022555 0.289793474 0.221088801\n",
       " [97] 0.018025982 0.116502449 0.058571326 0.137694944 0.329729762 0.635589906\n",
       "[103] 0.067096308 0.259761140 0.143894687 0.333581540 0.129856327 0.352909466\n",
       "[109] 0.472984919 0.792348630 0.268478280 0.020941454 0.305852533 0.645961082\n",
       "[115] 0.190763117 0.144786944 0.180726075 0.031487793 0.150300980 0.014009554\n",
       "[121] 0.057264981 0.004044978 0.323667763 0.011913155 0.054420645 0.187598795\n",
       "[127] 0.927456809 0.506844849 0.017104363 0.534841064 0.221174704 0.492774321\n",
       "[133] 0.021409905 0.001635778 0.080146559 0.141055568 0.003432450 0.017327335\n",
       "[139] 0.599409017 0.070202476 0.415479126 0.023772042 0.003612093 0.038881779\n",
       "[145] 0.015358565 0.537855190 0.038471518 0.056875177 0.129978255 0.079389498\n",
       "[151] 0.580686156 0.022504768 0.222028173 0.697623293 0.197341292 0.836629016\n",
       "[157] 0.130308906 0.159181897 0.141779225 0.577697000 0.304192468 0.304863314\n",
       "[163] 0.872175310 0.033503985 0.006790647 0.547561086 0.023176463 0.114028058\n",
       "[169] 0.029878613 0.029866448 0.983617037 0.476926476 0.258450841 0.657174245\n",
       "[175] 0.167561297 0.005249923 0.033547983 0.947914216 0.229634140 0.120156868\n",
       "[181] 0.142599606 0.510880483 0.492502423 0.126805547 0.407427982 0.581683597\n",
       "[187] 0.141220868 0.491239754 0.506108233 0.554117473 0.910908909 0.026922042\n",
       "[193] 0.754692854 0.003852085 0.237275522 0.016736358 0.025431990 0.042756743\n",
       "[199] 0.051363416 0.083060253 0.127811559 0.015304406 0.157569965 0.086745783\n",
       "[205] 0.260415248 0.536101435 0.755638271 0.111936309 0.094603554 0.431373346\n",
       "[211] 0.586116091 0.243447742 0.006924723 0.780407223 0.400909626 0.371780967\n",
       "[217] 0.286448908 0.860804894 0.654651004 0.703567200 0.209465642 0.568627478\n",
       "[223] 0.124654363 0.277632054 0.056137375 0.023672007 0.014623643 0.160746800\n",
       "[229] 0.024261531 0.009897023 0.734834096 0.014620977 0.700805781 0.023393914\n",
       "[235] 0.049290991 0.278628757 0.752031525 0.039345530 0.018637172 0.999999708\n",
       "[241] 0.093553646 0.623929138 0.402888193 0.126998702 0.024357279 0.448752348\n",
       "[247] 0.053238778 0.113177777 0.167676638 0.006548414 0.466087119 0.386747702\n",
       "[253] 0.016496530 0.036777820 0.002382588 0.003217970 0.283340408 0.008966440\n",
       "[259] 0.733352605 0.004417500 0.678317988 0.072083809 0.010707624 0.095579666\n",
       "[265] 0.202076772 0.009829382 0.234251785 0.715477465 0.819380810 0.034065779\n",
       "[271] 0.834428855 0.325728125 0.513442627 0.145989698 0.041613383 0.262299722\n",
       "[277] 0.136582835 0.008630821 0.058890546 0.079271450 0.242801812 0.864697526\n",
       "[283] 0.741507778 0.373944969 0.509642309 0.264162647 0.178624923 0.003222963\n",
       "[289] 0.021667495 0.019493263 0.596229293 0.776172264 0.399593539 0.606594059\n",
       "[295] 0.113553883 0.042541367 0.013672579 0.488215580 0.011983605 0.071998657\n",
       "[301] 0.435615925 0.003450510 0.037845921 0.011425179 0.125273075 0.157569069\n",
       "[307] 0.327168048 0.295916573 0.111041755 0.014955935 0.010614844 0.527001368\n",
       "[313] 0.002628959 0.395103978 0.203728443 0.658453967 0.119776075 0.009270842\n",
       "[319] 0.413609506 0.007299883 0.496842320 0.046751664 0.488966714 0.002926553\n",
       "[325] 0.097887263 0.409159877 0.007465596 0.290840580 0.935538612 0.166115984\n",
       "[331] 0.089471992 0.999999883 0.049219461 0.725142014 0.851325425 0.045386127\n",
       "[337] 0.234429222 0.067903500 0.003112684 0.258186422 0.673808286 0.385916734\n",
       "[343] 0.079119108 0.112803626 0.013906054 0.640587881 0.985543514 0.009456902\n",
       "[349] 0.647949182 0.290692961 0.920930207 0.452534393 0.761096085 0.511185475\n",
       "[355] 0.623218459 0.016214154 0.333177900 0.140060744 0.372381146 0.495830561\n",
       "[361] 0.007025473 0.638281170 0.642296764 0.306906721 0.604142272 0.078082415\n",
       "[367] 0.685527596 0.141919931 0.719426913 0.980936895 0.329592462 0.225619594\n",
       "[373] 0.758030217 0.061634312 0.722338100 0.597710955 0.601819735 0.999999690\n",
       "[379] 0.396457885 0.425650269 0.031663277 0.085104078 0.231992734 0.011451837\n",
       "[385] 0.650776669 0.420623258 0.542682303 0.181802958 0.014977140 0.569748133\n",
       "[391] 0.018792819 0.023477505 0.340417620 0.004009874 0.329470585 0.057472754\n",
       "[397] 0.010717399 0.355640669 0.017382993 0.062025610 0.219398175 0.565174117\n",
       "[403] 0.261167232 0.108724262 0.794849142 0.910915007 0.010809720 0.199673288\n",
       "[409] 0.004452145 0.005689542 0.015625274 0.043454284 0.379819426 0.936779544\n",
       "[415] 0.639230477 0.285950397 0.643706591 0.240804205 0.013317486 0.248732745\n",
       "[421] 0.015014981 0.086405018 0.021912478 0.047740367 0.343106389 0.019901892\n",
       "[427] 0.040227009 0.118351092 0.021514738 0.380268791 0.070909177 0.248539095\n",
       "[433] 0.045895957 0.338157017 0.074469436 0.087575613 0.234081540 0.371581350\n",
       "[439] 0.010544310 0.010312087 0.068029199 0.027631806 0.044008232 0.003771366\n",
       "[445] 0.007830417 0.040976359 0.018095112 0.420336652 0.233790557 0.829962285\n",
       "[451] 0.712566470 0.999999912 0.408515097 0.125435193 0.306269306 0.004306283\n",
       "[457] 0.025405967 0.062532224 0.775409684 0.137930277 0.050667847 0.216099554\n",
       "[463] 0.621063155 0.020266187 0.843686308 0.213026159 0.077424269 0.152747880\n",
       "[469] 0.503699642 0.044597434 0.437533083 0.066340065 0.403796668 0.032570909\n",
       "[475] 0.041141097 0.110801390 0.284565334 0.453694109 0.545860894 0.140119417\n",
       "[481] 0.050545222 0.985872533 0.187786309 0.026204628 0.003950452 0.013911304\n",
       "[487] 0.205325833 0.420938987 0.594037472 0.035177148 0.010124121 0.655772066\n",
       "[493] 0.041482342 0.344306643 0.151969112 0.527477580 0.305607768 0.004867046\n",
       "[499] 0.065909469 0.652951926 0.086409328 0.139178073 0.341995924 0.761104634\n",
       "[505] 0.003953023 0.328641035 0.113700152 0.028037344 0.441713651 0.392843827\n",
       "[511] 0.077026445 0.217891120 0.096324683 0.466468666 0.064699366 0.115774308\n",
       "[517] 0.231654071 0.028289391 0.224137906 0.011793350 0.063918202 0.008206939\n",
       "[523] 0.049741890 0.275763126 0.820409225 0.258760375 0.224002738 0.208821993\n",
       "[529] 0.150022187 0.010607178 0.647883306 0.379170961 0.365998455 0.878862923\n",
       "[535] 0.303307487 0.079278303 0.112109861 0.319550358 0.315691131 0.001367989\n",
       "[541] 0.646882617 0.559595760 0.081245229 0.080499020 0.697254993 0.099046204\n",
       "[547] 0.019403293 0.613112732 0.163373410 0.313347826 0.240213171 0.127873431\n",
       "[553] 0.064746615 0.212852339 0.512703953 0.867244624 0.104466079 0.008922268\n",
       "[559] 0.411225023 0.029322481 0.932105572 0.083169498 0.999998900 0.298514548\n",
       "[565] 0.076350737 0.041251860 0.172107091 0.015304265 0.008796559 0.040668576\n",
       "[571] 0.148778657 0.240627254 0.019433089 0.037153226 0.346003159 0.087198317\n",
       "[577] 0.056231015 0.196347895 0.048247490 0.078114164 0.013364192 0.631510296\n",
       "[583] 0.824502259 0.285033270 0.005661678 0.198725132 0.009779606 0.620715035\n",
       "[589] 0.127601721 0.313203256 0.006019278 0.114255686 0.455293684 0.552655531\n",
       "[595] 0.003852827 0.207449365 0.034221136 0.137632114 0.016001655 0.128871773\n",
       "[601] 0.084731595 0.027043320 0.306044260 0.311882082 0.056415880 0.064123689\n",
       "[607] 0.136150606 0.214066230 0.084362031 0.436706513 0.696452017 0.141759052\n",
       "[613] 0.358399928 0.734331335 0.029888329 0.100385833 0.713072753 0.081923908\n",
       "[619] 0.299475489 0.428429381 0.291351863"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predLasso$b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = a, case = b\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.889294130849512"
      ],
      "text/latex": [
       "0.889294130849512"
      ],
      "text/markdown": [
       "0.889294130849512"
      ],
      "text/plain": [
       "Area under the curve: 0.8893"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf.roc<-roc(traindata3$y,predLasso$b)\n",
    "auc(rf.roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(rf.roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.0199699476169238</li>\n",
       "\t<li>0.0458199977164465</li>\n",
       "\t<li>0.495266696384679</li>\n",
       "\t<li>0.0472587034954134</li>\n",
       "\t<li>0.610468334947603</li>\n",
       "\t<li>0.136159621575197</li>\n",
       "\t<li>0.31704976973032</li>\n",
       "\t<li>0.330788076270406</li>\n",
       "\t<li>0.0572359528828377</li>\n",
       "\t<li>0.0083233190846819</li>\n",
       "\t<li>0.196416341440513</li>\n",
       "\t<li>0.180200264230589</li>\n",
       "\t<li>0.0113471974905296</li>\n",
       "\t<li>0.999999874308373</li>\n",
       "\t<li>0.0774379836991409</li>\n",
       "\t<li>0.0343189363764838</li>\n",
       "\t<li>0.0117077239538832</li>\n",
       "\t<li>0.175135339069821</li>\n",
       "\t<li>0.151597975314201</li>\n",
       "\t<li>0.0113879153094041</li>\n",
       "\t<li>0.0390493668900996</li>\n",
       "\t<li>0.267976114670805</li>\n",
       "\t<li>0.92963137112946</li>\n",
       "\t<li>0.01331014904008</li>\n",
       "\t<li>0.632286991324739</li>\n",
       "\t<li>0.0681840936289838</li>\n",
       "\t<li>0.167867084684194</li>\n",
       "\t<li>0.783166172347892</li>\n",
       "\t<li>0.539091118054234</li>\n",
       "\t<li>0.0885465145955104</li>\n",
       "\t<li>0.218503708926811</li>\n",
       "\t<li>0.714245072008431</li>\n",
       "\t<li>0.0193011550320404</li>\n",
       "\t<li>0.417379060294064</li>\n",
       "\t<li>0.714314667153859</li>\n",
       "\t<li>0.0989819113680306</li>\n",
       "\t<li>0.63212634718896</li>\n",
       "\t<li>0.863068467443166</li>\n",
       "\t<li>0.838679177471337</li>\n",
       "\t<li>0.0123271799699895</li>\n",
       "\t<li>0.200794933483637</li>\n",
       "\t<li>0.188821192097363</li>\n",
       "\t<li>0.0700626377808473</li>\n",
       "\t<li>0.837945571103761</li>\n",
       "\t<li>0.276310621835661</li>\n",
       "\t<li>0.657691841484601</li>\n",
       "\t<li>0.205660255581229</li>\n",
       "\t<li>0.0239168649629574</li>\n",
       "\t<li>0.156224136042788</li>\n",
       "\t<li>0.0727436295232659</li>\n",
       "\t<li>0.0502401352718925</li>\n",
       "\t<li>0.0271617649981027</li>\n",
       "\t<li>0.132303470328694</li>\n",
       "\t<li>0.176673984898641</li>\n",
       "\t<li>0.936209049546387</li>\n",
       "\t<li>0.714462545999191</li>\n",
       "\t<li>0.611341433297742</li>\n",
       "\t<li>0.522927481877359</li>\n",
       "\t<li>0.0856755142255962</li>\n",
       "\t<li>0.262617148260062</li>\n",
       "\t<li>0.899308500498465</li>\n",
       "\t<li>0.0121847961244518</li>\n",
       "\t<li>0.0169264820611136</li>\n",
       "\t<li>0.116634527172926</li>\n",
       "\t<li>0.0672691467996576</li>\n",
       "\t<li>0.019559155120192</li>\n",
       "\t<li>0.00197235396077447</li>\n",
       "\t<li>0.0108449809442752</li>\n",
       "\t<li>0.510216724093831</li>\n",
       "\t<li>0.0904532460508593</li>\n",
       "\t<li>0.435372525685862</li>\n",
       "\t<li>0.0049295633283936</li>\n",
       "\t<li>0.012455078937811</li>\n",
       "\t<li>0.541889294949295</li>\n",
       "\t<li>0.00895807358566363</li>\n",
       "\t<li>0.0936020507485612</li>\n",
       "\t<li>0.564364049580987</li>\n",
       "\t<li>0.0175227074763854</li>\n",
       "\t<li>0.319530217052547</li>\n",
       "\t<li>0.0260328768217256</li>\n",
       "\t<li>0.782656773290995</li>\n",
       "\t<li>0.0300054899951376</li>\n",
       "\t<li>0.0092407087558277</li>\n",
       "\t<li>0.557042322489272</li>\n",
       "\t<li>0.172767899139369</li>\n",
       "\t<li>0.169500475758547</li>\n",
       "\t<li>0.0105496860722101</li>\n",
       "\t<li>0.047448975283293</li>\n",
       "\t<li>0.721803564859241</li>\n",
       "\t<li>0.236527042683651</li>\n",
       "\t<li>0.0400844925094205</li>\n",
       "\t<li>0.0323106117152453</li>\n",
       "\t<li>0.0551054029703938</li>\n",
       "\t<li>0.881513227076948</li>\n",
       "\t<li>0.127465269427721</li>\n",
       "\t<li>0.171626208605749</li>\n",
       "\t<li>0.0198552567487982</li>\n",
       "\t<li>0.0939341549905087</li>\n",
       "\t<li>0.384039317177223</li>\n",
       "\t<li>0.490064623868398</li>\n",
       "\t<li>0.810220301244794</li>\n",
       "\t<li>0.0128802300860469</li>\n",
       "\t<li>0.628930818844018</li>\n",
       "\t<li>0.477956918518179</li>\n",
       "\t<li>0.211239062974798</li>\n",
       "\t<li>0.450625519167723</li>\n",
       "\t<li>0.0489693549931018</li>\n",
       "\t<li>0.153292202001712</li>\n",
       "\t<li>0.263943026571787</li>\n",
       "\t<li>0.0361809239692963</li>\n",
       "\t<li>0.014737017055404</li>\n",
       "\t<li>0.817887964218033</li>\n",
       "\t<li>0.00572570066559266</li>\n",
       "\t<li>0.565256351818271</li>\n",
       "\t<li>0.236803485421299</li>\n",
       "\t<li>0.00297010101132321</li>\n",
       "\t<li>0.00219648975771379</li>\n",
       "\t<li>0.499704006968091</li>\n",
       "\t<li>0.0303396620690834</li>\n",
       "\t<li>0.00558038007284844</li>\n",
       "\t<li>0.0424689032462572</li>\n",
       "\t<li>0.0278690778886793</li>\n",
       "\t<li>0.00941158628724017</li>\n",
       "\t<li>0.0669573500977942</li>\n",
       "\t<li>0.11273279278212</li>\n",
       "\t<li>0.00286946198686716</li>\n",
       "\t<li>0.0271483676136606</li>\n",
       "\t<li>0.218614350829553</li>\n",
       "\t<li>0.571809577970821</li>\n",
       "\t<li>0.0161068956824048</li>\n",
       "\t<li>0.464076015443004</li>\n",
       "\t<li>0.187053377790224</li>\n",
       "\t<li>0.241979368573769</li>\n",
       "\t<li>0.260805827999097</li>\n",
       "\t<li>0.383374781060198</li>\n",
       "\t<li>0.433144602201799</li>\n",
       "\t<li>0.00436676477761932</li>\n",
       "\t<li>0.722942271523976</li>\n",
       "\t<li>0.0810959085557277</li>\n",
       "\t<li>0.457050515442807</li>\n",
       "\t<li>0.247592719205121</li>\n",
       "\t<li>0.149236598016692</li>\n",
       "\t<li>0.190278667026799</li>\n",
       "\t<li>0.711020806121186</li>\n",
       "\t<li>0.311137916522735</li>\n",
       "\t<li>0.292990647337669</li>\n",
       "\t<li>0.511568232699909</li>\n",
       "\t<li>0.229699173606277</li>\n",
       "\t<li>0.116477659097698</li>\n",
       "\t<li>0.0367492647376478</li>\n",
       "\t<li>0.143062726825331</li>\n",
       "\t<li>0.301657220879435</li>\n",
       "\t<li>0.147534430963637</li>\n",
       "\t<li>0.0841891611580271</li>\n",
       "\t<li>0.00643526070543069</li>\n",
       "\t<li>0.0556656082727995</li>\n",
       "\t<li>0.0220379683539955</li>\n",
       "\t<li>0.378662367379383</li>\n",
       "\t<li>0.180499966171909</li>\n",
       "\t<li>0.784403150893681</li>\n",
       "\t<li>0.108861026226306</li>\n",
       "\t<li>0.0603342786261118</li>\n",
       "\t<li>0.0107996713554488</li>\n",
       "\t<li>0.127146975736733</li>\n",
       "\t<li>0.0275128873508818</li>\n",
       "\t<li>0.526974796553326</li>\n",
       "\t<li>0.0220476828074766</li>\n",
       "\t<li>0.70057123408146</li>\n",
       "\t<li>0.00382296065149055</li>\n",
       "\t<li>0.134059162106367</li>\n",
       "\t<li>0.52661504239586</li>\n",
       "\t<li>0.236627709655711</li>\n",
       "\t<li>0.39244447464418</li>\n",
       "\t<li>0.0100239226706501</li>\n",
       "\t<li>0.444120282795447</li>\n",
       "\t<li>0.00614967158187709</li>\n",
       "\t<li>0.251881076367534</li>\n",
       "\t<li>0.276438051909961</li>\n",
       "\t<li>0.467511826672706</li>\n",
       "\t<li>0.86772384271406</li>\n",
       "\t<li>0.756015126393805</li>\n",
       "\t<li>0.664003048244369</li>\n",
       "\t<li>0.48620790605888</li>\n",
       "\t<li>0.00159154789704167</li>\n",
       "\t<li>0.797357617195317</li>\n",
       "\t<li>0.123754012887735</li>\n",
       "\t<li>0.00915150482932242</li>\n",
       "\t<li>0.0676467822304575</li>\n",
       "\t<li>0.119022463031599</li>\n",
       "\t<li>0.174293104904208</li>\n",
       "\t<li>0.0118006942527187</li>\n",
       "\t<li>0.53804781128433</li>\n",
       "\t<li>0.613603538229885</li>\n",
       "\t<li>0.119289128555705</li>\n",
       "\t<li>0.00803418840343704</li>\n",
       "\t<li>0.0138828887671821</li>\n",
       "\t<li>0.0361448537960523</li>\n",
       "\t<li>0.0429285456110043</li>\n",
       "\t<li>0.0104389900359683</li>\n",
       "\t<li>0.729467703963586</li>\n",
       "\t<li>0.459945572340207</li>\n",
       "\t<li>0.0552130150495399</li>\n",
       "\t<li>0.0367526429414071</li>\n",
       "\t<li>0.559102877454589</li>\n",
       "\t<li>0.28454384680674</li>\n",
       "\t<li>0.578174400044091</li>\n",
       "\t<li>0.00733794610488339</li>\n",
       "\t<li>0.773034341157126</li>\n",
       "\t<li>0.0123431907981087</li>\n",
       "\t<li>0.729826707497585</li>\n",
       "\t<li>0.0639325591940032</li>\n",
       "\t<li>0.00659662247244716</li>\n",
       "\t<li>0.0207966120870647</li>\n",
       "\t<li>0.698048462346474</li>\n",
       "\t<li>0.0832348719605707</li>\n",
       "\t<li>0.425743992736508</li>\n",
       "\t<li>0.0500112452744067</li>\n",
       "\t<li>0.0315595359783704</li>\n",
       "\t<li>0.0128139565343078</li>\n",
       "\t<li>0.0615207431367373</li>\n",
       "\t<li>0.0576847675261676</li>\n",
       "\t<li>0.00786206014013649</li>\n",
       "\t<li>0.236294197111729</li>\n",
       "\t<li>0.594878093697926</li>\n",
       "\t<li>0.145727523457501</li>\n",
       "\t<li>0.0107497251696278</li>\n",
       "\t<li>0.117193432905089</li>\n",
       "\t<li>0.742498161477231</li>\n",
       "\t<li>0.326045461851359</li>\n",
       "\t<li>0.0154746336086699</li>\n",
       "\t<li>0.691470479197498</li>\n",
       "\t<li>0.035472218114744</li>\n",
       "\t<li>0.3243238016297</li>\n",
       "\t<li>0.0379532609969916</li>\n",
       "\t<li>0.0162007700833543</li>\n",
       "\t<li>0.0598537057380022</li>\n",
       "\t<li>0.12993090212402</li>\n",
       "\t<li>0.055016377954057</li>\n",
       "\t<li>0.42532440282483</li>\n",
       "\t<li>0.603929006780376</li>\n",
       "\t<li>0.0417853173593943</li>\n",
       "\t<li>0.0130308787284185</li>\n",
       "\t<li>0.0213242023163649</li>\n",
       "\t<li>0.252132484152406</li>\n",
       "\t<li>0.0320103020527634</li>\n",
       "\t<li>0.046142420252636</li>\n",
       "\t<li>0.225944302645433</li>\n",
       "\t<li>0.814561937550337</li>\n",
       "\t<li>0.391556731308694</li>\n",
       "\t<li>0.00171532327448696</li>\n",
       "\t<li>0.0206665028291591</li>\n",
       "\t<li>0.682130373225299</li>\n",
       "\t<li>0.772062002357203</li>\n",
       "\t<li>0.333155318384347</li>\n",
       "\t<li>0.0165101749053747</li>\n",
       "\t<li>0.516591044093818</li>\n",
       "\t<li>0.841115959677762</li>\n",
       "\t<li>0.195898652395022</li>\n",
       "\t<li>0.0156843758209774</li>\n",
       "\t<li>0.0365511431414871</li>\n",
       "\t<li>0.41489042890736</li>\n",
       "\t<li>0.213119768257293</li>\n",
       "\t<li>0.0426793419907824</li>\n",
       "\t<li>0.252557401266546</li>\n",
       "\t<li>0.370312273922775</li>\n",
       "\t<li>0.30530040193884</li>\n",
       "\t<li>0.0870845766246387</li>\n",
       "\t<li>0.0533006539540388</li>\n",
       "\t<li>0.0517563694307511</li>\n",
       "\t<li>0.183546371926233</li>\n",
       "\t<li>0.251103787332291</li>\n",
       "\t<li>0.0477140619717467</li>\n",
       "\t<li>0.190894647316061</li>\n",
       "\t<li>0.220178971392864</li>\n",
       "\t<li>0.0152378694649064</li>\n",
       "\t<li>0.0790836143686536</li>\n",
       "\t<li>0.344426444871542</li>\n",
       "\t<li>0.0292482233504427</li>\n",
       "\t<li>0.269250724139821</li>\n",
       "\t<li>0.191362671593097</li>\n",
       "\t<li>0.205383741574171</li>\n",
       "\t<li>0.0896329720627386</li>\n",
       "\t<li>0.126061754194506</li>\n",
       "\t<li>0.136648052408895</li>\n",
       "\t<li>0.54583599187173</li>\n",
       "\t<li>0.182070444777865</li>\n",
       "\t<li>0.0276649996261271</li>\n",
       "\t<li>0.525221181201475</li>\n",
       "\t<li>0.617625570371258</li>\n",
       "\t<li>0.27138290942917</li>\n",
       "\t<li>0.364024274891242</li>\n",
       "\t<li>0.0390194583915117</li>\n",
       "\t<li>0.536313766726566</li>\n",
       "\t<li>0.316752941622576</li>\n",
       "\t<li>0.222694306314753</li>\n",
       "\t<li>0.9999999556635</li>\n",
       "\t<li>0.852776026502151</li>\n",
       "\t<li>0.173081373488694</li>\n",
       "\t<li>0.0106313525774898</li>\n",
       "\t<li>0.012958796082242</li>\n",
       "\t<li>0.605079039893046</li>\n",
       "\t<li>0.0209566285165233</li>\n",
       "\t<li>0.0117290602580811</li>\n",
       "\t<li>0.00208421400211895</li>\n",
       "\t<li>0.0474264752745083</li>\n",
       "\t<li>0.0452644869921135</li>\n",
       "\t<li>0.176047049527089</li>\n",
       "\t<li>0.288992155005493</li>\n",
       "\t<li>0.0484168059490443</li>\n",
       "\t<li>0.00935030328410211</li>\n",
       "\t<li>0.0735783355315092</li>\n",
       "\t<li>0.366352469147635</li>\n",
       "\t<li>0.411857493946701</li>\n",
       "\t<li>0.0428127447273097</li>\n",
       "\t<li>0.0466857273831911</li>\n",
       "\t<li>0.670354944651671</li>\n",
       "\t<li>0.135337592343281</li>\n",
       "\t<li>0.106430211907991</li>\n",
       "\t<li>0.430768947542698</li>\n",
       "\t<li>0.377499049946076</li>\n",
       "\t<li>0.0569596311322483</li>\n",
       "\t<li>0.0782487023213659</li>\n",
       "\t<li>0.0403133305175846</li>\n",
       "\t<li>0.146504416543012</li>\n",
       "\t<li>0.141851106686674</li>\n",
       "\t<li>0.407257277517452</li>\n",
       "\t<li>0.407308457442344</li>\n",
       "\t<li>0.375945914538592</li>\n",
       "\t<li>0.0591865119366488</li>\n",
       "\t<li>0.335869707310747</li>\n",
       "\t<li>0.373718891213413</li>\n",
       "\t<li>0.299704703366878</li>\n",
       "\t<li>0.436507471586192</li>\n",
       "\t<li>0.525239621301956</li>\n",
       "\t<li>0.748810689599713</li>\n",
       "\t<li>0.0444664072064231</li>\n",
       "\t<li>0.420971911242324</li>\n",
       "\t<li>0.479008623665864</li>\n",
       "\t<li>0.0225867450198073</li>\n",
       "\t<li>0.0336935618577512</li>\n",
       "\t<li>0.0526724333108519</li>\n",
       "\t<li>0.0921678686500227</li>\n",
       "\t<li>0.145484880868177</li>\n",
       "\t<li>0.35388327919847</li>\n",
       "\t<li>0.375723540882374</li>\n",
       "\t<li>0.0484635475531039</li>\n",
       "\t<li>0.0669329697001514</li>\n",
       "\t<li>0.275664993452405</li>\n",
       "\t<li>0.0187046100288029</li>\n",
       "\t<li>0.0373261337703594</li>\n",
       "\t<li>0.0361548542119007</li>\n",
       "\t<li>0.0180698408998763</li>\n",
       "\t<li>0.144307393591685</li>\n",
       "\t<li>0.999999765774209</li>\n",
       "\t<li>0.0874662983887632</li>\n",
       "\t<li>0.494653149058208</li>\n",
       "\t<li>0.766595370098011</li>\n",
       "\t<li>0.04937193390233</li>\n",
       "\t<li>0.0587688149414015</li>\n",
       "\t<li>0.113350902296175</li>\n",
       "\t<li>0.0641453110264208</li>\n",
       "\t<li>0.0306221130817041</li>\n",
       "\t<li>0.0364971643592527</li>\n",
       "\t<li>0.0746120938024865</li>\n",
       "\t<li>0.153070839608285</li>\n",
       "\t<li>0.0923529700092577</li>\n",
       "\t<li>0.646101390255116</li>\n",
       "\t<li>0.0161892993419189</li>\n",
       "\t<li>0.0033641288571327</li>\n",
       "\t<li>0.0304969068173873</li>\n",
       "\t<li>0.856436492437686</li>\n",
       "\t<li>0.188495916703216</li>\n",
       "\t<li>0.00534169786257923</li>\n",
       "\t<li>0.105919053645878</li>\n",
       "\t<li>0.0775022656123414</li>\n",
       "\t<li>0.883106882604116</li>\n",
       "\t<li>0.084328904048033</li>\n",
       "\t<li>0.0403205585624215</li>\n",
       "\t<li>0.0212326212804359</li>\n",
       "\t<li>0.291700811308681</li>\n",
       "\t<li>0.186420928028463</li>\n",
       "\t<li>0.372208222677709</li>\n",
       "\t<li>0.0841111297047274</li>\n",
       "\t<li>0.466491451713264</li>\n",
       "\t<li>0.123605364073339</li>\n",
       "\t<li>0.12082936013055</li>\n",
       "\t<li>0.0106835858471097</li>\n",
       "\t<li>0.139360357664107</li>\n",
       "\t<li>0.130757114898081</li>\n",
       "\t<li>0.212864098500351</li>\n",
       "\t<li>0.00337488996762014</li>\n",
       "\t<li>0.0796780446935557</li>\n",
       "\t<li>0.525696186552051</li>\n",
       "\t<li>0.23379409560094</li>\n",
       "\t<li>0.0360437163578946</li>\n",
       "\t<li>0.313261216347766</li>\n",
       "\t<li>0.0153695605345677</li>\n",
       "\t<li>0.0198241590102087</li>\n",
       "\t<li>0.0559440741759468</li>\n",
       "\t<li>0.00263272934508541</li>\n",
       "\t<li>0.0172846066915633</li>\n",
       "\t<li>0.637519180213675</li>\n",
       "\t<li>0.066087308482829</li>\n",
       "\t<li>0.766196541476464</li>\n",
       "\t<li>0.0313326585610782</li>\n",
       "\t<li>0.407844199740332</li>\n",
       "\t<li>0.0731464017427767</li>\n",
       "\t<li>0.340667917237113</li>\n",
       "\t<li>0.850718506308087</li>\n",
       "\t<li>0.0526048912150345</li>\n",
       "\t<li>0.125206536717809</li>\n",
       "\t<li>0.014018347795696</li>\n",
       "\t<li>0.380067430682013</li>\n",
       "\t<li>0.00806281398673934</li>\n",
       "\t<li>0.312416832563002</li>\n",
       "\t<li>0.516806109975632</li>\n",
       "\t<li>0.156617486628282</li>\n",
       "\t<li>0.500536052575628</li>\n",
       "\t<li>0.203053643003371</li>\n",
       "\t<li>0.606848366882435</li>\n",
       "\t<li>0.147635101544277</li>\n",
       "\t<li>0.592510764997326</li>\n",
       "\t<li>0.00268754556486882</li>\n",
       "\t<li>0.0191301065063439</li>\n",
       "\t<li>0.106800900073335</li>\n",
       "\t<li>0.385639891578442</li>\n",
       "\t<li>0.0988073904345365</li>\n",
       "\t<li>0.242881225374657</li>\n",
       "\t<li>0.102640797904684</li>\n",
       "\t<li>0.342644094226348</li>\n",
       "\t<li>0.579634240456382</li>\n",
       "\t<li>0.0880472218016967</li>\n",
       "\t<li>0.391731286741306</li>\n",
       "\t<li>0.232641098884727</li>\n",
       "\t<li>0.848298568694339</li>\n",
       "\t<li>0.236062058943315</li>\n",
       "\t<li>0.0232003913889166</li>\n",
       "\t<li>0.118158466623914</li>\n",
       "\t<li>0.530891970377014</li>\n",
       "\t<li>0.245601262968426</li>\n",
       "\t<li>0.127651060608124</li>\n",
       "\t<li>0.0566126191986855</li>\n",
       "\t<li>0.17812038081585</li>\n",
       "\t<li>0.944361527376599</li>\n",
       "\t<li>0.0628925297638546</li>\n",
       "\t<li>0.607099296657314</li>\n",
       "\t<li>0.00647036352417058</li>\n",
       "\t<li>0.014506728146203</li>\n",
       "\t<li>0.52762505968881</li>\n",
       "\t<li>0.00803542976488018</li>\n",
       "\t<li>0.55339847312494</li>\n",
       "\t<li>0.125021655152885</li>\n",
       "\t<li>0.0122782554025418</li>\n",
       "\t<li>0.721077080670495</li>\n",
       "\t<li>0.203762662847281</li>\n",
       "\t<li>0.00319297473404992</li>\n",
       "\t<li>0.248570352049798</li>\n",
       "\t<li>0.29862730608033</li>\n",
       "\t<li>0.999999668512615</li>\n",
       "\t<li>0.736981259442357</li>\n",
       "\t<li>0.0441224070278956</li>\n",
       "\t<li>0.0221582979391203</li>\n",
       "\t<li>0.189982247487777</li>\n",
       "\t<li>0.894282444692239</li>\n",
       "\t<li>0.0437765767775495</li>\n",
       "\t<li>0.367857676672516</li>\n",
       "\t<li>0.172466364280629</li>\n",
       "\t<li>0.0243349213940347</li>\n",
       "\t<li>0.0175794820056814</li>\n",
       "\t<li>0.452609346022816</li>\n",
       "\t<li>0.137613484637788</li>\n",
       "\t<li>0.656618216663544</li>\n",
       "\t<li>0.0781637715273055</li>\n",
       "\t<li>0.0246332312798398</li>\n",
       "\t<li>0.571694330649898</li>\n",
       "\t<li>0.016002881459772</li>\n",
       "\t<li>0.199758297954567</li>\n",
       "\t<li>0.601404930111302</li>\n",
       "\t<li>0.217279361750711</li>\n",
       "\t<li>0.00286133259752111</li>\n",
       "\t<li>0.00527884708402513</li>\n",
       "\t<li>0.0825537081087439</li>\n",
       "\t<li>0.899053576453538</li>\n",
       "\t<li>0.0498238978185774</li>\n",
       "\t<li>0.00999921756695854</li>\n",
       "\t<li>0.0286649510015706</li>\n",
       "\t<li>0.67590343466665</li>\n",
       "\t<li>0.100243824833728</li>\n",
       "\t<li>0.404647265904865</li>\n",
       "\t<li>0.0132329010918809</li>\n",
       "\t<li>0.622210290276593</li>\n",
       "\t<li>0.187974394598899</li>\n",
       "\t<li>0.75914933789834</li>\n",
       "\t<li>0.46460801488613</li>\n",
       "\t<li>0.0744935782972362</li>\n",
       "\t<li>0.016853624195665</li>\n",
       "\t<li>0.940676545822903</li>\n",
       "\t<li>0.0102856266755571</li>\n",
       "\t<li>0.801658041621613</li>\n",
       "\t<li>0.0477185915405782</li>\n",
       "\t<li>0.0732503424959294</li>\n",
       "\t<li>0.0302669930852424</li>\n",
       "\t<li>0.0280085447009265</li>\n",
       "\t<li>0.013871306182459</li>\n",
       "\t<li>0.0647383577023884</li>\n",
       "\t<li>0.00811055369306502</li>\n",
       "\t<li>0.29543868779346</li>\n",
       "\t<li>0.354904215660545</li>\n",
       "\t<li>0.0951801706426184</li>\n",
       "\t<li>0.0204777669926373</li>\n",
       "\t<li>0.677036779526389</li>\n",
       "\t<li>0.20985532459379</li>\n",
       "\t<li>0.0483360111963359</li>\n",
       "\t<li>0.0334912135273815</li>\n",
       "\t<li>0.0208440532932362</li>\n",
       "\t<li>0.162408618117541</li>\n",
       "\t<li>0.0342050573802004</li>\n",
       "\t<li>0.0302878476153293</li>\n",
       "\t<li>0.7100987248817</li>\n",
       "\t<li>0.381194836611222</li>\n",
       "\t<li>0.0094529771604589</li>\n",
       "\t<li>0.0402290172810124</li>\n",
       "\t<li>0.58695037075786</li>\n",
       "\t<li>0.00321619240820627</li>\n",
       "\t<li>0.172097600353855</li>\n",
       "\t<li>0.387063094017129</li>\n",
       "\t<li>0.00574809919706981</li>\n",
       "\t<li>0.632587707744512</li>\n",
       "\t<li>0.0739718968498494</li>\n",
       "\t<li>0.00075272394936441</li>\n",
       "\t<li>0.192655555635881</li>\n",
       "\t<li>0.00694355381487038</li>\n",
       "\t<li>0.400550517664137</li>\n",
       "\t<li>0.625299783731324</li>\n",
       "\t<li>0.503589400637887</li>\n",
       "\t<li>0.015247599790401</li>\n",
       "\t<li>0.966539426126986</li>\n",
       "\t<li>0.592102754547421</li>\n",
       "\t<li>0.181275564495835</li>\n",
       "\t<li>0.131093707166075</li>\n",
       "\t<li>0.0657437081635146</li>\n",
       "\t<li>0.983885077238754</li>\n",
       "\t<li>0.0408730272219029</li>\n",
       "\t<li>0.0569603826233853</li>\n",
       "\t<li>0.0355543301314578</li>\n",
       "\t<li>0.0325323960705489</li>\n",
       "\t<li>0.498212231811771</li>\n",
       "\t<li>0.612572923896403</li>\n",
       "\t<li>0.0079727847894024</li>\n",
       "\t<li>0.460081353769792</li>\n",
       "\t<li>0.0119379321131558</li>\n",
       "\t<li>0.0346133714336782</li>\n",
       "\t<li>0.0188672155751794</li>\n",
       "\t<li>0.810873556214912</li>\n",
       "\t<li>0.401400629063512</li>\n",
       "\t<li>0.276390544212923</li>\n",
       "\t<li>0.153843597894376</li>\n",
       "\t<li>0.060038027606361</li>\n",
       "\t<li>0.017804359463568</li>\n",
       "\t<li>0.265074272736638</li>\n",
       "\t<li>0.0277494323330679</li>\n",
       "\t<li>0.0455792688099718</li>\n",
       "\t<li>0.0251425224731764</li>\n",
       "\t<li>0.269636746779349</li>\n",
       "\t<li>0.289120442725711</li>\n",
       "\t<li>0.0064975369843315</li>\n",
       "\t<li>0.147841056205854</li>\n",
       "\t<li>0.0495796982764737</li>\n",
       "\t<li>0.654069287569774</li>\n",
       "\t<li>0.248166936639415</li>\n",
       "\t<li>0.0368513111243656</li>\n",
       "\t<li>0.19111719200314</li>\n",
       "\t<li>0.370488328776153</li>\n",
       "\t<li>0.471546277126646</li>\n",
       "\t<li>0.383419329479009</li>\n",
       "\t<li>0.284179548560016</li>\n",
       "\t<li>0.60487845731714</li>\n",
       "\t<li>0.214259307519855</li>\n",
       "\t<li>0.0590188301682051</li>\n",
       "\t<li>0.020525284866343</li>\n",
       "\t<li>0.0246309874298711</li>\n",
       "\t<li>0.0127786609236949</li>\n",
       "\t<li>0.313523041958294</li>\n",
       "\t<li>0.311842939913688</li>\n",
       "\t<li>0.101597741560868</li>\n",
       "\t<li>0.603513866791461</li>\n",
       "\t<li>0.00479214537332067</li>\n",
       "\t<li>0.117154947437688</li>\n",
       "\t<li>0.219800049557997</li>\n",
       "\t<li>0.0171034990162961</li>\n",
       "\t<li>0.540297716322392</li>\n",
       "\t<li>0.0689034896757411</li>\n",
       "\t<li>0.01820575584003</li>\n",
       "\t<li>0.00227838794919667</li>\n",
       "\t<li>0.0023905725602704</li>\n",
       "\t<li>0.373918558449697</li>\n",
       "\t<li>0.0123804855931148</li>\n",
       "\t<li>0.00342162955273444</li>\n",
       "\t<li>0.0454900500367966</li>\n",
       "\t<li>0.614405940171795</li>\n",
       "\t<li>0.0344217594887273</li>\n",
       "\t<li>0.14507590877353</li>\n",
       "\t<li>0.539178293519048</li>\n",
       "\t<li>0.0254165815037278</li>\n",
       "\t<li>0.054414746981464</li>\n",
       "\t<li>0.322385011952746</li>\n",
       "\t<li>0.923803474700752</li>\n",
       "\t<li>0.0395919920019561</li>\n",
       "\t<li>0.0650213372766423</li>\n",
       "\t<li>0.00324105580869746</li>\n",
       "\t<li>0.0973600067040383</li>\n",
       "\t<li>0.345475723251949</li>\n",
       "\t<li>0.628954732600845</li>\n",
       "\t<li>0.0315861561523127</li>\n",
       "\t<li>0.0162456139902508</li>\n",
       "\t<li>0.829958584056793</li>\n",
       "\t<li>0.0157824164366038</li>\n",
       "\t<li>0.193132686377787</li>\n",
       "\t<li>0.0817198656589979</li>\n",
       "\t<li>0.934610357737708</li>\n",
       "\t<li>0.170657763868179</li>\n",
       "\t<li>0.620558604533632</li>\n",
       "\t<li>0.704158168790487</li>\n",
       "\t<li>0.379998434870672</li>\n",
       "\t<li>0.302483219838451</li>\n",
       "\t<li>0.112226301412075</li>\n",
       "\t<li>0.0930631664990264</li>\n",
       "\t<li>0.00455253010717648</li>\n",
       "\t<li>0.138205228973645</li>\n",
       "\t<li>0.0922202816433377</li>\n",
       "\t<li>0.00626807308392644</li>\n",
       "\t<li>0.145079792273125</li>\n",
       "\t<li>0.642471744984563</li>\n",
       "\t<li>0.525146730827957</li>\n",
       "\t<li>0.272499556188591</li>\n",
       "\t<li>0.0112859975074698</li>\n",
       "\t<li>0.215974245773398</li>\n",
       "\t<li>0.119223020522359</li>\n",
       "\t<li>0.0108070503893455</li>\n",
       "\t<li>0.031710100455674</li>\n",
       "\t<li>0.250448896349346</li>\n",
       "\t<li>0.059307305251946</li>\n",
       "\t<li>0.00557992835427086</li>\n",
       "\t<li>0.0550400837910643</li>\n",
       "\t<li>0.0299165091943819</li>\n",
       "\t<li>0.0245991091285857</li>\n",
       "\t<li>0.0908313575875495</li>\n",
       "\t<li>0.102443088020665</li>\n",
       "\t<li>0.529037659511435</li>\n",
       "\t<li>0.212354389990612</li>\n",
       "\t<li>0.0735288174845334</li>\n",
       "\t<li>0.0277750722591762</li>\n",
       "\t<li>0.430901572274588</li>\n",
       "\t<li>0.378389026351761</li>\n",
       "\t<li>0.252816928381743</li>\n",
       "\t<li>0.218371966458501</li>\n",
       "\t<li>0.511594920477143</li>\n",
       "\t<li>0.426762267196003</li>\n",
       "\t<li>0.36313913607403</li>\n",
       "\t<li>0.229652722872118</li>\n",
       "\t<li>0.999982909447387</li>\n",
       "\t<li>0.00573476837917188</li>\n",
       "\t<li>0.0221284190372738</li>\n",
       "\t<li>0.473720125868222</li>\n",
       "\t<li>0.218775520151417</li>\n",
       "\t<li>0.153918079075086</li>\n",
       "\t<li>0.570078205080328</li>\n",
       "\t<li>0.00673497966191684</li>\n",
       "\t<li>0.0108676534344724</li>\n",
       "\t<li>0.584801385624825</li>\n",
       "\t<li>0.557958525617629</li>\n",
       "\t<li>0.127662345311259</li>\n",
       "\t<li>0.00759444902871862</li>\n",
       "\t<li>0.734756665300793</li>\n",
       "\t<li>0.286417672352117</li>\n",
       "\t<li>0.444254734078745</li>\n",
       "\t<li>0.0124171302645206</li>\n",
       "\t<li>0.394702502404232</li>\n",
       "\t<li>0.186638748017359</li>\n",
       "\t<li>0.169808008391833</li>\n",
       "\t<li>0.0416221124213114</li>\n",
       "\t<li>0.80069246464988</li>\n",
       "\t<li>0.0739007862256507</li>\n",
       "\t<li>0.18201739186026</li>\n",
       "\t<li>0.203678734082994</li>\n",
       "\t<li>0.593907026751536</li>\n",
       "\t<li>0.305569586998538</li>\n",
       "\t<li>0.263643382196554</li>\n",
       "\t<li>0.0134936619851492</li>\n",
       "\t<li>0.208820994640213</li>\n",
       "\t<li>0.0549664596862607</li>\n",
       "\t<li>0.0286693218212616</li>\n",
       "\t<li>0.541920854508511</li>\n",
       "\t<li>0.390828504609042</li>\n",
       "\t<li>0.0336965980949221</li>\n",
       "\t<li>0.364172401964305</li>\n",
       "\t<li>0.051075154008684</li>\n",
       "\t<li>0.094488609256491</li>\n",
       "\t<li>0.0313731202656018</li>\n",
       "\t<li>0.0995308146759852</li>\n",
       "\t<li>0.0554973631729953</li>\n",
       "\t<li>0.152083172205623</li>\n",
       "\t<li>0.722991566287879</li>\n",
       "\t<li>0.189711784402361</li>\n",
       "\t<li>0.547964874321728</li>\n",
       "\t<li>0.688318268307373</li>\n",
       "\t<li>0.462524239085874</li>\n",
       "\t<li>0.0258162878044023</li>\n",
       "\t<li>0.267599346710097</li>\n",
       "\t<li>0.0912615391614897</li>\n",
       "\t<li>0.421269112895809</li>\n",
       "\t<li>0.951909913594931</li>\n",
       "\t<li>0.0106417124853625</li>\n",
       "\t<li>0.0147647082047271</li>\n",
       "\t<li>0.399001617248235</li>\n",
       "\t<li>0.3652506032079</li>\n",
       "\t<li>0.359153996941063</li>\n",
       "\t<li>0.52293226570314</li>\n",
       "\t<li>0.931774186479659</li>\n",
       "\t<li>0.045009603948656</li>\n",
       "\t<li>0.118841196690333</li>\n",
       "\t<li>0.225255072158649</li>\n",
       "\t<li>0.509376765758929</li>\n",
       "\t<li>0.217679294960974</li>\n",
       "\t<li>0.013327865518525</li>\n",
       "\t<li>0.0165579512922611</li>\n",
       "\t<li>0.60389610576593</li>\n",
       "\t<li>0.569661028875765</li>\n",
       "\t<li>0.368408491903595</li>\n",
       "\t<li>0.0154710552313364</li>\n",
       "\t<li>0.275897407486218</li>\n",
       "\t<li>0.502188344505214</li>\n",
       "\t<li>0.0787388213398721</li>\n",
       "\t<li>0.0967300618243921</li>\n",
       "\t<li>0.00494883580375058</li>\n",
       "\t<li>0.0641508423486803</li>\n",
       "\t<li>0.0306752135803522</li>\n",
       "\t<li>0.402309389714621</li>\n",
       "\t<li>0.00383458180257253</li>\n",
       "\t<li>0.022162456940421</li>\n",
       "\t<li>0.00835678330015797</li>\n",
       "\t<li>0.0426203782200652</li>\n",
       "\t<li>0.549851594086313</li>\n",
       "\t<li>0.0474251305226223</li>\n",
       "\t<li>0.67539938725115</li>\n",
       "\t<li>0.0204900547140519</li>\n",
       "\t<li>0.549072635790886</li>\n",
       "\t<li>0.516366445615041</li>\n",
       "\t<li>0.0193906860951293</li>\n",
       "\t<li>0.152919852151923</li>\n",
       "\t<li>0.0115180746729149</li>\n",
       "\t<li>0.0131243438935188</li>\n",
       "\t<li>0.0669864820543327</li>\n",
       "\t<li>0.0770191340567978</li>\n",
       "\t<li>0.0337849859576272</li>\n",
       "\t<li>0.584499862875758</li>\n",
       "\t<li>0.174133168075228</li>\n",
       "\t<li>0.0168523973033843</li>\n",
       "\t<li>0.036492761553111</li>\n",
       "\t<li>0.03296429270335</li>\n",
       "\t<li>0.13522573705344</li>\n",
       "\t<li>0.441435132119064</li>\n",
       "\t<li>0.0631999099092396</li>\n",
       "\t<li>0.443591054899236</li>\n",
       "\t<li>0.820625405561113</li>\n",
       "\t<li>0.14760614406295</li>\n",
       "\t<li>0.00921031969447578</li>\n",
       "\t<li>0.0407275279789619</li>\n",
       "\t<li>0.496519971203934</li>\n",
       "\t<li>0.352683457730475</li>\n",
       "\t<li>0.0337052154495489</li>\n",
       "\t<li>0.041021725067448</li>\n",
       "\t<li>0.657159124804541</li>\n",
       "\t<li>0.674788662579156</li>\n",
       "\t<li>0.589624181563565</li>\n",
       "\t<li>0.0378380543825788</li>\n",
       "\t<li>0.770466683785002</li>\n",
       "\t<li>0.312030005477519</li>\n",
       "\t<li>0.0203791415008888</li>\n",
       "\t<li>0.25260727593978</li>\n",
       "\t<li>0.0370764573951389</li>\n",
       "\t<li>0.010213880400914</li>\n",
       "\t<li>0.379636352117403</li>\n",
       "\t<li>0.00403537276895975</li>\n",
       "\t<li>0.108292436662199</li>\n",
       "\t<li>0.249821608841375</li>\n",
       "\t<li>0.342332203115704</li>\n",
       "\t<li>0.584098659595383</li>\n",
       "\t<li>0.0439314807877648</li>\n",
       "\t<li>0.387596786008076</li>\n",
       "\t<li>0.0138209702150517</li>\n",
       "\t<li>0.260108634346888</li>\n",
       "\t<li>0.669261470397866</li>\n",
       "\t<li>0.0102872462659718</li>\n",
       "\t<li>0.123577193624098</li>\n",
       "\t<li>0.0234988787420555</li>\n",
       "\t<li>0.00315546696297043</li>\n",
       "\t<li>0.0123239365043939</li>\n",
       "\t<li>0.041466787478148</li>\n",
       "\t<li>0.307849208920438</li>\n",
       "\t<li>0.0981298036112295</li>\n",
       "\t<li>0.0118130657382479</li>\n",
       "\t<li>0.266928381781508</li>\n",
       "\t<li>0.0494469030388882</li>\n",
       "\t<li>0.180771557197106</li>\n",
       "\t<li>0.00582339154323416</li>\n",
       "\t<li>0.58083375330643</li>\n",
       "\t<li>0.0246091628848339</li>\n",
       "\t<li>0.0564855465442005</li>\n",
       "\t<li>0.644209414662118</li>\n",
       "\t<li>0.289100550018003</li>\n",
       "\t<li>0.0938396094680775</li>\n",
       "\t<li>0.00385549300521934</li>\n",
       "\t<li>0.0378153241286041</li>\n",
       "\t<li>0.363537742091485</li>\n",
       "\t<li>0.447053123345453</li>\n",
       "\t<li>0.18576784797277</li>\n",
       "\t<li>0.387679087563473</li>\n",
       "\t<li>0.691089208128983</li>\n",
       "\t<li>0.0383015816281294</li>\n",
       "\t<li>0.094289063137743</li>\n",
       "\t<li>0.0530877723967058</li>\n",
       "\t<li>0.979161767606413</li>\n",
       "\t<li>0.430368104133677</li>\n",
       "\t<li>0.0541403278161665</li>\n",
       "\t<li>0.0270043072110092</li>\n",
       "\t<li>0.512078551749623</li>\n",
       "\t<li>0.00480679602286807</li>\n",
       "\t<li>0.200703002730321</li>\n",
       "\t<li>0.683521656236424</li>\n",
       "\t<li>0.0471513790862581</li>\n",
       "\t<li>0.00142491766924496</li>\n",
       "\t<li>0.580801622126068</li>\n",
       "\t<li>0.432030142618772</li>\n",
       "\t<li>0.952026895787423</li>\n",
       "\t<li>0.283430277644257</li>\n",
       "\t<li>0.210257501672467</li>\n",
       "\t<li>0.197776730461394</li>\n",
       "\t<li>0.0314478917634595</li>\n",
       "\t<li>0.616894050279868</li>\n",
       "\t<li>0.223820706046476</li>\n",
       "\t<li>0.816932175283422</li>\n",
       "\t<li>0.0237137255225264</li>\n",
       "\t<li>0.525538330221074</li>\n",
       "\t<li>0.899365439141373</li>\n",
       "\t<li>0.648562218250217</li>\n",
       "\t<li>0.0413256660936873</li>\n",
       "\t<li>0.0530809885017832</li>\n",
       "\t<li>0.0371021460691384</li>\n",
       "\t<li>0.666036891337846</li>\n",
       "\t<li>0.0503403084487381</li>\n",
       "\t<li>0.216736085460578</li>\n",
       "\t<li>0.529816914001044</li>\n",
       "\t<li>0.0170300823100406</li>\n",
       "\t<li>0.845662369979206</li>\n",
       "\t<li>0.286608517639903</li>\n",
       "\t<li>0.0453429413868556</li>\n",
       "\t<li>0.0168998093054599</li>\n",
       "\t<li>0.0309378666679549</li>\n",
       "\t<li>0.460958104606825</li>\n",
       "\t<li>0.00168174614456759</li>\n",
       "\t<li>0.0575316655816865</li>\n",
       "\t<li>0.0295832576561265</li>\n",
       "\t<li>0.30211582040186</li>\n",
       "\t<li>0.359100467014012</li>\n",
       "\t<li>0.0142723785962464</li>\n",
       "\t<li>0.152992139306264</li>\n",
       "\t<li>0.290629304585038</li>\n",
       "\t<li>0.34477857883198</li>\n",
       "\t<li>0.0639895413299661</li>\n",
       "\t<li>0.0723829748932035</li>\n",
       "\t<li>0.00956408821996863</li>\n",
       "\t<li>0.0129948419248106</li>\n",
       "\t<li>0.0896109652454796</li>\n",
       "\t<li>0.218414632305991</li>\n",
       "\t<li>0.395518672176483</li>\n",
       "\t<li>0.164719917850441</li>\n",
       "\t<li>0.337378224554789</li>\n",
       "\t<li>0.409249367689963</li>\n",
       "\t<li>0.750958644764419</li>\n",
       "\t<li>0.993024241858559</li>\n",
       "\t<li>0.552509093205434</li>\n",
       "\t<li>0.300306271671101</li>\n",
       "\t<li>0.144804852792329</li>\n",
       "\t<li>0.0986315585450896</li>\n",
       "\t<li>0.170781558190222</li>\n",
       "\t<li>0.0388472689638898</li>\n",
       "\t<li>0.942579881928659</li>\n",
       "\t<li>0.100843439938957</li>\n",
       "\t<li>0.385625233480742</li>\n",
       "\t<li>0.511611566425847</li>\n",
       "\t<li>0.620134773196079</li>\n",
       "\t<li>0.214033042597848</li>\n",
       "\t<li>0.158314813526888</li>\n",
       "\t<li>0.129795050606655</li>\n",
       "\t<li>0.109386720230056</li>\n",
       "\t<li>0.0981704340271325</li>\n",
       "\t<li>0.175908978783608</li>\n",
       "\t<li>0.109022694256191</li>\n",
       "\t<li>0.228573754468922</li>\n",
       "\t<li>0.0902800566130318</li>\n",
       "\t<li>0.56496302270547</li>\n",
       "\t<li>0.00655918326418388</li>\n",
       "\t<li>0.253354366901994</li>\n",
       "\t<li>0.603017468139228</li>\n",
       "\t<li>0.0347655953967357</li>\n",
       "\t<li>0.10580000118631</li>\n",
       "\t<li>0.0083025719003252</li>\n",
       "\t<li>0.219575793226539</li>\n",
       "\t<li>0.662334978091723</li>\n",
       "\t<li>0.0192483334061113</li>\n",
       "\t<li>0.738566013046589</li>\n",
       "\t<li>0.777172976452723</li>\n",
       "\t<li>0.432590569735931</li>\n",
       "\t<li>0.0254074775950213</li>\n",
       "\t<li>0.0317159200826119</li>\n",
       "\t<li>0.599506332897089</li>\n",
       "\t<li>0.586511967250636</li>\n",
       "\t<li>0.621486116547452</li>\n",
       "\t<li>0.0115517258002166</li>\n",
       "\t<li>0.0605978514231961</li>\n",
       "\t<li>0.639683791847728</li>\n",
       "\t<li>0.401322189073391</li>\n",
       "\t<li>0.99999992964077</li>\n",
       "\t<li>0.0168575565473987</li>\n",
       "\t<li>0.00370177835154455</li>\n",
       "\t<li>0.458223529318648</li>\n",
       "\t<li>0.173668631513075</li>\n",
       "\t<li>0.154046361181832</li>\n",
       "\t<li>0.536322719123752</li>\n",
       "\t<li>0.915712765181158</li>\n",
       "\t<li>0.165488761489117</li>\n",
       "\t<li>0.0650764613495248</li>\n",
       "\t<li>0.819550839293484</li>\n",
       "\t<li>0.101598300655427</li>\n",
       "\t<li>0.0656982498830971</li>\n",
       "\t<li>0.321449929578181</li>\n",
       "\t<li>0.044652300764393</li>\n",
       "\t<li>0.0308784400477463</li>\n",
       "\t<li>0.286529895034282</li>\n",
       "\t<li>0.313468252418951</li>\n",
       "\t<li>0.320396366228584</li>\n",
       "\t<li>0.315096211324578</li>\n",
       "\t<li>0.0094626318285661</li>\n",
       "\t<li>0.234500854469321</li>\n",
       "\t<li>0.0485966680983412</li>\n",
       "\t<li>0.0306373039133289</li>\n",
       "\t<li>0.999999939875605</li>\n",
       "\t<li>0.876353206266361</li>\n",
       "\t<li>0.758531543035791</li>\n",
       "\t<li>0.222113709306038</li>\n",
       "\t<li>0.201757308451926</li>\n",
       "\t<li>0.111621401533538</li>\n",
       "\t<li>0.740042850946609</li>\n",
       "\t<li>0.189892130939609</li>\n",
       "\t<li>0.370649402159646</li>\n",
       "\t<li>0.0136711727458992</li>\n",
       "\t<li>0.0347085172703979</li>\n",
       "\t<li>0.423681544014339</li>\n",
       "\t<li>0.017429461665319</li>\n",
       "\t<li>0.0110954028292281</li>\n",
       "\t<li>0.116377241092142</li>\n",
       "\t<li>0.00382670423927343</li>\n",
       "\t<li>0.0217295531117031</li>\n",
       "\t<li>0.0461711130922531</li>\n",
       "\t<li>0.192904837551817</li>\n",
       "\t<li>0.254077723904435</li>\n",
       "\t<li>0.0211117847937291</li>\n",
       "\t<li>0.442869254979738</li>\n",
       "\t<li>0.578907224354884</li>\n",
       "\t<li>0.124759142525714</li>\n",
       "\t<li>0.459858852421076</li>\n",
       "\t<li>0.531920055779343</li>\n",
       "\t<li>0.413125221294555</li>\n",
       "\t<li>0.234420446840034</li>\n",
       "\t<li>0.021949157617395</li>\n",
       "\t<li>0.864688431053634</li>\n",
       "\t<li>0.203674400884231</li>\n",
       "\t<li>0.190250163913924</li>\n",
       "\t<li>0.570898236070186</li>\n",
       "\t<li>0.752585899180292</li>\n",
       "\t<li>0.525509188624906</li>\n",
       "\t<li>0.282242331063523</li>\n",
       "\t<li>0.572963794309999</li>\n",
       "\t<li>0.745021396124277</li>\n",
       "\t<li>0.241342983100793</li>\n",
       "\t<li>0.127341682541055</li>\n",
       "\t<li>0.295692908843781</li>\n",
       "\t<li>0.216927415072056</li>\n",
       "\t<li>0.00354988961628633</li>\n",
       "\t<li>0.00906657281069526</li>\n",
       "\t<li>0.0134610499532325</li>\n",
       "\t<li>0.032610587379341</li>\n",
       "\t<li>0.212721050174182</li>\n",
       "\t<li>0.0336941251163405</li>\n",
       "\t<li>0.845287429391015</li>\n",
       "\t<li>0.0772480311694506</li>\n",
       "\t<li>0.0415381055168681</li>\n",
       "\t<li>0.00294287677568198</li>\n",
       "\t<li>0.385000324228021</li>\n",
       "\t<li>0.0189692585269595</li>\n",
       "\t<li>0.10563684310018</li>\n",
       "\t<li>0.41006316102086</li>\n",
       "\t<li>0.206635460236618</li>\n",
       "\t<li>0.144453713311913</li>\n",
       "\t<li>0.999999930693917</li>\n",
       "\t<li>0.802362441796549</li>\n",
       "\t<li>0.0570869712885768</li>\n",
       "\t<li>0.0062163176607514</li>\n",
       "\t<li>0.022757733737513</li>\n",
       "\t<li>0.135026298725444</li>\n",
       "\t<li>0.0706772612898688</li>\n",
       "\t<li>0.0149102391900019</li>\n",
       "\t<li>0.530340540229543</li>\n",
       "\t<li>0.0700719598294253</li>\n",
       "\t<li>0.619797393031223</li>\n",
       "\t<li>0.249289951159632</li>\n",
       "\t<li>0.214522047115071</li>\n",
       "\t<li>0.241476905602369</li>\n",
       "\t<li>0.581025331166739</li>\n",
       "\t<li>0.229537729165737</li>\n",
       "\t<li>0.0156763071588183</li>\n",
       "\t<li>0.00838477061773456</li>\n",
       "\t<li>0.0112938677246485</li>\n",
       "\t<li>0.00922797566104582</li>\n",
       "\t<li>0.182233202279052</li>\n",
       "\t<li>0.186647817566892</li>\n",
       "\t<li>0.637529836804787</li>\n",
       "\t<li>0.0651077506175047</li>\n",
       "\t<li>0.00615605951304662</li>\n",
       "\t<li>0.0498163662896792</li>\n",
       "\t<li>0.717465430491587</li>\n",
       "\t<li>0.740822422748179</li>\n",
       "\t<li>0.320125745668472</li>\n",
       "\t<li>0.430572704256078</li>\n",
       "\t<li>0.0464950395948776</li>\n",
       "\t<li>0.0260596362568755</li>\n",
       "\t<li>0.578887276573786</li>\n",
       "\t<li>0.0175645577515171</li>\n",
       "\t<li>0.103409777760653</li>\n",
       "\t<li>0.85426144584604</li>\n",
       "\t<li>0.435147639488308</li>\n",
       "\t<li>0.586469146015563</li>\n",
       "\t<li>0.044495562481657</li>\n",
       "\t<li>0.0446792253207579</li>\n",
       "\t<li>0.0200192340447763</li>\n",
       "\t<li>0.202046766133388</li>\n",
       "\t<li>0.0197862102524262</li>\n",
       "\t<li>0.866168370910104</li>\n",
       "\t<li>0.220309653898714</li>\n",
       "\t<li>0.367168500933266</li>\n",
       "\t<li>0.095357834961527</li>\n",
       "\t<li>0.238694714569209</li>\n",
       "\t<li>0.0241320906170692</li>\n",
       "\t<li>0.0328750065959386</li>\n",
       "\t<li>0.377486053253873</li>\n",
       "\t<li>0.00395549132352947</li>\n",
       "\t<li>0.00527288751066556</li>\n",
       "\t<li>0.235086477165255</li>\n",
       "\t<li>0.0144739114041722</li>\n",
       "\t<li>0.00635522691008368</li>\n",
       "\t<li>0.183120070641252</li>\n",
       "\t<li>0.482654739214698</li>\n",
       "\t<li>0.253982616107924</li>\n",
       "\t<li>0.269642814761059</li>\n",
       "\t<li>0.174129800510415</li>\n",
       "\t<li>0.285165538749012</li>\n",
       "\t<li>0.00166446284316019</li>\n",
       "\t<li>0.428576756148541</li>\n",
       "\t<li>0.864750745392298</li>\n",
       "\t<li>0.262160250272491</li>\n",
       "\t<li>0.775361391122565</li>\n",
       "\t<li>0.437557998593944</li>\n",
       "\t<li>0.100396201320877</li>\n",
       "\t<li>0.0625584857790366</li>\n",
       "\t<li>0.0148640217706099</li>\n",
       "\t<li>0.289021388332522</li>\n",
       "\t<li>0.00734551191106758</li>\n",
       "\t<li>0.475649970566081</li>\n",
       "\t<li>0.788356995517267</li>\n",
       "\t<li>0.610339466071035</li>\n",
       "\t<li>0.0855946833948425</li>\n",
       "\t<li>0.016886262585918</li>\n",
       "\t<li>0.359334891770436</li>\n",
       "\t<li>0.531629040085054</li>\n",
       "\t<li>0.012570018880217</li>\n",
       "\t<li>0.401280961903602</li>\n",
       "\t<li>0.285800420983862</li>\n",
       "\t<li>0.999999899391652</li>\n",
       "\t<li>0.0888033516629491</li>\n",
       "\t<li>0.679274958966115</li>\n",
       "\t<li>0.699092361664451</li>\n",
       "\t<li>0.786369010386977</li>\n",
       "\t<li>0.0329311038983416</li>\n",
       "\t<li>0.291024670329289</li>\n",
       "\t<li>0.149266119225962</li>\n",
       "\t<li>0.28157247532322</li>\n",
       "\t<li>0.357494012178614</li>\n",
       "\t<li>0.0182277356518192</li>\n",
       "\t<li>0.925413282505606</li>\n",
       "\t<li>0.139220916203728</li>\n",
       "\t<li>0.0125064075422918</li>\n",
       "\t<li>0.213127628749023</li>\n",
       "\t<li>0.508402524250166</li>\n",
       "\t<li>0.126469982458178</li>\n",
       "\t<li>0.264497391266</li>\n",
       "\t<li>0.436664519151672</li>\n",
       "\t<li>0.179911045434649</li>\n",
       "\t<li>0.0675101178974709</li>\n",
       "\t<li>0.469758723503266</li>\n",
       "\t<li>0.00108476919380634</li>\n",
       "\t<li>0.0502871692102015</li>\n",
       "\t<li>0.0633866987054689</li>\n",
       "\t<li>0.119501709935923</li>\n",
       "\t<li>0.466676158912044</li>\n",
       "\t<li>0.310300878091165</li>\n",
       "\t<li>0.999999147447549</li>\n",
       "\t<li>0.0862621250468959</li>\n",
       "\t<li>0.589699002573824</li>\n",
       "\t<li>0.0340958106540846</li>\n",
       "\t<li>0.208527613932907</li>\n",
       "\t<li>0.14542061372271</li>\n",
       "\t<li>0.0629816046156896</li>\n",
       "\t<li>0.840423979594436</li>\n",
       "\t<li>0.554403333439654</li>\n",
       "\t<li>0.246514541512246</li>\n",
       "\t<li>0.0041174717361653</li>\n",
       "\t<li>0.0107919343847855</li>\n",
       "\t<li>0.0184330466702109</li>\n",
       "\t<li>0.030832747655235</li>\n",
       "\t<li>0.99999996259574</li>\n",
       "\t<li>0.36480875288212</li>\n",
       "\t<li>0.0386046069383293</li>\n",
       "\t<li>0.0668992513567373</li>\n",
       "\t<li>0.0647919048913934</li>\n",
       "\t<li>0.144246540228767</li>\n",
       "\t<li>0.143138610278439</li>\n",
       "\t<li>0.0879836958120621</li>\n",
       "\t<li>0.332259734391238</li>\n",
       "\t<li>0.0319947834971824</li>\n",
       "\t<li>0.344343915942589</li>\n",
       "\t<li>0.010622928800904</li>\n",
       "\t<li>0.56482551860699</li>\n",
       "\t<li>0.0209093634221607</li>\n",
       "\t<li>0.0042353714747113</li>\n",
       "\t<li>0.719543640498619</li>\n",
       "\t<li>0.51695317027421</li>\n",
       "\t<li>0.110624286226349</li>\n",
       "\t<li>0.0417178617689054</li>\n",
       "\t<li>0.0298749809326296</li>\n",
       "\t<li>0.0875270512091124</li>\n",
       "\t<li>0.312629836641127</li>\n",
       "\t<li>0.0135589788440718</li>\n",
       "\t<li>0.322783229655624</li>\n",
       "\t<li>0.0725996861118289</li>\n",
       "\t<li>0.00693125984291952</li>\n",
       "\t<li>0.0524067668501133</li>\n",
       "\t<li>0.351444765477373</li>\n",
       "\t<li>0.108756601726957</li>\n",
       "\t<li>0.483537054294353</li>\n",
       "\t<li>0.804947871059299</li>\n",
       "\t<li>0.0660140388927749</li>\n",
       "\t<li>0.64819436885816</li>\n",
       "\t<li>0.238233231595257</li>\n",
       "\t<li>0.478779491874137</li>\n",
       "\t<li>0.596184681051985</li>\n",
       "\t<li>0.0774260173405864</li>\n",
       "\t<li>0.392008998828665</li>\n",
       "\t<li>0.158985831577443</li>\n",
       "\t<li>0.0689556610031463</li>\n",
       "\t<li>0.131936731389168</li>\n",
       "\t<li>0.379500234156237</li>\n",
       "\t<li>0.505233325002846</li>\n",
       "\t<li>0.0805139074077529</li>\n",
       "\t<li>0.138195033618495</li>\n",
       "\t<li>0.000613492335005297</li>\n",
       "\t<li>0.224049473408494</li>\n",
       "\t<li>0.27039819626497</li>\n",
       "\t<li>0.0588262376106442</li>\n",
       "\t<li>0.290807952041867</li>\n",
       "\t<li>0.0311343738390187</li>\n",
       "\t<li>0.384981768804915</li>\n",
       "\t<li>0.0328636207144629</li>\n",
       "\t<li>0.446884847470565</li>\n",
       "\t<li>0.0796920235654402</li>\n",
       "\t<li>0.306153800007737</li>\n",
       "\t<li>0.105690419986473</li>\n",
       "\t<li>0.0284524744413658</li>\n",
       "\t<li>0.0151126405406884</li>\n",
       "\t<li>0.112106467371821</li>\n",
       "\t<li>0.175400492093786</li>\n",
       "\t<li>0.431800454102634</li>\n",
       "\t<li>0.229696382210033</li>\n",
       "\t<li>0.0461283801584418</li>\n",
       "\t<li>0.0426430728317225</li>\n",
       "\t<li>0.0102192254548317</li>\n",
       "\t<li>0.387426183864605</li>\n",
       "\t<li>0.103129189776603</li>\n",
       "\t<li>0.164353813995948</li>\n",
       "\t<li>0.160166633041282</li>\n",
       "\t<li>0.343960831943811</li>\n",
       "\t<li>0.128269789287326</li>\n",
       "\t<li>0.448067419210507</li>\n",
       "\t<li>0.102143033927492</li>\n",
       "\t<li>0.0446640244954609</li>\n",
       "\t<li>0.00632588283494273</li>\n",
       "\t<li>0.334917997819173</li>\n",
       "\t<li>0.117036448203577</li>\n",
       "\t<li>0.0684230284893524</li>\n",
       "\t<li>0.118512756297235</li>\n",
       "\t<li>0.0343752981565862</li>\n",
       "\t<li>0.432497680919793</li>\n",
       "\t<li>0.139479191659523</li>\n",
       "\t<li>0.0140017766187891</li>\n",
       "\t<li>0.581409386187053</li>\n",
       "\t<li>0.707262374805594</li>\n",
       "\t<li>0.676039987152588</li>\n",
       "\t<li>0.111035127230789</li>\n",
       "\t<li>0.076719758971674</li>\n",
       "\t<li>0.111091887589308</li>\n",
       "\t<li>0.295261334057446</li>\n",
       "\t<li>0.00274376627747775</li>\n",
       "\t<li>0.00171409763294865</li>\n",
       "\t<li>0.242794444901855</li>\n",
       "\t<li>0.0365185630712038</li>\n",
       "\t<li>0.0381762833441883</li>\n",
       "\t<li>0.420266907762725</li>\n",
       "\t<li>0.4144607621968</li>\n",
       "\t<li>0.255141737848591</li>\n",
       "\t<li>0.0087406159898517</li>\n",
       "\t<li>0.149267505949089</li>\n",
       "\t<li>0.716772960375039</li>\n",
       "\t<li>0.428396484675624</li>\n",
       "\t<li>0.0897521400617911</li>\n",
       "\t<li>0.134422453775103</li>\n",
       "\t<li>0.159400382979137</li>\n",
       "\t<li>0.248156331997309</li>\n",
       "\t<li>0.999999899755454</li>\n",
       "\t<li>0.621003227524054</li>\n",
       "\t<li>0.411794970047196</li>\n",
       "\t<li>0.109357350356801</li>\n",
       "\t<li>0.00719209312836932</li>\n",
       "\t<li>0.00392713457488431</li>\n",
       "\t<li>0.643664784228587</li>\n",
       "\t<li>0.34850135185967</li>\n",
       "\t<li>0.0711569847618557</li>\n",
       "\t<li>0.351863175367112</li>\n",
       "\t<li>0.0288360561995615</li>\n",
       "\t<li>0.121008690138617</li>\n",
       "\t<li>0.341693623528626</li>\n",
       "\t<li>0.450786884324715</li>\n",
       "\t<li>0.839378356495052</li>\n",
       "\t<li>0.115675304010426</li>\n",
       "\t<li>0.395658873994819</li>\n",
       "\t<li>0.0780376514842547</li>\n",
       "\t<li>0.937222670441764</li>\n",
       "\t<li>0.0185688506937918</li>\n",
       "\t<li>0.0365611011513383</li>\n",
       "\t<li>0.459464686231983</li>\n",
       "\t<li>0.0123405080055394</li>\n",
       "\t<li>0.115300206157326</li>\n",
       "\t<li>0.333059677674429</li>\n",
       "\t<li>0.0194341950572159</li>\n",
       "\t<li>0.0116899936915638</li>\n",
       "\t<li>0.513825402584578</li>\n",
       "\t<li>0.0225353090383776</li>\n",
       "\t<li>0.0822178093006131</li>\n",
       "\t<li>0.480661788833197</li>\n",
       "\t<li>0.0966890152262564</li>\n",
       "\t<li>0.376341598089679</li>\n",
       "\t<li>0.0188280634091753</li>\n",
       "\t<li>0.0644800530654544</li>\n",
       "\t<li>0.496476381976845</li>\n",
       "\t<li>0.044486125992141</li>\n",
       "\t<li>0.242296131164322</li>\n",
       "\t<li>0.5462276438816</li>\n",
       "\t<li>0.373565724186034</li>\n",
       "\t<li>0.283126386745984</li>\n",
       "\t<li>0.0197569766692492</li>\n",
       "\t<li>0.0541379122086214</li>\n",
       "\t<li>0.0218449492671842</li>\n",
       "\t<li>0.39961999690153</li>\n",
       "\t<li>0.0128226676513324</li>\n",
       "\t<li>0.0397736359132368</li>\n",
       "\t<li>0.0674757811603202</li>\n",
       "\t<li>0.0244701811847007</li>\n",
       "\t<li>0.332434853551037</li>\n",
       "\t<li>0.0669214706400757</li>\n",
       "\t<li>0.810979291451371</li>\n",
       "\t<li>0.109771755561888</li>\n",
       "\t<li>0.360699543748818</li>\n",
       "\t<li>0.940448253429991</li>\n",
       "\t<li>0.318456867667438</li>\n",
       "\t<li>0.629184993765762</li>\n",
       "\t<li>0.0813133729422563</li>\n",
       "\t<li>0.620041187193279</li>\n",
       "\t<li>0.789030638685885</li>\n",
       "\t<li>0.344998351965315</li>\n",
       "\t<li>0.0550173805859845</li>\n",
       "\t<li>0.153916759875194</li>\n",
       "\t<li>0.330755455678276</li>\n",
       "\t<li>0.0194895318232061</li>\n",
       "\t<li>0.205749882986029</li>\n",
       "\t<li>0.553154358672463</li>\n",
       "\t<li>0.213651706951662</li>\n",
       "\t<li>0.591771298601621</li>\n",
       "\t<li>0.00611919438776252</li>\n",
       "\t<li>0.0822670563834605</li>\n",
       "\t<li>0.725529709606978</li>\n",
       "\t<li>0.0023654921756232</li>\n",
       "\t<li>0.0443624987354101</li>\n",
       "\t<li>0.0734532134021451</li>\n",
       "\t<li>0.111738043298359</li>\n",
       "\t<li>0.0189999346797714</li>\n",
       "\t<li>0.58166001331911</li>\n",
       "\t<li>0.352322672091126</li>\n",
       "\t<li>0.00410926428703993</li>\n",
       "\t<li>0.0873441475972424</li>\n",
       "\t<li>0.408803335657935</li>\n",
       "\t<li>0.784146396581265</li>\n",
       "\t<li>0.635640629612414</li>\n",
       "\t<li>0.115891386401419</li>\n",
       "\t<li>0.200717926772197</li>\n",
       "\t<li>0.299687427534232</li>\n",
       "\t<li>0.00672402856835594</li>\n",
       "\t<li>0.801570047356619</li>\n",
       "\t<li>0.0426671777978862</li>\n",
       "\t<li>0.0127759868029564</li>\n",
       "\t<li>0.242675951233782</li>\n",
       "\t<li>0.126772723549678</li>\n",
       "\t<li>0.0439637533488899</li>\n",
       "\t<li>0.257337844571137</li>\n",
       "\t<li>0.543337799086199</li>\n",
       "\t<li>0.208106641889324</li>\n",
       "\t<li>0.132721146618251</li>\n",
       "\t<li>0.531606266396264</li>\n",
       "\t<li>0.292998879059461</li>\n",
       "\t<li>0.00966018862869711</li>\n",
       "\t<li>0.0193694640301211</li>\n",
       "\t<li>0.15510703520612</li>\n",
       "\t<li>0.0161150653010276</li>\n",
       "\t<li>0.282592032803302</li>\n",
       "\t<li>0.00694463553896689</li>\n",
       "\t<li>0.201647592687945</li>\n",
       "\t<li>0.422466741704975</li>\n",
       "\t<li>0.242953581399268</li>\n",
       "\t<li>0.00351089833854013</li>\n",
       "\t<li>0.400627588277948</li>\n",
       "\t<li>0.0389228867741802</li>\n",
       "\t<li>0.0521593751657165</li>\n",
       "\t<li>0.331435704194659</li>\n",
       "\t<li>0.0455245248572239</li>\n",
       "\t<li>0.0309064616653328</li>\n",
       "\t<li>0.0878612310292512</li>\n",
       "\t<li>0.0133130674654993</li>\n",
       "\t<li>0.68832676321178</li>\n",
       "\t<li>0.659228577916452</li>\n",
       "\t<li>0.227359436505122</li>\n",
       "\t<li>0.481600127096135</li>\n",
       "\t<li>0.261394497440249</li>\n",
       "\t<li>0.0130665052484996</li>\n",
       "\t<li>0.520518344967139</li>\n",
       "\t<li>0.647744267628216</li>\n",
       "\t<li>0.250722701012656</li>\n",
       "\t<li>0.373737729831715</li>\n",
       "\t<li>0.325611836248114</li>\n",
       "\t<li>0.00319015496483199</li>\n",
       "\t<li>0.339464434005485</li>\n",
       "\t<li>0.0966684552130676</li>\n",
       "\t<li>0.0617476612895288</li>\n",
       "\t<li>0.19152926225361</li>\n",
       "\t<li>0.11103601264329</li>\n",
       "\t<li>0.61590524200321</li>\n",
       "\t<li>0.0215583805673567</li>\n",
       "\t<li>0.140053997430894</li>\n",
       "\t<li>0.0477986234154584</li>\n",
       "\t<li>0.0826264263420942</li>\n",
       "\t<li>0.00702769078577563</li>\n",
       "\t<li>0.826192755491458</li>\n",
       "\t<li>0.257834891033145</li>\n",
       "\t<li>0.286794215658679</li>\n",
       "\t<li>0.106014480810485</li>\n",
       "\t<li>0.340629795332494</li>\n",
       "\t<li>0.480350855810903</li>\n",
       "\t<li>0.155249929153167</li>\n",
       "\t<li>0.285211952849519</li>\n",
       "\t<li>0.324050126089352</li>\n",
       "\t<li>0.0703373040441296</li>\n",
       "\t<li>0.365802250433597</li>\n",
       "\t<li>0.360178385500458</li>\n",
       "\t<li>0.083657639466234</li>\n",
       "\t<li>0.0120954505667411</li>\n",
       "\t<li>0.0157389085382795</li>\n",
       "\t<li>0.0510121515711138</li>\n",
       "\t<li>0.18566362469721</li>\n",
       "\t<li>0.21093029457333</li>\n",
       "\t<li>0.0339366834988751</li>\n",
       "\t<li>0.129181519812422</li>\n",
       "\t<li>0.0551722889180708</li>\n",
       "\t<li>0.0583173531946214</li>\n",
       "\t<li>0.0565857009501661</li>\n",
       "\t<li>0.195500676987564</li>\n",
       "\t<li>0.387298210598922</li>\n",
       "\t<li>0.0174095793128992</li>\n",
       "\t<li>0.54469816524027</li>\n",
       "\t<li>0.999999881272755</li>\n",
       "\t<li>0.1027611960477</li>\n",
       "\t<li>0.47667823007691</li>\n",
       "\t<li>0.0662688998929444</li>\n",
       "\t<li>0.116922734592427</li>\n",
       "\t<li>0.0850278394625899</li>\n",
       "\t<li>0.392637026170667</li>\n",
       "\t<li>0.0100271760538756</li>\n",
       "\t<li>0.882515089158017</li>\n",
       "\t<li>0.149166487271602</li>\n",
       "\t<li>0.0447689462679109</li>\n",
       "\t<li>0.780293803729918</li>\n",
       "\t<li>0.577707688286746</li>\n",
       "\t<li>0.0186315861936365</li>\n",
       "\t<li>0.00826138172958256</li>\n",
       "\t<li>0.208537293731533</li>\n",
       "\t<li>0.270569664407966</li>\n",
       "\t<li>0.51863093991967</li>\n",
       "\t<li>0.174336426356056</li>\n",
       "\t<li>0.31521906329962</li>\n",
       "\t<li>0.422335186398251</li>\n",
       "\t<li>0.417747688186922</li>\n",
       "\t<li>0.209491591975983</li>\n",
       "\t<li>0.00632646199144239</li>\n",
       "\t<li>0.749652383357013</li>\n",
       "\t<li>0.0135803141014883</li>\n",
       "\t<li>0.284565936383715</li>\n",
       "\t<li>0.0203262184445531</li>\n",
       "\t<li>0.837222888250718</li>\n",
       "\t<li>0.0557357281173115</li>\n",
       "\t<li>0.20565285399988</li>\n",
       "\t<li>0.339038777221873</li>\n",
       "\t<li>0.0100044063394905</li>\n",
       "\t<li>0.357838781913102</li>\n",
       "\t<li>0.392694598286668</li>\n",
       "\t<li>0.123665880291385</li>\n",
       "\t<li>0.024835452290815</li>\n",
       "\t<li>0.798304952283806</li>\n",
       "\t<li>0.0710472788783619</li>\n",
       "\t<li>0.217491479794416</li>\n",
       "\t<li>0.618326994120905</li>\n",
       "\t<li>0.0400660581512396</li>\n",
       "\t<li>0.176943638370039</li>\n",
       "\t<li>0.425878858065581</li>\n",
       "\t<li>0.0485012868145134</li>\n",
       "\t<li>0.011622000593429</li>\n",
       "\t<li>0.163495250067349</li>\n",
       "\t<li>0.28876832052196</li>\n",
       "\t<li>0.150084973932894</li>\n",
       "\t<li>0.761837899159859</li>\n",
       "\t<li>0.0905138213694239</li>\n",
       "\t<li>0.117727398337633</li>\n",
       "\t<li>0.0129693312408954</li>\n",
       "\t<li>0.270787542001476</li>\n",
       "\t<li>0.303445040822964</li>\n",
       "\t<li>0.0045746293789501</li>\n",
       "\t<li>0.673561166508947</li>\n",
       "\t<li>0.420120032207368</li>\n",
       "\t<li>0.718770945107793</li>\n",
       "\t<li>0.0157843933453791</li>\n",
       "\t<li>0.773586185200803</li>\n",
       "\t<li>0.0444909631293543</li>\n",
       "\t<li>0.101154237579664</li>\n",
       "\t<li>0.00362551629215413</li>\n",
       "\t<li>0.864277448109038</li>\n",
       "\t<li>0.401290367065925</li>\n",
       "\t<li>0.0044193786966831</li>\n",
       "\t<li>0.105168455461581</li>\n",
       "\t<li>0.000704355504237902</li>\n",
       "\t<li>0.0782380967993163</li>\n",
       "\t<li>0.0680032169510186</li>\n",
       "\t<li>0.325215314840005</li>\n",
       "\t<li>0.522137318069587</li>\n",
       "\t<li>0.0695479591239362</li>\n",
       "\t<li>0.910088474045432</li>\n",
       "\t<li>0.0225768752466563</li>\n",
       "\t<li>0.0830117248087445</li>\n",
       "\t<li>0.782424262078024</li>\n",
       "\t<li>0.00284140544328993</li>\n",
       "\t<li>0.181802023547386</li>\n",
       "\t<li>0.446750806198583</li>\n",
       "\t<li>0.175307512265613</li>\n",
       "\t<li>0.211994718778293</li>\n",
       "\t<li>0.362056016463592</li>\n",
       "\t<li>0.698494628531572</li>\n",
       "\t<li>0.796409402741523</li>\n",
       "\t<li>0.69701665386244</li>\n",
       "\t<li>0.60498841832336</li>\n",
       "\t<li>0.222151410916736</li>\n",
       "\t<li>0.23245232118953</li>\n",
       "\t<li>0.0312143806769226</li>\n",
       "\t<li>0.00597863316182652</li>\n",
       "\t<li>0.0474385359674159</li>\n",
       "\t<li>0.0692646562294439</li>\n",
       "\t<li>0.813293860136716</li>\n",
       "\t<li>0.37800470789794</li>\n",
       "\t<li>0.0826239869534869</li>\n",
       "\t<li>0.0535687457396391</li>\n",
       "\t<li>0.381792304137231</li>\n",
       "\t<li>0.0224097542310455</li>\n",
       "\t<li>0.0571640111552471</li>\n",
       "\t<li>0.500278393587007</li>\n",
       "\t<li>0.737791677638268</li>\n",
       "\t<li>0.401349247208118</li>\n",
       "\t<li>0.436029511070447</li>\n",
       "\t<li>0.0327230826765334</li>\n",
       "\t<li>0.00927603454896041</li>\n",
       "\t<li>0.122509979589628</li>\n",
       "\t<li>0.0278683778105911</li>\n",
       "\t<li>0.852582373974251</li>\n",
       "\t<li>0.015712446760872</li>\n",
       "\t<li>0.0940175246146619</li>\n",
       "\t<li>0.573673495045516</li>\n",
       "\t<li>0.5041167751129</li>\n",
       "\t<li>0.0102770308244628</li>\n",
       "\t<li>0.00118549280420903</li>\n",
       "\t<li>0.131375428121562</li>\n",
       "\t<li>0.438320910689265</li>\n",
       "\t<li>0.756097754809503</li>\n",
       "\t<li>0.378041113022159</li>\n",
       "\t<li>0.0357551574045644</li>\n",
       "\t<li>0.134654161705776</li>\n",
       "\t<li>0.259550862282099</li>\n",
       "\t<li>0.778064247830644</li>\n",
       "\t<li>0.0201462974622649</li>\n",
       "\t<li>0.24006009465598</li>\n",
       "\t<li>0.678198913733828</li>\n",
       "\t<li>0.300570735597846</li>\n",
       "\t<li>0.0469963534947108</li>\n",
       "\t<li>0.502486365272292</li>\n",
       "\t<li>0.0143151454453701</li>\n",
       "\t<li>0.0651614188522869</li>\n",
       "\t<li>0.836073441063167</li>\n",
       "\t<li>0.259320005941914</li>\n",
       "\t<li>0.0108894451187177</li>\n",
       "\t<li>0.30730254345272</li>\n",
       "\t<li>0.0983187936355585</li>\n",
       "\t<li>0.0380510346564614</li>\n",
       "\t<li>0.607227303527659</li>\n",
       "\t<li>0.00786637524450243</li>\n",
       "\t<li>0.345422292451516</li>\n",
       "\t<li>0.0183754642854709</li>\n",
       "\t<li>0.206154474247793</li>\n",
       "\t<li>0.0099216288357854</li>\n",
       "\t<li>0.0414194474098731</li>\n",
       "\t<li>0.0829841263907362</li>\n",
       "\t<li>0.0306311878548479</li>\n",
       "\t<li>0.0285561501172792</li>\n",
       "\t<li>0.0605196262065695</li>\n",
       "\t<li>0.11832520017891</li>\n",
       "\t<li>0.0109672154767565</li>\n",
       "\t<li>0.0127333624065195</li>\n",
       "\t<li>0.00224888673714246</li>\n",
       "\t<li>0.0429628899674597</li>\n",
       "\t<li>0.0251811856175837</li>\n",
       "\t<li>0.0295559817119572</li>\n",
       "\t<li>0.00388163350760284</li>\n",
       "\t<li>0.261194695234384</li>\n",
       "\t<li>0.882634420272317</li>\n",
       "\t<li>0.171042965249206</li>\n",
       "\t<li>0.0149085330205609</li>\n",
       "\t<li>0.00158657563942785</li>\n",
       "\t<li>0.556844563218709</li>\n",
       "\t<li>0.387655918204949</li>\n",
       "\t<li>0.0661323040382908</li>\n",
       "\t<li>0.608800003409554</li>\n",
       "\t<li>0.574721973559668</li>\n",
       "\t<li>0.0521771218150772</li>\n",
       "\t<li>0.902118697498264</li>\n",
       "\t<li>0.0820669258207579</li>\n",
       "\t<li>0.00754209020126828</li>\n",
       "\t<li>0.723437201507968</li>\n",
       "\t<li>0.101828009736096</li>\n",
       "\t<li>0.468876692554582</li>\n",
       "\t<li>0.2468317337222</li>\n",
       "\t<li>0.512240316374758</li>\n",
       "\t<li>0.087004829379109</li>\n",
       "\t<li>0.0149338140834091</li>\n",
       "\t<li>0.45103091765394</li>\n",
       "\t<li>0.0231078515925126</li>\n",
       "\t<li>0.0443657247274287</li>\n",
       "\t<li>0.290537482498241</li>\n",
       "\t<li>0.561325115718624</li>\n",
       "\t<li>0.342226939316761</li>\n",
       "\t<li>0.12864443652351</li>\n",
       "\t<li>0.689437807152223</li>\n",
       "\t<li>0.333644758404331</li>\n",
       "\t<li>0.0820764525219253</li>\n",
       "\t<li>0.042152085677163</li>\n",
       "\t<li>0.224722384138494</li>\n",
       "\t<li>0.529648383441074</li>\n",
       "\t<li>0.0207762852410577</li>\n",
       "\t<li>0.00440931528151489</li>\n",
       "\t<li>0.411697715892915</li>\n",
       "\t<li>0.0802832296717964</li>\n",
       "\t<li>0.346339647881345</li>\n",
       "\t<li>0.0130900437365671</li>\n",
       "\t<li>0.012744641309673</li>\n",
       "\t<li>0.429693038206176</li>\n",
       "\t<li>0.211733598664388</li>\n",
       "\t<li>0.713899951037443</li>\n",
       "\t<li>0.0204587633545007</li>\n",
       "\t<li>0.969105900109868</li>\n",
       "\t<li>0.0120568543931244</li>\n",
       "\t<li>0.900706976430349</li>\n",
       "\t<li>0.00322821636077589</li>\n",
       "\t<li>0.0133650536120327</li>\n",
       "\t<li>0.198050332159119</li>\n",
       "\t<li>0.702582904313514</li>\n",
       "\t<li>0.660678973776002</li>\n",
       "\t<li>0.00893567809480346</li>\n",
       "\t<li>0.0246127601627504</li>\n",
       "\t<li>0.0372872768121372</li>\n",
       "\t<li>0.137123364030737</li>\n",
       "\t<li>0.520023981229925</li>\n",
       "\t<li>0.0066450794922484</li>\n",
       "\t<li>0.999999934069721</li>\n",
       "\t<li>0.699187065784779</li>\n",
       "\t<li>0.604873083636888</li>\n",
       "\t<li>0.333363366288427</li>\n",
       "\t<li>0.666210889741808</li>\n",
       "\t<li>0.12270428875785</li>\n",
       "\t<li>0.67271433309237</li>\n",
       "\t<li>0.186352219335337</li>\n",
       "\t<li>0.0255251185695469</li>\n",
       "\t<li>0.103295434259188</li>\n",
       "\t<li>0.292404583057265</li>\n",
       "\t<li>0.107016716840353</li>\n",
       "\t<li>0.0191613585884338</li>\n",
       "\t<li>0.323385344438563</li>\n",
       "\t<li>0.323606556162005</li>\n",
       "\t<li>0.00266290871999711</li>\n",
       "\t<li>0.0429157016021198</li>\n",
       "\t<li>0.00486193696698884</li>\n",
       "\t<li>0.350003539491226</li>\n",
       "\t<li>0.372556174351423</li>\n",
       "\t<li>0.0626083960843705</li>\n",
       "\t<li>0.191544290861375</li>\n",
       "\t<li>0.0938000049788963</li>\n",
       "\t<li>0.153051598225422</li>\n",
       "\t<li>0.480187997110702</li>\n",
       "\t<li>0.0052630599731583</li>\n",
       "\t<li>0.27972661029505</li>\n",
       "\t<li>0.0398773138398133</li>\n",
       "\t<li>0.00162572191553019</li>\n",
       "\t<li>0.442823136141403</li>\n",
       "\t<li>0.0120391413903642</li>\n",
       "\t<li>0.10566201095036</li>\n",
       "\t<li>0.093718039648838</li>\n",
       "\t<li>0.164142373320656</li>\n",
       "\t<li>0.383839605878613</li>\n",
       "\t<li>0.00451305545018589</li>\n",
       "\t<li>0.537594286597542</li>\n",
       "\t<li>0.466383798477598</li>\n",
       "\t<li>0.0281582762622777</li>\n",
       "\t<li>0.29656351506812</li>\n",
       "\t<li>0.0500069544563138</li>\n",
       "\t<li>0.0179109290575799</li>\n",
       "\t<li>0.182654043547901</li>\n",
       "\t<li>0.173632291650515</li>\n",
       "\t<li>0.314874409741457</li>\n",
       "\t<li>0.00285495418430275</li>\n",
       "\t<li>0.144684627437276</li>\n",
       "\t<li>0.0117231450258738</li>\n",
       "\t<li>0.427994448144244</li>\n",
       "\t<li>0.16361830683771</li>\n",
       "\t<li>0.00846342302494718</li>\n",
       "\t<li>0.0162058441102335</li>\n",
       "\t<li>0.261560471705074</li>\n",
       "\t<li>0.335900678213885</li>\n",
       "\t<li>0.999999719882787</li>\n",
       "\t<li>0.0223168252846246</li>\n",
       "\t<li>0.171904897795075</li>\n",
       "\t<li>0.0132759069202664</li>\n",
       "\t<li>0.571779516237679</li>\n",
       "\t<li>0.143311262794378</li>\n",
       "\t<li>0.0636269799691191</li>\n",
       "\t<li>0.0986192994529642</li>\n",
       "\t<li>0.186532650875311</li>\n",
       "\t<li>0.10725450032835</li>\n",
       "\t<li>0.0587551753964388</li>\n",
       "\t<li>0.0113480679433616</li>\n",
       "\t<li>0.373801111008503</li>\n",
       "\t<li>0.146091797132755</li>\n",
       "\t<li>0.00472946298492059</li>\n",
       "\t<li>0.0148767186716644</li>\n",
       "\t<li>0.402492786015853</li>\n",
       "\t<li>0.599905774626979</li>\n",
       "\t<li>0.129156093581038</li>\n",
       "\t<li>0.0327122735411403</li>\n",
       "\t<li>0.00270956646316803</li>\n",
       "\t<li>0.414419234280421</li>\n",
       "\t<li>0.0748696899689292</li>\n",
       "\t<li>0.210084474844711</li>\n",
       "\t<li>0.0505849568379707</li>\n",
       "\t<li>0.37878641187</li>\n",
       "\t<li>0.0520068591141141</li>\n",
       "\t<li>0.191747610946361</li>\n",
       "\t<li>0.073796405649198</li>\n",
       "\t<li>0.857763278077361</li>\n",
       "\t<li>0.287226861085736</li>\n",
       "\t<li>0.156571082241875</li>\n",
       "\t<li>0.679872307420937</li>\n",
       "\t<li>0.607413476372354</li>\n",
       "\t<li>0.0259541990026681</li>\n",
       "\t<li>0.438749681108358</li>\n",
       "\t<li>0.27895648356056</li>\n",
       "\t<li>0.0302853846441377</li>\n",
       "\t<li>0.0359113469890458</li>\n",
       "\t<li>0.180162495915178</li>\n",
       "\t<li>0.0416986274509818</li>\n",
       "\t<li>0.376890684622951</li>\n",
       "\t<li>0.231963166688558</li>\n",
       "\t<li>0.494434450132828</li>\n",
       "\t<li>0.0209143587981374</li>\n",
       "\t<li>0.107206716581593</li>\n",
       "\t<li>0.708322155399093</li>\n",
       "\t<li>0.913731609334851</li>\n",
       "\t<li>0.104157703080455</li>\n",
       "\t<li>0.0556707696440846</li>\n",
       "\t<li>0.0175382849056347</li>\n",
       "\t<li>0.460250756300556</li>\n",
       "\t<li>0.00892961678058072</li>\n",
       "\t<li>0.0426768272972568</li>\n",
       "\t<li>0.738494445202362</li>\n",
       "\t<li>0.0139891946964155</li>\n",
       "\t<li>0.0173010271782344</li>\n",
       "\t<li>0.36643548507285</li>\n",
       "\t<li>0.744312090543482</li>\n",
       "\t<li>0.0881357595922892</li>\n",
       "\t<li>0.00942054215640495</li>\n",
       "\t<li>0.068128632951133</li>\n",
       "\t<li>0.0388245693052423</li>\n",
       "\t<li>0.392192135594782</li>\n",
       "\t<li>0.388390494088206</li>\n",
       "\t<li>0.651204649158032</li>\n",
       "\t<li>0.180637714329667</li>\n",
       "\t<li>0.102705415062379</li>\n",
       "\t<li>0.020707264284863</li>\n",
       "\t<li>0.0256075381477818</li>\n",
       "\t<li>0.00250860705648477</li>\n",
       "\t<li>0.681940048158735</li>\n",
       "\t<li>0.350262020578293</li>\n",
       "\t<li>0.0383984308896455</li>\n",
       "\t<li>0.658734181967604</li>\n",
       "\t<li>0.0669926729564842</li>\n",
       "\t<li>0.533022350312315</li>\n",
       "\t<li>0.212693264031104</li>\n",
       "\t<li>0.738803576604701</li>\n",
       "\t<li>0.0389983717122111</li>\n",
       "\t<li>0.747525660111215</li>\n",
       "\t<li>0.785958005108221</li>\n",
       "\t<li>0.152504421161402</li>\n",
       "\t<li>0.0189503594342853</li>\n",
       "\t<li>0.235368374657788</li>\n",
       "\t<li>0.0416053578011781</li>\n",
       "\t<li>0.166950509764835</li>\n",
       "\t<li>0.744520527611533</li>\n",
       "\t<li>0.0529810394056784</li>\n",
       "\t<li>0.00327444356051325</li>\n",
       "\t<li>0.149399902870609</li>\n",
       "\t<li>0.402424644821861</li>\n",
       "\t<li>0.0276814867259534</li>\n",
       "\t<li>0.00991715759274851</li>\n",
       "\t<li>0.4251773610361</li>\n",
       "\t<li>0.25052315786123</li>\n",
       "\t<li>0.109479032663</li>\n",
       "\t<li>0.00821826282945569</li>\n",
       "\t<li>0.227110410995884</li>\n",
       "\t<li>0.14806988830362</li>\n",
       "\t<li>0.359347359420511</li>\n",
       "\t<li>0.210979644010036</li>\n",
       "\t<li>0.0463032214188742</li>\n",
       "\t<li>0.46875383462234</li>\n",
       "\t<li>0.215888946331947</li>\n",
       "\t<li>0.713179985277368</li>\n",
       "\t<li>0.0133076759758265</li>\n",
       "\t<li>0.123259371522801</li>\n",
       "\t<li>0.0350621484165317</li>\n",
       "\t<li>0.0218596503142545</li>\n",
       "\t<li>0.00709909504330927</li>\n",
       "\t<li>0.246300702617571</li>\n",
       "\t<li>0.0722452739253546</li>\n",
       "\t<li>0.768767034364842</li>\n",
       "\t<li>0.103045751032903</li>\n",
       "\t<li>0.0427102616103358</li>\n",
       "\t<li>0.0220479418402836</li>\n",
       "\t<li>0.920016590050812</li>\n",
       "\t<li>0.0211256570489751</li>\n",
       "\t<li>0.39996253462366</li>\n",
       "\t<li>0.0548779363750377</li>\n",
       "\t<li>0.0252650412505952</li>\n",
       "\t<li>0.257208730113193</li>\n",
       "\t<li>0.680939629125579</li>\n",
       "\t<li>0.729255611723037</li>\n",
       "\t<li>0.596928459338522</li>\n",
       "\t<li>0.999999369186944</li>\n",
       "\t<li>0.0263022407673469</li>\n",
       "\t<li>0.384847862812985</li>\n",
       "\t<li>0.549343043856729</li>\n",
       "\t<li>0.173576609514886</li>\n",
       "\t<li>0.080540638126581</li>\n",
       "\t<li>0.737058762166601</li>\n",
       "\t<li>0.00306032494596797</li>\n",
       "\t<li>0.0717116554481647</li>\n",
       "\t<li>0.309395303411374</li>\n",
       "\t<li>0.0220249069614392</li>\n",
       "\t<li>0.353891615376487</li>\n",
       "\t<li>0.0516877938558486</li>\n",
       "\t<li>0.197422325508889</li>\n",
       "\t<li>0.625577234432896</li>\n",
       "\t<li>0.0555821964514438</li>\n",
       "\t<li>0.589136527507761</li>\n",
       "\t<li>0.0555530611510398</li>\n",
       "\t<li>0.475650326664653</li>\n",
       "\t<li>0.115993377295593</li>\n",
       "\t<li>0.358102933017236</li>\n",
       "\t<li>0.84324883081674</li>\n",
       "\t<li>0.0322303964399007</li>\n",
       "\t<li>0.00261417803905965</li>\n",
       "\t<li>0.0192613020090237</li>\n",
       "\t<li>0.783702346299451</li>\n",
       "\t<li>0.999999921868559</li>\n",
       "\t<li>0.0079345660994076</li>\n",
       "\t<li>0.00387438423090134</li>\n",
       "\t<li>0.0211480019974471</li>\n",
       "\t<li>0.050450558487875</li>\n",
       "\t<li>0.0445260716037124</li>\n",
       "\t<li>0.0293364700879047</li>\n",
       "\t<li>0.0855610339125738</li>\n",
       "\t<li>0.641965499370153</li>\n",
       "\t<li>0.0489075172471356</li>\n",
       "\t<li>0.0013621038091988</li>\n",
       "\t<li>0.537891437772786</li>\n",
       "\t<li>0.886347490567167</li>\n",
       "\t<li>0.16125326393139</li>\n",
       "\t<li>0.0309850610819549</li>\n",
       "\t<li>0.693530738886448</li>\n",
       "\t<li>0.00561970245310959</li>\n",
       "\t<li>0.302654046642452</li>\n",
       "\t<li>0.00785647756233152</li>\n",
       "\t<li>0.191613057594669</li>\n",
       "\t<li>0.011638934751399</li>\n",
       "\t<li>0.243860321451445</li>\n",
       "\t<li>0.444726091154691</li>\n",
       "\t<li>0.0572090029004352</li>\n",
       "\t<li>0.261446047313538</li>\n",
       "\t<li>0.0171908586343519</li>\n",
       "\t<li>0.575854903961387</li>\n",
       "\t<li>0.192877574350031</li>\n",
       "\t<li>0.00422424646968325</li>\n",
       "\t<li>0.922205852496056</li>\n",
       "\t<li>0.0704851393216222</li>\n",
       "\t<li>0.567080304207818</li>\n",
       "\t<li>0.47606140786739</li>\n",
       "\t<li>0.306320674105787</li>\n",
       "\t<li>0.0369710971674199</li>\n",
       "\t<li>0.819697805813227</li>\n",
       "\t<li>0.013248651624041</li>\n",
       "\t<li>0.0631829730650491</li>\n",
       "\t<li>0.378553828139803</li>\n",
       "\t<li>0.0413801184849777</li>\n",
       "\t<li>0.27543407874579</li>\n",
       "\t<li>0.545895504384078</li>\n",
       "\t<li>0.259934954639882</li>\n",
       "\t<li>0.0165679910614728</li>\n",
       "\t<li>0.262467341480874</li>\n",
       "\t<li>0.00851088909340637</li>\n",
       "\t<li>0.132238607019351</li>\n",
       "\t<li>0.00264874929875302</li>\n",
       "\t<li>0.514256243669981</li>\n",
       "\t<li>0.504662694825196</li>\n",
       "\t<li>0.46918137663433</li>\n",
       "\t<li>0.0206286273389961</li>\n",
       "\t<li>0.0130306565509383</li>\n",
       "\t<li>0.0216268255776056</li>\n",
       "\t<li>0.962873976004148</li>\n",
       "\t<li>0.0228586561112876</li>\n",
       "\t<li>0.415886038439882</li>\n",
       "\t<li>0.111644078337856</li>\n",
       "\t<li>0.035732224743968</li>\n",
       "\t<li>0.183323652433876</li>\n",
       "\t<li>0.00962911740164285</li>\n",
       "\t<li>0.472661642158482</li>\n",
       "\t<li>0.063094439257077</li>\n",
       "\t<li>0.739556130631472</li>\n",
       "\t<li>0.540561789482398</li>\n",
       "\t<li>0.476699904607004</li>\n",
       "\t<li>0.0574012928104084</li>\n",
       "\t<li>0.0248904132738773</li>\n",
       "\t<li>0.320685216707128</li>\n",
       "\t<li>0.0259059737871506</li>\n",
       "\t<li>0.375706690430921</li>\n",
       "\t<li>0.0318621776115162</li>\n",
       "\t<li>0.835147724968061</li>\n",
       "\t<li>0.579956282477218</li>\n",
       "\t<li>0.771459176592422</li>\n",
       "\t<li>0.0929458415001703</li>\n",
       "\t<li>0.140511726836116</li>\n",
       "\t<li>0.0944645275237256</li>\n",
       "\t<li>0.165131169764395</li>\n",
       "\t<li>0.779965172205315</li>\n",
       "\t<li>0.0135918638149194</li>\n",
       "\t<li>0.00310894475377708</li>\n",
       "\t<li>0.999999921059875</li>\n",
       "\t<li>0.241733153817061</li>\n",
       "\t<li>0.658845773041207</li>\n",
       "\t<li>0.6818222401056</li>\n",
       "\t<li>0.0271196274070777</li>\n",
       "\t<li>0.355537798682258</li>\n",
       "\t<li>0.0610618672068804</li>\n",
       "\t<li>0.356707039898808</li>\n",
       "\t<li>0.350476407031346</li>\n",
       "\t<li>0.343113774889433</li>\n",
       "\t<li>0.0295106260508025</li>\n",
       "\t<li>0.738953374357723</li>\n",
       "\t<li>0.284378209763666</li>\n",
       "\t<li>0.068611015867394</li>\n",
       "\t<li>0.00482401773356611</li>\n",
       "\t<li>0.630769530768676</li>\n",
       "\t<li>0.315158357608213</li>\n",
       "\t<li>0.356827702658612</li>\n",
       "\t<li>0.270527239661937</li>\n",
       "\t<li>0.251624881415677</li>\n",
       "\t<li>0.25760880755991</li>\n",
       "\t<li>0.180107416527999</li>\n",
       "\t<li>0.573906010950479</li>\n",
       "\t<li>0.333287983948135</li>\n",
       "\t<li>0.0994519735509173</li>\n",
       "\t<li>0.254373111131962</li>\n",
       "\t<li>0.127947766456593</li>\n",
       "\t<li>0.0287503878509145</li>\n",
       "\t<li>0.622280038298678</li>\n",
       "\t<li>0.00923154040183382</li>\n",
       "\t<li>0.00893073150299629</li>\n",
       "\t<li>0.4107363959673</li>\n",
       "\t<li>0.532240779589429</li>\n",
       "\t<li>0.0147120399520612</li>\n",
       "\t<li>0.0850992156468574</li>\n",
       "\t<li>0.0666759971171219</li>\n",
       "\t<li>0.0532227533905391</li>\n",
       "\t<li>0.944763104212636</li>\n",
       "\t<li>0.0421231961962477</li>\n",
       "\t<li>0.330871603608521</li>\n",
       "\t<li>0.00501890572748447</li>\n",
       "\t<li>0.0475675707248185</li>\n",
       "\t<li>0.0083735761559981</li>\n",
       "\t<li>0.844623338968075</li>\n",
       "\t<li>0.846679594057493</li>\n",
       "\t<li>0.603003049555463</li>\n",
       "\t<li>0.0576375333669081</li>\n",
       "\t<li>0.393734285994759</li>\n",
       "\t<li>0.00767541424809816</li>\n",
       "\t<li>0.199636721981925</li>\n",
       "\t<li>0.292213024120264</li>\n",
       "\t<li>0.76906982519554</li>\n",
       "\t<li>0.856685323240491</li>\n",
       "\t<li>0.595530358227373</li>\n",
       "\t<li>0.029703505549569</li>\n",
       "\t<li>0.170230360437168</li>\n",
       "\t<li>0.0590962158581206</li>\n",
       "\t<li>0.137961922417632</li>\n",
       "\t<li>0.0744322585941222</li>\n",
       "\t<li>0.615940912653162</li>\n",
       "\t<li>0.838822052148631</li>\n",
       "\t<li>0.191665871794086</li>\n",
       "\t<li>0.528119175483124</li>\n",
       "\t<li>0.18480304438479</li>\n",
       "\t<li>0.030103090076589</li>\n",
       "\t<li>0.313327982406951</li>\n",
       "\t<li>0.0917255709899279</li>\n",
       "\t<li>0.041551136464234</li>\n",
       "\t<li>0.187966788144977</li>\n",
       "\t<li>0.0506776774678144</li>\n",
       "\t<li>0.265329521353257</li>\n",
       "\t<li>0.0346522723301589</li>\n",
       "\t<li>0.726813785921841</li>\n",
       "\t<li>0.191463884285267</li>\n",
       "\t<li>0.0639038731282046</li>\n",
       "\t<li>0.0632602697916699</li>\n",
       "\t<li>0.803075849198999</li>\n",
       "\t<li>0.438268079689189</li>\n",
       "\t<li>0.0985781227947532</li>\n",
       "\t<li>0.0310326490016468</li>\n",
       "\t<li>0.248496611722856</li>\n",
       "\t<li>0.0268931479842507</li>\n",
       "\t<li>0.584490035996011</li>\n",
       "\t<li>0.767644550296345</li>\n",
       "\t<li>0.616942243443454</li>\n",
       "\t<li>0.181884391709822</li>\n",
       "\t<li>0.0292352580005547</li>\n",
       "\t<li>0.449170568819321</li>\n",
       "\t<li>0.037957275850432</li>\n",
       "\t<li>0.0214499858700358</li>\n",
       "\t<li>0.772209840371338</li>\n",
       "\t<li>0.103785471161419</li>\n",
       "\t<li>0.479325296521707</li>\n",
       "\t<li>0.0128060641739382</li>\n",
       "\t<li>0.0396358543490138</li>\n",
       "\t<li>0.346936039831818</li>\n",
       "\t<li>0.210000158269074</li>\n",
       "\t<li>0.181801412731248</li>\n",
       "\t<li>0.082041376717069</li>\n",
       "\t<li>0.182220291384141</li>\n",
       "\t<li>0.0081125633662825</li>\n",
       "\t<li>0.474099349262691</li>\n",
       "\t<li>0.218620140442417</li>\n",
       "\t<li>0.221512435317649</li>\n",
       "\t<li>0.345253143463621</li>\n",
       "\t<li>0.282029571528149</li>\n",
       "\t<li>0.123630940615102</li>\n",
       "\t<li>0.00312376623456277</li>\n",
       "\t<li>0.687484003092099</li>\n",
       "\t<li>0.167630842305543</li>\n",
       "\t<li>0.256873896398803</li>\n",
       "\t<li>0.138270720742715</li>\n",
       "\t<li>0.335698092889923</li>\n",
       "\t<li>0.0555016611674721</li>\n",
       "\t<li>0.559613891428735</li>\n",
       "\t<li>0.0815618215625696</li>\n",
       "\t<li>0.314753223939865</li>\n",
       "\t<li>0.13519587565503</li>\n",
       "\t<li>0.512628252536532</li>\n",
       "\t<li>0.310301419935886</li>\n",
       "\t<li>0.138872333665707</li>\n",
       "\t<li>0.0032387922514317</li>\n",
       "\t<li>0.0503488414695993</li>\n",
       "\t<li>0.65381973340819</li>\n",
       "\t<li>0.0613327135504364</li>\n",
       "\t<li>0.00933227759682214</li>\n",
       "\t<li>0.0143616311827729</li>\n",
       "\t<li>0.124348599066586</li>\n",
       "\t<li>0.00647149006353583</li>\n",
       "\t<li>0.252065847305571</li>\n",
       "\t<li>0.31145577732936</li>\n",
       "\t<li>0.0170022745551469</li>\n",
       "\t<li>0.0184837003196838</li>\n",
       "\t<li>0.0125543720114499</li>\n",
       "\t<li>0.118783058537979</li>\n",
       "\t<li>0.00908245556874083</li>\n",
       "\t<li>0.0209936551649666</li>\n",
       "\t<li>0.113606708659381</li>\n",
       "\t<li>0.132870896582193</li>\n",
       "\t<li>0.262417580337086</li>\n",
       "\t<li>0.0111836663784644</li>\n",
       "\t<li>0.0379309246642737</li>\n",
       "\t<li>0.489558874761568</li>\n",
       "\t<li>0.0357662129703509</li>\n",
       "\t<li>0.0753036946996795</li>\n",
       "\t<li>0.648942525448569</li>\n",
       "\t<li>0.274216563019518</li>\n",
       "\t<li>0.852575022184947</li>\n",
       "\t<li>0.0479075158651052</li>\n",
       "\t<li>0.0382467771585991</li>\n",
       "\t<li>0.0521301417772706</li>\n",
       "\t<li>0.160474064868337</li>\n",
       "\t<li>0.191231812054263</li>\n",
       "\t<li>0.0397004059039417</li>\n",
       "\t<li>0.166397618120375</li>\n",
       "\t<li>0.0118601763248746</li>\n",
       "\t<li>0.955225021979712</li>\n",
       "\t<li>0.0190896078550038</li>\n",
       "\t<li>0.211297604682006</li>\n",
       "\t<li>0.151360334432323</li>\n",
       "\t<li>0.00286926456983292</li>\n",
       "\t<li>0.210523662682554</li>\n",
       "\t<li>0.149085021179574</li>\n",
       "\t<li>0.264074540127379</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.0199699476169238\n",
       "\\item 0.0458199977164465\n",
       "\\item 0.495266696384679\n",
       "\\item 0.0472587034954134\n",
       "\\item 0.610468334947603\n",
       "\\item 0.136159621575197\n",
       "\\item 0.31704976973032\n",
       "\\item 0.330788076270406\n",
       "\\item 0.0572359528828377\n",
       "\\item 0.0083233190846819\n",
       "\\item 0.196416341440513\n",
       "\\item 0.180200264230589\n",
       "\\item 0.0113471974905296\n",
       "\\item 0.999999874308373\n",
       "\\item 0.0774379836991409\n",
       "\\item 0.0343189363764838\n",
       "\\item 0.0117077239538832\n",
       "\\item 0.175135339069821\n",
       "\\item 0.151597975314201\n",
       "\\item 0.0113879153094041\n",
       "\\item 0.0390493668900996\n",
       "\\item 0.267976114670805\n",
       "\\item 0.92963137112946\n",
       "\\item 0.01331014904008\n",
       "\\item 0.632286991324739\n",
       "\\item 0.0681840936289838\n",
       "\\item 0.167867084684194\n",
       "\\item 0.783166172347892\n",
       "\\item 0.539091118054234\n",
       "\\item 0.0885465145955104\n",
       "\\item 0.218503708926811\n",
       "\\item 0.714245072008431\n",
       "\\item 0.0193011550320404\n",
       "\\item 0.417379060294064\n",
       "\\item 0.714314667153859\n",
       "\\item 0.0989819113680306\n",
       "\\item 0.63212634718896\n",
       "\\item 0.863068467443166\n",
       "\\item 0.838679177471337\n",
       "\\item 0.0123271799699895\n",
       "\\item 0.200794933483637\n",
       "\\item 0.188821192097363\n",
       "\\item 0.0700626377808473\n",
       "\\item 0.837945571103761\n",
       "\\item 0.276310621835661\n",
       "\\item 0.657691841484601\n",
       "\\item 0.205660255581229\n",
       "\\item 0.0239168649629574\n",
       "\\item 0.156224136042788\n",
       "\\item 0.0727436295232659\n",
       "\\item 0.0502401352718925\n",
       "\\item 0.0271617649981027\n",
       "\\item 0.132303470328694\n",
       "\\item 0.176673984898641\n",
       "\\item 0.936209049546387\n",
       "\\item 0.714462545999191\n",
       "\\item 0.611341433297742\n",
       "\\item 0.522927481877359\n",
       "\\item 0.0856755142255962\n",
       "\\item 0.262617148260062\n",
       "\\item 0.899308500498465\n",
       "\\item 0.0121847961244518\n",
       "\\item 0.0169264820611136\n",
       "\\item 0.116634527172926\n",
       "\\item 0.0672691467996576\n",
       "\\item 0.019559155120192\n",
       "\\item 0.00197235396077447\n",
       "\\item 0.0108449809442752\n",
       "\\item 0.510216724093831\n",
       "\\item 0.0904532460508593\n",
       "\\item 0.435372525685862\n",
       "\\item 0.0049295633283936\n",
       "\\item 0.012455078937811\n",
       "\\item 0.541889294949295\n",
       "\\item 0.00895807358566363\n",
       "\\item 0.0936020507485612\n",
       "\\item 0.564364049580987\n",
       "\\item 0.0175227074763854\n",
       "\\item 0.319530217052547\n",
       "\\item 0.0260328768217256\n",
       "\\item 0.782656773290995\n",
       "\\item 0.0300054899951376\n",
       "\\item 0.0092407087558277\n",
       "\\item 0.557042322489272\n",
       "\\item 0.172767899139369\n",
       "\\item 0.169500475758547\n",
       "\\item 0.0105496860722101\n",
       "\\item 0.047448975283293\n",
       "\\item 0.721803564859241\n",
       "\\item 0.236527042683651\n",
       "\\item 0.0400844925094205\n",
       "\\item 0.0323106117152453\n",
       "\\item 0.0551054029703938\n",
       "\\item 0.881513227076948\n",
       "\\item 0.127465269427721\n",
       "\\item 0.171626208605749\n",
       "\\item 0.0198552567487982\n",
       "\\item 0.0939341549905087\n",
       "\\item 0.384039317177223\n",
       "\\item 0.490064623868398\n",
       "\\item 0.810220301244794\n",
       "\\item 0.0128802300860469\n",
       "\\item 0.628930818844018\n",
       "\\item 0.477956918518179\n",
       "\\item 0.211239062974798\n",
       "\\item 0.450625519167723\n",
       "\\item 0.0489693549931018\n",
       "\\item 0.153292202001712\n",
       "\\item 0.263943026571787\n",
       "\\item 0.0361809239692963\n",
       "\\item 0.014737017055404\n",
       "\\item 0.817887964218033\n",
       "\\item 0.00572570066559266\n",
       "\\item 0.565256351818271\n",
       "\\item 0.236803485421299\n",
       "\\item 0.00297010101132321\n",
       "\\item 0.00219648975771379\n",
       "\\item 0.499704006968091\n",
       "\\item 0.0303396620690834\n",
       "\\item 0.00558038007284844\n",
       "\\item 0.0424689032462572\n",
       "\\item 0.0278690778886793\n",
       "\\item 0.00941158628724017\n",
       "\\item 0.0669573500977942\n",
       "\\item 0.11273279278212\n",
       "\\item 0.00286946198686716\n",
       "\\item 0.0271483676136606\n",
       "\\item 0.218614350829553\n",
       "\\item 0.571809577970821\n",
       "\\item 0.0161068956824048\n",
       "\\item 0.464076015443004\n",
       "\\item 0.187053377790224\n",
       "\\item 0.241979368573769\n",
       "\\item 0.260805827999097\n",
       "\\item 0.383374781060198\n",
       "\\item 0.433144602201799\n",
       "\\item 0.00436676477761932\n",
       "\\item 0.722942271523976\n",
       "\\item 0.0810959085557277\n",
       "\\item 0.457050515442807\n",
       "\\item 0.247592719205121\n",
       "\\item 0.149236598016692\n",
       "\\item 0.190278667026799\n",
       "\\item 0.711020806121186\n",
       "\\item 0.311137916522735\n",
       "\\item 0.292990647337669\n",
       "\\item 0.511568232699909\n",
       "\\item 0.229699173606277\n",
       "\\item 0.116477659097698\n",
       "\\item 0.0367492647376478\n",
       "\\item 0.143062726825331\n",
       "\\item 0.301657220879435\n",
       "\\item 0.147534430963637\n",
       "\\item 0.0841891611580271\n",
       "\\item 0.00643526070543069\n",
       "\\item 0.0556656082727995\n",
       "\\item 0.0220379683539955\n",
       "\\item 0.378662367379383\n",
       "\\item 0.180499966171909\n",
       "\\item 0.784403150893681\n",
       "\\item 0.108861026226306\n",
       "\\item 0.0603342786261118\n",
       "\\item 0.0107996713554488\n",
       "\\item 0.127146975736733\n",
       "\\item 0.0275128873508818\n",
       "\\item 0.526974796553326\n",
       "\\item 0.0220476828074766\n",
       "\\item 0.70057123408146\n",
       "\\item 0.00382296065149055\n",
       "\\item 0.134059162106367\n",
       "\\item 0.52661504239586\n",
       "\\item 0.236627709655711\n",
       "\\item 0.39244447464418\n",
       "\\item 0.0100239226706501\n",
       "\\item 0.444120282795447\n",
       "\\item 0.00614967158187709\n",
       "\\item 0.251881076367534\n",
       "\\item 0.276438051909961\n",
       "\\item 0.467511826672706\n",
       "\\item 0.86772384271406\n",
       "\\item 0.756015126393805\n",
       "\\item 0.664003048244369\n",
       "\\item 0.48620790605888\n",
       "\\item 0.00159154789704167\n",
       "\\item 0.797357617195317\n",
       "\\item 0.123754012887735\n",
       "\\item 0.00915150482932242\n",
       "\\item 0.0676467822304575\n",
       "\\item 0.119022463031599\n",
       "\\item 0.174293104904208\n",
       "\\item 0.0118006942527187\n",
       "\\item 0.53804781128433\n",
       "\\item 0.613603538229885\n",
       "\\item 0.119289128555705\n",
       "\\item 0.00803418840343704\n",
       "\\item 0.0138828887671821\n",
       "\\item 0.0361448537960523\n",
       "\\item 0.0429285456110043\n",
       "\\item 0.0104389900359683\n",
       "\\item 0.729467703963586\n",
       "\\item 0.459945572340207\n",
       "\\item 0.0552130150495399\n",
       "\\item 0.0367526429414071\n",
       "\\item 0.559102877454589\n",
       "\\item 0.28454384680674\n",
       "\\item 0.578174400044091\n",
       "\\item 0.00733794610488339\n",
       "\\item 0.773034341157126\n",
       "\\item 0.0123431907981087\n",
       "\\item 0.729826707497585\n",
       "\\item 0.0639325591940032\n",
       "\\item 0.00659662247244716\n",
       "\\item 0.0207966120870647\n",
       "\\item 0.698048462346474\n",
       "\\item 0.0832348719605707\n",
       "\\item 0.425743992736508\n",
       "\\item 0.0500112452744067\n",
       "\\item 0.0315595359783704\n",
       "\\item 0.0128139565343078\n",
       "\\item 0.0615207431367373\n",
       "\\item 0.0576847675261676\n",
       "\\item 0.00786206014013649\n",
       "\\item 0.236294197111729\n",
       "\\item 0.594878093697926\n",
       "\\item 0.145727523457501\n",
       "\\item 0.0107497251696278\n",
       "\\item 0.117193432905089\n",
       "\\item 0.742498161477231\n",
       "\\item 0.326045461851359\n",
       "\\item 0.0154746336086699\n",
       "\\item 0.691470479197498\n",
       "\\item 0.035472218114744\n",
       "\\item 0.3243238016297\n",
       "\\item 0.0379532609969916\n",
       "\\item 0.0162007700833543\n",
       "\\item 0.0598537057380022\n",
       "\\item 0.12993090212402\n",
       "\\item 0.055016377954057\n",
       "\\item 0.42532440282483\n",
       "\\item 0.603929006780376\n",
       "\\item 0.0417853173593943\n",
       "\\item 0.0130308787284185\n",
       "\\item 0.0213242023163649\n",
       "\\item 0.252132484152406\n",
       "\\item 0.0320103020527634\n",
       "\\item 0.046142420252636\n",
       "\\item 0.225944302645433\n",
       "\\item 0.814561937550337\n",
       "\\item 0.391556731308694\n",
       "\\item 0.00171532327448696\n",
       "\\item 0.0206665028291591\n",
       "\\item 0.682130373225299\n",
       "\\item 0.772062002357203\n",
       "\\item 0.333155318384347\n",
       "\\item 0.0165101749053747\n",
       "\\item 0.516591044093818\n",
       "\\item 0.841115959677762\n",
       "\\item 0.195898652395022\n",
       "\\item 0.0156843758209774\n",
       "\\item 0.0365511431414871\n",
       "\\item 0.41489042890736\n",
       "\\item 0.213119768257293\n",
       "\\item 0.0426793419907824\n",
       "\\item 0.252557401266546\n",
       "\\item 0.370312273922775\n",
       "\\item 0.30530040193884\n",
       "\\item 0.0870845766246387\n",
       "\\item 0.0533006539540388\n",
       "\\item 0.0517563694307511\n",
       "\\item 0.183546371926233\n",
       "\\item 0.251103787332291\n",
       "\\item 0.0477140619717467\n",
       "\\item 0.190894647316061\n",
       "\\item 0.220178971392864\n",
       "\\item 0.0152378694649064\n",
       "\\item 0.0790836143686536\n",
       "\\item 0.344426444871542\n",
       "\\item 0.0292482233504427\n",
       "\\item 0.269250724139821\n",
       "\\item 0.191362671593097\n",
       "\\item 0.205383741574171\n",
       "\\item 0.0896329720627386\n",
       "\\item 0.126061754194506\n",
       "\\item 0.136648052408895\n",
       "\\item 0.54583599187173\n",
       "\\item 0.182070444777865\n",
       "\\item 0.0276649996261271\n",
       "\\item 0.525221181201475\n",
       "\\item 0.617625570371258\n",
       "\\item 0.27138290942917\n",
       "\\item 0.364024274891242\n",
       "\\item 0.0390194583915117\n",
       "\\item 0.536313766726566\n",
       "\\item 0.316752941622576\n",
       "\\item 0.222694306314753\n",
       "\\item 0.9999999556635\n",
       "\\item 0.852776026502151\n",
       "\\item 0.173081373488694\n",
       "\\item 0.0106313525774898\n",
       "\\item 0.012958796082242\n",
       "\\item 0.605079039893046\n",
       "\\item 0.0209566285165233\n",
       "\\item 0.0117290602580811\n",
       "\\item 0.00208421400211895\n",
       "\\item 0.0474264752745083\n",
       "\\item 0.0452644869921135\n",
       "\\item 0.176047049527089\n",
       "\\item 0.288992155005493\n",
       "\\item 0.0484168059490443\n",
       "\\item 0.00935030328410211\n",
       "\\item 0.0735783355315092\n",
       "\\item 0.366352469147635\n",
       "\\item 0.411857493946701\n",
       "\\item 0.0428127447273097\n",
       "\\item 0.0466857273831911\n",
       "\\item 0.670354944651671\n",
       "\\item 0.135337592343281\n",
       "\\item 0.106430211907991\n",
       "\\item 0.430768947542698\n",
       "\\item 0.377499049946076\n",
       "\\item 0.0569596311322483\n",
       "\\item 0.0782487023213659\n",
       "\\item 0.0403133305175846\n",
       "\\item 0.146504416543012\n",
       "\\item 0.141851106686674\n",
       "\\item 0.407257277517452\n",
       "\\item 0.407308457442344\n",
       "\\item 0.375945914538592\n",
       "\\item 0.0591865119366488\n",
       "\\item 0.335869707310747\n",
       "\\item 0.373718891213413\n",
       "\\item 0.299704703366878\n",
       "\\item 0.436507471586192\n",
       "\\item 0.525239621301956\n",
       "\\item 0.748810689599713\n",
       "\\item 0.0444664072064231\n",
       "\\item 0.420971911242324\n",
       "\\item 0.479008623665864\n",
       "\\item 0.0225867450198073\n",
       "\\item 0.0336935618577512\n",
       "\\item 0.0526724333108519\n",
       "\\item 0.0921678686500227\n",
       "\\item 0.145484880868177\n",
       "\\item 0.35388327919847\n",
       "\\item 0.375723540882374\n",
       "\\item 0.0484635475531039\n",
       "\\item 0.0669329697001514\n",
       "\\item 0.275664993452405\n",
       "\\item 0.0187046100288029\n",
       "\\item 0.0373261337703594\n",
       "\\item 0.0361548542119007\n",
       "\\item 0.0180698408998763\n",
       "\\item 0.144307393591685\n",
       "\\item 0.999999765774209\n",
       "\\item 0.0874662983887632\n",
       "\\item 0.494653149058208\n",
       "\\item 0.766595370098011\n",
       "\\item 0.04937193390233\n",
       "\\item 0.0587688149414015\n",
       "\\item 0.113350902296175\n",
       "\\item 0.0641453110264208\n",
       "\\item 0.0306221130817041\n",
       "\\item 0.0364971643592527\n",
       "\\item 0.0746120938024865\n",
       "\\item 0.153070839608285\n",
       "\\item 0.0923529700092577\n",
       "\\item 0.646101390255116\n",
       "\\item 0.0161892993419189\n",
       "\\item 0.0033641288571327\n",
       "\\item 0.0304969068173873\n",
       "\\item 0.856436492437686\n",
       "\\item 0.188495916703216\n",
       "\\item 0.00534169786257923\n",
       "\\item 0.105919053645878\n",
       "\\item 0.0775022656123414\n",
       "\\item 0.883106882604116\n",
       "\\item 0.084328904048033\n",
       "\\item 0.0403205585624215\n",
       "\\item 0.0212326212804359\n",
       "\\item 0.291700811308681\n",
       "\\item 0.186420928028463\n",
       "\\item 0.372208222677709\n",
       "\\item 0.0841111297047274\n",
       "\\item 0.466491451713264\n",
       "\\item 0.123605364073339\n",
       "\\item 0.12082936013055\n",
       "\\item 0.0106835858471097\n",
       "\\item 0.139360357664107\n",
       "\\item 0.130757114898081\n",
       "\\item 0.212864098500351\n",
       "\\item 0.00337488996762014\n",
       "\\item 0.0796780446935557\n",
       "\\item 0.525696186552051\n",
       "\\item 0.23379409560094\n",
       "\\item 0.0360437163578946\n",
       "\\item 0.313261216347766\n",
       "\\item 0.0153695605345677\n",
       "\\item 0.0198241590102087\n",
       "\\item 0.0559440741759468\n",
       "\\item 0.00263272934508541\n",
       "\\item 0.0172846066915633\n",
       "\\item 0.637519180213675\n",
       "\\item 0.066087308482829\n",
       "\\item 0.766196541476464\n",
       "\\item 0.0313326585610782\n",
       "\\item 0.407844199740332\n",
       "\\item 0.0731464017427767\n",
       "\\item 0.340667917237113\n",
       "\\item 0.850718506308087\n",
       "\\item 0.0526048912150345\n",
       "\\item 0.125206536717809\n",
       "\\item 0.014018347795696\n",
       "\\item 0.380067430682013\n",
       "\\item 0.00806281398673934\n",
       "\\item 0.312416832563002\n",
       "\\item 0.516806109975632\n",
       "\\item 0.156617486628282\n",
       "\\item 0.500536052575628\n",
       "\\item 0.203053643003371\n",
       "\\item 0.606848366882435\n",
       "\\item 0.147635101544277\n",
       "\\item 0.592510764997326\n",
       "\\item 0.00268754556486882\n",
       "\\item 0.0191301065063439\n",
       "\\item 0.106800900073335\n",
       "\\item 0.385639891578442\n",
       "\\item 0.0988073904345365\n",
       "\\item 0.242881225374657\n",
       "\\item 0.102640797904684\n",
       "\\item 0.342644094226348\n",
       "\\item 0.579634240456382\n",
       "\\item 0.0880472218016967\n",
       "\\item 0.391731286741306\n",
       "\\item 0.232641098884727\n",
       "\\item 0.848298568694339\n",
       "\\item 0.236062058943315\n",
       "\\item 0.0232003913889166\n",
       "\\item 0.118158466623914\n",
       "\\item 0.530891970377014\n",
       "\\item 0.245601262968426\n",
       "\\item 0.127651060608124\n",
       "\\item 0.0566126191986855\n",
       "\\item 0.17812038081585\n",
       "\\item 0.944361527376599\n",
       "\\item 0.0628925297638546\n",
       "\\item 0.607099296657314\n",
       "\\item 0.00647036352417058\n",
       "\\item 0.014506728146203\n",
       "\\item 0.52762505968881\n",
       "\\item 0.00803542976488018\n",
       "\\item 0.55339847312494\n",
       "\\item 0.125021655152885\n",
       "\\item 0.0122782554025418\n",
       "\\item 0.721077080670495\n",
       "\\item 0.203762662847281\n",
       "\\item 0.00319297473404992\n",
       "\\item 0.248570352049798\n",
       "\\item 0.29862730608033\n",
       "\\item 0.999999668512615\n",
       "\\item 0.736981259442357\n",
       "\\item 0.0441224070278956\n",
       "\\item 0.0221582979391203\n",
       "\\item 0.189982247487777\n",
       "\\item 0.894282444692239\n",
       "\\item 0.0437765767775495\n",
       "\\item 0.367857676672516\n",
       "\\item 0.172466364280629\n",
       "\\item 0.0243349213940347\n",
       "\\item 0.0175794820056814\n",
       "\\item 0.452609346022816\n",
       "\\item 0.137613484637788\n",
       "\\item 0.656618216663544\n",
       "\\item 0.0781637715273055\n",
       "\\item 0.0246332312798398\n",
       "\\item 0.571694330649898\n",
       "\\item 0.016002881459772\n",
       "\\item 0.199758297954567\n",
       "\\item 0.601404930111302\n",
       "\\item 0.217279361750711\n",
       "\\item 0.00286133259752111\n",
       "\\item 0.00527884708402513\n",
       "\\item 0.0825537081087439\n",
       "\\item 0.899053576453538\n",
       "\\item 0.0498238978185774\n",
       "\\item 0.00999921756695854\n",
       "\\item 0.0286649510015706\n",
       "\\item 0.67590343466665\n",
       "\\item 0.100243824833728\n",
       "\\item 0.404647265904865\n",
       "\\item 0.0132329010918809\n",
       "\\item 0.622210290276593\n",
       "\\item 0.187974394598899\n",
       "\\item 0.75914933789834\n",
       "\\item 0.46460801488613\n",
       "\\item 0.0744935782972362\n",
       "\\item 0.016853624195665\n",
       "\\item 0.940676545822903\n",
       "\\item 0.0102856266755571\n",
       "\\item 0.801658041621613\n",
       "\\item 0.0477185915405782\n",
       "\\item 0.0732503424959294\n",
       "\\item 0.0302669930852424\n",
       "\\item 0.0280085447009265\n",
       "\\item 0.013871306182459\n",
       "\\item 0.0647383577023884\n",
       "\\item 0.00811055369306502\n",
       "\\item 0.29543868779346\n",
       "\\item 0.354904215660545\n",
       "\\item 0.0951801706426184\n",
       "\\item 0.0204777669926373\n",
       "\\item 0.677036779526389\n",
       "\\item 0.20985532459379\n",
       "\\item 0.0483360111963359\n",
       "\\item 0.0334912135273815\n",
       "\\item 0.0208440532932362\n",
       "\\item 0.162408618117541\n",
       "\\item 0.0342050573802004\n",
       "\\item 0.0302878476153293\n",
       "\\item 0.7100987248817\n",
       "\\item 0.381194836611222\n",
       "\\item 0.0094529771604589\n",
       "\\item 0.0402290172810124\n",
       "\\item 0.58695037075786\n",
       "\\item 0.00321619240820627\n",
       "\\item 0.172097600353855\n",
       "\\item 0.387063094017129\n",
       "\\item 0.00574809919706981\n",
       "\\item 0.632587707744512\n",
       "\\item 0.0739718968498494\n",
       "\\item 0.00075272394936441\n",
       "\\item 0.192655555635881\n",
       "\\item 0.00694355381487038\n",
       "\\item 0.400550517664137\n",
       "\\item 0.625299783731324\n",
       "\\item 0.503589400637887\n",
       "\\item 0.015247599790401\n",
       "\\item 0.966539426126986\n",
       "\\item 0.592102754547421\n",
       "\\item 0.181275564495835\n",
       "\\item 0.131093707166075\n",
       "\\item 0.0657437081635146\n",
       "\\item 0.983885077238754\n",
       "\\item 0.0408730272219029\n",
       "\\item 0.0569603826233853\n",
       "\\item 0.0355543301314578\n",
       "\\item 0.0325323960705489\n",
       "\\item 0.498212231811771\n",
       "\\item 0.612572923896403\n",
       "\\item 0.0079727847894024\n",
       "\\item 0.460081353769792\n",
       "\\item 0.0119379321131558\n",
       "\\item 0.0346133714336782\n",
       "\\item 0.0188672155751794\n",
       "\\item 0.810873556214912\n",
       "\\item 0.401400629063512\n",
       "\\item 0.276390544212923\n",
       "\\item 0.153843597894376\n",
       "\\item 0.060038027606361\n",
       "\\item 0.017804359463568\n",
       "\\item 0.265074272736638\n",
       "\\item 0.0277494323330679\n",
       "\\item 0.0455792688099718\n",
       "\\item 0.0251425224731764\n",
       "\\item 0.269636746779349\n",
       "\\item 0.289120442725711\n",
       "\\item 0.0064975369843315\n",
       "\\item 0.147841056205854\n",
       "\\item 0.0495796982764737\n",
       "\\item 0.654069287569774\n",
       "\\item 0.248166936639415\n",
       "\\item 0.0368513111243656\n",
       "\\item 0.19111719200314\n",
       "\\item 0.370488328776153\n",
       "\\item 0.471546277126646\n",
       "\\item 0.383419329479009\n",
       "\\item 0.284179548560016\n",
       "\\item 0.60487845731714\n",
       "\\item 0.214259307519855\n",
       "\\item 0.0590188301682051\n",
       "\\item 0.020525284866343\n",
       "\\item 0.0246309874298711\n",
       "\\item 0.0127786609236949\n",
       "\\item 0.313523041958294\n",
       "\\item 0.311842939913688\n",
       "\\item 0.101597741560868\n",
       "\\item 0.603513866791461\n",
       "\\item 0.00479214537332067\n",
       "\\item 0.117154947437688\n",
       "\\item 0.219800049557997\n",
       "\\item 0.0171034990162961\n",
       "\\item 0.540297716322392\n",
       "\\item 0.0689034896757411\n",
       "\\item 0.01820575584003\n",
       "\\item 0.00227838794919667\n",
       "\\item 0.0023905725602704\n",
       "\\item 0.373918558449697\n",
       "\\item 0.0123804855931148\n",
       "\\item 0.00342162955273444\n",
       "\\item 0.0454900500367966\n",
       "\\item 0.614405940171795\n",
       "\\item 0.0344217594887273\n",
       "\\item 0.14507590877353\n",
       "\\item 0.539178293519048\n",
       "\\item 0.0254165815037278\n",
       "\\item 0.054414746981464\n",
       "\\item 0.322385011952746\n",
       "\\item 0.923803474700752\n",
       "\\item 0.0395919920019561\n",
       "\\item 0.0650213372766423\n",
       "\\item 0.00324105580869746\n",
       "\\item 0.0973600067040383\n",
       "\\item 0.345475723251949\n",
       "\\item 0.628954732600845\n",
       "\\item 0.0315861561523127\n",
       "\\item 0.0162456139902508\n",
       "\\item 0.829958584056793\n",
       "\\item 0.0157824164366038\n",
       "\\item 0.193132686377787\n",
       "\\item 0.0817198656589979\n",
       "\\item 0.934610357737708\n",
       "\\item 0.170657763868179\n",
       "\\item 0.620558604533632\n",
       "\\item 0.704158168790487\n",
       "\\item 0.379998434870672\n",
       "\\item 0.302483219838451\n",
       "\\item 0.112226301412075\n",
       "\\item 0.0930631664990264\n",
       "\\item 0.00455253010717648\n",
       "\\item 0.138205228973645\n",
       "\\item 0.0922202816433377\n",
       "\\item 0.00626807308392644\n",
       "\\item 0.145079792273125\n",
       "\\item 0.642471744984563\n",
       "\\item 0.525146730827957\n",
       "\\item 0.272499556188591\n",
       "\\item 0.0112859975074698\n",
       "\\item 0.215974245773398\n",
       "\\item 0.119223020522359\n",
       "\\item 0.0108070503893455\n",
       "\\item 0.031710100455674\n",
       "\\item 0.250448896349346\n",
       "\\item 0.059307305251946\n",
       "\\item 0.00557992835427086\n",
       "\\item 0.0550400837910643\n",
       "\\item 0.0299165091943819\n",
       "\\item 0.0245991091285857\n",
       "\\item 0.0908313575875495\n",
       "\\item 0.102443088020665\n",
       "\\item 0.529037659511435\n",
       "\\item 0.212354389990612\n",
       "\\item 0.0735288174845334\n",
       "\\item 0.0277750722591762\n",
       "\\item 0.430901572274588\n",
       "\\item 0.378389026351761\n",
       "\\item 0.252816928381743\n",
       "\\item 0.218371966458501\n",
       "\\item 0.511594920477143\n",
       "\\item 0.426762267196003\n",
       "\\item 0.36313913607403\n",
       "\\item 0.229652722872118\n",
       "\\item 0.999982909447387\n",
       "\\item 0.00573476837917188\n",
       "\\item 0.0221284190372738\n",
       "\\item 0.473720125868222\n",
       "\\item 0.218775520151417\n",
       "\\item 0.153918079075086\n",
       "\\item 0.570078205080328\n",
       "\\item 0.00673497966191684\n",
       "\\item 0.0108676534344724\n",
       "\\item 0.584801385624825\n",
       "\\item 0.557958525617629\n",
       "\\item 0.127662345311259\n",
       "\\item 0.00759444902871862\n",
       "\\item 0.734756665300793\n",
       "\\item 0.286417672352117\n",
       "\\item 0.444254734078745\n",
       "\\item 0.0124171302645206\n",
       "\\item 0.394702502404232\n",
       "\\item 0.186638748017359\n",
       "\\item 0.169808008391833\n",
       "\\item 0.0416221124213114\n",
       "\\item 0.80069246464988\n",
       "\\item 0.0739007862256507\n",
       "\\item 0.18201739186026\n",
       "\\item 0.203678734082994\n",
       "\\item 0.593907026751536\n",
       "\\item 0.305569586998538\n",
       "\\item 0.263643382196554\n",
       "\\item 0.0134936619851492\n",
       "\\item 0.208820994640213\n",
       "\\item 0.0549664596862607\n",
       "\\item 0.0286693218212616\n",
       "\\item 0.541920854508511\n",
       "\\item 0.390828504609042\n",
       "\\item 0.0336965980949221\n",
       "\\item 0.364172401964305\n",
       "\\item 0.051075154008684\n",
       "\\item 0.094488609256491\n",
       "\\item 0.0313731202656018\n",
       "\\item 0.0995308146759852\n",
       "\\item 0.0554973631729953\n",
       "\\item 0.152083172205623\n",
       "\\item 0.722991566287879\n",
       "\\item 0.189711784402361\n",
       "\\item 0.547964874321728\n",
       "\\item 0.688318268307373\n",
       "\\item 0.462524239085874\n",
       "\\item 0.0258162878044023\n",
       "\\item 0.267599346710097\n",
       "\\item 0.0912615391614897\n",
       "\\item 0.421269112895809\n",
       "\\item 0.951909913594931\n",
       "\\item 0.0106417124853625\n",
       "\\item 0.0147647082047271\n",
       "\\item 0.399001617248235\n",
       "\\item 0.3652506032079\n",
       "\\item 0.359153996941063\n",
       "\\item 0.52293226570314\n",
       "\\item 0.931774186479659\n",
       "\\item 0.045009603948656\n",
       "\\item 0.118841196690333\n",
       "\\item 0.225255072158649\n",
       "\\item 0.509376765758929\n",
       "\\item 0.217679294960974\n",
       "\\item 0.013327865518525\n",
       "\\item 0.0165579512922611\n",
       "\\item 0.60389610576593\n",
       "\\item 0.569661028875765\n",
       "\\item 0.368408491903595\n",
       "\\item 0.0154710552313364\n",
       "\\item 0.275897407486218\n",
       "\\item 0.502188344505214\n",
       "\\item 0.0787388213398721\n",
       "\\item 0.0967300618243921\n",
       "\\item 0.00494883580375058\n",
       "\\item 0.0641508423486803\n",
       "\\item 0.0306752135803522\n",
       "\\item 0.402309389714621\n",
       "\\item 0.00383458180257253\n",
       "\\item 0.022162456940421\n",
       "\\item 0.00835678330015797\n",
       "\\item 0.0426203782200652\n",
       "\\item 0.549851594086313\n",
       "\\item 0.0474251305226223\n",
       "\\item 0.67539938725115\n",
       "\\item 0.0204900547140519\n",
       "\\item 0.549072635790886\n",
       "\\item 0.516366445615041\n",
       "\\item 0.0193906860951293\n",
       "\\item 0.152919852151923\n",
       "\\item 0.0115180746729149\n",
       "\\item 0.0131243438935188\n",
       "\\item 0.0669864820543327\n",
       "\\item 0.0770191340567978\n",
       "\\item 0.0337849859576272\n",
       "\\item 0.584499862875758\n",
       "\\item 0.174133168075228\n",
       "\\item 0.0168523973033843\n",
       "\\item 0.036492761553111\n",
       "\\item 0.03296429270335\n",
       "\\item 0.13522573705344\n",
       "\\item 0.441435132119064\n",
       "\\item 0.0631999099092396\n",
       "\\item 0.443591054899236\n",
       "\\item 0.820625405561113\n",
       "\\item 0.14760614406295\n",
       "\\item 0.00921031969447578\n",
       "\\item 0.0407275279789619\n",
       "\\item 0.496519971203934\n",
       "\\item 0.352683457730475\n",
       "\\item 0.0337052154495489\n",
       "\\item 0.041021725067448\n",
       "\\item 0.657159124804541\n",
       "\\item 0.674788662579156\n",
       "\\item 0.589624181563565\n",
       "\\item 0.0378380543825788\n",
       "\\item 0.770466683785002\n",
       "\\item 0.312030005477519\n",
       "\\item 0.0203791415008888\n",
       "\\item 0.25260727593978\n",
       "\\item 0.0370764573951389\n",
       "\\item 0.010213880400914\n",
       "\\item 0.379636352117403\n",
       "\\item 0.00403537276895975\n",
       "\\item 0.108292436662199\n",
       "\\item 0.249821608841375\n",
       "\\item 0.342332203115704\n",
       "\\item 0.584098659595383\n",
       "\\item 0.0439314807877648\n",
       "\\item 0.387596786008076\n",
       "\\item 0.0138209702150517\n",
       "\\item 0.260108634346888\n",
       "\\item 0.669261470397866\n",
       "\\item 0.0102872462659718\n",
       "\\item 0.123577193624098\n",
       "\\item 0.0234988787420555\n",
       "\\item 0.00315546696297043\n",
       "\\item 0.0123239365043939\n",
       "\\item 0.041466787478148\n",
       "\\item 0.307849208920438\n",
       "\\item 0.0981298036112295\n",
       "\\item 0.0118130657382479\n",
       "\\item 0.266928381781508\n",
       "\\item 0.0494469030388882\n",
       "\\item 0.180771557197106\n",
       "\\item 0.00582339154323416\n",
       "\\item 0.58083375330643\n",
       "\\item 0.0246091628848339\n",
       "\\item 0.0564855465442005\n",
       "\\item 0.644209414662118\n",
       "\\item 0.289100550018003\n",
       "\\item 0.0938396094680775\n",
       "\\item 0.00385549300521934\n",
       "\\item 0.0378153241286041\n",
       "\\item 0.363537742091485\n",
       "\\item 0.447053123345453\n",
       "\\item 0.18576784797277\n",
       "\\item 0.387679087563473\n",
       "\\item 0.691089208128983\n",
       "\\item 0.0383015816281294\n",
       "\\item 0.094289063137743\n",
       "\\item 0.0530877723967058\n",
       "\\item 0.979161767606413\n",
       "\\item 0.430368104133677\n",
       "\\item 0.0541403278161665\n",
       "\\item 0.0270043072110092\n",
       "\\item 0.512078551749623\n",
       "\\item 0.00480679602286807\n",
       "\\item 0.200703002730321\n",
       "\\item 0.683521656236424\n",
       "\\item 0.0471513790862581\n",
       "\\item 0.00142491766924496\n",
       "\\item 0.580801622126068\n",
       "\\item 0.432030142618772\n",
       "\\item 0.952026895787423\n",
       "\\item 0.283430277644257\n",
       "\\item 0.210257501672467\n",
       "\\item 0.197776730461394\n",
       "\\item 0.0314478917634595\n",
       "\\item 0.616894050279868\n",
       "\\item 0.223820706046476\n",
       "\\item 0.816932175283422\n",
       "\\item 0.0237137255225264\n",
       "\\item 0.525538330221074\n",
       "\\item 0.899365439141373\n",
       "\\item 0.648562218250217\n",
       "\\item 0.0413256660936873\n",
       "\\item 0.0530809885017832\n",
       "\\item 0.0371021460691384\n",
       "\\item 0.666036891337846\n",
       "\\item 0.0503403084487381\n",
       "\\item 0.216736085460578\n",
       "\\item 0.529816914001044\n",
       "\\item 0.0170300823100406\n",
       "\\item 0.845662369979206\n",
       "\\item 0.286608517639903\n",
       "\\item 0.0453429413868556\n",
       "\\item 0.0168998093054599\n",
       "\\item 0.0309378666679549\n",
       "\\item 0.460958104606825\n",
       "\\item 0.00168174614456759\n",
       "\\item 0.0575316655816865\n",
       "\\item 0.0295832576561265\n",
       "\\item 0.30211582040186\n",
       "\\item 0.359100467014012\n",
       "\\item 0.0142723785962464\n",
       "\\item 0.152992139306264\n",
       "\\item 0.290629304585038\n",
       "\\item 0.34477857883198\n",
       "\\item 0.0639895413299661\n",
       "\\item 0.0723829748932035\n",
       "\\item 0.00956408821996863\n",
       "\\item 0.0129948419248106\n",
       "\\item 0.0896109652454796\n",
       "\\item 0.218414632305991\n",
       "\\item 0.395518672176483\n",
       "\\item 0.164719917850441\n",
       "\\item 0.337378224554789\n",
       "\\item 0.409249367689963\n",
       "\\item 0.750958644764419\n",
       "\\item 0.993024241858559\n",
       "\\item 0.552509093205434\n",
       "\\item 0.300306271671101\n",
       "\\item 0.144804852792329\n",
       "\\item 0.0986315585450896\n",
       "\\item 0.170781558190222\n",
       "\\item 0.0388472689638898\n",
       "\\item 0.942579881928659\n",
       "\\item 0.100843439938957\n",
       "\\item 0.385625233480742\n",
       "\\item 0.511611566425847\n",
       "\\item 0.620134773196079\n",
       "\\item 0.214033042597848\n",
       "\\item 0.158314813526888\n",
       "\\item 0.129795050606655\n",
       "\\item 0.109386720230056\n",
       "\\item 0.0981704340271325\n",
       "\\item 0.175908978783608\n",
       "\\item 0.109022694256191\n",
       "\\item 0.228573754468922\n",
       "\\item 0.0902800566130318\n",
       "\\item 0.56496302270547\n",
       "\\item 0.00655918326418388\n",
       "\\item 0.253354366901994\n",
       "\\item 0.603017468139228\n",
       "\\item 0.0347655953967357\n",
       "\\item 0.10580000118631\n",
       "\\item 0.0083025719003252\n",
       "\\item 0.219575793226539\n",
       "\\item 0.662334978091723\n",
       "\\item 0.0192483334061113\n",
       "\\item 0.738566013046589\n",
       "\\item 0.777172976452723\n",
       "\\item 0.432590569735931\n",
       "\\item 0.0254074775950213\n",
       "\\item 0.0317159200826119\n",
       "\\item 0.599506332897089\n",
       "\\item 0.586511967250636\n",
       "\\item 0.621486116547452\n",
       "\\item 0.0115517258002166\n",
       "\\item 0.0605978514231961\n",
       "\\item 0.639683791847728\n",
       "\\item 0.401322189073391\n",
       "\\item 0.99999992964077\n",
       "\\item 0.0168575565473987\n",
       "\\item 0.00370177835154455\n",
       "\\item 0.458223529318648\n",
       "\\item 0.173668631513075\n",
       "\\item 0.154046361181832\n",
       "\\item 0.536322719123752\n",
       "\\item 0.915712765181158\n",
       "\\item 0.165488761489117\n",
       "\\item 0.0650764613495248\n",
       "\\item 0.819550839293484\n",
       "\\item 0.101598300655427\n",
       "\\item 0.0656982498830971\n",
       "\\item 0.321449929578181\n",
       "\\item 0.044652300764393\n",
       "\\item 0.0308784400477463\n",
       "\\item 0.286529895034282\n",
       "\\item 0.313468252418951\n",
       "\\item 0.320396366228584\n",
       "\\item 0.315096211324578\n",
       "\\item 0.0094626318285661\n",
       "\\item 0.234500854469321\n",
       "\\item 0.0485966680983412\n",
       "\\item 0.0306373039133289\n",
       "\\item 0.999999939875605\n",
       "\\item 0.876353206266361\n",
       "\\item 0.758531543035791\n",
       "\\item 0.222113709306038\n",
       "\\item 0.201757308451926\n",
       "\\item 0.111621401533538\n",
       "\\item 0.740042850946609\n",
       "\\item 0.189892130939609\n",
       "\\item 0.370649402159646\n",
       "\\item 0.0136711727458992\n",
       "\\item 0.0347085172703979\n",
       "\\item 0.423681544014339\n",
       "\\item 0.017429461665319\n",
       "\\item 0.0110954028292281\n",
       "\\item 0.116377241092142\n",
       "\\item 0.00382670423927343\n",
       "\\item 0.0217295531117031\n",
       "\\item 0.0461711130922531\n",
       "\\item 0.192904837551817\n",
       "\\item 0.254077723904435\n",
       "\\item 0.0211117847937291\n",
       "\\item 0.442869254979738\n",
       "\\item 0.578907224354884\n",
       "\\item 0.124759142525714\n",
       "\\item 0.459858852421076\n",
       "\\item 0.531920055779343\n",
       "\\item 0.413125221294555\n",
       "\\item 0.234420446840034\n",
       "\\item 0.021949157617395\n",
       "\\item 0.864688431053634\n",
       "\\item 0.203674400884231\n",
       "\\item 0.190250163913924\n",
       "\\item 0.570898236070186\n",
       "\\item 0.752585899180292\n",
       "\\item 0.525509188624906\n",
       "\\item 0.282242331063523\n",
       "\\item 0.572963794309999\n",
       "\\item 0.745021396124277\n",
       "\\item 0.241342983100793\n",
       "\\item 0.127341682541055\n",
       "\\item 0.295692908843781\n",
       "\\item 0.216927415072056\n",
       "\\item 0.00354988961628633\n",
       "\\item 0.00906657281069526\n",
       "\\item 0.0134610499532325\n",
       "\\item 0.032610587379341\n",
       "\\item 0.212721050174182\n",
       "\\item 0.0336941251163405\n",
       "\\item 0.845287429391015\n",
       "\\item 0.0772480311694506\n",
       "\\item 0.0415381055168681\n",
       "\\item 0.00294287677568198\n",
       "\\item 0.385000324228021\n",
       "\\item 0.0189692585269595\n",
       "\\item 0.10563684310018\n",
       "\\item 0.41006316102086\n",
       "\\item 0.206635460236618\n",
       "\\item 0.144453713311913\n",
       "\\item 0.999999930693917\n",
       "\\item 0.802362441796549\n",
       "\\item 0.0570869712885768\n",
       "\\item 0.0062163176607514\n",
       "\\item 0.022757733737513\n",
       "\\item 0.135026298725444\n",
       "\\item 0.0706772612898688\n",
       "\\item 0.0149102391900019\n",
       "\\item 0.530340540229543\n",
       "\\item 0.0700719598294253\n",
       "\\item 0.619797393031223\n",
       "\\item 0.249289951159632\n",
       "\\item 0.214522047115071\n",
       "\\item 0.241476905602369\n",
       "\\item 0.581025331166739\n",
       "\\item 0.229537729165737\n",
       "\\item 0.0156763071588183\n",
       "\\item 0.00838477061773456\n",
       "\\item 0.0112938677246485\n",
       "\\item 0.00922797566104582\n",
       "\\item 0.182233202279052\n",
       "\\item 0.186647817566892\n",
       "\\item 0.637529836804787\n",
       "\\item 0.0651077506175047\n",
       "\\item 0.00615605951304662\n",
       "\\item 0.0498163662896792\n",
       "\\item 0.717465430491587\n",
       "\\item 0.740822422748179\n",
       "\\item 0.320125745668472\n",
       "\\item 0.430572704256078\n",
       "\\item 0.0464950395948776\n",
       "\\item 0.0260596362568755\n",
       "\\item 0.578887276573786\n",
       "\\item 0.0175645577515171\n",
       "\\item 0.103409777760653\n",
       "\\item 0.85426144584604\n",
       "\\item 0.435147639488308\n",
       "\\item 0.586469146015563\n",
       "\\item 0.044495562481657\n",
       "\\item 0.0446792253207579\n",
       "\\item 0.0200192340447763\n",
       "\\item 0.202046766133388\n",
       "\\item 0.0197862102524262\n",
       "\\item 0.866168370910104\n",
       "\\item 0.220309653898714\n",
       "\\item 0.367168500933266\n",
       "\\item 0.095357834961527\n",
       "\\item 0.238694714569209\n",
       "\\item 0.0241320906170692\n",
       "\\item 0.0328750065959386\n",
       "\\item 0.377486053253873\n",
       "\\item 0.00395549132352947\n",
       "\\item 0.00527288751066556\n",
       "\\item 0.235086477165255\n",
       "\\item 0.0144739114041722\n",
       "\\item 0.00635522691008368\n",
       "\\item 0.183120070641252\n",
       "\\item 0.482654739214698\n",
       "\\item 0.253982616107924\n",
       "\\item 0.269642814761059\n",
       "\\item 0.174129800510415\n",
       "\\item 0.285165538749012\n",
       "\\item 0.00166446284316019\n",
       "\\item 0.428576756148541\n",
       "\\item 0.864750745392298\n",
       "\\item 0.262160250272491\n",
       "\\item 0.775361391122565\n",
       "\\item 0.437557998593944\n",
       "\\item 0.100396201320877\n",
       "\\item 0.0625584857790366\n",
       "\\item 0.0148640217706099\n",
       "\\item 0.289021388332522\n",
       "\\item 0.00734551191106758\n",
       "\\item 0.475649970566081\n",
       "\\item 0.788356995517267\n",
       "\\item 0.610339466071035\n",
       "\\item 0.0855946833948425\n",
       "\\item 0.016886262585918\n",
       "\\item 0.359334891770436\n",
       "\\item 0.531629040085054\n",
       "\\item 0.012570018880217\n",
       "\\item 0.401280961903602\n",
       "\\item 0.285800420983862\n",
       "\\item 0.999999899391652\n",
       "\\item 0.0888033516629491\n",
       "\\item 0.679274958966115\n",
       "\\item 0.699092361664451\n",
       "\\item 0.786369010386977\n",
       "\\item 0.0329311038983416\n",
       "\\item 0.291024670329289\n",
       "\\item 0.149266119225962\n",
       "\\item 0.28157247532322\n",
       "\\item 0.357494012178614\n",
       "\\item 0.0182277356518192\n",
       "\\item 0.925413282505606\n",
       "\\item 0.139220916203728\n",
       "\\item 0.0125064075422918\n",
       "\\item 0.213127628749023\n",
       "\\item 0.508402524250166\n",
       "\\item 0.126469982458178\n",
       "\\item 0.264497391266\n",
       "\\item 0.436664519151672\n",
       "\\item 0.179911045434649\n",
       "\\item 0.0675101178974709\n",
       "\\item 0.469758723503266\n",
       "\\item 0.00108476919380634\n",
       "\\item 0.0502871692102015\n",
       "\\item 0.0633866987054689\n",
       "\\item 0.119501709935923\n",
       "\\item 0.466676158912044\n",
       "\\item 0.310300878091165\n",
       "\\item 0.999999147447549\n",
       "\\item 0.0862621250468959\n",
       "\\item 0.589699002573824\n",
       "\\item 0.0340958106540846\n",
       "\\item 0.208527613932907\n",
       "\\item 0.14542061372271\n",
       "\\item 0.0629816046156896\n",
       "\\item 0.840423979594436\n",
       "\\item 0.554403333439654\n",
       "\\item 0.246514541512246\n",
       "\\item 0.0041174717361653\n",
       "\\item 0.0107919343847855\n",
       "\\item 0.0184330466702109\n",
       "\\item 0.030832747655235\n",
       "\\item 0.99999996259574\n",
       "\\item 0.36480875288212\n",
       "\\item 0.0386046069383293\n",
       "\\item 0.0668992513567373\n",
       "\\item 0.0647919048913934\n",
       "\\item 0.144246540228767\n",
       "\\item 0.143138610278439\n",
       "\\item 0.0879836958120621\n",
       "\\item 0.332259734391238\n",
       "\\item 0.0319947834971824\n",
       "\\item 0.344343915942589\n",
       "\\item 0.010622928800904\n",
       "\\item 0.56482551860699\n",
       "\\item 0.0209093634221607\n",
       "\\item 0.0042353714747113\n",
       "\\item 0.719543640498619\n",
       "\\item 0.51695317027421\n",
       "\\item 0.110624286226349\n",
       "\\item 0.0417178617689054\n",
       "\\item 0.0298749809326296\n",
       "\\item 0.0875270512091124\n",
       "\\item 0.312629836641127\n",
       "\\item 0.0135589788440718\n",
       "\\item 0.322783229655624\n",
       "\\item 0.0725996861118289\n",
       "\\item 0.00693125984291952\n",
       "\\item 0.0524067668501133\n",
       "\\item 0.351444765477373\n",
       "\\item 0.108756601726957\n",
       "\\item 0.483537054294353\n",
       "\\item 0.804947871059299\n",
       "\\item 0.0660140388927749\n",
       "\\item 0.64819436885816\n",
       "\\item 0.238233231595257\n",
       "\\item 0.478779491874137\n",
       "\\item 0.596184681051985\n",
       "\\item 0.0774260173405864\n",
       "\\item 0.392008998828665\n",
       "\\item 0.158985831577443\n",
       "\\item 0.0689556610031463\n",
       "\\item 0.131936731389168\n",
       "\\item 0.379500234156237\n",
       "\\item 0.505233325002846\n",
       "\\item 0.0805139074077529\n",
       "\\item 0.138195033618495\n",
       "\\item 0.000613492335005297\n",
       "\\item 0.224049473408494\n",
       "\\item 0.27039819626497\n",
       "\\item 0.0588262376106442\n",
       "\\item 0.290807952041867\n",
       "\\item 0.0311343738390187\n",
       "\\item 0.384981768804915\n",
       "\\item 0.0328636207144629\n",
       "\\item 0.446884847470565\n",
       "\\item 0.0796920235654402\n",
       "\\item 0.306153800007737\n",
       "\\item 0.105690419986473\n",
       "\\item 0.0284524744413658\n",
       "\\item 0.0151126405406884\n",
       "\\item 0.112106467371821\n",
       "\\item 0.175400492093786\n",
       "\\item 0.431800454102634\n",
       "\\item 0.229696382210033\n",
       "\\item 0.0461283801584418\n",
       "\\item 0.0426430728317225\n",
       "\\item 0.0102192254548317\n",
       "\\item 0.387426183864605\n",
       "\\item 0.103129189776603\n",
       "\\item 0.164353813995948\n",
       "\\item 0.160166633041282\n",
       "\\item 0.343960831943811\n",
       "\\item 0.128269789287326\n",
       "\\item 0.448067419210507\n",
       "\\item 0.102143033927492\n",
       "\\item 0.0446640244954609\n",
       "\\item 0.00632588283494273\n",
       "\\item 0.334917997819173\n",
       "\\item 0.117036448203577\n",
       "\\item 0.0684230284893524\n",
       "\\item 0.118512756297235\n",
       "\\item 0.0343752981565862\n",
       "\\item 0.432497680919793\n",
       "\\item 0.139479191659523\n",
       "\\item 0.0140017766187891\n",
       "\\item 0.581409386187053\n",
       "\\item 0.707262374805594\n",
       "\\item 0.676039987152588\n",
       "\\item 0.111035127230789\n",
       "\\item 0.076719758971674\n",
       "\\item 0.111091887589308\n",
       "\\item 0.295261334057446\n",
       "\\item 0.00274376627747775\n",
       "\\item 0.00171409763294865\n",
       "\\item 0.242794444901855\n",
       "\\item 0.0365185630712038\n",
       "\\item 0.0381762833441883\n",
       "\\item 0.420266907762725\n",
       "\\item 0.4144607621968\n",
       "\\item 0.255141737848591\n",
       "\\item 0.0087406159898517\n",
       "\\item 0.149267505949089\n",
       "\\item 0.716772960375039\n",
       "\\item 0.428396484675624\n",
       "\\item 0.0897521400617911\n",
       "\\item 0.134422453775103\n",
       "\\item 0.159400382979137\n",
       "\\item 0.248156331997309\n",
       "\\item 0.999999899755454\n",
       "\\item 0.621003227524054\n",
       "\\item 0.411794970047196\n",
       "\\item 0.109357350356801\n",
       "\\item 0.00719209312836932\n",
       "\\item 0.00392713457488431\n",
       "\\item 0.643664784228587\n",
       "\\item 0.34850135185967\n",
       "\\item 0.0711569847618557\n",
       "\\item 0.351863175367112\n",
       "\\item 0.0288360561995615\n",
       "\\item 0.121008690138617\n",
       "\\item 0.341693623528626\n",
       "\\item 0.450786884324715\n",
       "\\item 0.839378356495052\n",
       "\\item 0.115675304010426\n",
       "\\item 0.395658873994819\n",
       "\\item 0.0780376514842547\n",
       "\\item 0.937222670441764\n",
       "\\item 0.0185688506937918\n",
       "\\item 0.0365611011513383\n",
       "\\item 0.459464686231983\n",
       "\\item 0.0123405080055394\n",
       "\\item 0.115300206157326\n",
       "\\item 0.333059677674429\n",
       "\\item 0.0194341950572159\n",
       "\\item 0.0116899936915638\n",
       "\\item 0.513825402584578\n",
       "\\item 0.0225353090383776\n",
       "\\item 0.0822178093006131\n",
       "\\item 0.480661788833197\n",
       "\\item 0.0966890152262564\n",
       "\\item 0.376341598089679\n",
       "\\item 0.0188280634091753\n",
       "\\item 0.0644800530654544\n",
       "\\item 0.496476381976845\n",
       "\\item 0.044486125992141\n",
       "\\item 0.242296131164322\n",
       "\\item 0.5462276438816\n",
       "\\item 0.373565724186034\n",
       "\\item 0.283126386745984\n",
       "\\item 0.0197569766692492\n",
       "\\item 0.0541379122086214\n",
       "\\item 0.0218449492671842\n",
       "\\item 0.39961999690153\n",
       "\\item 0.0128226676513324\n",
       "\\item 0.0397736359132368\n",
       "\\item 0.0674757811603202\n",
       "\\item 0.0244701811847007\n",
       "\\item 0.332434853551037\n",
       "\\item 0.0669214706400757\n",
       "\\item 0.810979291451371\n",
       "\\item 0.109771755561888\n",
       "\\item 0.360699543748818\n",
       "\\item 0.940448253429991\n",
       "\\item 0.318456867667438\n",
       "\\item 0.629184993765762\n",
       "\\item 0.0813133729422563\n",
       "\\item 0.620041187193279\n",
       "\\item 0.789030638685885\n",
       "\\item 0.344998351965315\n",
       "\\item 0.0550173805859845\n",
       "\\item 0.153916759875194\n",
       "\\item 0.330755455678276\n",
       "\\item 0.0194895318232061\n",
       "\\item 0.205749882986029\n",
       "\\item 0.553154358672463\n",
       "\\item 0.213651706951662\n",
       "\\item 0.591771298601621\n",
       "\\item 0.00611919438776252\n",
       "\\item 0.0822670563834605\n",
       "\\item 0.725529709606978\n",
       "\\item 0.0023654921756232\n",
       "\\item 0.0443624987354101\n",
       "\\item 0.0734532134021451\n",
       "\\item 0.111738043298359\n",
       "\\item 0.0189999346797714\n",
       "\\item 0.58166001331911\n",
       "\\item 0.352322672091126\n",
       "\\item 0.00410926428703993\n",
       "\\item 0.0873441475972424\n",
       "\\item 0.408803335657935\n",
       "\\item 0.784146396581265\n",
       "\\item 0.635640629612414\n",
       "\\item 0.115891386401419\n",
       "\\item 0.200717926772197\n",
       "\\item 0.299687427534232\n",
       "\\item 0.00672402856835594\n",
       "\\item 0.801570047356619\n",
       "\\item 0.0426671777978862\n",
       "\\item 0.0127759868029564\n",
       "\\item 0.242675951233782\n",
       "\\item 0.126772723549678\n",
       "\\item 0.0439637533488899\n",
       "\\item 0.257337844571137\n",
       "\\item 0.543337799086199\n",
       "\\item 0.208106641889324\n",
       "\\item 0.132721146618251\n",
       "\\item 0.531606266396264\n",
       "\\item 0.292998879059461\n",
       "\\item 0.00966018862869711\n",
       "\\item 0.0193694640301211\n",
       "\\item 0.15510703520612\n",
       "\\item 0.0161150653010276\n",
       "\\item 0.282592032803302\n",
       "\\item 0.00694463553896689\n",
       "\\item 0.201647592687945\n",
       "\\item 0.422466741704975\n",
       "\\item 0.242953581399268\n",
       "\\item 0.00351089833854013\n",
       "\\item 0.400627588277948\n",
       "\\item 0.0389228867741802\n",
       "\\item 0.0521593751657165\n",
       "\\item 0.331435704194659\n",
       "\\item 0.0455245248572239\n",
       "\\item 0.0309064616653328\n",
       "\\item 0.0878612310292512\n",
       "\\item 0.0133130674654993\n",
       "\\item 0.68832676321178\n",
       "\\item 0.659228577916452\n",
       "\\item 0.227359436505122\n",
       "\\item 0.481600127096135\n",
       "\\item 0.261394497440249\n",
       "\\item 0.0130665052484996\n",
       "\\item 0.520518344967139\n",
       "\\item 0.647744267628216\n",
       "\\item 0.250722701012656\n",
       "\\item 0.373737729831715\n",
       "\\item 0.325611836248114\n",
       "\\item 0.00319015496483199\n",
       "\\item 0.339464434005485\n",
       "\\item 0.0966684552130676\n",
       "\\item 0.0617476612895288\n",
       "\\item 0.19152926225361\n",
       "\\item 0.11103601264329\n",
       "\\item 0.61590524200321\n",
       "\\item 0.0215583805673567\n",
       "\\item 0.140053997430894\n",
       "\\item 0.0477986234154584\n",
       "\\item 0.0826264263420942\n",
       "\\item 0.00702769078577563\n",
       "\\item 0.826192755491458\n",
       "\\item 0.257834891033145\n",
       "\\item 0.286794215658679\n",
       "\\item 0.106014480810485\n",
       "\\item 0.340629795332494\n",
       "\\item 0.480350855810903\n",
       "\\item 0.155249929153167\n",
       "\\item 0.285211952849519\n",
       "\\item 0.324050126089352\n",
       "\\item 0.0703373040441296\n",
       "\\item 0.365802250433597\n",
       "\\item 0.360178385500458\n",
       "\\item 0.083657639466234\n",
       "\\item 0.0120954505667411\n",
       "\\item 0.0157389085382795\n",
       "\\item 0.0510121515711138\n",
       "\\item 0.18566362469721\n",
       "\\item 0.21093029457333\n",
       "\\item 0.0339366834988751\n",
       "\\item 0.129181519812422\n",
       "\\item 0.0551722889180708\n",
       "\\item 0.0583173531946214\n",
       "\\item 0.0565857009501661\n",
       "\\item 0.195500676987564\n",
       "\\item 0.387298210598922\n",
       "\\item 0.0174095793128992\n",
       "\\item 0.54469816524027\n",
       "\\item 0.999999881272755\n",
       "\\item 0.1027611960477\n",
       "\\item 0.47667823007691\n",
       "\\item 0.0662688998929444\n",
       "\\item 0.116922734592427\n",
       "\\item 0.0850278394625899\n",
       "\\item 0.392637026170667\n",
       "\\item 0.0100271760538756\n",
       "\\item 0.882515089158017\n",
       "\\item 0.149166487271602\n",
       "\\item 0.0447689462679109\n",
       "\\item 0.780293803729918\n",
       "\\item 0.577707688286746\n",
       "\\item 0.0186315861936365\n",
       "\\item 0.00826138172958256\n",
       "\\item 0.208537293731533\n",
       "\\item 0.270569664407966\n",
       "\\item 0.51863093991967\n",
       "\\item 0.174336426356056\n",
       "\\item 0.31521906329962\n",
       "\\item 0.422335186398251\n",
       "\\item 0.417747688186922\n",
       "\\item 0.209491591975983\n",
       "\\item 0.00632646199144239\n",
       "\\item 0.749652383357013\n",
       "\\item 0.0135803141014883\n",
       "\\item 0.284565936383715\n",
       "\\item 0.0203262184445531\n",
       "\\item 0.837222888250718\n",
       "\\item 0.0557357281173115\n",
       "\\item 0.20565285399988\n",
       "\\item 0.339038777221873\n",
       "\\item 0.0100044063394905\n",
       "\\item 0.357838781913102\n",
       "\\item 0.392694598286668\n",
       "\\item 0.123665880291385\n",
       "\\item 0.024835452290815\n",
       "\\item 0.798304952283806\n",
       "\\item 0.0710472788783619\n",
       "\\item 0.217491479794416\n",
       "\\item 0.618326994120905\n",
       "\\item 0.0400660581512396\n",
       "\\item 0.176943638370039\n",
       "\\item 0.425878858065581\n",
       "\\item 0.0485012868145134\n",
       "\\item 0.011622000593429\n",
       "\\item 0.163495250067349\n",
       "\\item 0.28876832052196\n",
       "\\item 0.150084973932894\n",
       "\\item 0.761837899159859\n",
       "\\item 0.0905138213694239\n",
       "\\item 0.117727398337633\n",
       "\\item 0.0129693312408954\n",
       "\\item 0.270787542001476\n",
       "\\item 0.303445040822964\n",
       "\\item 0.0045746293789501\n",
       "\\item 0.673561166508947\n",
       "\\item 0.420120032207368\n",
       "\\item 0.718770945107793\n",
       "\\item 0.0157843933453791\n",
       "\\item 0.773586185200803\n",
       "\\item 0.0444909631293543\n",
       "\\item 0.101154237579664\n",
       "\\item 0.00362551629215413\n",
       "\\item 0.864277448109038\n",
       "\\item 0.401290367065925\n",
       "\\item 0.0044193786966831\n",
       "\\item 0.105168455461581\n",
       "\\item 0.000704355504237902\n",
       "\\item 0.0782380967993163\n",
       "\\item 0.0680032169510186\n",
       "\\item 0.325215314840005\n",
       "\\item 0.522137318069587\n",
       "\\item 0.0695479591239362\n",
       "\\item 0.910088474045432\n",
       "\\item 0.0225768752466563\n",
       "\\item 0.0830117248087445\n",
       "\\item 0.782424262078024\n",
       "\\item 0.00284140544328993\n",
       "\\item 0.181802023547386\n",
       "\\item 0.446750806198583\n",
       "\\item 0.175307512265613\n",
       "\\item 0.211994718778293\n",
       "\\item 0.362056016463592\n",
       "\\item 0.698494628531572\n",
       "\\item 0.796409402741523\n",
       "\\item 0.69701665386244\n",
       "\\item 0.60498841832336\n",
       "\\item 0.222151410916736\n",
       "\\item 0.23245232118953\n",
       "\\item 0.0312143806769226\n",
       "\\item 0.00597863316182652\n",
       "\\item 0.0474385359674159\n",
       "\\item 0.0692646562294439\n",
       "\\item 0.813293860136716\n",
       "\\item 0.37800470789794\n",
       "\\item 0.0826239869534869\n",
       "\\item 0.0535687457396391\n",
       "\\item 0.381792304137231\n",
       "\\item 0.0224097542310455\n",
       "\\item 0.0571640111552471\n",
       "\\item 0.500278393587007\n",
       "\\item 0.737791677638268\n",
       "\\item 0.401349247208118\n",
       "\\item 0.436029511070447\n",
       "\\item 0.0327230826765334\n",
       "\\item 0.00927603454896041\n",
       "\\item 0.122509979589628\n",
       "\\item 0.0278683778105911\n",
       "\\item 0.852582373974251\n",
       "\\item 0.015712446760872\n",
       "\\item 0.0940175246146619\n",
       "\\item 0.573673495045516\n",
       "\\item 0.5041167751129\n",
       "\\item 0.0102770308244628\n",
       "\\item 0.00118549280420903\n",
       "\\item 0.131375428121562\n",
       "\\item 0.438320910689265\n",
       "\\item 0.756097754809503\n",
       "\\item 0.378041113022159\n",
       "\\item 0.0357551574045644\n",
       "\\item 0.134654161705776\n",
       "\\item 0.259550862282099\n",
       "\\item 0.778064247830644\n",
       "\\item 0.0201462974622649\n",
       "\\item 0.24006009465598\n",
       "\\item 0.678198913733828\n",
       "\\item 0.300570735597846\n",
       "\\item 0.0469963534947108\n",
       "\\item 0.502486365272292\n",
       "\\item 0.0143151454453701\n",
       "\\item 0.0651614188522869\n",
       "\\item 0.836073441063167\n",
       "\\item 0.259320005941914\n",
       "\\item 0.0108894451187177\n",
       "\\item 0.30730254345272\n",
       "\\item 0.0983187936355585\n",
       "\\item 0.0380510346564614\n",
       "\\item 0.607227303527659\n",
       "\\item 0.00786637524450243\n",
       "\\item 0.345422292451516\n",
       "\\item 0.0183754642854709\n",
       "\\item 0.206154474247793\n",
       "\\item 0.0099216288357854\n",
       "\\item 0.0414194474098731\n",
       "\\item 0.0829841263907362\n",
       "\\item 0.0306311878548479\n",
       "\\item 0.0285561501172792\n",
       "\\item 0.0605196262065695\n",
       "\\item 0.11832520017891\n",
       "\\item 0.0109672154767565\n",
       "\\item 0.0127333624065195\n",
       "\\item 0.00224888673714246\n",
       "\\item 0.0429628899674597\n",
       "\\item 0.0251811856175837\n",
       "\\item 0.0295559817119572\n",
       "\\item 0.00388163350760284\n",
       "\\item 0.261194695234384\n",
       "\\item 0.882634420272317\n",
       "\\item 0.171042965249206\n",
       "\\item 0.0149085330205609\n",
       "\\item 0.00158657563942785\n",
       "\\item 0.556844563218709\n",
       "\\item 0.387655918204949\n",
       "\\item 0.0661323040382908\n",
       "\\item 0.608800003409554\n",
       "\\item 0.574721973559668\n",
       "\\item 0.0521771218150772\n",
       "\\item 0.902118697498264\n",
       "\\item 0.0820669258207579\n",
       "\\item 0.00754209020126828\n",
       "\\item 0.723437201507968\n",
       "\\item 0.101828009736096\n",
       "\\item 0.468876692554582\n",
       "\\item 0.2468317337222\n",
       "\\item 0.512240316374758\n",
       "\\item 0.087004829379109\n",
       "\\item 0.0149338140834091\n",
       "\\item 0.45103091765394\n",
       "\\item 0.0231078515925126\n",
       "\\item 0.0443657247274287\n",
       "\\item 0.290537482498241\n",
       "\\item 0.561325115718624\n",
       "\\item 0.342226939316761\n",
       "\\item 0.12864443652351\n",
       "\\item 0.689437807152223\n",
       "\\item 0.333644758404331\n",
       "\\item 0.0820764525219253\n",
       "\\item 0.042152085677163\n",
       "\\item 0.224722384138494\n",
       "\\item 0.529648383441074\n",
       "\\item 0.0207762852410577\n",
       "\\item 0.00440931528151489\n",
       "\\item 0.411697715892915\n",
       "\\item 0.0802832296717964\n",
       "\\item 0.346339647881345\n",
       "\\item 0.0130900437365671\n",
       "\\item 0.012744641309673\n",
       "\\item 0.429693038206176\n",
       "\\item 0.211733598664388\n",
       "\\item 0.713899951037443\n",
       "\\item 0.0204587633545007\n",
       "\\item 0.969105900109868\n",
       "\\item 0.0120568543931244\n",
       "\\item 0.900706976430349\n",
       "\\item 0.00322821636077589\n",
       "\\item 0.0133650536120327\n",
       "\\item 0.198050332159119\n",
       "\\item 0.702582904313514\n",
       "\\item 0.660678973776002\n",
       "\\item 0.00893567809480346\n",
       "\\item 0.0246127601627504\n",
       "\\item 0.0372872768121372\n",
       "\\item 0.137123364030737\n",
       "\\item 0.520023981229925\n",
       "\\item 0.0066450794922484\n",
       "\\item 0.999999934069721\n",
       "\\item 0.699187065784779\n",
       "\\item 0.604873083636888\n",
       "\\item 0.333363366288427\n",
       "\\item 0.666210889741808\n",
       "\\item 0.12270428875785\n",
       "\\item 0.67271433309237\n",
       "\\item 0.186352219335337\n",
       "\\item 0.0255251185695469\n",
       "\\item 0.103295434259188\n",
       "\\item 0.292404583057265\n",
       "\\item 0.107016716840353\n",
       "\\item 0.0191613585884338\n",
       "\\item 0.323385344438563\n",
       "\\item 0.323606556162005\n",
       "\\item 0.00266290871999711\n",
       "\\item 0.0429157016021198\n",
       "\\item 0.00486193696698884\n",
       "\\item 0.350003539491226\n",
       "\\item 0.372556174351423\n",
       "\\item 0.0626083960843705\n",
       "\\item 0.191544290861375\n",
       "\\item 0.0938000049788963\n",
       "\\item 0.153051598225422\n",
       "\\item 0.480187997110702\n",
       "\\item 0.0052630599731583\n",
       "\\item 0.27972661029505\n",
       "\\item 0.0398773138398133\n",
       "\\item 0.00162572191553019\n",
       "\\item 0.442823136141403\n",
       "\\item 0.0120391413903642\n",
       "\\item 0.10566201095036\n",
       "\\item 0.093718039648838\n",
       "\\item 0.164142373320656\n",
       "\\item 0.383839605878613\n",
       "\\item 0.00451305545018589\n",
       "\\item 0.537594286597542\n",
       "\\item 0.466383798477598\n",
       "\\item 0.0281582762622777\n",
       "\\item 0.29656351506812\n",
       "\\item 0.0500069544563138\n",
       "\\item 0.0179109290575799\n",
       "\\item 0.182654043547901\n",
       "\\item 0.173632291650515\n",
       "\\item 0.314874409741457\n",
       "\\item 0.00285495418430275\n",
       "\\item 0.144684627437276\n",
       "\\item 0.0117231450258738\n",
       "\\item 0.427994448144244\n",
       "\\item 0.16361830683771\n",
       "\\item 0.00846342302494718\n",
       "\\item 0.0162058441102335\n",
       "\\item 0.261560471705074\n",
       "\\item 0.335900678213885\n",
       "\\item 0.999999719882787\n",
       "\\item 0.0223168252846246\n",
       "\\item 0.171904897795075\n",
       "\\item 0.0132759069202664\n",
       "\\item 0.571779516237679\n",
       "\\item 0.143311262794378\n",
       "\\item 0.0636269799691191\n",
       "\\item 0.0986192994529642\n",
       "\\item 0.186532650875311\n",
       "\\item 0.10725450032835\n",
       "\\item 0.0587551753964388\n",
       "\\item 0.0113480679433616\n",
       "\\item 0.373801111008503\n",
       "\\item 0.146091797132755\n",
       "\\item 0.00472946298492059\n",
       "\\item 0.0148767186716644\n",
       "\\item 0.402492786015853\n",
       "\\item 0.599905774626979\n",
       "\\item 0.129156093581038\n",
       "\\item 0.0327122735411403\n",
       "\\item 0.00270956646316803\n",
       "\\item 0.414419234280421\n",
       "\\item 0.0748696899689292\n",
       "\\item 0.210084474844711\n",
       "\\item 0.0505849568379707\n",
       "\\item 0.37878641187\n",
       "\\item 0.0520068591141141\n",
       "\\item 0.191747610946361\n",
       "\\item 0.073796405649198\n",
       "\\item 0.857763278077361\n",
       "\\item 0.287226861085736\n",
       "\\item 0.156571082241875\n",
       "\\item 0.679872307420937\n",
       "\\item 0.607413476372354\n",
       "\\item 0.0259541990026681\n",
       "\\item 0.438749681108358\n",
       "\\item 0.27895648356056\n",
       "\\item 0.0302853846441377\n",
       "\\item 0.0359113469890458\n",
       "\\item 0.180162495915178\n",
       "\\item 0.0416986274509818\n",
       "\\item 0.376890684622951\n",
       "\\item 0.231963166688558\n",
       "\\item 0.494434450132828\n",
       "\\item 0.0209143587981374\n",
       "\\item 0.107206716581593\n",
       "\\item 0.708322155399093\n",
       "\\item 0.913731609334851\n",
       "\\item 0.104157703080455\n",
       "\\item 0.0556707696440846\n",
       "\\item 0.0175382849056347\n",
       "\\item 0.460250756300556\n",
       "\\item 0.00892961678058072\n",
       "\\item 0.0426768272972568\n",
       "\\item 0.738494445202362\n",
       "\\item 0.0139891946964155\n",
       "\\item 0.0173010271782344\n",
       "\\item 0.36643548507285\n",
       "\\item 0.744312090543482\n",
       "\\item 0.0881357595922892\n",
       "\\item 0.00942054215640495\n",
       "\\item 0.068128632951133\n",
       "\\item 0.0388245693052423\n",
       "\\item 0.392192135594782\n",
       "\\item 0.388390494088206\n",
       "\\item 0.651204649158032\n",
       "\\item 0.180637714329667\n",
       "\\item 0.102705415062379\n",
       "\\item 0.020707264284863\n",
       "\\item 0.0256075381477818\n",
       "\\item 0.00250860705648477\n",
       "\\item 0.681940048158735\n",
       "\\item 0.350262020578293\n",
       "\\item 0.0383984308896455\n",
       "\\item 0.658734181967604\n",
       "\\item 0.0669926729564842\n",
       "\\item 0.533022350312315\n",
       "\\item 0.212693264031104\n",
       "\\item 0.738803576604701\n",
       "\\item 0.0389983717122111\n",
       "\\item 0.747525660111215\n",
       "\\item 0.785958005108221\n",
       "\\item 0.152504421161402\n",
       "\\item 0.0189503594342853\n",
       "\\item 0.235368374657788\n",
       "\\item 0.0416053578011781\n",
       "\\item 0.166950509764835\n",
       "\\item 0.744520527611533\n",
       "\\item 0.0529810394056784\n",
       "\\item 0.00327444356051325\n",
       "\\item 0.149399902870609\n",
       "\\item 0.402424644821861\n",
       "\\item 0.0276814867259534\n",
       "\\item 0.00991715759274851\n",
       "\\item 0.4251773610361\n",
       "\\item 0.25052315786123\n",
       "\\item 0.109479032663\n",
       "\\item 0.00821826282945569\n",
       "\\item 0.227110410995884\n",
       "\\item 0.14806988830362\n",
       "\\item 0.359347359420511\n",
       "\\item 0.210979644010036\n",
       "\\item 0.0463032214188742\n",
       "\\item 0.46875383462234\n",
       "\\item 0.215888946331947\n",
       "\\item 0.713179985277368\n",
       "\\item 0.0133076759758265\n",
       "\\item 0.123259371522801\n",
       "\\item 0.0350621484165317\n",
       "\\item 0.0218596503142545\n",
       "\\item 0.00709909504330927\n",
       "\\item 0.246300702617571\n",
       "\\item 0.0722452739253546\n",
       "\\item 0.768767034364842\n",
       "\\item 0.103045751032903\n",
       "\\item 0.0427102616103358\n",
       "\\item 0.0220479418402836\n",
       "\\item 0.920016590050812\n",
       "\\item 0.0211256570489751\n",
       "\\item 0.39996253462366\n",
       "\\item 0.0548779363750377\n",
       "\\item 0.0252650412505952\n",
       "\\item 0.257208730113193\n",
       "\\item 0.680939629125579\n",
       "\\item 0.729255611723037\n",
       "\\item 0.596928459338522\n",
       "\\item 0.999999369186944\n",
       "\\item 0.0263022407673469\n",
       "\\item 0.384847862812985\n",
       "\\item 0.549343043856729\n",
       "\\item 0.173576609514886\n",
       "\\item 0.080540638126581\n",
       "\\item 0.737058762166601\n",
       "\\item 0.00306032494596797\n",
       "\\item 0.0717116554481647\n",
       "\\item 0.309395303411374\n",
       "\\item 0.0220249069614392\n",
       "\\item 0.353891615376487\n",
       "\\item 0.0516877938558486\n",
       "\\item 0.197422325508889\n",
       "\\item 0.625577234432896\n",
       "\\item 0.0555821964514438\n",
       "\\item 0.589136527507761\n",
       "\\item 0.0555530611510398\n",
       "\\item 0.475650326664653\n",
       "\\item 0.115993377295593\n",
       "\\item 0.358102933017236\n",
       "\\item 0.84324883081674\n",
       "\\item 0.0322303964399007\n",
       "\\item 0.00261417803905965\n",
       "\\item 0.0192613020090237\n",
       "\\item 0.783702346299451\n",
       "\\item 0.999999921868559\n",
       "\\item 0.0079345660994076\n",
       "\\item 0.00387438423090134\n",
       "\\item 0.0211480019974471\n",
       "\\item 0.050450558487875\n",
       "\\item 0.0445260716037124\n",
       "\\item 0.0293364700879047\n",
       "\\item 0.0855610339125738\n",
       "\\item 0.641965499370153\n",
       "\\item 0.0489075172471356\n",
       "\\item 0.0013621038091988\n",
       "\\item 0.537891437772786\n",
       "\\item 0.886347490567167\n",
       "\\item 0.16125326393139\n",
       "\\item 0.0309850610819549\n",
       "\\item 0.693530738886448\n",
       "\\item 0.00561970245310959\n",
       "\\item 0.302654046642452\n",
       "\\item 0.00785647756233152\n",
       "\\item 0.191613057594669\n",
       "\\item 0.011638934751399\n",
       "\\item 0.243860321451445\n",
       "\\item 0.444726091154691\n",
       "\\item 0.0572090029004352\n",
       "\\item 0.261446047313538\n",
       "\\item 0.0171908586343519\n",
       "\\item 0.575854903961387\n",
       "\\item 0.192877574350031\n",
       "\\item 0.00422424646968325\n",
       "\\item 0.922205852496056\n",
       "\\item 0.0704851393216222\n",
       "\\item 0.567080304207818\n",
       "\\item 0.47606140786739\n",
       "\\item 0.306320674105787\n",
       "\\item 0.0369710971674199\n",
       "\\item 0.819697805813227\n",
       "\\item 0.013248651624041\n",
       "\\item 0.0631829730650491\n",
       "\\item 0.378553828139803\n",
       "\\item 0.0413801184849777\n",
       "\\item 0.27543407874579\n",
       "\\item 0.545895504384078\n",
       "\\item 0.259934954639882\n",
       "\\item 0.0165679910614728\n",
       "\\item 0.262467341480874\n",
       "\\item 0.00851088909340637\n",
       "\\item 0.132238607019351\n",
       "\\item 0.00264874929875302\n",
       "\\item 0.514256243669981\n",
       "\\item 0.504662694825196\n",
       "\\item 0.46918137663433\n",
       "\\item 0.0206286273389961\n",
       "\\item 0.0130306565509383\n",
       "\\item 0.0216268255776056\n",
       "\\item 0.962873976004148\n",
       "\\item 0.0228586561112876\n",
       "\\item 0.415886038439882\n",
       "\\item 0.111644078337856\n",
       "\\item 0.035732224743968\n",
       "\\item 0.183323652433876\n",
       "\\item 0.00962911740164285\n",
       "\\item 0.472661642158482\n",
       "\\item 0.063094439257077\n",
       "\\item 0.739556130631472\n",
       "\\item 0.540561789482398\n",
       "\\item 0.476699904607004\n",
       "\\item 0.0574012928104084\n",
       "\\item 0.0248904132738773\n",
       "\\item 0.320685216707128\n",
       "\\item 0.0259059737871506\n",
       "\\item 0.375706690430921\n",
       "\\item 0.0318621776115162\n",
       "\\item 0.835147724968061\n",
       "\\item 0.579956282477218\n",
       "\\item 0.771459176592422\n",
       "\\item 0.0929458415001703\n",
       "\\item 0.140511726836116\n",
       "\\item 0.0944645275237256\n",
       "\\item 0.165131169764395\n",
       "\\item 0.779965172205315\n",
       "\\item 0.0135918638149194\n",
       "\\item 0.00310894475377708\n",
       "\\item 0.999999921059875\n",
       "\\item 0.241733153817061\n",
       "\\item 0.658845773041207\n",
       "\\item 0.6818222401056\n",
       "\\item 0.0271196274070777\n",
       "\\item 0.355537798682258\n",
       "\\item 0.0610618672068804\n",
       "\\item 0.356707039898808\n",
       "\\item 0.350476407031346\n",
       "\\item 0.343113774889433\n",
       "\\item 0.0295106260508025\n",
       "\\item 0.738953374357723\n",
       "\\item 0.284378209763666\n",
       "\\item 0.068611015867394\n",
       "\\item 0.00482401773356611\n",
       "\\item 0.630769530768676\n",
       "\\item 0.315158357608213\n",
       "\\item 0.356827702658612\n",
       "\\item 0.270527239661937\n",
       "\\item 0.251624881415677\n",
       "\\item 0.25760880755991\n",
       "\\item 0.180107416527999\n",
       "\\item 0.573906010950479\n",
       "\\item 0.333287983948135\n",
       "\\item 0.0994519735509173\n",
       "\\item 0.254373111131962\n",
       "\\item 0.127947766456593\n",
       "\\item 0.0287503878509145\n",
       "\\item 0.622280038298678\n",
       "\\item 0.00923154040183382\n",
       "\\item 0.00893073150299629\n",
       "\\item 0.4107363959673\n",
       "\\item 0.532240779589429\n",
       "\\item 0.0147120399520612\n",
       "\\item 0.0850992156468574\n",
       "\\item 0.0666759971171219\n",
       "\\item 0.0532227533905391\n",
       "\\item 0.944763104212636\n",
       "\\item 0.0421231961962477\n",
       "\\item 0.330871603608521\n",
       "\\item 0.00501890572748447\n",
       "\\item 0.0475675707248185\n",
       "\\item 0.0083735761559981\n",
       "\\item 0.844623338968075\n",
       "\\item 0.846679594057493\n",
       "\\item 0.603003049555463\n",
       "\\item 0.0576375333669081\n",
       "\\item 0.393734285994759\n",
       "\\item 0.00767541424809816\n",
       "\\item 0.199636721981925\n",
       "\\item 0.292213024120264\n",
       "\\item 0.76906982519554\n",
       "\\item 0.856685323240491\n",
       "\\item 0.595530358227373\n",
       "\\item 0.029703505549569\n",
       "\\item 0.170230360437168\n",
       "\\item 0.0590962158581206\n",
       "\\item 0.137961922417632\n",
       "\\item 0.0744322585941222\n",
       "\\item 0.615940912653162\n",
       "\\item 0.838822052148631\n",
       "\\item 0.191665871794086\n",
       "\\item 0.528119175483124\n",
       "\\item 0.18480304438479\n",
       "\\item 0.030103090076589\n",
       "\\item 0.313327982406951\n",
       "\\item 0.0917255709899279\n",
       "\\item 0.041551136464234\n",
       "\\item 0.187966788144977\n",
       "\\item 0.0506776774678144\n",
       "\\item 0.265329521353257\n",
       "\\item 0.0346522723301589\n",
       "\\item 0.726813785921841\n",
       "\\item 0.191463884285267\n",
       "\\item 0.0639038731282046\n",
       "\\item 0.0632602697916699\n",
       "\\item 0.803075849198999\n",
       "\\item 0.438268079689189\n",
       "\\item 0.0985781227947532\n",
       "\\item 0.0310326490016468\n",
       "\\item 0.248496611722856\n",
       "\\item 0.0268931479842507\n",
       "\\item 0.584490035996011\n",
       "\\item 0.767644550296345\n",
       "\\item 0.616942243443454\n",
       "\\item 0.181884391709822\n",
       "\\item 0.0292352580005547\n",
       "\\item 0.449170568819321\n",
       "\\item 0.037957275850432\n",
       "\\item 0.0214499858700358\n",
       "\\item 0.772209840371338\n",
       "\\item 0.103785471161419\n",
       "\\item 0.479325296521707\n",
       "\\item 0.0128060641739382\n",
       "\\item 0.0396358543490138\n",
       "\\item 0.346936039831818\n",
       "\\item 0.210000158269074\n",
       "\\item 0.181801412731248\n",
       "\\item 0.082041376717069\n",
       "\\item 0.182220291384141\n",
       "\\item 0.0081125633662825\n",
       "\\item 0.474099349262691\n",
       "\\item 0.218620140442417\n",
       "\\item 0.221512435317649\n",
       "\\item 0.345253143463621\n",
       "\\item 0.282029571528149\n",
       "\\item 0.123630940615102\n",
       "\\item 0.00312376623456277\n",
       "\\item 0.687484003092099\n",
       "\\item 0.167630842305543\n",
       "\\item 0.256873896398803\n",
       "\\item 0.138270720742715\n",
       "\\item 0.335698092889923\n",
       "\\item 0.0555016611674721\n",
       "\\item 0.559613891428735\n",
       "\\item 0.0815618215625696\n",
       "\\item 0.314753223939865\n",
       "\\item 0.13519587565503\n",
       "\\item 0.512628252536532\n",
       "\\item 0.310301419935886\n",
       "\\item 0.138872333665707\n",
       "\\item 0.0032387922514317\n",
       "\\item 0.0503488414695993\n",
       "\\item 0.65381973340819\n",
       "\\item 0.0613327135504364\n",
       "\\item 0.00933227759682214\n",
       "\\item 0.0143616311827729\n",
       "\\item 0.124348599066586\n",
       "\\item 0.00647149006353583\n",
       "\\item 0.252065847305571\n",
       "\\item 0.31145577732936\n",
       "\\item 0.0170022745551469\n",
       "\\item 0.0184837003196838\n",
       "\\item 0.0125543720114499\n",
       "\\item 0.118783058537979\n",
       "\\item 0.00908245556874083\n",
       "\\item 0.0209936551649666\n",
       "\\item 0.113606708659381\n",
       "\\item 0.132870896582193\n",
       "\\item 0.262417580337086\n",
       "\\item 0.0111836663784644\n",
       "\\item 0.0379309246642737\n",
       "\\item 0.489558874761568\n",
       "\\item 0.0357662129703509\n",
       "\\item 0.0753036946996795\n",
       "\\item 0.648942525448569\n",
       "\\item 0.274216563019518\n",
       "\\item 0.852575022184947\n",
       "\\item 0.0479075158651052\n",
       "\\item 0.0382467771585991\n",
       "\\item 0.0521301417772706\n",
       "\\item 0.160474064868337\n",
       "\\item 0.191231812054263\n",
       "\\item 0.0397004059039417\n",
       "\\item 0.166397618120375\n",
       "\\item 0.0118601763248746\n",
       "\\item 0.955225021979712\n",
       "\\item 0.0190896078550038\n",
       "\\item 0.211297604682006\n",
       "\\item 0.151360334432323\n",
       "\\item 0.00286926456983292\n",
       "\\item 0.210523662682554\n",
       "\\item 0.149085021179574\n",
       "\\item 0.264074540127379\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.0199699476169238\n",
       "2. 0.0458199977164465\n",
       "3. 0.495266696384679\n",
       "4. 0.0472587034954134\n",
       "5. 0.610468334947603\n",
       "6. 0.136159621575197\n",
       "7. 0.31704976973032\n",
       "8. 0.330788076270406\n",
       "9. 0.0572359528828377\n",
       "10. 0.0083233190846819\n",
       "11. 0.196416341440513\n",
       "12. 0.180200264230589\n",
       "13. 0.0113471974905296\n",
       "14. 0.999999874308373\n",
       "15. 0.0774379836991409\n",
       "16. 0.0343189363764838\n",
       "17. 0.0117077239538832\n",
       "18. 0.175135339069821\n",
       "19. 0.151597975314201\n",
       "20. 0.0113879153094041\n",
       "21. 0.0390493668900996\n",
       "22. 0.267976114670805\n",
       "23. 0.92963137112946\n",
       "24. 0.01331014904008\n",
       "25. 0.632286991324739\n",
       "26. 0.0681840936289838\n",
       "27. 0.167867084684194\n",
       "28. 0.783166172347892\n",
       "29. 0.539091118054234\n",
       "30. 0.0885465145955104\n",
       "31. 0.218503708926811\n",
       "32. 0.714245072008431\n",
       "33. 0.0193011550320404\n",
       "34. 0.417379060294064\n",
       "35. 0.714314667153859\n",
       "36. 0.0989819113680306\n",
       "37. 0.63212634718896\n",
       "38. 0.863068467443166\n",
       "39. 0.838679177471337\n",
       "40. 0.0123271799699895\n",
       "41. 0.200794933483637\n",
       "42. 0.188821192097363\n",
       "43. 0.0700626377808473\n",
       "44. 0.837945571103761\n",
       "45. 0.276310621835661\n",
       "46. 0.657691841484601\n",
       "47. 0.205660255581229\n",
       "48. 0.0239168649629574\n",
       "49. 0.156224136042788\n",
       "50. 0.0727436295232659\n",
       "51. 0.0502401352718925\n",
       "52. 0.0271617649981027\n",
       "53. 0.132303470328694\n",
       "54. 0.176673984898641\n",
       "55. 0.936209049546387\n",
       "56. 0.714462545999191\n",
       "57. 0.611341433297742\n",
       "58. 0.522927481877359\n",
       "59. 0.0856755142255962\n",
       "60. 0.262617148260062\n",
       "61. 0.899308500498465\n",
       "62. 0.0121847961244518\n",
       "63. 0.0169264820611136\n",
       "64. 0.116634527172926\n",
       "65. 0.0672691467996576\n",
       "66. 0.019559155120192\n",
       "67. 0.00197235396077447\n",
       "68. 0.0108449809442752\n",
       "69. 0.510216724093831\n",
       "70. 0.0904532460508593\n",
       "71. 0.435372525685862\n",
       "72. 0.0049295633283936\n",
       "73. 0.012455078937811\n",
       "74. 0.541889294949295\n",
       "75. 0.00895807358566363\n",
       "76. 0.0936020507485612\n",
       "77. 0.564364049580987\n",
       "78. 0.0175227074763854\n",
       "79. 0.319530217052547\n",
       "80. 0.0260328768217256\n",
       "81. 0.782656773290995\n",
       "82. 0.0300054899951376\n",
       "83. 0.0092407087558277\n",
       "84. 0.557042322489272\n",
       "85. 0.172767899139369\n",
       "86. 0.169500475758547\n",
       "87. 0.0105496860722101\n",
       "88. 0.047448975283293\n",
       "89. 0.721803564859241\n",
       "90. 0.236527042683651\n",
       "91. 0.0400844925094205\n",
       "92. 0.0323106117152453\n",
       "93. 0.0551054029703938\n",
       "94. 0.881513227076948\n",
       "95. 0.127465269427721\n",
       "96. 0.171626208605749\n",
       "97. 0.0198552567487982\n",
       "98. 0.0939341549905087\n",
       "99. 0.384039317177223\n",
       "100. 0.490064623868398\n",
       "101. 0.810220301244794\n",
       "102. 0.0128802300860469\n",
       "103. 0.628930818844018\n",
       "104. 0.477956918518179\n",
       "105. 0.211239062974798\n",
       "106. 0.450625519167723\n",
       "107. 0.0489693549931018\n",
       "108. 0.153292202001712\n",
       "109. 0.263943026571787\n",
       "110. 0.0361809239692963\n",
       "111. 0.014737017055404\n",
       "112. 0.817887964218033\n",
       "113. 0.00572570066559266\n",
       "114. 0.565256351818271\n",
       "115. 0.236803485421299\n",
       "116. 0.00297010101132321\n",
       "117. 0.00219648975771379\n",
       "118. 0.499704006968091\n",
       "119. 0.0303396620690834\n",
       "120. 0.00558038007284844\n",
       "121. 0.0424689032462572\n",
       "122. 0.0278690778886793\n",
       "123. 0.00941158628724017\n",
       "124. 0.0669573500977942\n",
       "125. 0.11273279278212\n",
       "126. 0.00286946198686716\n",
       "127. 0.0271483676136606\n",
       "128. 0.218614350829553\n",
       "129. 0.571809577970821\n",
       "130. 0.0161068956824048\n",
       "131. 0.464076015443004\n",
       "132. 0.187053377790224\n",
       "133. 0.241979368573769\n",
       "134. 0.260805827999097\n",
       "135. 0.383374781060198\n",
       "136. 0.433144602201799\n",
       "137. 0.00436676477761932\n",
       "138. 0.722942271523976\n",
       "139. 0.0810959085557277\n",
       "140. 0.457050515442807\n",
       "141. 0.247592719205121\n",
       "142. 0.149236598016692\n",
       "143. 0.190278667026799\n",
       "144. 0.711020806121186\n",
       "145. 0.311137916522735\n",
       "146. 0.292990647337669\n",
       "147. 0.511568232699909\n",
       "148. 0.229699173606277\n",
       "149. 0.116477659097698\n",
       "150. 0.0367492647376478\n",
       "151. 0.143062726825331\n",
       "152. 0.301657220879435\n",
       "153. 0.147534430963637\n",
       "154. 0.0841891611580271\n",
       "155. 0.00643526070543069\n",
       "156. 0.0556656082727995\n",
       "157. 0.0220379683539955\n",
       "158. 0.378662367379383\n",
       "159. 0.180499966171909\n",
       "160. 0.784403150893681\n",
       "161. 0.108861026226306\n",
       "162. 0.0603342786261118\n",
       "163. 0.0107996713554488\n",
       "164. 0.127146975736733\n",
       "165. 0.0275128873508818\n",
       "166. 0.526974796553326\n",
       "167. 0.0220476828074766\n",
       "168. 0.70057123408146\n",
       "169. 0.00382296065149055\n",
       "170. 0.134059162106367\n",
       "171. 0.52661504239586\n",
       "172. 0.236627709655711\n",
       "173. 0.39244447464418\n",
       "174. 0.0100239226706501\n",
       "175. 0.444120282795447\n",
       "176. 0.00614967158187709\n",
       "177. 0.251881076367534\n",
       "178. 0.276438051909961\n",
       "179. 0.467511826672706\n",
       "180. 0.86772384271406\n",
       "181. 0.756015126393805\n",
       "182. 0.664003048244369\n",
       "183. 0.48620790605888\n",
       "184. 0.00159154789704167\n",
       "185. 0.797357617195317\n",
       "186. 0.123754012887735\n",
       "187. 0.00915150482932242\n",
       "188. 0.0676467822304575\n",
       "189. 0.119022463031599\n",
       "190. 0.174293104904208\n",
       "191. 0.0118006942527187\n",
       "192. 0.53804781128433\n",
       "193. 0.613603538229885\n",
       "194. 0.119289128555705\n",
       "195. 0.00803418840343704\n",
       "196. 0.0138828887671821\n",
       "197. 0.0361448537960523\n",
       "198. 0.0429285456110043\n",
       "199. 0.0104389900359683\n",
       "200. 0.729467703963586\n",
       "201. 0.459945572340207\n",
       "202. 0.0552130150495399\n",
       "203. 0.0367526429414071\n",
       "204. 0.559102877454589\n",
       "205. 0.28454384680674\n",
       "206. 0.578174400044091\n",
       "207. 0.00733794610488339\n",
       "208. 0.773034341157126\n",
       "209. 0.0123431907981087\n",
       "210. 0.729826707497585\n",
       "211. 0.0639325591940032\n",
       "212. 0.00659662247244716\n",
       "213. 0.0207966120870647\n",
       "214. 0.698048462346474\n",
       "215. 0.0832348719605707\n",
       "216. 0.425743992736508\n",
       "217. 0.0500112452744067\n",
       "218. 0.0315595359783704\n",
       "219. 0.0128139565343078\n",
       "220. 0.0615207431367373\n",
       "221. 0.0576847675261676\n",
       "222. 0.00786206014013649\n",
       "223. 0.236294197111729\n",
       "224. 0.594878093697926\n",
       "225. 0.145727523457501\n",
       "226. 0.0107497251696278\n",
       "227. 0.117193432905089\n",
       "228. 0.742498161477231\n",
       "229. 0.326045461851359\n",
       "230. 0.0154746336086699\n",
       "231. 0.691470479197498\n",
       "232. 0.035472218114744\n",
       "233. 0.3243238016297\n",
       "234. 0.0379532609969916\n",
       "235. 0.0162007700833543\n",
       "236. 0.0598537057380022\n",
       "237. 0.12993090212402\n",
       "238. 0.055016377954057\n",
       "239. 0.42532440282483\n",
       "240. 0.603929006780376\n",
       "241. 0.0417853173593943\n",
       "242. 0.0130308787284185\n",
       "243. 0.0213242023163649\n",
       "244. 0.252132484152406\n",
       "245. 0.0320103020527634\n",
       "246. 0.046142420252636\n",
       "247. 0.225944302645433\n",
       "248. 0.814561937550337\n",
       "249. 0.391556731308694\n",
       "250. 0.00171532327448696\n",
       "251. 0.0206665028291591\n",
       "252. 0.682130373225299\n",
       "253. 0.772062002357203\n",
       "254. 0.333155318384347\n",
       "255. 0.0165101749053747\n",
       "256. 0.516591044093818\n",
       "257. 0.841115959677762\n",
       "258. 0.195898652395022\n",
       "259. 0.0156843758209774\n",
       "260. 0.0365511431414871\n",
       "261. 0.41489042890736\n",
       "262. 0.213119768257293\n",
       "263. 0.0426793419907824\n",
       "264. 0.252557401266546\n",
       "265. 0.370312273922775\n",
       "266. 0.30530040193884\n",
       "267. 0.0870845766246387\n",
       "268. 0.0533006539540388\n",
       "269. 0.0517563694307511\n",
       "270. 0.183546371926233\n",
       "271. 0.251103787332291\n",
       "272. 0.0477140619717467\n",
       "273. 0.190894647316061\n",
       "274. 0.220178971392864\n",
       "275. 0.0152378694649064\n",
       "276. 0.0790836143686536\n",
       "277. 0.344426444871542\n",
       "278. 0.0292482233504427\n",
       "279. 0.269250724139821\n",
       "280. 0.191362671593097\n",
       "281. 0.205383741574171\n",
       "282. 0.0896329720627386\n",
       "283. 0.126061754194506\n",
       "284. 0.136648052408895\n",
       "285. 0.54583599187173\n",
       "286. 0.182070444777865\n",
       "287. 0.0276649996261271\n",
       "288. 0.525221181201475\n",
       "289. 0.617625570371258\n",
       "290. 0.27138290942917\n",
       "291. 0.364024274891242\n",
       "292. 0.0390194583915117\n",
       "293. 0.536313766726566\n",
       "294. 0.316752941622576\n",
       "295. 0.222694306314753\n",
       "296. 0.9999999556635\n",
       "297. 0.852776026502151\n",
       "298. 0.173081373488694\n",
       "299. 0.0106313525774898\n",
       "300. 0.012958796082242\n",
       "301. 0.605079039893046\n",
       "302. 0.0209566285165233\n",
       "303. 0.0117290602580811\n",
       "304. 0.00208421400211895\n",
       "305. 0.0474264752745083\n",
       "306. 0.0452644869921135\n",
       "307. 0.176047049527089\n",
       "308. 0.288992155005493\n",
       "309. 0.0484168059490443\n",
       "310. 0.00935030328410211\n",
       "311. 0.0735783355315092\n",
       "312. 0.366352469147635\n",
       "313. 0.411857493946701\n",
       "314. 0.0428127447273097\n",
       "315. 0.0466857273831911\n",
       "316. 0.670354944651671\n",
       "317. 0.135337592343281\n",
       "318. 0.106430211907991\n",
       "319. 0.430768947542698\n",
       "320. 0.377499049946076\n",
       "321. 0.0569596311322483\n",
       "322. 0.0782487023213659\n",
       "323. 0.0403133305175846\n",
       "324. 0.146504416543012\n",
       "325. 0.141851106686674\n",
       "326. 0.407257277517452\n",
       "327. 0.407308457442344\n",
       "328. 0.375945914538592\n",
       "329. 0.0591865119366488\n",
       "330. 0.335869707310747\n",
       "331. 0.373718891213413\n",
       "332. 0.299704703366878\n",
       "333. 0.436507471586192\n",
       "334. 0.525239621301956\n",
       "335. 0.748810689599713\n",
       "336. 0.0444664072064231\n",
       "337. 0.420971911242324\n",
       "338. 0.479008623665864\n",
       "339. 0.0225867450198073\n",
       "340. 0.0336935618577512\n",
       "341. 0.0526724333108519\n",
       "342. 0.0921678686500227\n",
       "343. 0.145484880868177\n",
       "344. 0.35388327919847\n",
       "345. 0.375723540882374\n",
       "346. 0.0484635475531039\n",
       "347. 0.0669329697001514\n",
       "348. 0.275664993452405\n",
       "349. 0.0187046100288029\n",
       "350. 0.0373261337703594\n",
       "351. 0.0361548542119007\n",
       "352. 0.0180698408998763\n",
       "353. 0.144307393591685\n",
       "354. 0.999999765774209\n",
       "355. 0.0874662983887632\n",
       "356. 0.494653149058208\n",
       "357. 0.766595370098011\n",
       "358. 0.04937193390233\n",
       "359. 0.0587688149414015\n",
       "360. 0.113350902296175\n",
       "361. 0.0641453110264208\n",
       "362. 0.0306221130817041\n",
       "363. 0.0364971643592527\n",
       "364. 0.0746120938024865\n",
       "365. 0.153070839608285\n",
       "366. 0.0923529700092577\n",
       "367. 0.646101390255116\n",
       "368. 0.0161892993419189\n",
       "369. 0.0033641288571327\n",
       "370. 0.0304969068173873\n",
       "371. 0.856436492437686\n",
       "372. 0.188495916703216\n",
       "373. 0.00534169786257923\n",
       "374. 0.105919053645878\n",
       "375. 0.0775022656123414\n",
       "376. 0.883106882604116\n",
       "377. 0.084328904048033\n",
       "378. 0.0403205585624215\n",
       "379. 0.0212326212804359\n",
       "380. 0.291700811308681\n",
       "381. 0.186420928028463\n",
       "382. 0.372208222677709\n",
       "383. 0.0841111297047274\n",
       "384. 0.466491451713264\n",
       "385. 0.123605364073339\n",
       "386. 0.12082936013055\n",
       "387. 0.0106835858471097\n",
       "388. 0.139360357664107\n",
       "389. 0.130757114898081\n",
       "390. 0.212864098500351\n",
       "391. 0.00337488996762014\n",
       "392. 0.0796780446935557\n",
       "393. 0.525696186552051\n",
       "394. 0.23379409560094\n",
       "395. 0.0360437163578946\n",
       "396. 0.313261216347766\n",
       "397. 0.0153695605345677\n",
       "398. 0.0198241590102087\n",
       "399. 0.0559440741759468\n",
       "400. 0.00263272934508541\n",
       "401. 0.0172846066915633\n",
       "402. 0.637519180213675\n",
       "403. 0.066087308482829\n",
       "404. 0.766196541476464\n",
       "405. 0.0313326585610782\n",
       "406. 0.407844199740332\n",
       "407. 0.0731464017427767\n",
       "408. 0.340667917237113\n",
       "409. 0.850718506308087\n",
       "410. 0.0526048912150345\n",
       "411. 0.125206536717809\n",
       "412. 0.014018347795696\n",
       "413. 0.380067430682013\n",
       "414. 0.00806281398673934\n",
       "415. 0.312416832563002\n",
       "416. 0.516806109975632\n",
       "417. 0.156617486628282\n",
       "418. 0.500536052575628\n",
       "419. 0.203053643003371\n",
       "420. 0.606848366882435\n",
       "421. 0.147635101544277\n",
       "422. 0.592510764997326\n",
       "423. 0.00268754556486882\n",
       "424. 0.0191301065063439\n",
       "425. 0.106800900073335\n",
       "426. 0.385639891578442\n",
       "427. 0.0988073904345365\n",
       "428. 0.242881225374657\n",
       "429. 0.102640797904684\n",
       "430. 0.342644094226348\n",
       "431. 0.579634240456382\n",
       "432. 0.0880472218016967\n",
       "433. 0.391731286741306\n",
       "434. 0.232641098884727\n",
       "435. 0.848298568694339\n",
       "436. 0.236062058943315\n",
       "437. 0.0232003913889166\n",
       "438. 0.118158466623914\n",
       "439. 0.530891970377014\n",
       "440. 0.245601262968426\n",
       "441. 0.127651060608124\n",
       "442. 0.0566126191986855\n",
       "443. 0.17812038081585\n",
       "444. 0.944361527376599\n",
       "445. 0.0628925297638546\n",
       "446. 0.607099296657314\n",
       "447. 0.00647036352417058\n",
       "448. 0.014506728146203\n",
       "449. 0.52762505968881\n",
       "450. 0.00803542976488018\n",
       "451. 0.55339847312494\n",
       "452. 0.125021655152885\n",
       "453. 0.0122782554025418\n",
       "454. 0.721077080670495\n",
       "455. 0.203762662847281\n",
       "456. 0.00319297473404992\n",
       "457. 0.248570352049798\n",
       "458. 0.29862730608033\n",
       "459. 0.999999668512615\n",
       "460. 0.736981259442357\n",
       "461. 0.0441224070278956\n",
       "462. 0.0221582979391203\n",
       "463. 0.189982247487777\n",
       "464. 0.894282444692239\n",
       "465. 0.0437765767775495\n",
       "466. 0.367857676672516\n",
       "467. 0.172466364280629\n",
       "468. 0.0243349213940347\n",
       "469. 0.0175794820056814\n",
       "470. 0.452609346022816\n",
       "471. 0.137613484637788\n",
       "472. 0.656618216663544\n",
       "473. 0.0781637715273055\n",
       "474. 0.0246332312798398\n",
       "475. 0.571694330649898\n",
       "476. 0.016002881459772\n",
       "477. 0.199758297954567\n",
       "478. 0.601404930111302\n",
       "479. 0.217279361750711\n",
       "480. 0.00286133259752111\n",
       "481. 0.00527884708402513\n",
       "482. 0.0825537081087439\n",
       "483. 0.899053576453538\n",
       "484. 0.0498238978185774\n",
       "485. 0.00999921756695854\n",
       "486. 0.0286649510015706\n",
       "487. 0.67590343466665\n",
       "488. 0.100243824833728\n",
       "489. 0.404647265904865\n",
       "490. 0.0132329010918809\n",
       "491. 0.622210290276593\n",
       "492. 0.187974394598899\n",
       "493. 0.75914933789834\n",
       "494. 0.46460801488613\n",
       "495. 0.0744935782972362\n",
       "496. 0.016853624195665\n",
       "497. 0.940676545822903\n",
       "498. 0.0102856266755571\n",
       "499. 0.801658041621613\n",
       "500. 0.0477185915405782\n",
       "501. 0.0732503424959294\n",
       "502. 0.0302669930852424\n",
       "503. 0.0280085447009265\n",
       "504. 0.013871306182459\n",
       "505. 0.0647383577023884\n",
       "506. 0.00811055369306502\n",
       "507. 0.29543868779346\n",
       "508. 0.354904215660545\n",
       "509. 0.0951801706426184\n",
       "510. 0.0204777669926373\n",
       "511. 0.677036779526389\n",
       "512. 0.20985532459379\n",
       "513. 0.0483360111963359\n",
       "514. 0.0334912135273815\n",
       "515. 0.0208440532932362\n",
       "516. 0.162408618117541\n",
       "517. 0.0342050573802004\n",
       "518. 0.0302878476153293\n",
       "519. 0.7100987248817\n",
       "520. 0.381194836611222\n",
       "521. 0.0094529771604589\n",
       "522. 0.0402290172810124\n",
       "523. 0.58695037075786\n",
       "524. 0.00321619240820627\n",
       "525. 0.172097600353855\n",
       "526. 0.387063094017129\n",
       "527. 0.00574809919706981\n",
       "528. 0.632587707744512\n",
       "529. 0.0739718968498494\n",
       "530. 0.00075272394936441\n",
       "531. 0.192655555635881\n",
       "532. 0.00694355381487038\n",
       "533. 0.400550517664137\n",
       "534. 0.625299783731324\n",
       "535. 0.503589400637887\n",
       "536. 0.015247599790401\n",
       "537. 0.966539426126986\n",
       "538. 0.592102754547421\n",
       "539. 0.181275564495835\n",
       "540. 0.131093707166075\n",
       "541. 0.0657437081635146\n",
       "542. 0.983885077238754\n",
       "543. 0.0408730272219029\n",
       "544. 0.0569603826233853\n",
       "545. 0.0355543301314578\n",
       "546. 0.0325323960705489\n",
       "547. 0.498212231811771\n",
       "548. 0.612572923896403\n",
       "549. 0.0079727847894024\n",
       "550. 0.460081353769792\n",
       "551. 0.0119379321131558\n",
       "552. 0.0346133714336782\n",
       "553. 0.0188672155751794\n",
       "554. 0.810873556214912\n",
       "555. 0.401400629063512\n",
       "556. 0.276390544212923\n",
       "557. 0.153843597894376\n",
       "558. 0.060038027606361\n",
       "559. 0.017804359463568\n",
       "560. 0.265074272736638\n",
       "561. 0.0277494323330679\n",
       "562. 0.0455792688099718\n",
       "563. 0.0251425224731764\n",
       "564. 0.269636746779349\n",
       "565. 0.289120442725711\n",
       "566. 0.0064975369843315\n",
       "567. 0.147841056205854\n",
       "568. 0.0495796982764737\n",
       "569. 0.654069287569774\n",
       "570. 0.248166936639415\n",
       "571. 0.0368513111243656\n",
       "572. 0.19111719200314\n",
       "573. 0.370488328776153\n",
       "574. 0.471546277126646\n",
       "575. 0.383419329479009\n",
       "576. 0.284179548560016\n",
       "577. 0.60487845731714\n",
       "578. 0.214259307519855\n",
       "579. 0.0590188301682051\n",
       "580. 0.020525284866343\n",
       "581. 0.0246309874298711\n",
       "582. 0.0127786609236949\n",
       "583. 0.313523041958294\n",
       "584. 0.311842939913688\n",
       "585. 0.101597741560868\n",
       "586. 0.603513866791461\n",
       "587. 0.00479214537332067\n",
       "588. 0.117154947437688\n",
       "589. 0.219800049557997\n",
       "590. 0.0171034990162961\n",
       "591. 0.540297716322392\n",
       "592. 0.0689034896757411\n",
       "593. 0.01820575584003\n",
       "594. 0.00227838794919667\n",
       "595. 0.0023905725602704\n",
       "596. 0.373918558449697\n",
       "597. 0.0123804855931148\n",
       "598. 0.00342162955273444\n",
       "599. 0.0454900500367966\n",
       "600. 0.614405940171795\n",
       "601. 0.0344217594887273\n",
       "602. 0.14507590877353\n",
       "603. 0.539178293519048\n",
       "604. 0.0254165815037278\n",
       "605. 0.054414746981464\n",
       "606. 0.322385011952746\n",
       "607. 0.923803474700752\n",
       "608. 0.0395919920019561\n",
       "609. 0.0650213372766423\n",
       "610. 0.00324105580869746\n",
       "611. 0.0973600067040383\n",
       "612. 0.345475723251949\n",
       "613. 0.628954732600845\n",
       "614. 0.0315861561523127\n",
       "615. 0.0162456139902508\n",
       "616. 0.829958584056793\n",
       "617. 0.0157824164366038\n",
       "618. 0.193132686377787\n",
       "619. 0.0817198656589979\n",
       "620. 0.934610357737708\n",
       "621. 0.170657763868179\n",
       "622. 0.620558604533632\n",
       "623. 0.704158168790487\n",
       "624. 0.379998434870672\n",
       "625. 0.302483219838451\n",
       "626. 0.112226301412075\n",
       "627. 0.0930631664990264\n",
       "628. 0.00455253010717648\n",
       "629. 0.138205228973645\n",
       "630. 0.0922202816433377\n",
       "631. 0.00626807308392644\n",
       "632. 0.145079792273125\n",
       "633. 0.642471744984563\n",
       "634. 0.525146730827957\n",
       "635. 0.272499556188591\n",
       "636. 0.0112859975074698\n",
       "637. 0.215974245773398\n",
       "638. 0.119223020522359\n",
       "639. 0.0108070503893455\n",
       "640. 0.031710100455674\n",
       "641. 0.250448896349346\n",
       "642. 0.059307305251946\n",
       "643. 0.00557992835427086\n",
       "644. 0.0550400837910643\n",
       "645. 0.0299165091943819\n",
       "646. 0.0245991091285857\n",
       "647. 0.0908313575875495\n",
       "648. 0.102443088020665\n",
       "649. 0.529037659511435\n",
       "650. 0.212354389990612\n",
       "651. 0.0735288174845334\n",
       "652. 0.0277750722591762\n",
       "653. 0.430901572274588\n",
       "654. 0.378389026351761\n",
       "655. 0.252816928381743\n",
       "656. 0.218371966458501\n",
       "657. 0.511594920477143\n",
       "658. 0.426762267196003\n",
       "659. 0.36313913607403\n",
       "660. 0.229652722872118\n",
       "661. 0.999982909447387\n",
       "662. 0.00573476837917188\n",
       "663. 0.0221284190372738\n",
       "664. 0.473720125868222\n",
       "665. 0.218775520151417\n",
       "666. 0.153918079075086\n",
       "667. 0.570078205080328\n",
       "668. 0.00673497966191684\n",
       "669. 0.0108676534344724\n",
       "670. 0.584801385624825\n",
       "671. 0.557958525617629\n",
       "672. 0.127662345311259\n",
       "673. 0.00759444902871862\n",
       "674. 0.734756665300793\n",
       "675. 0.286417672352117\n",
       "676. 0.444254734078745\n",
       "677. 0.0124171302645206\n",
       "678. 0.394702502404232\n",
       "679. 0.186638748017359\n",
       "680. 0.169808008391833\n",
       "681. 0.0416221124213114\n",
       "682. 0.80069246464988\n",
       "683. 0.0739007862256507\n",
       "684. 0.18201739186026\n",
       "685. 0.203678734082994\n",
       "686. 0.593907026751536\n",
       "687. 0.305569586998538\n",
       "688. 0.263643382196554\n",
       "689. 0.0134936619851492\n",
       "690. 0.208820994640213\n",
       "691. 0.0549664596862607\n",
       "692. 0.0286693218212616\n",
       "693. 0.541920854508511\n",
       "694. 0.390828504609042\n",
       "695. 0.0336965980949221\n",
       "696. 0.364172401964305\n",
       "697. 0.051075154008684\n",
       "698. 0.094488609256491\n",
       "699. 0.0313731202656018\n",
       "700. 0.0995308146759852\n",
       "701. 0.0554973631729953\n",
       "702. 0.152083172205623\n",
       "703. 0.722991566287879\n",
       "704. 0.189711784402361\n",
       "705. 0.547964874321728\n",
       "706. 0.688318268307373\n",
       "707. 0.462524239085874\n",
       "708. 0.0258162878044023\n",
       "709. 0.267599346710097\n",
       "710. 0.0912615391614897\n",
       "711. 0.421269112895809\n",
       "712. 0.951909913594931\n",
       "713. 0.0106417124853625\n",
       "714. 0.0147647082047271\n",
       "715. 0.399001617248235\n",
       "716. 0.3652506032079\n",
       "717. 0.359153996941063\n",
       "718. 0.52293226570314\n",
       "719. 0.931774186479659\n",
       "720. 0.045009603948656\n",
       "721. 0.118841196690333\n",
       "722. 0.225255072158649\n",
       "723. 0.509376765758929\n",
       "724. 0.217679294960974\n",
       "725. 0.013327865518525\n",
       "726. 0.0165579512922611\n",
       "727. 0.60389610576593\n",
       "728. 0.569661028875765\n",
       "729. 0.368408491903595\n",
       "730. 0.0154710552313364\n",
       "731. 0.275897407486218\n",
       "732. 0.502188344505214\n",
       "733. 0.0787388213398721\n",
       "734. 0.0967300618243921\n",
       "735. 0.00494883580375058\n",
       "736. 0.0641508423486803\n",
       "737. 0.0306752135803522\n",
       "738. 0.402309389714621\n",
       "739. 0.00383458180257253\n",
       "740. 0.022162456940421\n",
       "741. 0.00835678330015797\n",
       "742. 0.0426203782200652\n",
       "743. 0.549851594086313\n",
       "744. 0.0474251305226223\n",
       "745. 0.67539938725115\n",
       "746. 0.0204900547140519\n",
       "747. 0.549072635790886\n",
       "748. 0.516366445615041\n",
       "749. 0.0193906860951293\n",
       "750. 0.152919852151923\n",
       "751. 0.0115180746729149\n",
       "752. 0.0131243438935188\n",
       "753. 0.0669864820543327\n",
       "754. 0.0770191340567978\n",
       "755. 0.0337849859576272\n",
       "756. 0.584499862875758\n",
       "757. 0.174133168075228\n",
       "758. 0.0168523973033843\n",
       "759. 0.036492761553111\n",
       "760. 0.03296429270335\n",
       "761. 0.13522573705344\n",
       "762. 0.441435132119064\n",
       "763. 0.0631999099092396\n",
       "764. 0.443591054899236\n",
       "765. 0.820625405561113\n",
       "766. 0.14760614406295\n",
       "767. 0.00921031969447578\n",
       "768. 0.0407275279789619\n",
       "769. 0.496519971203934\n",
       "770. 0.352683457730475\n",
       "771. 0.0337052154495489\n",
       "772. 0.041021725067448\n",
       "773. 0.657159124804541\n",
       "774. 0.674788662579156\n",
       "775. 0.589624181563565\n",
       "776. 0.0378380543825788\n",
       "777. 0.770466683785002\n",
       "778. 0.312030005477519\n",
       "779. 0.0203791415008888\n",
       "780. 0.25260727593978\n",
       "781. 0.0370764573951389\n",
       "782. 0.010213880400914\n",
       "783. 0.379636352117403\n",
       "784. 0.00403537276895975\n",
       "785. 0.108292436662199\n",
       "786. 0.249821608841375\n",
       "787. 0.342332203115704\n",
       "788. 0.584098659595383\n",
       "789. 0.0439314807877648\n",
       "790. 0.387596786008076\n",
       "791. 0.0138209702150517\n",
       "792. 0.260108634346888\n",
       "793. 0.669261470397866\n",
       "794. 0.0102872462659718\n",
       "795. 0.123577193624098\n",
       "796. 0.0234988787420555\n",
       "797. 0.00315546696297043\n",
       "798. 0.0123239365043939\n",
       "799. 0.041466787478148\n",
       "800. 0.307849208920438\n",
       "801. 0.0981298036112295\n",
       "802. 0.0118130657382479\n",
       "803. 0.266928381781508\n",
       "804. 0.0494469030388882\n",
       "805. 0.180771557197106\n",
       "806. 0.00582339154323416\n",
       "807. 0.58083375330643\n",
       "808. 0.0246091628848339\n",
       "809. 0.0564855465442005\n",
       "810. 0.644209414662118\n",
       "811. 0.289100550018003\n",
       "812. 0.0938396094680775\n",
       "813. 0.00385549300521934\n",
       "814. 0.0378153241286041\n",
       "815. 0.363537742091485\n",
       "816. 0.447053123345453\n",
       "817. 0.18576784797277\n",
       "818. 0.387679087563473\n",
       "819. 0.691089208128983\n",
       "820. 0.0383015816281294\n",
       "821. 0.094289063137743\n",
       "822. 0.0530877723967058\n",
       "823. 0.979161767606413\n",
       "824. 0.430368104133677\n",
       "825. 0.0541403278161665\n",
       "826. 0.0270043072110092\n",
       "827. 0.512078551749623\n",
       "828. 0.00480679602286807\n",
       "829. 0.200703002730321\n",
       "830. 0.683521656236424\n",
       "831. 0.0471513790862581\n",
       "832. 0.00142491766924496\n",
       "833. 0.580801622126068\n",
       "834. 0.432030142618772\n",
       "835. 0.952026895787423\n",
       "836. 0.283430277644257\n",
       "837. 0.210257501672467\n",
       "838. 0.197776730461394\n",
       "839. 0.0314478917634595\n",
       "840. 0.616894050279868\n",
       "841. 0.223820706046476\n",
       "842. 0.816932175283422\n",
       "843. 0.0237137255225264\n",
       "844. 0.525538330221074\n",
       "845. 0.899365439141373\n",
       "846. 0.648562218250217\n",
       "847. 0.0413256660936873\n",
       "848. 0.0530809885017832\n",
       "849. 0.0371021460691384\n",
       "850. 0.666036891337846\n",
       "851. 0.0503403084487381\n",
       "852. 0.216736085460578\n",
       "853. 0.529816914001044\n",
       "854. 0.0170300823100406\n",
       "855. 0.845662369979206\n",
       "856. 0.286608517639903\n",
       "857. 0.0453429413868556\n",
       "858. 0.0168998093054599\n",
       "859. 0.0309378666679549\n",
       "860. 0.460958104606825\n",
       "861. 0.00168174614456759\n",
       "862. 0.0575316655816865\n",
       "863. 0.0295832576561265\n",
       "864. 0.30211582040186\n",
       "865. 0.359100467014012\n",
       "866. 0.0142723785962464\n",
       "867. 0.152992139306264\n",
       "868. 0.290629304585038\n",
       "869. 0.34477857883198\n",
       "870. 0.0639895413299661\n",
       "871. 0.0723829748932035\n",
       "872. 0.00956408821996863\n",
       "873. 0.0129948419248106\n",
       "874. 0.0896109652454796\n",
       "875. 0.218414632305991\n",
       "876. 0.395518672176483\n",
       "877. 0.164719917850441\n",
       "878. 0.337378224554789\n",
       "879. 0.409249367689963\n",
       "880. 0.750958644764419\n",
       "881. 0.993024241858559\n",
       "882. 0.552509093205434\n",
       "883. 0.300306271671101\n",
       "884. 0.144804852792329\n",
       "885. 0.0986315585450896\n",
       "886. 0.170781558190222\n",
       "887. 0.0388472689638898\n",
       "888. 0.942579881928659\n",
       "889. 0.100843439938957\n",
       "890. 0.385625233480742\n",
       "891. 0.511611566425847\n",
       "892. 0.620134773196079\n",
       "893. 0.214033042597848\n",
       "894. 0.158314813526888\n",
       "895. 0.129795050606655\n",
       "896. 0.109386720230056\n",
       "897. 0.0981704340271325\n",
       "898. 0.175908978783608\n",
       "899. 0.109022694256191\n",
       "900. 0.228573754468922\n",
       "901. 0.0902800566130318\n",
       "902. 0.56496302270547\n",
       "903. 0.00655918326418388\n",
       "904. 0.253354366901994\n",
       "905. 0.603017468139228\n",
       "906. 0.0347655953967357\n",
       "907. 0.10580000118631\n",
       "908. 0.0083025719003252\n",
       "909. 0.219575793226539\n",
       "910. 0.662334978091723\n",
       "911. 0.0192483334061113\n",
       "912. 0.738566013046589\n",
       "913. 0.777172976452723\n",
       "914. 0.432590569735931\n",
       "915. 0.0254074775950213\n",
       "916. 0.0317159200826119\n",
       "917. 0.599506332897089\n",
       "918. 0.586511967250636\n",
       "919. 0.621486116547452\n",
       "920. 0.0115517258002166\n",
       "921. 0.0605978514231961\n",
       "922. 0.639683791847728\n",
       "923. 0.401322189073391\n",
       "924. 0.99999992964077\n",
       "925. 0.0168575565473987\n",
       "926. 0.00370177835154455\n",
       "927. 0.458223529318648\n",
       "928. 0.173668631513075\n",
       "929. 0.154046361181832\n",
       "930. 0.536322719123752\n",
       "931. 0.915712765181158\n",
       "932. 0.165488761489117\n",
       "933. 0.0650764613495248\n",
       "934. 0.819550839293484\n",
       "935. 0.101598300655427\n",
       "936. 0.0656982498830971\n",
       "937. 0.321449929578181\n",
       "938. 0.044652300764393\n",
       "939. 0.0308784400477463\n",
       "940. 0.286529895034282\n",
       "941. 0.313468252418951\n",
       "942. 0.320396366228584\n",
       "943. 0.315096211324578\n",
       "944. 0.0094626318285661\n",
       "945. 0.234500854469321\n",
       "946. 0.0485966680983412\n",
       "947. 0.0306373039133289\n",
       "948. 0.999999939875605\n",
       "949. 0.876353206266361\n",
       "950. 0.758531543035791\n",
       "951. 0.222113709306038\n",
       "952. 0.201757308451926\n",
       "953. 0.111621401533538\n",
       "954. 0.740042850946609\n",
       "955. 0.189892130939609\n",
       "956. 0.370649402159646\n",
       "957. 0.0136711727458992\n",
       "958. 0.0347085172703979\n",
       "959. 0.423681544014339\n",
       "960. 0.017429461665319\n",
       "961. 0.0110954028292281\n",
       "962. 0.116377241092142\n",
       "963. 0.00382670423927343\n",
       "964. 0.0217295531117031\n",
       "965. 0.0461711130922531\n",
       "966. 0.192904837551817\n",
       "967. 0.254077723904435\n",
       "968. 0.0211117847937291\n",
       "969. 0.442869254979738\n",
       "970. 0.578907224354884\n",
       "971. 0.124759142525714\n",
       "972. 0.459858852421076\n",
       "973. 0.531920055779343\n",
       "974. 0.413125221294555\n",
       "975. 0.234420446840034\n",
       "976. 0.021949157617395\n",
       "977. 0.864688431053634\n",
       "978. 0.203674400884231\n",
       "979. 0.190250163913924\n",
       "980. 0.570898236070186\n",
       "981. 0.752585899180292\n",
       "982. 0.525509188624906\n",
       "983. 0.282242331063523\n",
       "984. 0.572963794309999\n",
       "985. 0.745021396124277\n",
       "986. 0.241342983100793\n",
       "987. 0.127341682541055\n",
       "988. 0.295692908843781\n",
       "989. 0.216927415072056\n",
       "990. 0.00354988961628633\n",
       "991. 0.00906657281069526\n",
       "992. 0.0134610499532325\n",
       "993. 0.032610587379341\n",
       "994. 0.212721050174182\n",
       "995. 0.0336941251163405\n",
       "996. 0.845287429391015\n",
       "997. 0.0772480311694506\n",
       "998. 0.0415381055168681\n",
       "999. 0.00294287677568198\n",
       "1000. 0.385000324228021\n",
       "1001. 0.0189692585269595\n",
       "1002. 0.10563684310018\n",
       "1003. 0.41006316102086\n",
       "1004. 0.206635460236618\n",
       "1005. 0.144453713311913\n",
       "1006. 0.999999930693917\n",
       "1007. 0.802362441796549\n",
       "1008. 0.0570869712885768\n",
       "1009. 0.0062163176607514\n",
       "1010. 0.022757733737513\n",
       "1011. 0.135026298725444\n",
       "1012. 0.0706772612898688\n",
       "1013. 0.0149102391900019\n",
       "1014. 0.530340540229543\n",
       "1015. 0.0700719598294253\n",
       "1016. 0.619797393031223\n",
       "1017. 0.249289951159632\n",
       "1018. 0.214522047115071\n",
       "1019. 0.241476905602369\n",
       "1020. 0.581025331166739\n",
       "1021. 0.229537729165737\n",
       "1022. 0.0156763071588183\n",
       "1023. 0.00838477061773456\n",
       "1024. 0.0112938677246485\n",
       "1025. 0.00922797566104582\n",
       "1026. 0.182233202279052\n",
       "1027. 0.186647817566892\n",
       "1028. 0.637529836804787\n",
       "1029. 0.0651077506175047\n",
       "1030. 0.00615605951304662\n",
       "1031. 0.0498163662896792\n",
       "1032. 0.717465430491587\n",
       "1033. 0.740822422748179\n",
       "1034. 0.320125745668472\n",
       "1035. 0.430572704256078\n",
       "1036. 0.0464950395948776\n",
       "1037. 0.0260596362568755\n",
       "1038. 0.578887276573786\n",
       "1039. 0.0175645577515171\n",
       "1040. 0.103409777760653\n",
       "1041. 0.85426144584604\n",
       "1042. 0.435147639488308\n",
       "1043. 0.586469146015563\n",
       "1044. 0.044495562481657\n",
       "1045. 0.0446792253207579\n",
       "1046. 0.0200192340447763\n",
       "1047. 0.202046766133388\n",
       "1048. 0.0197862102524262\n",
       "1049. 0.866168370910104\n",
       "1050. 0.220309653898714\n",
       "1051. 0.367168500933266\n",
       "1052. 0.095357834961527\n",
       "1053. 0.238694714569209\n",
       "1054. 0.0241320906170692\n",
       "1055. 0.0328750065959386\n",
       "1056. 0.377486053253873\n",
       "1057. 0.00395549132352947\n",
       "1058. 0.00527288751066556\n",
       "1059. 0.235086477165255\n",
       "1060. 0.0144739114041722\n",
       "1061. 0.00635522691008368\n",
       "1062. 0.183120070641252\n",
       "1063. 0.482654739214698\n",
       "1064. 0.253982616107924\n",
       "1065. 0.269642814761059\n",
       "1066. 0.174129800510415\n",
       "1067. 0.285165538749012\n",
       "1068. 0.00166446284316019\n",
       "1069. 0.428576756148541\n",
       "1070. 0.864750745392298\n",
       "1071. 0.262160250272491\n",
       "1072. 0.775361391122565\n",
       "1073. 0.437557998593944\n",
       "1074. 0.100396201320877\n",
       "1075. 0.0625584857790366\n",
       "1076. 0.0148640217706099\n",
       "1077. 0.289021388332522\n",
       "1078. 0.00734551191106758\n",
       "1079. 0.475649970566081\n",
       "1080. 0.788356995517267\n",
       "1081. 0.610339466071035\n",
       "1082. 0.0855946833948425\n",
       "1083. 0.016886262585918\n",
       "1084. 0.359334891770436\n",
       "1085. 0.531629040085054\n",
       "1086. 0.012570018880217\n",
       "1087. 0.401280961903602\n",
       "1088. 0.285800420983862\n",
       "1089. 0.999999899391652\n",
       "1090. 0.0888033516629491\n",
       "1091. 0.679274958966115\n",
       "1092. 0.699092361664451\n",
       "1093. 0.786369010386977\n",
       "1094. 0.0329311038983416\n",
       "1095. 0.291024670329289\n",
       "1096. 0.149266119225962\n",
       "1097. 0.28157247532322\n",
       "1098. 0.357494012178614\n",
       "1099. 0.0182277356518192\n",
       "1100. 0.925413282505606\n",
       "1101. 0.139220916203728\n",
       "1102. 0.0125064075422918\n",
       "1103. 0.213127628749023\n",
       "1104. 0.508402524250166\n",
       "1105. 0.126469982458178\n",
       "1106. 0.264497391266\n",
       "1107. 0.436664519151672\n",
       "1108. 0.179911045434649\n",
       "1109. 0.0675101178974709\n",
       "1110. 0.469758723503266\n",
       "1111. 0.00108476919380634\n",
       "1112. 0.0502871692102015\n",
       "1113. 0.0633866987054689\n",
       "1114. 0.119501709935923\n",
       "1115. 0.466676158912044\n",
       "1116. 0.310300878091165\n",
       "1117. 0.999999147447549\n",
       "1118. 0.0862621250468959\n",
       "1119. 0.589699002573824\n",
       "1120. 0.0340958106540846\n",
       "1121. 0.208527613932907\n",
       "1122. 0.14542061372271\n",
       "1123. 0.0629816046156896\n",
       "1124. 0.840423979594436\n",
       "1125. 0.554403333439654\n",
       "1126. 0.246514541512246\n",
       "1127. 0.0041174717361653\n",
       "1128. 0.0107919343847855\n",
       "1129. 0.0184330466702109\n",
       "1130. 0.030832747655235\n",
       "1131. 0.99999996259574\n",
       "1132. 0.36480875288212\n",
       "1133. 0.0386046069383293\n",
       "1134. 0.0668992513567373\n",
       "1135. 0.0647919048913934\n",
       "1136. 0.144246540228767\n",
       "1137. 0.143138610278439\n",
       "1138. 0.0879836958120621\n",
       "1139. 0.332259734391238\n",
       "1140. 0.0319947834971824\n",
       "1141. 0.344343915942589\n",
       "1142. 0.010622928800904\n",
       "1143. 0.56482551860699\n",
       "1144. 0.0209093634221607\n",
       "1145. 0.0042353714747113\n",
       "1146. 0.719543640498619\n",
       "1147. 0.51695317027421\n",
       "1148. 0.110624286226349\n",
       "1149. 0.0417178617689054\n",
       "1150. 0.0298749809326296\n",
       "1151. 0.0875270512091124\n",
       "1152. 0.312629836641127\n",
       "1153. 0.0135589788440718\n",
       "1154. 0.322783229655624\n",
       "1155. 0.0725996861118289\n",
       "1156. 0.00693125984291952\n",
       "1157. 0.0524067668501133\n",
       "1158. 0.351444765477373\n",
       "1159. 0.108756601726957\n",
       "1160. 0.483537054294353\n",
       "1161. 0.804947871059299\n",
       "1162. 0.0660140388927749\n",
       "1163. 0.64819436885816\n",
       "1164. 0.238233231595257\n",
       "1165. 0.478779491874137\n",
       "1166. 0.596184681051985\n",
       "1167. 0.0774260173405864\n",
       "1168. 0.392008998828665\n",
       "1169. 0.158985831577443\n",
       "1170. 0.0689556610031463\n",
       "1171. 0.131936731389168\n",
       "1172. 0.379500234156237\n",
       "1173. 0.505233325002846\n",
       "1174. 0.0805139074077529\n",
       "1175. 0.138195033618495\n",
       "1176. 0.000613492335005297\n",
       "1177. 0.224049473408494\n",
       "1178. 0.27039819626497\n",
       "1179. 0.0588262376106442\n",
       "1180. 0.290807952041867\n",
       "1181. 0.0311343738390187\n",
       "1182. 0.384981768804915\n",
       "1183. 0.0328636207144629\n",
       "1184. 0.446884847470565\n",
       "1185. 0.0796920235654402\n",
       "1186. 0.306153800007737\n",
       "1187. 0.105690419986473\n",
       "1188. 0.0284524744413658\n",
       "1189. 0.0151126405406884\n",
       "1190. 0.112106467371821\n",
       "1191. 0.175400492093786\n",
       "1192. 0.431800454102634\n",
       "1193. 0.229696382210033\n",
       "1194. 0.0461283801584418\n",
       "1195. 0.0426430728317225\n",
       "1196. 0.0102192254548317\n",
       "1197. 0.387426183864605\n",
       "1198. 0.103129189776603\n",
       "1199. 0.164353813995948\n",
       "1200. 0.160166633041282\n",
       "1201. 0.343960831943811\n",
       "1202. 0.128269789287326\n",
       "1203. 0.448067419210507\n",
       "1204. 0.102143033927492\n",
       "1205. 0.0446640244954609\n",
       "1206. 0.00632588283494273\n",
       "1207. 0.334917997819173\n",
       "1208. 0.117036448203577\n",
       "1209. 0.0684230284893524\n",
       "1210. 0.118512756297235\n",
       "1211. 0.0343752981565862\n",
       "1212. 0.432497680919793\n",
       "1213. 0.139479191659523\n",
       "1214. 0.0140017766187891\n",
       "1215. 0.581409386187053\n",
       "1216. 0.707262374805594\n",
       "1217. 0.676039987152588\n",
       "1218. 0.111035127230789\n",
       "1219. 0.076719758971674\n",
       "1220. 0.111091887589308\n",
       "1221. 0.295261334057446\n",
       "1222. 0.00274376627747775\n",
       "1223. 0.00171409763294865\n",
       "1224. 0.242794444901855\n",
       "1225. 0.0365185630712038\n",
       "1226. 0.0381762833441883\n",
       "1227. 0.420266907762725\n",
       "1228. 0.4144607621968\n",
       "1229. 0.255141737848591\n",
       "1230. 0.0087406159898517\n",
       "1231. 0.149267505949089\n",
       "1232. 0.716772960375039\n",
       "1233. 0.428396484675624\n",
       "1234. 0.0897521400617911\n",
       "1235. 0.134422453775103\n",
       "1236. 0.159400382979137\n",
       "1237. 0.248156331997309\n",
       "1238. 0.999999899755454\n",
       "1239. 0.621003227524054\n",
       "1240. 0.411794970047196\n",
       "1241. 0.109357350356801\n",
       "1242. 0.00719209312836932\n",
       "1243. 0.00392713457488431\n",
       "1244. 0.643664784228587\n",
       "1245. 0.34850135185967\n",
       "1246. 0.0711569847618557\n",
       "1247. 0.351863175367112\n",
       "1248. 0.0288360561995615\n",
       "1249. 0.121008690138617\n",
       "1250. 0.341693623528626\n",
       "1251. 0.450786884324715\n",
       "1252. 0.839378356495052\n",
       "1253. 0.115675304010426\n",
       "1254. 0.395658873994819\n",
       "1255. 0.0780376514842547\n",
       "1256. 0.937222670441764\n",
       "1257. 0.0185688506937918\n",
       "1258. 0.0365611011513383\n",
       "1259. 0.459464686231983\n",
       "1260. 0.0123405080055394\n",
       "1261. 0.115300206157326\n",
       "1262. 0.333059677674429\n",
       "1263. 0.0194341950572159\n",
       "1264. 0.0116899936915638\n",
       "1265. 0.513825402584578\n",
       "1266. 0.0225353090383776\n",
       "1267. 0.0822178093006131\n",
       "1268. 0.480661788833197\n",
       "1269. 0.0966890152262564\n",
       "1270. 0.376341598089679\n",
       "1271. 0.0188280634091753\n",
       "1272. 0.0644800530654544\n",
       "1273. 0.496476381976845\n",
       "1274. 0.044486125992141\n",
       "1275. 0.242296131164322\n",
       "1276. 0.5462276438816\n",
       "1277. 0.373565724186034\n",
       "1278. 0.283126386745984\n",
       "1279. 0.0197569766692492\n",
       "1280. 0.0541379122086214\n",
       "1281. 0.0218449492671842\n",
       "1282. 0.39961999690153\n",
       "1283. 0.0128226676513324\n",
       "1284. 0.0397736359132368\n",
       "1285. 0.0674757811603202\n",
       "1286. 0.0244701811847007\n",
       "1287. 0.332434853551037\n",
       "1288. 0.0669214706400757\n",
       "1289. 0.810979291451371\n",
       "1290. 0.109771755561888\n",
       "1291. 0.360699543748818\n",
       "1292. 0.940448253429991\n",
       "1293. 0.318456867667438\n",
       "1294. 0.629184993765762\n",
       "1295. 0.0813133729422563\n",
       "1296. 0.620041187193279\n",
       "1297. 0.789030638685885\n",
       "1298. 0.344998351965315\n",
       "1299. 0.0550173805859845\n",
       "1300. 0.153916759875194\n",
       "1301. 0.330755455678276\n",
       "1302. 0.0194895318232061\n",
       "1303. 0.205749882986029\n",
       "1304. 0.553154358672463\n",
       "1305. 0.213651706951662\n",
       "1306. 0.591771298601621\n",
       "1307. 0.00611919438776252\n",
       "1308. 0.0822670563834605\n",
       "1309. 0.725529709606978\n",
       "1310. 0.0023654921756232\n",
       "1311. 0.0443624987354101\n",
       "1312. 0.0734532134021451\n",
       "1313. 0.111738043298359\n",
       "1314. 0.0189999346797714\n",
       "1315. 0.58166001331911\n",
       "1316. 0.352322672091126\n",
       "1317. 0.00410926428703993\n",
       "1318. 0.0873441475972424\n",
       "1319. 0.408803335657935\n",
       "1320. 0.784146396581265\n",
       "1321. 0.635640629612414\n",
       "1322. 0.115891386401419\n",
       "1323. 0.200717926772197\n",
       "1324. 0.299687427534232\n",
       "1325. 0.00672402856835594\n",
       "1326. 0.801570047356619\n",
       "1327. 0.0426671777978862\n",
       "1328. 0.0127759868029564\n",
       "1329. 0.242675951233782\n",
       "1330. 0.126772723549678\n",
       "1331. 0.0439637533488899\n",
       "1332. 0.257337844571137\n",
       "1333. 0.543337799086199\n",
       "1334. 0.208106641889324\n",
       "1335. 0.132721146618251\n",
       "1336. 0.531606266396264\n",
       "1337. 0.292998879059461\n",
       "1338. 0.00966018862869711\n",
       "1339. 0.0193694640301211\n",
       "1340. 0.15510703520612\n",
       "1341. 0.0161150653010276\n",
       "1342. 0.282592032803302\n",
       "1343. 0.00694463553896689\n",
       "1344. 0.201647592687945\n",
       "1345. 0.422466741704975\n",
       "1346. 0.242953581399268\n",
       "1347. 0.00351089833854013\n",
       "1348. 0.400627588277948\n",
       "1349. 0.0389228867741802\n",
       "1350. 0.0521593751657165\n",
       "1351. 0.331435704194659\n",
       "1352. 0.0455245248572239\n",
       "1353. 0.0309064616653328\n",
       "1354. 0.0878612310292512\n",
       "1355. 0.0133130674654993\n",
       "1356. 0.68832676321178\n",
       "1357. 0.659228577916452\n",
       "1358. 0.227359436505122\n",
       "1359. 0.481600127096135\n",
       "1360. 0.261394497440249\n",
       "1361. 0.0130665052484996\n",
       "1362. 0.520518344967139\n",
       "1363. 0.647744267628216\n",
       "1364. 0.250722701012656\n",
       "1365. 0.373737729831715\n",
       "1366. 0.325611836248114\n",
       "1367. 0.00319015496483199\n",
       "1368. 0.339464434005485\n",
       "1369. 0.0966684552130676\n",
       "1370. 0.0617476612895288\n",
       "1371. 0.19152926225361\n",
       "1372. 0.11103601264329\n",
       "1373. 0.61590524200321\n",
       "1374. 0.0215583805673567\n",
       "1375. 0.140053997430894\n",
       "1376. 0.0477986234154584\n",
       "1377. 0.0826264263420942\n",
       "1378. 0.00702769078577563\n",
       "1379. 0.826192755491458\n",
       "1380. 0.257834891033145\n",
       "1381. 0.286794215658679\n",
       "1382. 0.106014480810485\n",
       "1383. 0.340629795332494\n",
       "1384. 0.480350855810903\n",
       "1385. 0.155249929153167\n",
       "1386. 0.285211952849519\n",
       "1387. 0.324050126089352\n",
       "1388. 0.0703373040441296\n",
       "1389. 0.365802250433597\n",
       "1390. 0.360178385500458\n",
       "1391. 0.083657639466234\n",
       "1392. 0.0120954505667411\n",
       "1393. 0.0157389085382795\n",
       "1394. 0.0510121515711138\n",
       "1395. 0.18566362469721\n",
       "1396. 0.21093029457333\n",
       "1397. 0.0339366834988751\n",
       "1398. 0.129181519812422\n",
       "1399. 0.0551722889180708\n",
       "1400. 0.0583173531946214\n",
       "1401. 0.0565857009501661\n",
       "1402. 0.195500676987564\n",
       "1403. 0.387298210598922\n",
       "1404. 0.0174095793128992\n",
       "1405. 0.54469816524027\n",
       "1406. 0.999999881272755\n",
       "1407. 0.1027611960477\n",
       "1408. 0.47667823007691\n",
       "1409. 0.0662688998929444\n",
       "1410. 0.116922734592427\n",
       "1411. 0.0850278394625899\n",
       "1412. 0.392637026170667\n",
       "1413. 0.0100271760538756\n",
       "1414. 0.882515089158017\n",
       "1415. 0.149166487271602\n",
       "1416. 0.0447689462679109\n",
       "1417. 0.780293803729918\n",
       "1418. 0.577707688286746\n",
       "1419. 0.0186315861936365\n",
       "1420. 0.00826138172958256\n",
       "1421. 0.208537293731533\n",
       "1422. 0.270569664407966\n",
       "1423. 0.51863093991967\n",
       "1424. 0.174336426356056\n",
       "1425. 0.31521906329962\n",
       "1426. 0.422335186398251\n",
       "1427. 0.417747688186922\n",
       "1428. 0.209491591975983\n",
       "1429. 0.00632646199144239\n",
       "1430. 0.749652383357013\n",
       "1431. 0.0135803141014883\n",
       "1432. 0.284565936383715\n",
       "1433. 0.0203262184445531\n",
       "1434. 0.837222888250718\n",
       "1435. 0.0557357281173115\n",
       "1436. 0.20565285399988\n",
       "1437. 0.339038777221873\n",
       "1438. 0.0100044063394905\n",
       "1439. 0.357838781913102\n",
       "1440. 0.392694598286668\n",
       "1441. 0.123665880291385\n",
       "1442. 0.024835452290815\n",
       "1443. 0.798304952283806\n",
       "1444. 0.0710472788783619\n",
       "1445. 0.217491479794416\n",
       "1446. 0.618326994120905\n",
       "1447. 0.0400660581512396\n",
       "1448. 0.176943638370039\n",
       "1449. 0.425878858065581\n",
       "1450. 0.0485012868145134\n",
       "1451. 0.011622000593429\n",
       "1452. 0.163495250067349\n",
       "1453. 0.28876832052196\n",
       "1454. 0.150084973932894\n",
       "1455. 0.761837899159859\n",
       "1456. 0.0905138213694239\n",
       "1457. 0.117727398337633\n",
       "1458. 0.0129693312408954\n",
       "1459. 0.270787542001476\n",
       "1460. 0.303445040822964\n",
       "1461. 0.0045746293789501\n",
       "1462. 0.673561166508947\n",
       "1463. 0.420120032207368\n",
       "1464. 0.718770945107793\n",
       "1465. 0.0157843933453791\n",
       "1466. 0.773586185200803\n",
       "1467. 0.0444909631293543\n",
       "1468. 0.101154237579664\n",
       "1469. 0.00362551629215413\n",
       "1470. 0.864277448109038\n",
       "1471. 0.401290367065925\n",
       "1472. 0.0044193786966831\n",
       "1473. 0.105168455461581\n",
       "1474. 0.000704355504237902\n",
       "1475. 0.0782380967993163\n",
       "1476. 0.0680032169510186\n",
       "1477. 0.325215314840005\n",
       "1478. 0.522137318069587\n",
       "1479. 0.0695479591239362\n",
       "1480. 0.910088474045432\n",
       "1481. 0.0225768752466563\n",
       "1482. 0.0830117248087445\n",
       "1483. 0.782424262078024\n",
       "1484. 0.00284140544328993\n",
       "1485. 0.181802023547386\n",
       "1486. 0.446750806198583\n",
       "1487. 0.175307512265613\n",
       "1488. 0.211994718778293\n",
       "1489. 0.362056016463592\n",
       "1490. 0.698494628531572\n",
       "1491. 0.796409402741523\n",
       "1492. 0.69701665386244\n",
       "1493. 0.60498841832336\n",
       "1494. 0.222151410916736\n",
       "1495. 0.23245232118953\n",
       "1496. 0.0312143806769226\n",
       "1497. 0.00597863316182652\n",
       "1498. 0.0474385359674159\n",
       "1499. 0.0692646562294439\n",
       "1500. 0.813293860136716\n",
       "1501. 0.37800470789794\n",
       "1502. 0.0826239869534869\n",
       "1503. 0.0535687457396391\n",
       "1504. 0.381792304137231\n",
       "1505. 0.0224097542310455\n",
       "1506. 0.0571640111552471\n",
       "1507. 0.500278393587007\n",
       "1508. 0.737791677638268\n",
       "1509. 0.401349247208118\n",
       "1510. 0.436029511070447\n",
       "1511. 0.0327230826765334\n",
       "1512. 0.00927603454896041\n",
       "1513. 0.122509979589628\n",
       "1514. 0.0278683778105911\n",
       "1515. 0.852582373974251\n",
       "1516. 0.015712446760872\n",
       "1517. 0.0940175246146619\n",
       "1518. 0.573673495045516\n",
       "1519. 0.5041167751129\n",
       "1520. 0.0102770308244628\n",
       "1521. 0.00118549280420903\n",
       "1522. 0.131375428121562\n",
       "1523. 0.438320910689265\n",
       "1524. 0.756097754809503\n",
       "1525. 0.378041113022159\n",
       "1526. 0.0357551574045644\n",
       "1527. 0.134654161705776\n",
       "1528. 0.259550862282099\n",
       "1529. 0.778064247830644\n",
       "1530. 0.0201462974622649\n",
       "1531. 0.24006009465598\n",
       "1532. 0.678198913733828\n",
       "1533. 0.300570735597846\n",
       "1534. 0.0469963534947108\n",
       "1535. 0.502486365272292\n",
       "1536. 0.0143151454453701\n",
       "1537. 0.0651614188522869\n",
       "1538. 0.836073441063167\n",
       "1539. 0.259320005941914\n",
       "1540. 0.0108894451187177\n",
       "1541. 0.30730254345272\n",
       "1542. 0.0983187936355585\n",
       "1543. 0.0380510346564614\n",
       "1544. 0.607227303527659\n",
       "1545. 0.00786637524450243\n",
       "1546. 0.345422292451516\n",
       "1547. 0.0183754642854709\n",
       "1548. 0.206154474247793\n",
       "1549. 0.0099216288357854\n",
       "1550. 0.0414194474098731\n",
       "1551. 0.0829841263907362\n",
       "1552. 0.0306311878548479\n",
       "1553. 0.0285561501172792\n",
       "1554. 0.0605196262065695\n",
       "1555. 0.11832520017891\n",
       "1556. 0.0109672154767565\n",
       "1557. 0.0127333624065195\n",
       "1558. 0.00224888673714246\n",
       "1559. 0.0429628899674597\n",
       "1560. 0.0251811856175837\n",
       "1561. 0.0295559817119572\n",
       "1562. 0.00388163350760284\n",
       "1563. 0.261194695234384\n",
       "1564. 0.882634420272317\n",
       "1565. 0.171042965249206\n",
       "1566. 0.0149085330205609\n",
       "1567. 0.00158657563942785\n",
       "1568. 0.556844563218709\n",
       "1569. 0.387655918204949\n",
       "1570. 0.0661323040382908\n",
       "1571. 0.608800003409554\n",
       "1572. 0.574721973559668\n",
       "1573. 0.0521771218150772\n",
       "1574. 0.902118697498264\n",
       "1575. 0.0820669258207579\n",
       "1576. 0.00754209020126828\n",
       "1577. 0.723437201507968\n",
       "1578. 0.101828009736096\n",
       "1579. 0.468876692554582\n",
       "1580. 0.2468317337222\n",
       "1581. 0.512240316374758\n",
       "1582. 0.087004829379109\n",
       "1583. 0.0149338140834091\n",
       "1584. 0.45103091765394\n",
       "1585. 0.0231078515925126\n",
       "1586. 0.0443657247274287\n",
       "1587. 0.290537482498241\n",
       "1588. 0.561325115718624\n",
       "1589. 0.342226939316761\n",
       "1590. 0.12864443652351\n",
       "1591. 0.689437807152223\n",
       "1592. 0.333644758404331\n",
       "1593. 0.0820764525219253\n",
       "1594. 0.042152085677163\n",
       "1595. 0.224722384138494\n",
       "1596. 0.529648383441074\n",
       "1597. 0.0207762852410577\n",
       "1598. 0.00440931528151489\n",
       "1599. 0.411697715892915\n",
       "1600. 0.0802832296717964\n",
       "1601. 0.346339647881345\n",
       "1602. 0.0130900437365671\n",
       "1603. 0.012744641309673\n",
       "1604. 0.429693038206176\n",
       "1605. 0.211733598664388\n",
       "1606. 0.713899951037443\n",
       "1607. 0.0204587633545007\n",
       "1608. 0.969105900109868\n",
       "1609. 0.0120568543931244\n",
       "1610. 0.900706976430349\n",
       "1611. 0.00322821636077589\n",
       "1612. 0.0133650536120327\n",
       "1613. 0.198050332159119\n",
       "1614. 0.702582904313514\n",
       "1615. 0.660678973776002\n",
       "1616. 0.00893567809480346\n",
       "1617. 0.0246127601627504\n",
       "1618. 0.0372872768121372\n",
       "1619. 0.137123364030737\n",
       "1620. 0.520023981229925\n",
       "1621. 0.0066450794922484\n",
       "1622. 0.999999934069721\n",
       "1623. 0.699187065784779\n",
       "1624. 0.604873083636888\n",
       "1625. 0.333363366288427\n",
       "1626. 0.666210889741808\n",
       "1627. 0.12270428875785\n",
       "1628. 0.67271433309237\n",
       "1629. 0.186352219335337\n",
       "1630. 0.0255251185695469\n",
       "1631. 0.103295434259188\n",
       "1632. 0.292404583057265\n",
       "1633. 0.107016716840353\n",
       "1634. 0.0191613585884338\n",
       "1635. 0.323385344438563\n",
       "1636. 0.323606556162005\n",
       "1637. 0.00266290871999711\n",
       "1638. 0.0429157016021198\n",
       "1639. 0.00486193696698884\n",
       "1640. 0.350003539491226\n",
       "1641. 0.372556174351423\n",
       "1642. 0.0626083960843705\n",
       "1643. 0.191544290861375\n",
       "1644. 0.0938000049788963\n",
       "1645. 0.153051598225422\n",
       "1646. 0.480187997110702\n",
       "1647. 0.0052630599731583\n",
       "1648. 0.27972661029505\n",
       "1649. 0.0398773138398133\n",
       "1650. 0.00162572191553019\n",
       "1651. 0.442823136141403\n",
       "1652. 0.0120391413903642\n",
       "1653. 0.10566201095036\n",
       "1654. 0.093718039648838\n",
       "1655. 0.164142373320656\n",
       "1656. 0.383839605878613\n",
       "1657. 0.00451305545018589\n",
       "1658. 0.537594286597542\n",
       "1659. 0.466383798477598\n",
       "1660. 0.0281582762622777\n",
       "1661. 0.29656351506812\n",
       "1662. 0.0500069544563138\n",
       "1663. 0.0179109290575799\n",
       "1664. 0.182654043547901\n",
       "1665. 0.173632291650515\n",
       "1666. 0.314874409741457\n",
       "1667. 0.00285495418430275\n",
       "1668. 0.144684627437276\n",
       "1669. 0.0117231450258738\n",
       "1670. 0.427994448144244\n",
       "1671. 0.16361830683771\n",
       "1672. 0.00846342302494718\n",
       "1673. 0.0162058441102335\n",
       "1674. 0.261560471705074\n",
       "1675. 0.335900678213885\n",
       "1676. 0.999999719882787\n",
       "1677. 0.0223168252846246\n",
       "1678. 0.171904897795075\n",
       "1679. 0.0132759069202664\n",
       "1680. 0.571779516237679\n",
       "1681. 0.143311262794378\n",
       "1682. 0.0636269799691191\n",
       "1683. 0.0986192994529642\n",
       "1684. 0.186532650875311\n",
       "1685. 0.10725450032835\n",
       "1686. 0.0587551753964388\n",
       "1687. 0.0113480679433616\n",
       "1688. 0.373801111008503\n",
       "1689. 0.146091797132755\n",
       "1690. 0.00472946298492059\n",
       "1691. 0.0148767186716644\n",
       "1692. 0.402492786015853\n",
       "1693. 0.599905774626979\n",
       "1694. 0.129156093581038\n",
       "1695. 0.0327122735411403\n",
       "1696. 0.00270956646316803\n",
       "1697. 0.414419234280421\n",
       "1698. 0.0748696899689292\n",
       "1699. 0.210084474844711\n",
       "1700. 0.0505849568379707\n",
       "1701. 0.37878641187\n",
       "1702. 0.0520068591141141\n",
       "1703. 0.191747610946361\n",
       "1704. 0.073796405649198\n",
       "1705. 0.857763278077361\n",
       "1706. 0.287226861085736\n",
       "1707. 0.156571082241875\n",
       "1708. 0.679872307420937\n",
       "1709. 0.607413476372354\n",
       "1710. 0.0259541990026681\n",
       "1711. 0.438749681108358\n",
       "1712. 0.27895648356056\n",
       "1713. 0.0302853846441377\n",
       "1714. 0.0359113469890458\n",
       "1715. 0.180162495915178\n",
       "1716. 0.0416986274509818\n",
       "1717. 0.376890684622951\n",
       "1718. 0.231963166688558\n",
       "1719. 0.494434450132828\n",
       "1720. 0.0209143587981374\n",
       "1721. 0.107206716581593\n",
       "1722. 0.708322155399093\n",
       "1723. 0.913731609334851\n",
       "1724. 0.104157703080455\n",
       "1725. 0.0556707696440846\n",
       "1726. 0.0175382849056347\n",
       "1727. 0.460250756300556\n",
       "1728. 0.00892961678058072\n",
       "1729. 0.0426768272972568\n",
       "1730. 0.738494445202362\n",
       "1731. 0.0139891946964155\n",
       "1732. 0.0173010271782344\n",
       "1733. 0.36643548507285\n",
       "1734. 0.744312090543482\n",
       "1735. 0.0881357595922892\n",
       "1736. 0.00942054215640495\n",
       "1737. 0.068128632951133\n",
       "1738. 0.0388245693052423\n",
       "1739. 0.392192135594782\n",
       "1740. 0.388390494088206\n",
       "1741. 0.651204649158032\n",
       "1742. 0.180637714329667\n",
       "1743. 0.102705415062379\n",
       "1744. 0.020707264284863\n",
       "1745. 0.0256075381477818\n",
       "1746. 0.00250860705648477\n",
       "1747. 0.681940048158735\n",
       "1748. 0.350262020578293\n",
       "1749. 0.0383984308896455\n",
       "1750. 0.658734181967604\n",
       "1751. 0.0669926729564842\n",
       "1752. 0.533022350312315\n",
       "1753. 0.212693264031104\n",
       "1754. 0.738803576604701\n",
       "1755. 0.0389983717122111\n",
       "1756. 0.747525660111215\n",
       "1757. 0.785958005108221\n",
       "1758. 0.152504421161402\n",
       "1759. 0.0189503594342853\n",
       "1760. 0.235368374657788\n",
       "1761. 0.0416053578011781\n",
       "1762. 0.166950509764835\n",
       "1763. 0.744520527611533\n",
       "1764. 0.0529810394056784\n",
       "1765. 0.00327444356051325\n",
       "1766. 0.149399902870609\n",
       "1767. 0.402424644821861\n",
       "1768. 0.0276814867259534\n",
       "1769. 0.00991715759274851\n",
       "1770. 0.4251773610361\n",
       "1771. 0.25052315786123\n",
       "1772. 0.109479032663\n",
       "1773. 0.00821826282945569\n",
       "1774. 0.227110410995884\n",
       "1775. 0.14806988830362\n",
       "1776. 0.359347359420511\n",
       "1777. 0.210979644010036\n",
       "1778. 0.0463032214188742\n",
       "1779. 0.46875383462234\n",
       "1780. 0.215888946331947\n",
       "1781. 0.713179985277368\n",
       "1782. 0.0133076759758265\n",
       "1783. 0.123259371522801\n",
       "1784. 0.0350621484165317\n",
       "1785. 0.0218596503142545\n",
       "1786. 0.00709909504330927\n",
       "1787. 0.246300702617571\n",
       "1788. 0.0722452739253546\n",
       "1789. 0.768767034364842\n",
       "1790. 0.103045751032903\n",
       "1791. 0.0427102616103358\n",
       "1792. 0.0220479418402836\n",
       "1793. 0.920016590050812\n",
       "1794. 0.0211256570489751\n",
       "1795. 0.39996253462366\n",
       "1796. 0.0548779363750377\n",
       "1797. 0.0252650412505952\n",
       "1798. 0.257208730113193\n",
       "1799. 0.680939629125579\n",
       "1800. 0.729255611723037\n",
       "1801. 0.596928459338522\n",
       "1802. 0.999999369186944\n",
       "1803. 0.0263022407673469\n",
       "1804. 0.384847862812985\n",
       "1805. 0.549343043856729\n",
       "1806. 0.173576609514886\n",
       "1807. 0.080540638126581\n",
       "1808. 0.737058762166601\n",
       "1809. 0.00306032494596797\n",
       "1810. 0.0717116554481647\n",
       "1811. 0.309395303411374\n",
       "1812. 0.0220249069614392\n",
       "1813. 0.353891615376487\n",
       "1814. 0.0516877938558486\n",
       "1815. 0.197422325508889\n",
       "1816. 0.625577234432896\n",
       "1817. 0.0555821964514438\n",
       "1818. 0.589136527507761\n",
       "1819. 0.0555530611510398\n",
       "1820. 0.475650326664653\n",
       "1821. 0.115993377295593\n",
       "1822. 0.358102933017236\n",
       "1823. 0.84324883081674\n",
       "1824. 0.0322303964399007\n",
       "1825. 0.00261417803905965\n",
       "1826. 0.0192613020090237\n",
       "1827. 0.783702346299451\n",
       "1828. 0.999999921868559\n",
       "1829. 0.0079345660994076\n",
       "1830. 0.00387438423090134\n",
       "1831. 0.0211480019974471\n",
       "1832. 0.050450558487875\n",
       "1833. 0.0445260716037124\n",
       "1834. 0.0293364700879047\n",
       "1835. 0.0855610339125738\n",
       "1836. 0.641965499370153\n",
       "1837. 0.0489075172471356\n",
       "1838. 0.0013621038091988\n",
       "1839. 0.537891437772786\n",
       "1840. 0.886347490567167\n",
       "1841. 0.16125326393139\n",
       "1842. 0.0309850610819549\n",
       "1843. 0.693530738886448\n",
       "1844. 0.00561970245310959\n",
       "1845. 0.302654046642452\n",
       "1846. 0.00785647756233152\n",
       "1847. 0.191613057594669\n",
       "1848. 0.011638934751399\n",
       "1849. 0.243860321451445\n",
       "1850. 0.444726091154691\n",
       "1851. 0.0572090029004352\n",
       "1852. 0.261446047313538\n",
       "1853. 0.0171908586343519\n",
       "1854. 0.575854903961387\n",
       "1855. 0.192877574350031\n",
       "1856. 0.00422424646968325\n",
       "1857. 0.922205852496056\n",
       "1858. 0.0704851393216222\n",
       "1859. 0.567080304207818\n",
       "1860. 0.47606140786739\n",
       "1861. 0.306320674105787\n",
       "1862. 0.0369710971674199\n",
       "1863. 0.819697805813227\n",
       "1864. 0.013248651624041\n",
       "1865. 0.0631829730650491\n",
       "1866. 0.378553828139803\n",
       "1867. 0.0413801184849777\n",
       "1868. 0.27543407874579\n",
       "1869. 0.545895504384078\n",
       "1870. 0.259934954639882\n",
       "1871. 0.0165679910614728\n",
       "1872. 0.262467341480874\n",
       "1873. 0.00851088909340637\n",
       "1874. 0.132238607019351\n",
       "1875. 0.00264874929875302\n",
       "1876. 0.514256243669981\n",
       "1877. 0.504662694825196\n",
       "1878. 0.46918137663433\n",
       "1879. 0.0206286273389961\n",
       "1880. 0.0130306565509383\n",
       "1881. 0.0216268255776056\n",
       "1882. 0.962873976004148\n",
       "1883. 0.0228586561112876\n",
       "1884. 0.415886038439882\n",
       "1885. 0.111644078337856\n",
       "1886. 0.035732224743968\n",
       "1887. 0.183323652433876\n",
       "1888. 0.00962911740164285\n",
       "1889. 0.472661642158482\n",
       "1890. 0.063094439257077\n",
       "1891. 0.739556130631472\n",
       "1892. 0.540561789482398\n",
       "1893. 0.476699904607004\n",
       "1894. 0.0574012928104084\n",
       "1895. 0.0248904132738773\n",
       "1896. 0.320685216707128\n",
       "1897. 0.0259059737871506\n",
       "1898. 0.375706690430921\n",
       "1899. 0.0318621776115162\n",
       "1900. 0.835147724968061\n",
       "1901. 0.579956282477218\n",
       "1902. 0.771459176592422\n",
       "1903. 0.0929458415001703\n",
       "1904. 0.140511726836116\n",
       "1905. 0.0944645275237256\n",
       "1906. 0.165131169764395\n",
       "1907. 0.779965172205315\n",
       "1908. 0.0135918638149194\n",
       "1909. 0.00310894475377708\n",
       "1910. 0.999999921059875\n",
       "1911. 0.241733153817061\n",
       "1912. 0.658845773041207\n",
       "1913. 0.6818222401056\n",
       "1914. 0.0271196274070777\n",
       "1915. 0.355537798682258\n",
       "1916. 0.0610618672068804\n",
       "1917. 0.356707039898808\n",
       "1918. 0.350476407031346\n",
       "1919. 0.343113774889433\n",
       "1920. 0.0295106260508025\n",
       "1921. 0.738953374357723\n",
       "1922. 0.284378209763666\n",
       "1923. 0.068611015867394\n",
       "1924. 0.00482401773356611\n",
       "1925. 0.630769530768676\n",
       "1926. 0.315158357608213\n",
       "1927. 0.356827702658612\n",
       "1928. 0.270527239661937\n",
       "1929. 0.251624881415677\n",
       "1930. 0.25760880755991\n",
       "1931. 0.180107416527999\n",
       "1932. 0.573906010950479\n",
       "1933. 0.333287983948135\n",
       "1934. 0.0994519735509173\n",
       "1935. 0.254373111131962\n",
       "1936. 0.127947766456593\n",
       "1937. 0.0287503878509145\n",
       "1938. 0.622280038298678\n",
       "1939. 0.00923154040183382\n",
       "1940. 0.00893073150299629\n",
       "1941. 0.4107363959673\n",
       "1942. 0.532240779589429\n",
       "1943. 0.0147120399520612\n",
       "1944. 0.0850992156468574\n",
       "1945. 0.0666759971171219\n",
       "1946. 0.0532227533905391\n",
       "1947. 0.944763104212636\n",
       "1948. 0.0421231961962477\n",
       "1949. 0.330871603608521\n",
       "1950. 0.00501890572748447\n",
       "1951. 0.0475675707248185\n",
       "1952. 0.0083735761559981\n",
       "1953. 0.844623338968075\n",
       "1954. 0.846679594057493\n",
       "1955. 0.603003049555463\n",
       "1956. 0.0576375333669081\n",
       "1957. 0.393734285994759\n",
       "1958. 0.00767541424809816\n",
       "1959. 0.199636721981925\n",
       "1960. 0.292213024120264\n",
       "1961. 0.76906982519554\n",
       "1962. 0.856685323240491\n",
       "1963. 0.595530358227373\n",
       "1964. 0.029703505549569\n",
       "1965. 0.170230360437168\n",
       "1966. 0.0590962158581206\n",
       "1967. 0.137961922417632\n",
       "1968. 0.0744322585941222\n",
       "1969. 0.615940912653162\n",
       "1970. 0.838822052148631\n",
       "1971. 0.191665871794086\n",
       "1972. 0.528119175483124\n",
       "1973. 0.18480304438479\n",
       "1974. 0.030103090076589\n",
       "1975. 0.313327982406951\n",
       "1976. 0.0917255709899279\n",
       "1977. 0.041551136464234\n",
       "1978. 0.187966788144977\n",
       "1979. 0.0506776774678144\n",
       "1980. 0.265329521353257\n",
       "1981. 0.0346522723301589\n",
       "1982. 0.726813785921841\n",
       "1983. 0.191463884285267\n",
       "1984. 0.0639038731282046\n",
       "1985. 0.0632602697916699\n",
       "1986. 0.803075849198999\n",
       "1987. 0.438268079689189\n",
       "1988. 0.0985781227947532\n",
       "1989. 0.0310326490016468\n",
       "1990. 0.248496611722856\n",
       "1991. 0.0268931479842507\n",
       "1992. 0.584490035996011\n",
       "1993. 0.767644550296345\n",
       "1994. 0.616942243443454\n",
       "1995. 0.181884391709822\n",
       "1996. 0.0292352580005547\n",
       "1997. 0.449170568819321\n",
       "1998. 0.037957275850432\n",
       "1999. 0.0214499858700358\n",
       "2000. 0.772209840371338\n",
       "2001. 0.103785471161419\n",
       "2002. 0.479325296521707\n",
       "2003. 0.0128060641739382\n",
       "2004. 0.0396358543490138\n",
       "2005. 0.346936039831818\n",
       "2006. 0.210000158269074\n",
       "2007. 0.181801412731248\n",
       "2008. 0.082041376717069\n",
       "2009. 0.182220291384141\n",
       "2010. 0.0081125633662825\n",
       "2011. 0.474099349262691\n",
       "2012. 0.218620140442417\n",
       "2013. 0.221512435317649\n",
       "2014. 0.345253143463621\n",
       "2015. 0.282029571528149\n",
       "2016. 0.123630940615102\n",
       "2017. 0.00312376623456277\n",
       "2018. 0.687484003092099\n",
       "2019. 0.167630842305543\n",
       "2020. 0.256873896398803\n",
       "2021. 0.138270720742715\n",
       "2022. 0.335698092889923\n",
       "2023. 0.0555016611674721\n",
       "2024. 0.559613891428735\n",
       "2025. 0.0815618215625696\n",
       "2026. 0.314753223939865\n",
       "2027. 0.13519587565503\n",
       "2028. 0.512628252536532\n",
       "2029. 0.310301419935886\n",
       "2030. 0.138872333665707\n",
       "2031. 0.0032387922514317\n",
       "2032. 0.0503488414695993\n",
       "2033. 0.65381973340819\n",
       "2034. 0.0613327135504364\n",
       "2035. 0.00933227759682214\n",
       "2036. 0.0143616311827729\n",
       "2037. 0.124348599066586\n",
       "2038. 0.00647149006353583\n",
       "2039. 0.252065847305571\n",
       "2040. 0.31145577732936\n",
       "2041. 0.0170022745551469\n",
       "2042. 0.0184837003196838\n",
       "2043. 0.0125543720114499\n",
       "2044. 0.118783058537979\n",
       "2045. 0.00908245556874083\n",
       "2046. 0.0209936551649666\n",
       "2047. 0.113606708659381\n",
       "2048. 0.132870896582193\n",
       "2049. 0.262417580337086\n",
       "2050. 0.0111836663784644\n",
       "2051. 0.0379309246642737\n",
       "2052. 0.489558874761568\n",
       "2053. 0.0357662129703509\n",
       "2054. 0.0753036946996795\n",
       "2055. 0.648942525448569\n",
       "2056. 0.274216563019518\n",
       "2057. 0.852575022184947\n",
       "2058. 0.0479075158651052\n",
       "2059. 0.0382467771585991\n",
       "2060. 0.0521301417772706\n",
       "2061. 0.160474064868337\n",
       "2062. 0.191231812054263\n",
       "2063. 0.0397004059039417\n",
       "2064. 0.166397618120375\n",
       "2065. 0.0118601763248746\n",
       "2066. 0.955225021979712\n",
       "2067. 0.0190896078550038\n",
       "2068. 0.211297604682006\n",
       "2069. 0.151360334432323\n",
       "2070. 0.00286926456983292\n",
       "2071. 0.210523662682554\n",
       "2072. 0.149085021179574\n",
       "2073. 0.264074540127379\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   [1] 0.0199699476 0.0458199977 0.4952666964 0.0472587035 0.6104683349\n",
       "   [6] 0.1361596216 0.3170497697 0.3307880763 0.0572359529 0.0083233191\n",
       "  [11] 0.1964163414 0.1802002642 0.0113471975 0.9999998743 0.0774379837\n",
       "  [16] 0.0343189364 0.0117077240 0.1751353391 0.1515979753 0.0113879153\n",
       "  [21] 0.0390493669 0.2679761147 0.9296313711 0.0133101490 0.6322869913\n",
       "  [26] 0.0681840936 0.1678670847 0.7831661723 0.5390911181 0.0885465146\n",
       "  [31] 0.2185037089 0.7142450720 0.0193011550 0.4173790603 0.7143146672\n",
       "  [36] 0.0989819114 0.6321263472 0.8630684674 0.8386791775 0.0123271800\n",
       "  [41] 0.2007949335 0.1888211921 0.0700626378 0.8379455711 0.2763106218\n",
       "  [46] 0.6576918415 0.2056602556 0.0239168650 0.1562241360 0.0727436295\n",
       "  [51] 0.0502401353 0.0271617650 0.1323034703 0.1766739849 0.9362090495\n",
       "  [56] 0.7144625460 0.6113414333 0.5229274819 0.0856755142 0.2626171483\n",
       "  [61] 0.8993085005 0.0121847961 0.0169264821 0.1166345272 0.0672691468\n",
       "  [66] 0.0195591551 0.0019723540 0.0108449809 0.5102167241 0.0904532461\n",
       "  [71] 0.4353725257 0.0049295633 0.0124550789 0.5418892949 0.0089580736\n",
       "  [76] 0.0936020507 0.5643640496 0.0175227075 0.3195302171 0.0260328768\n",
       "  [81] 0.7826567733 0.0300054900 0.0092407088 0.5570423225 0.1727678991\n",
       "  [86] 0.1695004758 0.0105496861 0.0474489753 0.7218035649 0.2365270427\n",
       "  [91] 0.0400844925 0.0323106117 0.0551054030 0.8815132271 0.1274652694\n",
       "  [96] 0.1716262086 0.0198552567 0.0939341550 0.3840393172 0.4900646239\n",
       " [101] 0.8102203012 0.0128802301 0.6289308188 0.4779569185 0.2112390630\n",
       " [106] 0.4506255192 0.0489693550 0.1532922020 0.2639430266 0.0361809240\n",
       " [111] 0.0147370171 0.8178879642 0.0057257007 0.5652563518 0.2368034854\n",
       " [116] 0.0029701010 0.0021964898 0.4997040070 0.0303396621 0.0055803801\n",
       " [121] 0.0424689032 0.0278690779 0.0094115863 0.0669573501 0.1127327928\n",
       " [126] 0.0028694620 0.0271483676 0.2186143508 0.5718095780 0.0161068957\n",
       " [131] 0.4640760154 0.1870533778 0.2419793686 0.2608058280 0.3833747811\n",
       " [136] 0.4331446022 0.0043667648 0.7229422715 0.0810959086 0.4570505154\n",
       " [141] 0.2475927192 0.1492365980 0.1902786670 0.7110208061 0.3111379165\n",
       " [146] 0.2929906473 0.5115682327 0.2296991736 0.1164776591 0.0367492647\n",
       " [151] 0.1430627268 0.3016572209 0.1475344310 0.0841891612 0.0064352607\n",
       " [156] 0.0556656083 0.0220379684 0.3786623674 0.1804999662 0.7844031509\n",
       " [161] 0.1088610262 0.0603342786 0.0107996714 0.1271469757 0.0275128874\n",
       " [166] 0.5269747966 0.0220476828 0.7005712341 0.0038229607 0.1340591621\n",
       " [171] 0.5266150424 0.2366277097 0.3924444746 0.0100239227 0.4441202828\n",
       " [176] 0.0061496716 0.2518810764 0.2764380519 0.4675118267 0.8677238427\n",
       " [181] 0.7560151264 0.6640030482 0.4862079061 0.0015915479 0.7973576172\n",
       " [186] 0.1237540129 0.0091515048 0.0676467822 0.1190224630 0.1742931049\n",
       " [191] 0.0118006943 0.5380478113 0.6136035382 0.1192891286 0.0080341884\n",
       " [196] 0.0138828888 0.0361448538 0.0429285456 0.0104389900 0.7294677040\n",
       " [201] 0.4599455723 0.0552130150 0.0367526429 0.5591028775 0.2845438468\n",
       " [206] 0.5781744000 0.0073379461 0.7730343412 0.0123431908 0.7298267075\n",
       " [211] 0.0639325592 0.0065966225 0.0207966121 0.6980484623 0.0832348720\n",
       " [216] 0.4257439927 0.0500112453 0.0315595360 0.0128139565 0.0615207431\n",
       " [221] 0.0576847675 0.0078620601 0.2362941971 0.5948780937 0.1457275235\n",
       " [226] 0.0107497252 0.1171934329 0.7424981615 0.3260454619 0.0154746336\n",
       " [231] 0.6914704792 0.0354722181 0.3243238016 0.0379532610 0.0162007701\n",
       " [236] 0.0598537057 0.1299309021 0.0550163780 0.4253244028 0.6039290068\n",
       " [241] 0.0417853174 0.0130308787 0.0213242023 0.2521324842 0.0320103021\n",
       " [246] 0.0461424203 0.2259443026 0.8145619376 0.3915567313 0.0017153233\n",
       " [251] 0.0206665028 0.6821303732 0.7720620024 0.3331553184 0.0165101749\n",
       " [256] 0.5165910441 0.8411159597 0.1958986524 0.0156843758 0.0365511431\n",
       " [261] 0.4148904289 0.2131197683 0.0426793420 0.2525574013 0.3703122739\n",
       " [266] 0.3053004019 0.0870845766 0.0533006540 0.0517563694 0.1835463719\n",
       " [271] 0.2511037873 0.0477140620 0.1908946473 0.2201789714 0.0152378695\n",
       " [276] 0.0790836144 0.3444264449 0.0292482234 0.2692507241 0.1913626716\n",
       " [281] 0.2053837416 0.0896329721 0.1260617542 0.1366480524 0.5458359919\n",
       " [286] 0.1820704448 0.0276649996 0.5252211812 0.6176255704 0.2713829094\n",
       " [291] 0.3640242749 0.0390194584 0.5363137667 0.3167529416 0.2226943063\n",
       " [296] 0.9999999557 0.8527760265 0.1730813735 0.0106313526 0.0129587961\n",
       " [301] 0.6050790399 0.0209566285 0.0117290603 0.0020842140 0.0474264753\n",
       " [306] 0.0452644870 0.1760470495 0.2889921550 0.0484168059 0.0093503033\n",
       " [311] 0.0735783355 0.3663524691 0.4118574939 0.0428127447 0.0466857274\n",
       " [316] 0.6703549447 0.1353375923 0.1064302119 0.4307689475 0.3774990499\n",
       " [321] 0.0569596311 0.0782487023 0.0403133305 0.1465044165 0.1418511067\n",
       " [326] 0.4072572775 0.4073084574 0.3759459145 0.0591865119 0.3358697073\n",
       " [331] 0.3737188912 0.2997047034 0.4365074716 0.5252396213 0.7488106896\n",
       " [336] 0.0444664072 0.4209719112 0.4790086237 0.0225867450 0.0336935619\n",
       " [341] 0.0526724333 0.0921678687 0.1454848809 0.3538832792 0.3757235409\n",
       " [346] 0.0484635476 0.0669329697 0.2756649935 0.0187046100 0.0373261338\n",
       " [351] 0.0361548542 0.0180698409 0.1443073936 0.9999997658 0.0874662984\n",
       " [356] 0.4946531491 0.7665953701 0.0493719339 0.0587688149 0.1133509023\n",
       " [361] 0.0641453110 0.0306221131 0.0364971644 0.0746120938 0.1530708396\n",
       " [366] 0.0923529700 0.6461013903 0.0161892993 0.0033641289 0.0304969068\n",
       " [371] 0.8564364924 0.1884959167 0.0053416979 0.1059190536 0.0775022656\n",
       " [376] 0.8831068826 0.0843289040 0.0403205586 0.0212326213 0.2917008113\n",
       " [381] 0.1864209280 0.3722082227 0.0841111297 0.4664914517 0.1236053641\n",
       " [386] 0.1208293601 0.0106835858 0.1393603577 0.1307571149 0.2128640985\n",
       " [391] 0.0033748900 0.0796780447 0.5256961866 0.2337940956 0.0360437164\n",
       " [396] 0.3132612163 0.0153695605 0.0198241590 0.0559440742 0.0026327293\n",
       " [401] 0.0172846067 0.6375191802 0.0660873085 0.7661965415 0.0313326586\n",
       " [406] 0.4078441997 0.0731464017 0.3406679172 0.8507185063 0.0526048912\n",
       " [411] 0.1252065367 0.0140183478 0.3800674307 0.0080628140 0.3124168326\n",
       " [416] 0.5168061100 0.1566174866 0.5005360526 0.2030536430 0.6068483669\n",
       " [421] 0.1476351015 0.5925107650 0.0026875456 0.0191301065 0.1068009001\n",
       " [426] 0.3856398916 0.0988073904 0.2428812254 0.1026407979 0.3426440942\n",
       " [431] 0.5796342405 0.0880472218 0.3917312867 0.2326410989 0.8482985687\n",
       " [436] 0.2360620589 0.0232003914 0.1181584666 0.5308919704 0.2456012630\n",
       " [441] 0.1276510606 0.0566126192 0.1781203808 0.9443615274 0.0628925298\n",
       " [446] 0.6070992967 0.0064703635 0.0145067281 0.5276250597 0.0080354298\n",
       " [451] 0.5533984731 0.1250216552 0.0122782554 0.7210770807 0.2037626628\n",
       " [456] 0.0031929747 0.2485703520 0.2986273061 0.9999996685 0.7369812594\n",
       " [461] 0.0441224070 0.0221582979 0.1899822475 0.8942824447 0.0437765768\n",
       " [466] 0.3678576767 0.1724663643 0.0243349214 0.0175794820 0.4526093460\n",
       " [471] 0.1376134846 0.6566182167 0.0781637715 0.0246332313 0.5716943306\n",
       " [476] 0.0160028815 0.1997582980 0.6014049301 0.2172793618 0.0028613326\n",
       " [481] 0.0052788471 0.0825537081 0.8990535765 0.0498238978 0.0099992176\n",
       " [486] 0.0286649510 0.6759034347 0.1002438248 0.4046472659 0.0132329011\n",
       " [491] 0.6222102903 0.1879743946 0.7591493379 0.4646080149 0.0744935783\n",
       " [496] 0.0168536242 0.9406765458 0.0102856267 0.8016580416 0.0477185915\n",
       " [501] 0.0732503425 0.0302669931 0.0280085447 0.0138713062 0.0647383577\n",
       " [506] 0.0081105537 0.2954386878 0.3549042157 0.0951801706 0.0204777670\n",
       " [511] 0.6770367795 0.2098553246 0.0483360112 0.0334912135 0.0208440533\n",
       " [516] 0.1624086181 0.0342050574 0.0302878476 0.7100987249 0.3811948366\n",
       " [521] 0.0094529772 0.0402290173 0.5869503708 0.0032161924 0.1720976004\n",
       " [526] 0.3870630940 0.0057480992 0.6325877077 0.0739718968 0.0007527239\n",
       " [531] 0.1926555556 0.0069435538 0.4005505177 0.6252997837 0.5035894006\n",
       " [536] 0.0152475998 0.9665394261 0.5921027545 0.1812755645 0.1310937072\n",
       " [541] 0.0657437082 0.9838850772 0.0408730272 0.0569603826 0.0355543301\n",
       " [546] 0.0325323961 0.4982122318 0.6125729239 0.0079727848 0.4600813538\n",
       " [551] 0.0119379321 0.0346133714 0.0188672156 0.8108735562 0.4014006291\n",
       " [556] 0.2763905442 0.1538435979 0.0600380276 0.0178043595 0.2650742727\n",
       " [561] 0.0277494323 0.0455792688 0.0251425225 0.2696367468 0.2891204427\n",
       " [566] 0.0064975370 0.1478410562 0.0495796983 0.6540692876 0.2481669366\n",
       " [571] 0.0368513111 0.1911171920 0.3704883288 0.4715462771 0.3834193295\n",
       " [576] 0.2841795486 0.6048784573 0.2142593075 0.0590188302 0.0205252849\n",
       " [581] 0.0246309874 0.0127786609 0.3135230420 0.3118429399 0.1015977416\n",
       " [586] 0.6035138668 0.0047921454 0.1171549474 0.2198000496 0.0171034990\n",
       " [591] 0.5402977163 0.0689034897 0.0182057558 0.0022783879 0.0023905726\n",
       " [596] 0.3739185584 0.0123804856 0.0034216296 0.0454900500 0.6144059402\n",
       " [601] 0.0344217595 0.1450759088 0.5391782935 0.0254165815 0.0544147470\n",
       " [606] 0.3223850120 0.9238034747 0.0395919920 0.0650213373 0.0032410558\n",
       " [611] 0.0973600067 0.3454757233 0.6289547326 0.0315861562 0.0162456140\n",
       " [616] 0.8299585841 0.0157824164 0.1931326864 0.0817198657 0.9346103577\n",
       " [621] 0.1706577639 0.6205586045 0.7041581688 0.3799984349 0.3024832198\n",
       " [626] 0.1122263014 0.0930631665 0.0045525301 0.1382052290 0.0922202816\n",
       " [631] 0.0062680731 0.1450797923 0.6424717450 0.5251467308 0.2724995562\n",
       " [636] 0.0112859975 0.2159742458 0.1192230205 0.0108070504 0.0317101005\n",
       " [641] 0.2504488963 0.0593073053 0.0055799284 0.0550400838 0.0299165092\n",
       " [646] 0.0245991091 0.0908313576 0.1024430880 0.5290376595 0.2123543900\n",
       " [651] 0.0735288175 0.0277750723 0.4309015723 0.3783890264 0.2528169284\n",
       " [656] 0.2183719665 0.5115949205 0.4267622672 0.3631391361 0.2296527229\n",
       " [661] 0.9999829094 0.0057347684 0.0221284190 0.4737201259 0.2187755202\n",
       " [666] 0.1539180791 0.5700782051 0.0067349797 0.0108676534 0.5848013856\n",
       " [671] 0.5579585256 0.1276623453 0.0075944490 0.7347566653 0.2864176724\n",
       " [676] 0.4442547341 0.0124171303 0.3947025024 0.1866387480 0.1698080084\n",
       " [681] 0.0416221124 0.8006924646 0.0739007862 0.1820173919 0.2036787341\n",
       " [686] 0.5939070268 0.3055695870 0.2636433822 0.0134936620 0.2088209946\n",
       " [691] 0.0549664597 0.0286693218 0.5419208545 0.3908285046 0.0336965981\n",
       " [696] 0.3641724020 0.0510751540 0.0944886093 0.0313731203 0.0995308147\n",
       " [701] 0.0554973632 0.1520831722 0.7229915663 0.1897117844 0.5479648743\n",
       " [706] 0.6883182683 0.4625242391 0.0258162878 0.2675993467 0.0912615392\n",
       " [711] 0.4212691129 0.9519099136 0.0106417125 0.0147647082 0.3990016172\n",
       " [716] 0.3652506032 0.3591539969 0.5229322657 0.9317741865 0.0450096039\n",
       " [721] 0.1188411967 0.2252550722 0.5093767658 0.2176792950 0.0133278655\n",
       " [726] 0.0165579513 0.6038961058 0.5696610289 0.3684084919 0.0154710552\n",
       " [731] 0.2758974075 0.5021883445 0.0787388213 0.0967300618 0.0049488358\n",
       " [736] 0.0641508423 0.0306752136 0.4023093897 0.0038345818 0.0221624569\n",
       " [741] 0.0083567833 0.0426203782 0.5498515941 0.0474251305 0.6753993873\n",
       " [746] 0.0204900547 0.5490726358 0.5163664456 0.0193906861 0.1529198522\n",
       " [751] 0.0115180747 0.0131243439 0.0669864821 0.0770191341 0.0337849860\n",
       " [756] 0.5844998629 0.1741331681 0.0168523973 0.0364927616 0.0329642927\n",
       " [761] 0.1352257371 0.4414351321 0.0631999099 0.4435910549 0.8206254056\n",
       " [766] 0.1476061441 0.0092103197 0.0407275280 0.4965199712 0.3526834577\n",
       " [771] 0.0337052154 0.0410217251 0.6571591248 0.6747886626 0.5896241816\n",
       " [776] 0.0378380544 0.7704666838 0.3120300055 0.0203791415 0.2526072759\n",
       " [781] 0.0370764574 0.0102138804 0.3796363521 0.0040353728 0.1082924367\n",
       " [786] 0.2498216088 0.3423322031 0.5840986596 0.0439314808 0.3875967860\n",
       " [791] 0.0138209702 0.2601086343 0.6692614704 0.0102872463 0.1235771936\n",
       " [796] 0.0234988787 0.0031554670 0.0123239365 0.0414667875 0.3078492089\n",
       " [801] 0.0981298036 0.0118130657 0.2669283818 0.0494469030 0.1807715572\n",
       " [806] 0.0058233915 0.5808337533 0.0246091629 0.0564855465 0.6442094147\n",
       " [811] 0.2891005500 0.0938396095 0.0038554930 0.0378153241 0.3635377421\n",
       " [816] 0.4470531233 0.1857678480 0.3876790876 0.6910892081 0.0383015816\n",
       " [821] 0.0942890631 0.0530877724 0.9791617676 0.4303681041 0.0541403278\n",
       " [826] 0.0270043072 0.5120785517 0.0048067960 0.2007030027 0.6835216562\n",
       " [831] 0.0471513791 0.0014249177 0.5808016221 0.4320301426 0.9520268958\n",
       " [836] 0.2834302776 0.2102575017 0.1977767305 0.0314478918 0.6168940503\n",
       " [841] 0.2238207060 0.8169321753 0.0237137255 0.5255383302 0.8993654391\n",
       " [846] 0.6485622183 0.0413256661 0.0530809885 0.0371021461 0.6660368913\n",
       " [851] 0.0503403084 0.2167360855 0.5298169140 0.0170300823 0.8456623700\n",
       " [856] 0.2866085176 0.0453429414 0.0168998093 0.0309378667 0.4609581046\n",
       " [861] 0.0016817461 0.0575316656 0.0295832577 0.3021158204 0.3591004670\n",
       " [866] 0.0142723786 0.1529921393 0.2906293046 0.3447785788 0.0639895413\n",
       " [871] 0.0723829749 0.0095640882 0.0129948419 0.0896109652 0.2184146323\n",
       " [876] 0.3955186722 0.1647199179 0.3373782246 0.4092493677 0.7509586448\n",
       " [881] 0.9930242419 0.5525090932 0.3003062717 0.1448048528 0.0986315585\n",
       " [886] 0.1707815582 0.0388472690 0.9425798819 0.1008434399 0.3856252335\n",
       " [891] 0.5116115664 0.6201347732 0.2140330426 0.1583148135 0.1297950506\n",
       " [896] 0.1093867202 0.0981704340 0.1759089788 0.1090226943 0.2285737545\n",
       " [901] 0.0902800566 0.5649630227 0.0065591833 0.2533543669 0.6030174681\n",
       " [906] 0.0347655954 0.1058000012 0.0083025719 0.2195757932 0.6623349781\n",
       " [911] 0.0192483334 0.7385660130 0.7771729765 0.4325905697 0.0254074776\n",
       " [916] 0.0317159201 0.5995063329 0.5865119673 0.6214861165 0.0115517258\n",
       " [921] 0.0605978514 0.6396837918 0.4013221891 0.9999999296 0.0168575565\n",
       " [926] 0.0037017784 0.4582235293 0.1736686315 0.1540463612 0.5363227191\n",
       " [931] 0.9157127652 0.1654887615 0.0650764613 0.8195508393 0.1015983007\n",
       " [936] 0.0656982499 0.3214499296 0.0446523008 0.0308784400 0.2865298950\n",
       " [941] 0.3134682524 0.3203963662 0.3150962113 0.0094626318 0.2345008545\n",
       " [946] 0.0485966681 0.0306373039 0.9999999399 0.8763532063 0.7585315430\n",
       " [951] 0.2221137093 0.2017573085 0.1116214015 0.7400428509 0.1898921309\n",
       " [956] 0.3706494022 0.0136711727 0.0347085173 0.4236815440 0.0174294617\n",
       " [961] 0.0110954028 0.1163772411 0.0038267042 0.0217295531 0.0461711131\n",
       " [966] 0.1929048376 0.2540777239 0.0211117848 0.4428692550 0.5789072244\n",
       " [971] 0.1247591425 0.4598588524 0.5319200558 0.4131252213 0.2344204468\n",
       " [976] 0.0219491576 0.8646884311 0.2036744009 0.1902501639 0.5708982361\n",
       " [981] 0.7525858992 0.5255091886 0.2822423311 0.5729637943 0.7450213961\n",
       " [986] 0.2413429831 0.1273416825 0.2956929088 0.2169274151 0.0035498896\n",
       " [991] 0.0090665728 0.0134610500 0.0326105874 0.2127210502 0.0336941251\n",
       " [996] 0.8452874294 0.0772480312 0.0415381055 0.0029428768 0.3850003242\n",
       "[1001] 0.0189692585 0.1056368431 0.4100631610 0.2066354602 0.1444537133\n",
       "[1006] 0.9999999307 0.8023624418 0.0570869713 0.0062163177 0.0227577337\n",
       "[1011] 0.1350262987 0.0706772613 0.0149102392 0.5303405402 0.0700719598\n",
       "[1016] 0.6197973930 0.2492899512 0.2145220471 0.2414769056 0.5810253312\n",
       "[1021] 0.2295377292 0.0156763072 0.0083847706 0.0112938677 0.0092279757\n",
       "[1026] 0.1822332023 0.1866478176 0.6375298368 0.0651077506 0.0061560595\n",
       "[1031] 0.0498163663 0.7174654305 0.7408224227 0.3201257457 0.4305727043\n",
       "[1036] 0.0464950396 0.0260596363 0.5788872766 0.0175645578 0.1034097778\n",
       "[1041] 0.8542614458 0.4351476395 0.5864691460 0.0444955625 0.0446792253\n",
       "[1046] 0.0200192340 0.2020467661 0.0197862103 0.8661683709 0.2203096539\n",
       "[1051] 0.3671685009 0.0953578350 0.2386947146 0.0241320906 0.0328750066\n",
       "[1056] 0.3774860533 0.0039554913 0.0052728875 0.2350864772 0.0144739114\n",
       "[1061] 0.0063552269 0.1831200706 0.4826547392 0.2539826161 0.2696428148\n",
       "[1066] 0.1741298005 0.2851655387 0.0016644628 0.4285767561 0.8647507454\n",
       "[1071] 0.2621602503 0.7753613911 0.4375579986 0.1003962013 0.0625584858\n",
       "[1076] 0.0148640218 0.2890213883 0.0073455119 0.4756499706 0.7883569955\n",
       "[1081] 0.6103394661 0.0855946834 0.0168862626 0.3593348918 0.5316290401\n",
       "[1086] 0.0125700189 0.4012809619 0.2858004210 0.9999998994 0.0888033517\n",
       "[1091] 0.6792749590 0.6990923617 0.7863690104 0.0329311039 0.2910246703\n",
       "[1096] 0.1492661192 0.2815724753 0.3574940122 0.0182277357 0.9254132825\n",
       "[1101] 0.1392209162 0.0125064075 0.2131276287 0.5084025243 0.1264699825\n",
       "[1106] 0.2644973913 0.4366645192 0.1799110454 0.0675101179 0.4697587235\n",
       "[1111] 0.0010847692 0.0502871692 0.0633866987 0.1195017099 0.4666761589\n",
       "[1116] 0.3103008781 0.9999991474 0.0862621250 0.5896990026 0.0340958107\n",
       "[1121] 0.2085276139 0.1454206137 0.0629816046 0.8404239796 0.5544033334\n",
       "[1126] 0.2465145415 0.0041174717 0.0107919344 0.0184330467 0.0308327477\n",
       "[1131] 0.9999999626 0.3648087529 0.0386046069 0.0668992514 0.0647919049\n",
       "[1136] 0.1442465402 0.1431386103 0.0879836958 0.3322597344 0.0319947835\n",
       "[1141] 0.3443439159 0.0106229288 0.5648255186 0.0209093634 0.0042353715\n",
       "[1146] 0.7195436405 0.5169531703 0.1106242862 0.0417178618 0.0298749809\n",
       "[1151] 0.0875270512 0.3126298366 0.0135589788 0.3227832297 0.0725996861\n",
       "[1156] 0.0069312598 0.0524067669 0.3514447655 0.1087566017 0.4835370543\n",
       "[1161] 0.8049478711 0.0660140389 0.6481943689 0.2382332316 0.4787794919\n",
       "[1166] 0.5961846811 0.0774260173 0.3920089988 0.1589858316 0.0689556610\n",
       "[1171] 0.1319367314 0.3795002342 0.5052333250 0.0805139074 0.1381950336\n",
       "[1176] 0.0006134923 0.2240494734 0.2703981963 0.0588262376 0.2908079520\n",
       "[1181] 0.0311343738 0.3849817688 0.0328636207 0.4468848475 0.0796920236\n",
       "[1186] 0.3061538000 0.1056904200 0.0284524744 0.0151126405 0.1121064674\n",
       "[1191] 0.1754004921 0.4318004541 0.2296963822 0.0461283802 0.0426430728\n",
       "[1196] 0.0102192255 0.3874261839 0.1031291898 0.1643538140 0.1601666330\n",
       "[1201] 0.3439608319 0.1282697893 0.4480674192 0.1021430339 0.0446640245\n",
       "[1206] 0.0063258828 0.3349179978 0.1170364482 0.0684230285 0.1185127563\n",
       "[1211] 0.0343752982 0.4324976809 0.1394791917 0.0140017766 0.5814093862\n",
       "[1216] 0.7072623748 0.6760399872 0.1110351272 0.0767197590 0.1110918876\n",
       "[1221] 0.2952613341 0.0027437663 0.0017140976 0.2427944449 0.0365185631\n",
       "[1226] 0.0381762833 0.4202669078 0.4144607622 0.2551417378 0.0087406160\n",
       "[1231] 0.1492675059 0.7167729604 0.4283964847 0.0897521401 0.1344224538\n",
       "[1236] 0.1594003830 0.2481563320 0.9999998998 0.6210032275 0.4117949700\n",
       "[1241] 0.1093573504 0.0071920931 0.0039271346 0.6436647842 0.3485013519\n",
       "[1246] 0.0711569848 0.3518631754 0.0288360562 0.1210086901 0.3416936235\n",
       "[1251] 0.4507868843 0.8393783565 0.1156753040 0.3956588740 0.0780376515\n",
       "[1256] 0.9372226704 0.0185688507 0.0365611012 0.4594646862 0.0123405080\n",
       "[1261] 0.1153002062 0.3330596777 0.0194341951 0.0116899937 0.5138254026\n",
       "[1266] 0.0225353090 0.0822178093 0.4806617888 0.0966890152 0.3763415981\n",
       "[1271] 0.0188280634 0.0644800531 0.4964763820 0.0444861260 0.2422961312\n",
       "[1276] 0.5462276439 0.3735657242 0.2831263867 0.0197569767 0.0541379122\n",
       "[1281] 0.0218449493 0.3996199969 0.0128226677 0.0397736359 0.0674757812\n",
       "[1286] 0.0244701812 0.3324348536 0.0669214706 0.8109792915 0.1097717556\n",
       "[1291] 0.3606995437 0.9404482534 0.3184568677 0.6291849938 0.0813133729\n",
       "[1296] 0.6200411872 0.7890306387 0.3449983520 0.0550173806 0.1539167599\n",
       "[1301] 0.3307554557 0.0194895318 0.2057498830 0.5531543587 0.2136517070\n",
       "[1306] 0.5917712986 0.0061191944 0.0822670564 0.7255297096 0.0023654922\n",
       "[1311] 0.0443624987 0.0734532134 0.1117380433 0.0189999347 0.5816600133\n",
       "[1316] 0.3523226721 0.0041092643 0.0873441476 0.4088033357 0.7841463966\n",
       "[1321] 0.6356406296 0.1158913864 0.2007179268 0.2996874275 0.0067240286\n",
       "[1326] 0.8015700474 0.0426671778 0.0127759868 0.2426759512 0.1267727235\n",
       "[1331] 0.0439637533 0.2573378446 0.5433377991 0.2081066419 0.1327211466\n",
       "[1336] 0.5316062664 0.2929988791 0.0096601886 0.0193694640 0.1551070352\n",
       "[1341] 0.0161150653 0.2825920328 0.0069446355 0.2016475927 0.4224667417\n",
       "[1346] 0.2429535814 0.0035108983 0.4006275883 0.0389228868 0.0521593752\n",
       "[1351] 0.3314357042 0.0455245249 0.0309064617 0.0878612310 0.0133130675\n",
       "[1356] 0.6883267632 0.6592285779 0.2273594365 0.4816001271 0.2613944974\n",
       "[1361] 0.0130665052 0.5205183450 0.6477442676 0.2507227010 0.3737377298\n",
       "[1366] 0.3256118362 0.0031901550 0.3394644340 0.0966684552 0.0617476613\n",
       "[1371] 0.1915292623 0.1110360126 0.6159052420 0.0215583806 0.1400539974\n",
       "[1376] 0.0477986234 0.0826264263 0.0070276908 0.8261927555 0.2578348910\n",
       "[1381] 0.2867942157 0.1060144808 0.3406297953 0.4803508558 0.1552499292\n",
       "[1386] 0.2852119528 0.3240501261 0.0703373040 0.3658022504 0.3601783855\n",
       "[1391] 0.0836576395 0.0120954506 0.0157389085 0.0510121516 0.1856636247\n",
       "[1396] 0.2109302946 0.0339366835 0.1291815198 0.0551722889 0.0583173532\n",
       "[1401] 0.0565857010 0.1955006770 0.3872982106 0.0174095793 0.5446981652\n",
       "[1406] 0.9999998813 0.1027611960 0.4766782301 0.0662688999 0.1169227346\n",
       "[1411] 0.0850278395 0.3926370262 0.0100271761 0.8825150892 0.1491664873\n",
       "[1416] 0.0447689463 0.7802938037 0.5777076883 0.0186315862 0.0082613817\n",
       "[1421] 0.2085372937 0.2705696644 0.5186309399 0.1743364264 0.3152190633\n",
       "[1426] 0.4223351864 0.4177476882 0.2094915920 0.0063264620 0.7496523834\n",
       "[1431] 0.0135803141 0.2845659364 0.0203262184 0.8372228883 0.0557357281\n",
       "[1436] 0.2056528540 0.3390387772 0.0100044063 0.3578387819 0.3926945983\n",
       "[1441] 0.1236658803 0.0248354523 0.7983049523 0.0710472789 0.2174914798\n",
       "[1446] 0.6183269941 0.0400660582 0.1769436384 0.4258788581 0.0485012868\n",
       "[1451] 0.0116220006 0.1634952501 0.2887683205 0.1500849739 0.7618378992\n",
       "[1456] 0.0905138214 0.1177273983 0.0129693312 0.2707875420 0.3034450408\n",
       "[1461] 0.0045746294 0.6735611665 0.4201200322 0.7187709451 0.0157843933\n",
       "[1466] 0.7735861852 0.0444909631 0.1011542376 0.0036255163 0.8642774481\n",
       "[1471] 0.4012903671 0.0044193787 0.1051684555 0.0007043555 0.0782380968\n",
       "[1476] 0.0680032170 0.3252153148 0.5221373181 0.0695479591 0.9100884740\n",
       "[1481] 0.0225768752 0.0830117248 0.7824242621 0.0028414054 0.1818020235\n",
       "[1486] 0.4467508062 0.1753075123 0.2119947188 0.3620560165 0.6984946285\n",
       "[1491] 0.7964094027 0.6970166539 0.6049884183 0.2221514109 0.2324523212\n",
       "[1496] 0.0312143807 0.0059786332 0.0474385360 0.0692646562 0.8132938601\n",
       "[1501] 0.3780047079 0.0826239870 0.0535687457 0.3817923041 0.0224097542\n",
       "[1506] 0.0571640112 0.5002783936 0.7377916776 0.4013492472 0.4360295111\n",
       "[1511] 0.0327230827 0.0092760345 0.1225099796 0.0278683778 0.8525823740\n",
       "[1516] 0.0157124468 0.0940175246 0.5736734950 0.5041167751 0.0102770308\n",
       "[1521] 0.0011854928 0.1313754281 0.4383209107 0.7560977548 0.3780411130\n",
       "[1526] 0.0357551574 0.1346541617 0.2595508623 0.7780642478 0.0201462975\n",
       "[1531] 0.2400600947 0.6781989137 0.3005707356 0.0469963535 0.5024863653\n",
       "[1536] 0.0143151454 0.0651614189 0.8360734411 0.2593200059 0.0108894451\n",
       "[1541] 0.3073025435 0.0983187936 0.0380510347 0.6072273035 0.0078663752\n",
       "[1546] 0.3454222925 0.0183754643 0.2061544742 0.0099216288 0.0414194474\n",
       "[1551] 0.0829841264 0.0306311879 0.0285561501 0.0605196262 0.1183252002\n",
       "[1556] 0.0109672155 0.0127333624 0.0022488867 0.0429628900 0.0251811856\n",
       "[1561] 0.0295559817 0.0038816335 0.2611946952 0.8826344203 0.1710429652\n",
       "[1566] 0.0149085330 0.0015865756 0.5568445632 0.3876559182 0.0661323040\n",
       "[1571] 0.6088000034 0.5747219736 0.0521771218 0.9021186975 0.0820669258\n",
       "[1576] 0.0075420902 0.7234372015 0.1018280097 0.4688766926 0.2468317337\n",
       "[1581] 0.5122403164 0.0870048294 0.0149338141 0.4510309177 0.0231078516\n",
       "[1586] 0.0443657247 0.2905374825 0.5613251157 0.3422269393 0.1286444365\n",
       "[1591] 0.6894378072 0.3336447584 0.0820764525 0.0421520857 0.2247223841\n",
       "[1596] 0.5296483834 0.0207762852 0.0044093153 0.4116977159 0.0802832297\n",
       "[1601] 0.3463396479 0.0130900437 0.0127446413 0.4296930382 0.2117335987\n",
       "[1606] 0.7138999510 0.0204587634 0.9691059001 0.0120568544 0.9007069764\n",
       "[1611] 0.0032282164 0.0133650536 0.1980503322 0.7025829043 0.6606789738\n",
       "[1616] 0.0089356781 0.0246127602 0.0372872768 0.1371233640 0.5200239812\n",
       "[1621] 0.0066450795 0.9999999341 0.6991870658 0.6048730836 0.3333633663\n",
       "[1626] 0.6662108897 0.1227042888 0.6727143331 0.1863522193 0.0255251186\n",
       "[1631] 0.1032954343 0.2924045831 0.1070167168 0.0191613586 0.3233853444\n",
       "[1636] 0.3236065562 0.0026629087 0.0429157016 0.0048619370 0.3500035395\n",
       "[1641] 0.3725561744 0.0626083961 0.1915442909 0.0938000050 0.1530515982\n",
       "[1646] 0.4801879971 0.0052630600 0.2797266103 0.0398773138 0.0016257219\n",
       "[1651] 0.4428231361 0.0120391414 0.1056620110 0.0937180396 0.1641423733\n",
       "[1656] 0.3838396059 0.0045130555 0.5375942866 0.4663837985 0.0281582763\n",
       "[1661] 0.2965635151 0.0500069545 0.0179109291 0.1826540435 0.1736322917\n",
       "[1666] 0.3148744097 0.0028549542 0.1446846274 0.0117231450 0.4279944481\n",
       "[1671] 0.1636183068 0.0084634230 0.0162058441 0.2615604717 0.3359006782\n",
       "[1676] 0.9999997199 0.0223168253 0.1719048978 0.0132759069 0.5717795162\n",
       "[1681] 0.1433112628 0.0636269800 0.0986192995 0.1865326509 0.1072545003\n",
       "[1686] 0.0587551754 0.0113480679 0.3738011110 0.1460917971 0.0047294630\n",
       "[1691] 0.0148767187 0.4024927860 0.5999057746 0.1291560936 0.0327122735\n",
       "[1696] 0.0027095665 0.4144192343 0.0748696900 0.2100844748 0.0505849568\n",
       "[1701] 0.3787864119 0.0520068591 0.1917476109 0.0737964056 0.8577632781\n",
       "[1706] 0.2872268611 0.1565710822 0.6798723074 0.6074134764 0.0259541990\n",
       "[1711] 0.4387496811 0.2789564836 0.0302853846 0.0359113470 0.1801624959\n",
       "[1716] 0.0416986275 0.3768906846 0.2319631667 0.4944344501 0.0209143588\n",
       "[1721] 0.1072067166 0.7083221554 0.9137316093 0.1041577031 0.0556707696\n",
       "[1726] 0.0175382849 0.4602507563 0.0089296168 0.0426768273 0.7384944452\n",
       "[1731] 0.0139891947 0.0173010272 0.3664354851 0.7443120905 0.0881357596\n",
       "[1736] 0.0094205422 0.0681286330 0.0388245693 0.3921921356 0.3883904941\n",
       "[1741] 0.6512046492 0.1806377143 0.1027054151 0.0207072643 0.0256075381\n",
       "[1746] 0.0025086071 0.6819400482 0.3502620206 0.0383984309 0.6587341820\n",
       "[1751] 0.0669926730 0.5330223503 0.2126932640 0.7388035766 0.0389983717\n",
       "[1756] 0.7475256601 0.7859580051 0.1525044212 0.0189503594 0.2353683747\n",
       "[1761] 0.0416053578 0.1669505098 0.7445205276 0.0529810394 0.0032744436\n",
       "[1766] 0.1493999029 0.4024246448 0.0276814867 0.0099171576 0.4251773610\n",
       "[1771] 0.2505231579 0.1094790327 0.0082182628 0.2271104110 0.1480698883\n",
       "[1776] 0.3593473594 0.2109796440 0.0463032214 0.4687538346 0.2158889463\n",
       "[1781] 0.7131799853 0.0133076760 0.1232593715 0.0350621484 0.0218596503\n",
       "[1786] 0.0070990950 0.2463007026 0.0722452739 0.7687670344 0.1030457510\n",
       "[1791] 0.0427102616 0.0220479418 0.9200165901 0.0211256570 0.3999625346\n",
       "[1796] 0.0548779364 0.0252650413 0.2572087301 0.6809396291 0.7292556117\n",
       "[1801] 0.5969284593 0.9999993692 0.0263022408 0.3848478628 0.5493430439\n",
       "[1806] 0.1735766095 0.0805406381 0.7370587622 0.0030603249 0.0717116554\n",
       "[1811] 0.3093953034 0.0220249070 0.3538916154 0.0516877939 0.1974223255\n",
       "[1816] 0.6255772344 0.0555821965 0.5891365275 0.0555530612 0.4756503267\n",
       "[1821] 0.1159933773 0.3581029330 0.8432488308 0.0322303964 0.0026141780\n",
       "[1826] 0.0192613020 0.7837023463 0.9999999219 0.0079345661 0.0038743842\n",
       "[1831] 0.0211480020 0.0504505585 0.0445260716 0.0293364701 0.0855610339\n",
       "[1836] 0.6419654994 0.0489075172 0.0013621038 0.5378914378 0.8863474906\n",
       "[1841] 0.1612532639 0.0309850611 0.6935307389 0.0056197025 0.3026540466\n",
       "[1846] 0.0078564776 0.1916130576 0.0116389348 0.2438603215 0.4447260912\n",
       "[1851] 0.0572090029 0.2614460473 0.0171908586 0.5758549040 0.1928775744\n",
       "[1856] 0.0042242465 0.9222058525 0.0704851393 0.5670803042 0.4760614079\n",
       "[1861] 0.3063206741 0.0369710972 0.8196978058 0.0132486516 0.0631829731\n",
       "[1866] 0.3785538281 0.0413801185 0.2754340787 0.5458955044 0.2599349546\n",
       "[1871] 0.0165679911 0.2624673415 0.0085108891 0.1322386070 0.0026487493\n",
       "[1876] 0.5142562437 0.5046626948 0.4691813766 0.0206286273 0.0130306566\n",
       "[1881] 0.0216268256 0.9628739760 0.0228586561 0.4158860384 0.1116440783\n",
       "[1886] 0.0357322247 0.1833236524 0.0096291174 0.4726616422 0.0630944393\n",
       "[1891] 0.7395561306 0.5405617895 0.4766999046 0.0574012928 0.0248904133\n",
       "[1896] 0.3206852167 0.0259059738 0.3757066904 0.0318621776 0.8351477250\n",
       "[1901] 0.5799562825 0.7714591766 0.0929458415 0.1405117268 0.0944645275\n",
       "[1906] 0.1651311698 0.7799651722 0.0135918638 0.0031089448 0.9999999211\n",
       "[1911] 0.2417331538 0.6588457730 0.6818222401 0.0271196274 0.3555377987\n",
       "[1916] 0.0610618672 0.3567070399 0.3504764070 0.3431137749 0.0295106261\n",
       "[1921] 0.7389533744 0.2843782098 0.0686110159 0.0048240177 0.6307695308\n",
       "[1926] 0.3151583576 0.3568277027 0.2705272397 0.2516248814 0.2576088076\n",
       "[1931] 0.1801074165 0.5739060110 0.3332879839 0.0994519736 0.2543731111\n",
       "[1936] 0.1279477665 0.0287503879 0.6222800383 0.0092315404 0.0089307315\n",
       "[1941] 0.4107363960 0.5322407796 0.0147120400 0.0850992156 0.0666759971\n",
       "[1946] 0.0532227534 0.9447631042 0.0421231962 0.3308716036 0.0050189057\n",
       "[1951] 0.0475675707 0.0083735762 0.8446233390 0.8466795941 0.6030030496\n",
       "[1956] 0.0576375334 0.3937342860 0.0076754142 0.1996367220 0.2922130241\n",
       "[1961] 0.7690698252 0.8566853232 0.5955303582 0.0297035055 0.1702303604\n",
       "[1966] 0.0590962159 0.1379619224 0.0744322586 0.6159409127 0.8388220521\n",
       "[1971] 0.1916658718 0.5281191755 0.1848030444 0.0301030901 0.3133279824\n",
       "[1976] 0.0917255710 0.0415511365 0.1879667881 0.0506776775 0.2653295214\n",
       "[1981] 0.0346522723 0.7268137859 0.1914638843 0.0639038731 0.0632602698\n",
       "[1986] 0.8030758492 0.4382680797 0.0985781228 0.0310326490 0.2484966117\n",
       "[1991] 0.0268931480 0.5844900360 0.7676445503 0.6169422434 0.1818843917\n",
       "[1996] 0.0292352580 0.4491705688 0.0379572759 0.0214499859 0.7722098404\n",
       "[2001] 0.1037854712 0.4793252965 0.0128060642 0.0396358543 0.3469360398\n",
       "[2006] 0.2100001583 0.1818014127 0.0820413767 0.1822202914 0.0081125634\n",
       "[2011] 0.4740993493 0.2186201404 0.2215124353 0.3452531435 0.2820295715\n",
       "[2016] 0.1236309406 0.0031237662 0.6874840031 0.1676308423 0.2568738964\n",
       "[2021] 0.1382707207 0.3356980929 0.0555016612 0.5596138914 0.0815618216\n",
       "[2026] 0.3147532239 0.1351958757 0.5126282525 0.3103014199 0.1388723337\n",
       "[2031] 0.0032387923 0.0503488415 0.6538197334 0.0613327136 0.0093322776\n",
       "[2036] 0.0143616312 0.1243485991 0.0064714901 0.2520658473 0.3114557773\n",
       "[2041] 0.0170022746 0.0184837003 0.0125543720 0.1187830585 0.0090824556\n",
       "[2046] 0.0209936552 0.1136067087 0.1328708966 0.2624175803 0.0111836664\n",
       "[2051] 0.0379309247 0.4895588748 0.0357662130 0.0753036947 0.6489425254\n",
       "[2056] 0.2742165630 0.8525750222 0.0479075159 0.0382467772 0.0521301418\n",
       "[2061] 0.1604740649 0.1912318121 0.0397004059 0.1663976181 0.0118601763\n",
       "[2066] 0.9552250220 0.0190896079 0.2112976047 0.1513603344 0.0028692646\n",
       "[2071] 0.2105236627 0.1490850212 0.2640745401"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predtest <- predict(Lasso,submitdata,type = 'prob')  #testdata \n",
    "predtest$b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions <- predtest$b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Format OK\"\n",
      "$submission\n",
      "[0.02,0.0458,0.4953,0.0473,0.6105,0.1362,0.317,0.3308,0.0572,0.0083,0.1964,0.1802,0.0113,1,0.0774,0.0343,0.0117,0.1751,0.1516,0.0114,0.039,0.268,0.9296,0.0133,0.6323,0.0682,0.1679,0.7832,0.5391,0.0885,0.2185,0.7142,0.0193,0.4174,0.7143,0.099,0.6321,0.8631,0.8387,0.0123,0.2008,0.1888,0.0701,0.8379,0.2763,0.6577,0.2057,0.0239,0.1562,0.0727,0.0502,0.0272,0.1323,0.1767,0.9362,0.7145,0.6113,0.5229,0.0857,0.2626,0.8993,0.0122,0.0169,0.1166,0.0673,0.0196,0.002,0.0108,0.5102,0.0905,0.4354,0.0049,0.0125,0.5419,0.009,0.0936,0.5644,0.0175,0.3195,0.026,0.7827,0.03,0.0092,0.557,0.1728,0.1695,0.0105,0.0474,0.7218,0.2365,0.0401,0.0323,0.0551,0.8815,0.1275,0.1716,0.0199,0.0939,0.384,0.4901,0.8102,0.0129,0.6289,0.478,0.2112,0.4506,0.049,0.1533,0.2639,0.0362,0.0147,0.8179,0.0057,0.5653,0.2368,0.003,0.0022,0.4997,0.0303,0.0056,0.0425,0.0279,0.0094,0.067,0.1127,0.0029,0.0271,0.2186,0.5718,0.0161,0.4641,0.1871,0.242,0.2608,0.3834,0.4331,0.0044,0.7229,0.0811,0.4571,0.2476,0.1492,0.1903,0.711,0.3111,0.293,0.5116,0.2297,0.1165,0.0367,0.1431,0.3017,0.1475,0.0842,0.0064,0.0557,0.022,0.3787,0.1805,0.7844,0.1089,0.0603,0.0108,0.1271,0.0275,0.527,0.022,0.7006,0.0038,0.1341,0.5266,0.2366,0.3924,0.01,0.4441,0.0061,0.2519,0.2764,0.4675,0.8677,0.756,0.664,0.4862,0.0016,0.7974,0.1238,0.0092,0.0676,0.119,0.1743,0.0118,0.538,0.6136,0.1193,0.008,0.0139,0.0361,0.0429,0.0104,0.7295,0.4599,0.0552,0.0368,0.5591,0.2845,0.5782,0.0073,0.773,0.0123,0.7298,0.0639,0.0066,0.0208,0.698,0.0832,0.4257,0.05,0.0316,0.0128,0.0615,0.0577,0.0079,0.2363,0.5949,0.1457,0.0107,0.1172,0.7425,0.326,0.0155,0.6915,0.0355,0.3243,0.038,0.0162,0.0599,0.1299,0.055,0.4253,0.6039,0.0418,0.013,0.0213,0.2521,0.032,0.0461,0.2259,0.8146,0.3916,0.0017,0.0207,0.6821,0.7721,0.3332,0.0165,0.5166,0.8411,0.1959,0.0157,0.0366,0.4149,0.2131,0.0427,0.2526,0.3703,0.3053,0.0871,0.0533,0.0518,0.1835,0.2511,0.0477,0.1909,0.2202,0.0152,0.0791,0.3444,0.0292,0.2693,0.1914,0.2054,0.0896,0.1261,0.1366,0.5458,0.1821,0.0277,0.5252,0.6176,0.2714,0.364,0.039,0.5363,0.3168,0.2227,1,0.8528,0.1731,0.0106,0.013,0.6051,0.021,0.0117,0.0021,0.0474,0.0453,0.176,0.289,0.0484,0.0094,0.0736,0.3664,0.4119,0.0428,0.0467,0.6704,0.1353,0.1064,0.4308,0.3775,0.057,0.0782,0.0403,0.1465,0.1419,0.4073,0.4073,0.3759,0.0592,0.3359,0.3737,0.2997,0.4365,0.5252,0.7488,0.0445,0.421,0.479,0.0226,0.0337,0.0527,0.0922,0.1455,0.3539,0.3757,0.0485,0.0669,0.2757,0.0187,0.0373,0.0362,0.0181,0.1443,1,0.0875,0.4947,0.7666,0.0494,0.0588,0.1134,0.0641,0.0306,0.0365,0.0746,0.1531,0.0924,0.6461,0.0162,0.0034,0.0305,0.8564,0.1885,0.0053,0.1059,0.0775,0.8831,0.0843,0.0403,0.0212,0.2917,0.1864,0.3722,0.0841,0.4665,0.1236,0.1208,0.0107,0.1394,0.1308,0.2129,0.0034,0.0797,0.5257,0.2338,0.036,0.3133,0.0154,0.0198,0.0559,0.0026,0.0173,0.6375,0.0661,0.7662,0.0313,0.4078,0.0731,0.3407,0.8507,0.0526,0.1252,0.014,0.3801,0.0081,0.3124,0.5168,0.1566,0.5005,0.2031,0.6068,0.1476,0.5925,0.0027,0.0191,0.1068,0.3856,0.0988,0.2429,0.1026,0.3426,0.5796,0.088,0.3917,0.2326,0.8483,0.2361,0.0232,0.1182,0.5309,0.2456,0.1277,0.0566,0.1781,0.9444,0.0629,0.6071,0.0065,0.0145,0.5276,0.008,0.5534,0.125,0.0123,0.7211,0.2038,0.0032,0.2486,0.2986,1,0.737,0.0441,0.0222,0.19,0.8943,0.0438,0.3679,0.1725,0.0243,0.0176,0.4526,0.1376,0.6566,0.0782,0.0246,0.5717,0.016,0.1998,0.6014,0.2173,0.0029,0.0053,0.0826,0.8991,0.0498,0.01,0.0287,0.6759,0.1002,0.4046,0.0132,0.6222,0.188,0.7591,0.4646,0.0745,0.0169,0.9407,0.0103,0.8017,0.0477,0.0733,0.0303,0.028,0.0139,0.0647,0.0081,0.2954,0.3549,0.0952,0.0205,0.677,0.2099,0.0483,0.0335,0.0208,0.1624,0.0342,0.0303,0.7101,0.3812,0.0095,0.0402,0.587,0.0032,0.1721,0.3871,0.0057,0.6326,0.074,0.0008,0.1927,0.0069,0.4006,0.6253,0.5036,0.0152,0.9665,0.5921,0.1813,0.1311,0.0657,0.9839,0.0409,0.057,0.0356,0.0325,0.4982,0.6126,0.008,0.4601,0.0119,0.0346,0.0189,0.8109,0.4014,0.2764,0.1538,0.06,0.0178,0.2651,0.0277,0.0456,0.0251,0.2696,0.2891,0.0065,0.1478,0.0496,0.6541,0.2482,0.0369,0.1911,0.3705,0.4715,0.3834,0.2842,0.6049,0.2143,0.059,0.0205,0.0246,0.0128,0.3135,0.3118,0.1016,0.6035,0.0048,0.1172,0.2198,0.0171,0.5403,0.0689,0.0182,0.0023,0.0024,0.3739,0.0124,0.0034,0.0455,0.6144,0.0344,0.1451,0.5392,0.0254,0.0544,0.3224,0.9238,0.0396,0.065,0.0032,0.0974,0.3455,0.629,0.0316,0.0162,0.83,0.0158,0.1931,0.0817,0.9346,0.1707,0.6206,0.7042,0.38,0.3025,0.1122,0.0931,0.0046,0.1382,0.0922,0.0063,0.1451,0.6425,0.5251,0.2725,0.0113,0.216,0.1192,0.0108,0.0317,0.2504,0.0593,0.0056,0.055,0.0299,0.0246,0.0908,0.1024,0.529,0.2124,0.0735,0.0278,0.4309,0.3784,0.2528,0.2184,0.5116,0.4268,0.3631,0.2297,1,0.0057,0.0221,0.4737,0.2188,0.1539,0.5701,0.0067,0.0109,0.5848,0.558,0.1277,0.0076,0.7348,0.2864,0.4443,0.0124,0.3947,0.1866,0.1698,0.0416,0.8007,0.0739,0.182,0.2037,0.5939,0.3056,0.2636,0.0135,0.2088,0.055,0.0287,0.5419,0.3908,0.0337,0.3642,0.0511,0.0945,0.0314,0.0995,0.0555,0.1521,0.723,0.1897,0.548,0.6883,0.4625,0.0258,0.2676,0.0913,0.4213,0.9519,0.0106,0.0148,0.399,0.3653,0.3592,0.5229,0.9318,0.045,0.1188,0.2253,0.5094,0.2177,0.0133,0.0166,0.6039,0.5697,0.3684,0.0155,0.2759,0.5022,0.0787,0.0967,0.0049,0.0642,0.0307,0.4023,0.0038,0.0222,0.0084,0.0426,0.5499,0.0474,0.6754,0.0205,0.5491,0.5164,0.0194,0.1529,0.0115,0.0131,0.067,0.077,0.0338,0.5845,0.1741,0.0169,0.0365,0.033,0.1352,0.4414,0.0632,0.4436,0.8206,0.1476,0.0092,0.0407,0.4965,0.3527,0.0337,0.041,0.6572,0.6748,0.5896,0.0378,0.7705,0.312,0.0204,0.2526,0.0371,0.0102,0.3796,0.004,0.1083,0.2498,0.3423,0.5841,0.0439,0.3876,0.0138,0.2601,0.6693,0.0103,0.1236,0.0235,0.0032,0.0123,0.0415,0.3078,0.0981,0.0118,0.2669,0.0494,0.1808,0.0058,0.5808,0.0246,0.0565,0.6442,0.2891,0.0938,0.0039,0.0378,0.3635,0.4471,0.1858,0.3877,0.6911,0.0383,0.0943,0.0531,0.9792,0.4304,0.0541,0.027,0.5121,0.0048,0.2007,0.6835,0.0472,0.0014,0.5808,0.432,0.952,0.2834,0.2103,0.1978,0.0314,0.6169,0.2238,0.8169,0.0237,0.5255,0.8994,0.6486,0.0413,0.0531,0.0371,0.666,0.0503,0.2167,0.5298,0.017,0.8457,0.2866,0.0453,0.0169,0.0309,0.461,0.0017,0.0575,0.0296,0.3021,0.3591,0.0143,0.153,0.2906,0.3448,0.064,0.0724,0.0096,0.013,0.0896,0.2184,0.3955,0.1647,0.3374,0.4092,0.751,0.993,0.5525,0.3003,0.1448,0.0986,0.1708,0.0388,0.9426,0.1008,0.3856,0.5116,0.6201,0.214,0.1583,0.1298,0.1094,0.0982,0.1759,0.109,0.2286,0.0903,0.565,0.0066,0.2534,0.603,0.0348,0.1058,0.0083,0.2196,0.6623,0.0192,0.7386,0.7772,0.4326,0.0254,0.0317,0.5995,0.5865,0.6215,0.0116,0.0606,0.6397,0.4013,1,0.0169,0.0037,0.4582,0.1737,0.154,0.5363,0.9157,0.1655,0.0651,0.8196,0.1016,0.0657,0.3214,0.0447,0.0309,0.2865,0.3135,0.3204,0.3151,0.0095,0.2345,0.0486,0.0306,1,0.8764,0.7585,0.2221,0.2018,0.1116,0.74,0.1899,0.3706,0.0137,0.0347,0.4237,0.0174,0.0111,0.1164,0.0038,0.0217,0.0462,0.1929,0.2541,0.0211,0.4429,0.5789,0.1248,0.4599,0.5319,0.4131,0.2344,0.0219,0.8647,0.2037,0.1903,0.5709,0.7526,0.5255,0.2822,0.573,0.745,0.2413,0.1273,0.2957,0.2169,0.0035,0.0091,0.0135,0.0326,0.2127,0.0337,0.8453,0.0772,0.0415,0.0029,0.385,0.019,0.1056,0.4101,0.2066,0.1445,1,0.8024,0.0571,0.0062,0.0228,0.135,0.0707,0.0149,0.5303,0.0701,0.6198,0.2493,0.2145,0.2415,0.581,0.2295,0.0157,0.0084,0.0113,0.0092,0.1822,0.1866,0.6375,0.0651,0.0062,0.0498,0.7175,0.7408,0.3201,0.4306,0.0465,0.0261,0.5789,0.0176,0.1034,0.8543,0.4351,0.5865,0.0445,0.0447,0.02,0.202,0.0198,0.8662,0.2203,0.3672,0.0954,0.2387,0.0241,0.0329,0.3775,0.004,0.0053,0.2351,0.0145,0.0064,0.1831,0.4827,0.254,0.2696,0.1741,0.2852,0.0017,0.4286,0.8648,0.2622,0.7754,0.4376,0.1004,0.0626,0.0149,0.289,0.0073,0.4756,0.7884,0.6103,0.0856,0.0169,0.3593,0.5316,0.0126,0.4013,0.2858,1,0.0888,0.6793,0.6991,0.7864,0.0329,0.291,0.1493,0.2816,0.3575,0.0182,0.9254,0.1392,0.0125,0.2131,0.5084,0.1265,0.2645,0.4367,0.1799,0.0675,0.4698,0.0011,0.0503,0.0634,0.1195,0.4667,0.3103,1,0.0863,0.5897,0.0341,0.2085,0.1454,0.063,0.8404,0.5544,0.2465,0.0041,0.0108,0.0184,0.0308,1,0.3648,0.0386,0.0669,0.0648,0.1442,0.1431,0.088,0.3323,0.032,0.3443,0.0106,0.5648,0.0209,0.0042,0.7195,0.517,0.1106,0.0417,0.0299,0.0875,0.3126,0.0136,0.3228,0.0726,0.0069,0.0524,0.3514,0.1088,0.4835,0.8049,0.066,0.6482,0.2382,0.4788,0.5962,0.0774,0.392,0.159,0.069,0.1319,0.3795,0.5052,0.0805,0.1382,0.0006,0.224,0.2704,0.0588,0.2908,0.0311,0.385,0.0329,0.4469,0.0797,0.3062,0.1057,0.0285,0.0151,0.1121,0.1754,0.4318,0.2297,0.0461,0.0426,0.0102,0.3874,0.1031,0.1644,0.1602,0.344,0.1283,0.4481,0.1021,0.0447,0.0063,0.3349,0.117,0.0684,0.1185,0.0344,0.4325,0.1395,0.014,0.5814,0.7073,0.676,0.111,0.0767,0.1111,0.2953,0.0027,0.0017,0.2428,0.0365,0.0382,0.4203,0.4145,0.2551,0.0087,0.1493,0.7168,0.4284,0.0898,0.1344,0.1594,0.2482,1,0.621,0.4118,0.1094,0.0072,0.0039,0.6437,0.3485,0.0712,0.3519,0.0288,0.121,0.3417,0.4508,0.8394,0.1157,0.3957,0.078,0.9372,0.0186,0.0366,0.4595,0.0123,0.1153,0.3331,0.0194,0.0117,0.5138,0.0225,0.0822,0.4807,0.0967,0.3763,0.0188,0.0645,0.4965,0.0445,0.2423,0.5462,0.3736,0.2831,0.0198,0.0541,0.0218,0.3996,0.0128,0.0398,0.0675,0.0245,0.3324,0.0669,0.811,0.1098,0.3607,0.9404,0.3185,0.6292,0.0813,0.62,0.789,0.345,0.055,0.1539,0.3308,0.0195,0.2057,0.5532,0.2137,0.5918,0.0061,0.0823,0.7255,0.0024,0.0444,0.0735,0.1117,0.019,0.5817,0.3523,0.0041,0.0873,0.4088,0.7841,0.6356,0.1159,0.2007,0.2997,0.0067,0.8016,0.0427,0.0128,0.2427,0.1268,0.044,0.2573,0.5433,0.2081,0.1327,0.5316,0.293,0.0097,0.0194,0.1551,0.0161,0.2826,0.0069,0.2016,0.4225,0.243,0.0035,0.4006,0.0389,0.0522,0.3314,0.0455,0.0309,0.0879,0.0133,0.6883,0.6592,0.2274,0.4816,0.2614,0.0131,0.5205,0.6477,0.2507,0.3737,0.3256,0.0032,0.3395,0.0967,0.0617,0.1915,0.111,0.6159,0.0216,0.1401,0.0478,0.0826,0.007,0.8262,0.2578,0.2868,0.106,0.3406,0.4804,0.1552,0.2852,0.3241,0.0703,0.3658,0.3602,0.0837,0.0121,0.0157,0.051,0.1857,0.2109,0.0339,0.1292,0.0552,0.0583,0.0566,0.1955,0.3873,0.0174,0.5447,1,0.1028,0.4767,0.0663,0.1169,0.085,0.3926,0.01,0.8825,0.1492,0.0448,0.7803,0.5777,0.0186,0.0083,0.2085,0.2706,0.5186,0.1743,0.3152,0.4223,0.4177,0.2095,0.0063,0.7497,0.0136,0.2846,0.0203,0.8372,0.0557,0.2057,0.339,0.01,0.3578,0.3927,0.1237,0.0248,0.7983,0.071,0.2175,0.6183,0.0401,0.1769,0.4259,0.0485,0.0116,0.1635,0.2888,0.1501,0.7618,0.0905,0.1177,0.013,0.2708,0.3034,0.0046,0.6736,0.4201,0.7188,0.0158,0.7736,0.0445,0.1012,0.0036,0.8643,0.4013,0.0044,0.1052,0.0007,0.0782,0.068,0.3252,0.5221,0.0695,0.9101,0.0226,0.083,0.7824,0.0028,0.1818,0.4468,0.1753,0.212,0.3621,0.6985,0.7964,0.697,0.605,0.2222,0.2325,0.0312,0.006,0.0474,0.0693,0.8133,0.378,0.0826,0.0536,0.3818,0.0224,0.0572,0.5003,0.7378,0.4013,0.436,0.0327,0.0093,0.1225,0.0279,0.8526,0.0157,0.094,0.5737,0.5041,0.0103,0.0012,0.1314,0.4383,0.7561,0.378,0.0358,0.1347,0.2596,0.7781,0.0201,0.2401,0.6782,0.3006,0.047,0.5025,0.0143,0.0652,0.8361,0.2593,0.0109,0.3073,0.0983,0.0381,0.6072,0.0079,0.3454,0.0184,0.2062,0.0099,0.0414,0.083,0.0306,0.0286,0.0605,0.1183,0.011,0.0127,0.0022,0.043,0.0252,0.0296,0.0039,0.2612,0.8826,0.171,0.0149,0.0016,0.5568,0.3877,0.0661,0.6088,0.5747,0.0522,0.9021,0.0821,0.0075,0.7234,0.1018,0.4689,0.2468,0.5122,0.087,0.0149,0.451,0.0231,0.0444,0.2905,0.5613,0.3422,0.1286,0.6894,0.3336,0.0821,0.0422,0.2247,0.5296,0.0208,0.0044,0.4117,0.0803,0.3463,0.0131,0.0127,0.4297,0.2117,0.7139,0.0205,0.9691,0.0121,0.9007,0.0032,0.0134,0.1981,0.7026,0.6607,0.0089,0.0246,0.0373,0.1371,0.52,0.0066,1,0.6992,0.6049,0.3334,0.6662,0.1227,0.6727,0.1864,0.0255,0.1033,0.2924,0.107,0.0192,0.3234,0.3236,0.0027,0.0429,0.0049,0.35,0.3726,0.0626,0.1915,0.0938,0.1531,0.4802,0.0053,0.2797,0.0399,0.0016,0.4428,0.012,0.1057,0.0937,0.1641,0.3838,0.0045,0.5376,0.4664,0.0282,0.2966,0.05,0.0179,0.1827,0.1736,0.3149,0.0029,0.1447,0.0117,0.428,0.1636,0.0085,0.0162,0.2616,0.3359,1,0.0223,0.1719,0.0133,0.5718,0.1433,0.0636,0.0986,0.1865,0.1073,0.0588,0.0113,0.3738,0.1461,0.0047,0.0149,0.4025,0.5999,0.1292,0.0327,0.0027,0.4144,0.0749,0.2101,0.0506,0.3788,0.052,0.1917,0.0738,0.8578,0.2872,0.1566,0.6799,0.6074,0.026,0.4387,0.279,0.0303,0.0359,0.1802,0.0417,0.3769,0.232,0.4944,0.0209,0.1072,0.7083,0.9137,0.1042,0.0557,0.0175,0.4603,0.0089,0.0427,0.7385,0.014,0.0173,0.3664,0.7443,0.0881,0.0094,0.0681,0.0388,0.3922,0.3884,0.6512,0.1806,0.1027,0.0207,0.0256,0.0025,0.6819,0.3503,0.0384,0.6587,0.067,0.533,0.2127,0.7388,0.039,0.7475,0.786,0.1525,0.019,0.2354,0.0416,0.167,0.7445,0.053,0.0033,0.1494,0.4024,0.0277,0.0099,0.4252,0.2505,0.1095,0.0082,0.2271,0.1481,0.3593,0.211,0.0463,0.4688,0.2159,0.7132,0.0133,0.1233,0.0351,0.0219,0.0071,0.2463,0.0722,0.7688,0.103,0.0427,0.022,0.92,0.0211,0.4,0.0549,0.0253,0.2572,0.6809,0.7293,0.5969,1,0.0263,0.3848,0.5493,0.1736,0.0805,0.7371,0.0031,0.0717,0.3094,0.022,0.3539,0.0517,0.1974,0.6256,0.0556,0.5891,0.0556,0.4757,0.116,0.3581,0.8432,0.0322,0.0026,0.0193,0.7837,1,0.0079,0.0039,0.0211,0.0505,0.0445,0.0293,0.0856,0.642,0.0489,0.0014,0.5379,0.8863,0.1613,0.031,0.6935,0.0056,0.3027,0.0079,0.1916,0.0116,0.2439,0.4447,0.0572,0.2614,0.0172,0.5759,0.1929,0.0042,0.9222,0.0705,0.5671,0.4761,0.3063,0.037,0.8197,0.0132,0.0632,0.3786,0.0414,0.2754,0.5459,0.2599,0.0166,0.2625,0.0085,0.1322,0.0026,0.5143,0.5047,0.4692,0.0206,0.013,0.0216,0.9629,0.0229,0.4159,0.1116,0.0357,0.1833,0.0096,0.4727,0.0631,0.7396,0.5406,0.4767,0.0574,0.0249,0.3207,0.0259,0.3757,0.0319,0.8351,0.58,0.7715,0.0929,0.1405,0.0945,0.1651,0.78,0.0136,0.0031,1,0.2417,0.6588,0.6818,0.0271,0.3555,0.0611,0.3567,0.3505,0.3431,0.0295,0.739,0.2844,0.0686,0.0048,0.6308,0.3152,0.3568,0.2705,0.2516,0.2576,0.1801,0.5739,0.3333,0.0995,0.2544,0.1279,0.0288,0.6223,0.0092,0.0089,0.4107,0.5322,0.0147,0.0851,0.0667,0.0532,0.9448,0.0421,0.3309,0.005,0.0476,0.0084,0.8446,0.8467,0.603,0.0576,0.3937,0.0077,0.1996,0.2922,0.7691,0.8567,0.5955,0.0297,0.1702,0.0591,0.138,0.0744,0.6159,0.8388,0.1917,0.5281,0.1848,0.0301,0.3133,0.0917,0.0416,0.188,0.0507,0.2653,0.0347,0.7268,0.1915,0.0639,0.0633,0.8031,0.4383,0.0986,0.031,0.2485,0.0269,0.5845,0.7676,0.6169,0.1819,0.0292,0.4492,0.038,0.0214,0.7722,0.1038,0.4793,0.0128,0.0396,0.3469,0.21,0.1818,0.082,0.1822,0.0081,0.4741,0.2186,0.2215,0.3453,0.282,0.1236,0.0031,0.6875,0.1676,0.2569,0.1383,0.3357,0.0555,0.5596,0.0816,0.3148,0.1352,0.5126,0.3103,0.1389,0.0032,0.0503,0.6538,0.0613,0.0093,0.0144,0.1243,0.0065,0.2521,0.3115,0.017,0.0185,0.0126,0.1188,0.0091,0.021,0.1136,0.1329,0.2624,0.0112,0.0379,0.4896,0.0358,0.0753,0.6489,0.2742,0.8526,0.0479,0.0382,0.0521,0.1605,0.1912,0.0397,0.1664,0.0119,0.9552,0.0191,0.2113,0.1514,0.0029,0.2105,0.1491,0.2641] \n",
      "\n",
      "[1] \"You did not submit.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "FALSE"
      ],
      "text/latex": [
       "FALSE"
      ],
      "text/markdown": [
       "FALSE"
      ],
      "text/plain": [
       "[1] FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "send_submission(predictions, token, url=subm_url, submit_now= submit_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. Random Forest I - 0.854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RF1: 500 Tree (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tunegrid <- expand.grid(.mtry=c(3,5,10,15), .splitrule = \"gini\", .min.node.size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 5, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    }
   ],
   "source": [
    "set.seed(300)\n",
    "RF <- train(y~., data=traindata7, method=\"ranger\", trControl = fitControl, tuneGrid = tunegrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "1453 samples\n",
       "  58 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1309, 1307, 1308, 1308, 1307, 1308, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   3    0.8341272  0.4678762\n",
       "   5    0.8397692  0.5042960\n",
       "  10    0.8375584  0.5086975\n",
       "  15    0.8368745  0.5148579\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 5, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predRF <- predict(RF, traindata3, type = 'prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = a, case = b\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.884903490068455"
      ],
      "text/latex": [
       "0.884903490068455"
      ],
      "text/markdown": [
       "0.884903490068455"
      ],
      "text/plain": [
       "Area under the curve: 0.8849"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf.roc<-roc(traindata3$y,predRF$b)\n",
    "auc(rf.roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RF2: 250 Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tunegrid2 <- expand.grid(.mtry=c(3,5,10,15), .splitrule = \"gini\", .min.node.size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 5, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    }
   ],
   "source": [
    "set.seed(350)\n",
    "RF2 <- train(y~., data=traindata7, method=\"ranger\", trControl = fitControl, tuneGrid = tunegrid2,num.trees = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "1453 samples\n",
       "  58 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1307, 1309, 1308, 1307, 1307, 1308, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   3    0.8338684  0.4678193\n",
       "   5    0.8377335  0.4999499\n",
       "  10    0.8356568  0.5032019\n",
       "  15    0.8364731  0.5147640\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 5, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predRF2 <- predict(RF2, testdata, type = 'prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = a, case = b\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.88366906071148"
      ],
      "text/latex": [
       "0.88366906071148"
      ],
      "text/markdown": [
       "0.88366906071148"
      ],
      "text/plain": [
       "Area under the curve: 0.8837"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf.roc<-roc(traindata3$y,predRF2$b)\n",
    "auc(rf.roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RF3: 350 Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 5, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    }
   ],
   "source": [
    "set.seed(450)\n",
    "RF3 <- train(y~., data=traindata7, method=\"ranger\", trControl = fitControl, tuneGrid = tunegrid2,num.trees = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "1453 samples\n",
       "  58 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1307, 1309, 1308, 1307, 1307, 1308, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   3    0.8344117  0.4700823\n",
       "   5    0.8392223  0.5035497\n",
       "  10    0.8364760  0.5070506\n",
       "  15    0.8374292  0.5176639\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 5, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predRF3 <- predict(RF3, traindata3, type = 'prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = a, case = b\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.883206149702615"
      ],
      "text/latex": [
       "0.883206149702615"
      ],
      "text/markdown": [
       "0.883206149702615"
      ],
      "text/plain": [
       "Area under the curve: 0.8832"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf.roc<-roc(traindata3$y,predRF3$b)\n",
    "auc(rf.roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 5, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    }
   ],
   "source": [
    "set.seed(500)\n",
    "RF4 <- train(y~., data=traindata, method=\"ranger\", trControl = fitControl,\n",
    "             tuneGrid = tunegrid2,num.trees = 250, importance = 'permutation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "1453 samples\n",
       "  60 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1309, 1308, 1308, 1308, 1307, 1308, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   3    0.8327548  0.4597760\n",
       "   5    0.8373010  0.4943773\n",
       "  10    0.8359359  0.5060683\n",
       "  15    0.8349608  0.5066433\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 5, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predRF4 <- predict(RF4, testdata, type = 'prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = a, case = b\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.88460891033554"
      ],
      "text/latex": [
       "0.88460891033554"
      ],
      "text/markdown": [
       "0.88460891033554"
      ],
      "text/plain": [
       "Area under the curve: 0.8846"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf.roc<-roc(testdata$y,predRF4$b)\n",
    "auc(rf.roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RF 4: Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imptable = data.table(varImp(RF4)$importance)\n",
    "imptable[,order := order(imptable)]\n",
    "imptable[,variable := 1:dim(imptable)[1] ]\n",
    "imptable = imptable[order(rank(order))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Overall</th><th scope=col>order</th><th scope=col>variable</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>  5.2523535</td><td> 1         </td><td> 8         </td></tr>\n",
       "\t<tr><td>  3.9635170</td><td> 2         </td><td> 3         </td></tr>\n",
       "\t<tr><td>  2.5911049</td><td> 3         </td><td>29         </td></tr>\n",
       "\t<tr><td>  2.4468338</td><td> 4         </td><td> 1         </td></tr>\n",
       "\t<tr><td>  3.6410346</td><td> 5         </td><td>31         </td></tr>\n",
       "\t<tr><td> 60.4881043</td><td> 6         </td><td>42         </td></tr>\n",
       "\t<tr><td>  0.0000000</td><td> 7         </td><td> 4         </td></tr>\n",
       "\t<tr><td>  3.0271534</td><td> 8         </td><td>37         </td></tr>\n",
       "\t<tr><td>  5.4694629</td><td> 9         </td><td>40         </td></tr>\n",
       "\t<tr><td>  1.1329437</td><td>10         </td><td> 2         </td></tr>\n",
       "\t<tr><td>  4.1346771</td><td>11         </td><td> 5         </td></tr>\n",
       "\t<tr><td>  2.3977826</td><td>12         </td><td>47         </td></tr>\n",
       "\t<tr><td> 17.3594678</td><td>13         </td><td>38         </td></tr>\n",
       "\t<tr><td> 34.7530994</td><td>14         </td><td>54         </td></tr>\n",
       "\t<tr><td>  8.3643014</td><td>15         </td><td>17         </td></tr>\n",
       "\t<tr><td>  2.9308669</td><td>16         </td><td>49         </td></tr>\n",
       "\t<tr><td>  4.6009079</td><td>17         </td><td>45         </td></tr>\n",
       "\t<tr><td>  4.5820360</td><td>18         </td><td>25         </td></tr>\n",
       "\t<tr><td>  9.7486652</td><td>19         </td><td>12         </td></tr>\n",
       "\t<tr><td> 15.7683984</td><td>20         </td><td>44         </td></tr>\n",
       "\t<tr><td> 39.3317936</td><td>21         </td><td>32         </td></tr>\n",
       "\t<tr><td>  3.1849859</td><td>22         </td><td>22         </td></tr>\n",
       "\t<tr><td>  3.8383782</td><td>23         </td><td>60         </td></tr>\n",
       "\t<tr><td>  3.7619849</td><td>24         </td><td>43         </td></tr>\n",
       "\t<tr><td>  3.0908014</td><td>25         </td><td>35         </td></tr>\n",
       "\t<tr><td>  2.0559297</td><td>26         </td><td>11         </td></tr>\n",
       "\t<tr><td>  3.3650123</td><td>27         </td><td>34         </td></tr>\n",
       "\t<tr><td> 10.4854727</td><td>28         </td><td>39         </td></tr>\n",
       "\t<tr><td>  5.4462208</td><td>29         </td><td> 9         </td></tr>\n",
       "\t<tr><td>  3.1008920</td><td>30         </td><td>59         </td></tr>\n",
       "\t<tr><td>  2.9313457</td><td>31         </td><td>26         </td></tr>\n",
       "\t<tr><td> 65.4144135</td><td>32         </td><td>56         </td></tr>\n",
       "\t<tr><td>  6.1680288</td><td>33         </td><td> 6         </td></tr>\n",
       "\t<tr><td>100.0000000</td><td>34         </td><td>23         </td></tr>\n",
       "\t<tr><td>  3.5873782</td><td>35         </td><td>18         </td></tr>\n",
       "\t<tr><td>  3.1150039</td><td>36         </td><td>46         </td></tr>\n",
       "\t<tr><td>  5.2663770</td><td>37         </td><td>13         </td></tr>\n",
       "\t<tr><td>  3.0271534</td><td>38         </td><td>52         </td></tr>\n",
       "\t<tr><td> 21.5224756</td><td>39         </td><td>48         </td></tr>\n",
       "\t<tr><td>  4.4060575</td><td>40         </td><td>41         </td></tr>\n",
       "\t<tr><td>  2.2986373</td><td>41         </td><td>33         </td></tr>\n",
       "\t<tr><td>  3.0271534</td><td>42         </td><td>57         </td></tr>\n",
       "\t<tr><td>  4.4638497</td><td>43         </td><td>27         </td></tr>\n",
       "\t<tr><td>  3.4550301</td><td>44         </td><td>51         </td></tr>\n",
       "\t<tr><td>  9.1156463</td><td>45         </td><td>36         </td></tr>\n",
       "\t<tr><td>  8.1010733</td><td>46         </td><td>20         </td></tr>\n",
       "\t<tr><td>  1.1992286</td><td>47         </td><td> 7         </td></tr>\n",
       "\t<tr><td>  4.0183825</td><td>48         </td><td>53         </td></tr>\n",
       "\t<tr><td>  0.4965527</td><td>49         </td><td>10         </td></tr>\n",
       "\t<tr><td> 30.4065818</td><td>50         </td><td>14         </td></tr>\n",
       "\t<tr><td>  7.6278542</td><td>51         </td><td>24         </td></tr>\n",
       "\t<tr><td>  3.0889760</td><td>52         </td><td>15         </td></tr>\n",
       "\t<tr><td> 73.2416894</td><td>53         </td><td>30         </td></tr>\n",
       "\t<tr><td>  3.1397165</td><td>54         </td><td>55         </td></tr>\n",
       "\t<tr><td>  4.2626567</td><td>55         </td><td>21         </td></tr>\n",
       "\t<tr><td> 11.6559602</td><td>56         </td><td>58         </td></tr>\n",
       "\t<tr><td> 10.5565582</td><td>57         </td><td>16         </td></tr>\n",
       "\t<tr><td>  3.0271534</td><td>58         </td><td>50         </td></tr>\n",
       "\t<tr><td>  2.9634891</td><td>59         </td><td>19         </td></tr>\n",
       "\t<tr><td>  5.2972162</td><td>60         </td><td>28         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " Overall & order & variable\\\\\n",
       "\\hline\n",
       "\t   5.2523535 &  1          &  8         \\\\\n",
       "\t   3.9635170 &  2          &  3         \\\\\n",
       "\t   2.5911049 &  3          & 29         \\\\\n",
       "\t   2.4468338 &  4          &  1         \\\\\n",
       "\t   3.6410346 &  5          & 31         \\\\\n",
       "\t  60.4881043 &  6          & 42         \\\\\n",
       "\t   0.0000000 &  7          &  4         \\\\\n",
       "\t   3.0271534 &  8          & 37         \\\\\n",
       "\t   5.4694629 &  9          & 40         \\\\\n",
       "\t   1.1329437 & 10          &  2         \\\\\n",
       "\t   4.1346771 & 11          &  5         \\\\\n",
       "\t   2.3977826 & 12          & 47         \\\\\n",
       "\t  17.3594678 & 13          & 38         \\\\\n",
       "\t  34.7530994 & 14          & 54         \\\\\n",
       "\t   8.3643014 & 15          & 17         \\\\\n",
       "\t   2.9308669 & 16          & 49         \\\\\n",
       "\t   4.6009079 & 17          & 45         \\\\\n",
       "\t   4.5820360 & 18          & 25         \\\\\n",
       "\t   9.7486652 & 19          & 12         \\\\\n",
       "\t  15.7683984 & 20          & 44         \\\\\n",
       "\t  39.3317936 & 21          & 32         \\\\\n",
       "\t   3.1849859 & 22          & 22         \\\\\n",
       "\t   3.8383782 & 23          & 60         \\\\\n",
       "\t   3.7619849 & 24          & 43         \\\\\n",
       "\t   3.0908014 & 25          & 35         \\\\\n",
       "\t   2.0559297 & 26          & 11         \\\\\n",
       "\t   3.3650123 & 27          & 34         \\\\\n",
       "\t  10.4854727 & 28          & 39         \\\\\n",
       "\t   5.4462208 & 29          &  9         \\\\\n",
       "\t   3.1008920 & 30          & 59         \\\\\n",
       "\t   2.9313457 & 31          & 26         \\\\\n",
       "\t  65.4144135 & 32          & 56         \\\\\n",
       "\t   6.1680288 & 33          &  6         \\\\\n",
       "\t 100.0000000 & 34          & 23         \\\\\n",
       "\t   3.5873782 & 35          & 18         \\\\\n",
       "\t   3.1150039 & 36          & 46         \\\\\n",
       "\t   5.2663770 & 37          & 13         \\\\\n",
       "\t   3.0271534 & 38          & 52         \\\\\n",
       "\t  21.5224756 & 39          & 48         \\\\\n",
       "\t   4.4060575 & 40          & 41         \\\\\n",
       "\t   2.2986373 & 41          & 33         \\\\\n",
       "\t   3.0271534 & 42          & 57         \\\\\n",
       "\t   4.4638497 & 43          & 27         \\\\\n",
       "\t   3.4550301 & 44          & 51         \\\\\n",
       "\t   9.1156463 & 45          & 36         \\\\\n",
       "\t   8.1010733 & 46          & 20         \\\\\n",
       "\t   1.1992286 & 47          &  7         \\\\\n",
       "\t   4.0183825 & 48          & 53         \\\\\n",
       "\t   0.4965527 & 49          & 10         \\\\\n",
       "\t  30.4065818 & 50          & 14         \\\\\n",
       "\t   7.6278542 & 51          & 24         \\\\\n",
       "\t   3.0889760 & 52          & 15         \\\\\n",
       "\t  73.2416894 & 53          & 30         \\\\\n",
       "\t   3.1397165 & 54          & 55         \\\\\n",
       "\t   4.2626567 & 55          & 21         \\\\\n",
       "\t  11.6559602 & 56          & 58         \\\\\n",
       "\t  10.5565582 & 57          & 16         \\\\\n",
       "\t   3.0271534 & 58          & 50         \\\\\n",
       "\t   2.9634891 & 59          & 19         \\\\\n",
       "\t   5.2972162 & 60          & 28         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Overall | order | variable |\n",
       "|---|---|---|\n",
       "|   5.2523535 |  1          |  8          |\n",
       "|   3.9635170 |  2          |  3          |\n",
       "|   2.5911049 |  3          | 29          |\n",
       "|   2.4468338 |  4          |  1          |\n",
       "|   3.6410346 |  5          | 31          |\n",
       "|  60.4881043 |  6          | 42          |\n",
       "|   0.0000000 |  7          |  4          |\n",
       "|   3.0271534 |  8          | 37          |\n",
       "|   5.4694629 |  9          | 40          |\n",
       "|   1.1329437 | 10          |  2          |\n",
       "|   4.1346771 | 11          |  5          |\n",
       "|   2.3977826 | 12          | 47          |\n",
       "|  17.3594678 | 13          | 38          |\n",
       "|  34.7530994 | 14          | 54          |\n",
       "|   8.3643014 | 15          | 17          |\n",
       "|   2.9308669 | 16          | 49          |\n",
       "|   4.6009079 | 17          | 45          |\n",
       "|   4.5820360 | 18          | 25          |\n",
       "|   9.7486652 | 19          | 12          |\n",
       "|  15.7683984 | 20          | 44          |\n",
       "|  39.3317936 | 21          | 32          |\n",
       "|   3.1849859 | 22          | 22          |\n",
       "|   3.8383782 | 23          | 60          |\n",
       "|   3.7619849 | 24          | 43          |\n",
       "|   3.0908014 | 25          | 35          |\n",
       "|   2.0559297 | 26          | 11          |\n",
       "|   3.3650123 | 27          | 34          |\n",
       "|  10.4854727 | 28          | 39          |\n",
       "|   5.4462208 | 29          |  9          |\n",
       "|   3.1008920 | 30          | 59          |\n",
       "|   2.9313457 | 31          | 26          |\n",
       "|  65.4144135 | 32          | 56          |\n",
       "|   6.1680288 | 33          |  6          |\n",
       "| 100.0000000 | 34          | 23          |\n",
       "|   3.5873782 | 35          | 18          |\n",
       "|   3.1150039 | 36          | 46          |\n",
       "|   5.2663770 | 37          | 13          |\n",
       "|   3.0271534 | 38          | 52          |\n",
       "|  21.5224756 | 39          | 48          |\n",
       "|   4.4060575 | 40          | 41          |\n",
       "|   2.2986373 | 41          | 33          |\n",
       "|   3.0271534 | 42          | 57          |\n",
       "|   4.4638497 | 43          | 27          |\n",
       "|   3.4550301 | 44          | 51          |\n",
       "|   9.1156463 | 45          | 36          |\n",
       "|   8.1010733 | 46          | 20          |\n",
       "|   1.1992286 | 47          |  7          |\n",
       "|   4.0183825 | 48          | 53          |\n",
       "|   0.4965527 | 49          | 10          |\n",
       "|  30.4065818 | 50          | 14          |\n",
       "|   7.6278542 | 51          | 24          |\n",
       "|   3.0889760 | 52          | 15          |\n",
       "|  73.2416894 | 53          | 30          |\n",
       "|   3.1397165 | 54          | 55          |\n",
       "|   4.2626567 | 55          | 21          |\n",
       "|  11.6559602 | 56          | 58          |\n",
       "|  10.5565582 | 57          | 16          |\n",
       "|   3.0271534 | 58          | 50          |\n",
       "|   2.9634891 | 59          | 19          |\n",
       "|   5.2972162 | 60          | 28          |\n",
       "\n"
      ],
      "text/plain": [
       "   Overall     order variable\n",
       "1    5.2523535  1     8      \n",
       "2    3.9635170  2     3      \n",
       "3    2.5911049  3    29      \n",
       "4    2.4468338  4     1      \n",
       "5    3.6410346  5    31      \n",
       "6   60.4881043  6    42      \n",
       "7    0.0000000  7     4      \n",
       "8    3.0271534  8    37      \n",
       "9    5.4694629  9    40      \n",
       "10   1.1329437 10     2      \n",
       "11   4.1346771 11     5      \n",
       "12   2.3977826 12    47      \n",
       "13  17.3594678 13    38      \n",
       "14  34.7530994 14    54      \n",
       "15   8.3643014 15    17      \n",
       "16   2.9308669 16    49      \n",
       "17   4.6009079 17    45      \n",
       "18   4.5820360 18    25      \n",
       "19   9.7486652 19    12      \n",
       "20  15.7683984 20    44      \n",
       "21  39.3317936 21    32      \n",
       "22   3.1849859 22    22      \n",
       "23   3.8383782 23    60      \n",
       "24   3.7619849 24    43      \n",
       "25   3.0908014 25    35      \n",
       "26   2.0559297 26    11      \n",
       "27   3.3650123 27    34      \n",
       "28  10.4854727 28    39      \n",
       "29   5.4462208 29     9      \n",
       "30   3.1008920 30    59      \n",
       "31   2.9313457 31    26      \n",
       "32  65.4144135 32    56      \n",
       "33   6.1680288 33     6      \n",
       "34 100.0000000 34    23      \n",
       "35   3.5873782 35    18      \n",
       "36   3.1150039 36    46      \n",
       "37   5.2663770 37    13      \n",
       "38   3.0271534 38    52      \n",
       "39  21.5224756 39    48      \n",
       "40   4.4060575 40    41      \n",
       "41   2.2986373 41    33      \n",
       "42   3.0271534 42    57      \n",
       "43   4.4638497 43    27      \n",
       "44   3.4550301 44    51      \n",
       "45   9.1156463 45    36      \n",
       "46   8.1010733 46    20      \n",
       "47   1.1992286 47     7      \n",
       "48   4.0183825 48    53      \n",
       "49   0.4965527 49    10      \n",
       "50  30.4065818 50    14      \n",
       "51   7.6278542 51    24      \n",
       "52   3.0889760 52    15      \n",
       "53  73.2416894 53    30      \n",
       "54   3.1397165 54    55      \n",
       "55   4.2626567 55    21      \n",
       "56  11.6559602 56    58      \n",
       "57  10.5565582 57    16      \n",
       "58   3.0271534 58    50      \n",
       "59   2.9634891 59    19      \n",
       "60   5.2972162 60    28      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAAgP9NTU1oaGh8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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(varImp(RF4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in `[.data.table`(imptable, ..idx, \"Overall\"): ..idx is not found in calling scope and it is not a column of type logical. When the first argument inside DT[...] is a single symbol, data.table looks for it in calling scope.\n",
     "output_type": "error",
     "traceback": [
      "Error in `[.data.table`(imptable, ..idx, \"Overall\"): ..idx is not found in calling scope and it is not a column of type logical. When the first argument inside DT[...] is a single symbol, data.table looks for it in calling scope.\nTraceback:\n",
      "1. imptable[..idx, \"Overall\"]",
      "2. `[.data.table`(imptable, ..idx, \"Overall\")",
      "3. stop(as.character(isub), \" is not found in calling scope and it is not a column of type logical. When the first argument inside DT[...] is a single symbol, data.table looks for it in calling scope.\")"
     ]
    }
   ],
   "source": [
    "idx <- varImp(RF4)$importance >5\n",
    "imptable <- as.data.table(varImp(RF4)$importance)\n",
    "imptable[..idx,\"Overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.0386666666666667</li>\n",
       "\t<li>0.284307793173011</li>\n",
       "\t<li>0.357987802849658</li>\n",
       "\t<li>0.0494333333333333</li>\n",
       "\t<li>0.589378795671079</li>\n",
       "\t<li>0.111455237641615</li>\n",
       "\t<li>0.323146097272539</li>\n",
       "\t<li>0.323419225306165</li>\n",
       "\t<li>0.0732036075036075</li>\n",
       "\t<li>9.09090909090909e-05</li>\n",
       "\t<li>0.371806204993215</li>\n",
       "\t<li>0.253524318826233</li>\n",
       "\t<li>0.0327115646258503</li>\n",
       "\t<li>0.768577930732082</li>\n",
       "\t<li>0.118376479076479</li>\n",
       "\t<li>0.0389967320261438</li>\n",
       "\t<li>0.030564267866067</li>\n",
       "\t<li>0.153095203946263</li>\n",
       "\t<li>0.202483166814215</li>\n",
       "\t<li>0.0114024737167594</li>\n",
       "\t<li>0.106046405228758</li>\n",
       "\t<li>0.29346375424886</li>\n",
       "\t<li>0.799570000538655</li>\n",
       "\t<li>0.00573333333333333</li>\n",
       "\t<li>0.5085797649002</li>\n",
       "\t<li>0.0682929499072356</li>\n",
       "\t<li>0.287933901925218</li>\n",
       "\t<li>0.418726586129839</li>\n",
       "\t<li>0.426580377603335</li>\n",
       "\t<li>0.314469341223495</li>\n",
       "\t<li>0.292820313987962</li>\n",
       "\t<li>0.657049667521002</li>\n",
       "\t<li>0.0256076371389804</li>\n",
       "\t<li>0.351996273468985</li>\n",
       "\t<li>0.566468441782719</li>\n",
       "\t<li>0.268638978801203</li>\n",
       "\t<li>0.673271758263628</li>\n",
       "\t<li>0.639288675661089</li>\n",
       "\t<li>0.787783287780627</li>\n",
       "\t<li>0.0233333333333333</li>\n",
       "\t<li>0.171500477300477</li>\n",
       "\t<li>0.119285409016808</li>\n",
       "\t<li>0.0653362745098039</li>\n",
       "\t<li>0.627611595913476</li>\n",
       "\t<li>0.259306722689076</li>\n",
       "\t<li>0.657464618843982</li>\n",
       "\t<li>0.223395785757563</li>\n",
       "\t<li>0.0201636363636364</li>\n",
       "\t<li>0.216319848984649</li>\n",
       "\t<li>0.131109351432881</li>\n",
       "\t<li>0.0198444444444444</li>\n",
       "\t<li>0.0243603318250377</li>\n",
       "\t<li>0.204613680522376</li>\n",
       "\t<li>0.232045146520104</li>\n",
       "\t<li>0.779745979852686</li>\n",
       "\t<li>0.375555555555556</li>\n",
       "\t<li>0.575470191632187</li>\n",
       "\t<li>0.444164448582921</li>\n",
       "\t<li>0.123793530543531</li>\n",
       "\t<li>0.394624221962658</li>\n",
       "\t<li>0.787017562218569</li>\n",
       "\t<li>0.0126242424242424</li>\n",
       "\t<li>0.0957952380952381</li>\n",
       "\t<li>0.143419180819181</li>\n",
       "\t<li>0.0484333333333333</li>\n",
       "\t<li>0.0992571428571428</li>\n",
       "\t<li>0.00917142857142857</li>\n",
       "\t<li>0.0188188110026619</li>\n",
       "\t<li>0.544796632996633</li>\n",
       "\t<li>0.125974548440066</li>\n",
       "\t<li>0.481474910143135</li>\n",
       "\t<li>0.0384257703081232</li>\n",
       "\t<li>0.0466444444444444</li>\n",
       "\t<li>0.503750648882545</li>\n",
       "\t<li>0.0580952380952381</li>\n",
       "\t<li>0.203208080808081</li>\n",
       "\t<li>0.530479579137357</li>\n",
       "\t<li>0.0368943977591036</li>\n",
       "\t<li>0.25924207414433</li>\n",
       "\t<li>0.0973109557109557</li>\n",
       "\t<li>0.794891299403227</li>\n",
       "\t<li>0.0474490028490029</li>\n",
       "\t<li>0.0257497326203209</li>\n",
       "\t<li>0.54952819565919</li>\n",
       "\t<li>0.233513556115693</li>\n",
       "\t<li>0.220663402613324</li>\n",
       "\t<li>0.00313333333333333</li>\n",
       "\t<li>0.0868405797101449</li>\n",
       "\t<li>0.710100044877115</li>\n",
       "\t<li>0.2369</li>\n",
       "\t<li>0.0212975723622782</li>\n",
       "\t<li>0.0691290043290043</li>\n",
       "\t<li>0.0969240468804379</li>\n",
       "\t<li>0.663681517942318</li>\n",
       "\t<li>0.142473367487064</li>\n",
       "\t<li>0.292054991818389</li>\n",
       "\t<li>0.0947949096880131</li>\n",
       "\t<li>0.064574358974359</li>\n",
       "\t<li>0.332497741192073</li>\n",
       "\t<li>0.333328000196019</li>\n",
       "\t<li>0.525190712488606</li>\n",
       "\t<li>0.0287575757575758</li>\n",
       "\t<li>0.489178923361334</li>\n",
       "\t<li>0.242720091605708</li>\n",
       "\t<li>0.386952646541243</li>\n",
       "\t<li>0.342856513695812</li>\n",
       "\t<li>0.032830303030303</li>\n",
       "\t<li>0.189535353535354</li>\n",
       "\t<li>0.210482655945651</li>\n",
       "\t<li>0.0498095238095238</li>\n",
       "\t<li>0.0331333333333333</li>\n",
       "\t<li>0.644927177881025</li>\n",
       "\t<li>0.141847619047619</li>\n",
       "\t<li>0.583307089324737</li>\n",
       "\t<li>0.213456439993848</li>\n",
       "\t<li>0.0228242424242424</li>\n",
       "\t<li>0.0101480519480519</li>\n",
       "\t<li>0.30572274359553</li>\n",
       "\t<li>0.040411167945439</li>\n",
       "\t<li>0.0219428571428571</li>\n",
       "\t<li>0.0237333333333333</li>\n",
       "\t<li>0.0600539542194715</li>\n",
       "\t<li>0.0337307189542484</li>\n",
       "\t<li>0.171074208304011</li>\n",
       "\t<li>0.181488568614938</li>\n",
       "\t<li>0.0301389978213508</li>\n",
       "\t<li>0.0276758620689655</li>\n",
       "\t<li>0.294828571428571</li>\n",
       "\t<li>0.629823419909178</li>\n",
       "\t<li>0.0543151515151515</li>\n",
       "\t<li>0.59868781383623</li>\n",
       "\t<li>0.256419523585888</li>\n",
       "\t<li>0.306866698214982</li>\n",
       "\t<li>0.282727478564128</li>\n",
       "\t<li>0.340124195763443</li>\n",
       "\t<li>0.50100325846369</li>\n",
       "\t<li>0.0508666666666667</li>\n",
       "\t<li>0.469653497438885</li>\n",
       "\t<li>0.118086150015562</li>\n",
       "\t<li>0.572841387756388</li>\n",
       "\t<li>0.314598026815766</li>\n",
       "\t<li>0.172211644749381</li>\n",
       "\t<li>0.312275030741055</li>\n",
       "\t<li>0.428704688112038</li>\n",
       "\t<li>0.316485521866962</li>\n",
       "\t<li>0.218884753213629</li>\n",
       "\t<li>0.538241060137602</li>\n",
       "\t<li>0.212618794480247</li>\n",
       "\t<li>0.271734902537344</li>\n",
       "\t<li>0.0656476190476191</li>\n",
       "\t<li>0.0681892849447471</li>\n",
       "\t<li>0.270012535356496</li>\n",
       "\t<li>0.219599945265462</li>\n",
       "\t<li>0.127282352941176</li>\n",
       "\t<li>0.0117241830065359</li>\n",
       "\t<li>0.129743353682011</li>\n",
       "\t<li>0.0368705882352941</li>\n",
       "\t<li>0.425313918564177</li>\n",
       "\t<li>0.250565328386548</li>\n",
       "\t<li>0.811754268262775</li>\n",
       "\t<li>0.10218750106103</li>\n",
       "\t<li>0.118322222222222</li>\n",
       "\t<li>0.0282564102564103</li>\n",
       "\t<li>0.0904044334975369</li>\n",
       "\t<li>0.0226837789661319</li>\n",
       "\t<li>0.441826362146999</li>\n",
       "\t<li>0.0640064935064935</li>\n",
       "\t<li>0.371058461660636</li>\n",
       "\t<li>0.0340666666666667</li>\n",
       "\t<li>0.137093351424695</li>\n",
       "\t<li>0.621160071352579</li>\n",
       "\t<li>0.240783019252167</li>\n",
       "\t<li>0.358916487664258</li>\n",
       "\t<li>0.0433751322751323</li>\n",
       "\t<li>0.299365270831587</li>\n",
       "\t<li>0.0096</li>\n",
       "\t<li>0.245954102884316</li>\n",
       "\t<li>0.253102089913986</li>\n",
       "\t<li>0.578974293628385</li>\n",
       "\t<li>0.775791193067877</li>\n",
       "\t<li>0.733348996767748</li>\n",
       "\t<li>0.64141104881558</li>\n",
       "\t<li>0.437606539316601</li>\n",
       "\t<li>0.00446666666666667</li>\n",
       "\t<li>0.693518958187248</li>\n",
       "\t<li>0.0239629465354663</li>\n",
       "\t<li>0.001</li>\n",
       "\t<li>0.134897917177162</li>\n",
       "\t<li>0.226644142647396</li>\n",
       "\t<li>0.159959516856069</li>\n",
       "\t<li>0.000244897959183673</li>\n",
       "\t<li>0.359045390437543</li>\n",
       "\t<li>0.477865833621991</li>\n",
       "\t<li>0.214059271331079</li>\n",
       "\t<li>0.00755555555555555</li>\n",
       "\t<li>0.000333333333333333</li>\n",
       "\t<li>0.0472571428571429</li>\n",
       "\t<li>0.0151666666666667</li>\n",
       "\t<li>0.0358242424242424</li>\n",
       "\t<li>0.637158012728281</li>\n",
       "\t<li>0.482731520614185</li>\n",
       "\t<li>0.111332323232323</li>\n",
       "\t<li>0.152250127437984</li>\n",
       "\t<li>0.497167088916823</li>\n",
       "\t<li>0.271479273781657</li>\n",
       "\t<li>0.455203818836828</li>\n",
       "\t<li>0.0448757575757576</li>\n",
       "\t<li>0.763935244230151</li>\n",
       "\t<li>0.105061904761905</li>\n",
       "\t<li>0.606830139245331</li>\n",
       "\t<li>0.0266</li>\n",
       "\t<li>8e-04</li>\n",
       "\t<li>0.0942739541160594</li>\n",
       "\t<li>0.438700079703044</li>\n",
       "\t<li>0.243512348779888</li>\n",
       "\t<li>0.503079037870049</li>\n",
       "\t<li>0.133569126842656</li>\n",
       "\t<li>0.0412037037037037</li>\n",
       "\t<li>0.0102036923860453</li>\n",
       "\t<li>0.0580067873303167</li>\n",
       "\t<li>0.0811474842767296</li>\n",
       "\t<li>0.00489090909090909</li>\n",
       "\t<li>0.330810339039293</li>\n",
       "\t<li>0.464420454461405</li>\n",
       "\t<li>0.0700666666666667</li>\n",
       "\t<li>0.0973904761904762</li>\n",
       "\t<li>0.163766416955655</li>\n",
       "\t<li>0.523274217931655</li>\n",
       "\t<li>0.199467024929094</li>\n",
       "\t<li>0.00769090909090909</li>\n",
       "\t<li>0.677031551758182</li>\n",
       "\t<li>0.0696396825396825</li>\n",
       "\t<li>0.413304805937685</li>\n",
       "\t<li>0.0611194805194805</li>\n",
       "\t<li>0.0169884057971014</li>\n",
       "\t<li>0.0607285714285714</li>\n",
       "\t<li>0.43492685720077</li>\n",
       "\t<li>0.216448023096463</li>\n",
       "\t<li>0.355853453920927</li>\n",
       "\t<li>0.527556616736744</li>\n",
       "\t<li>0.0530505494505495</li>\n",
       "\t<li>0.0183686274509804</li>\n",
       "\t<li>0.0113011879804333</li>\n",
       "\t<li>0.0813967679379444</li>\n",
       "\t<li>0.0760971448228414</li>\n",
       "\t<li>0.0506560846560846</li>\n",
       "\t<li>0.400735353535354</li>\n",
       "\t<li>0.515018519229181</li>\n",
       "\t<li>0.360409195402299</li>\n",
       "\t<li>0.00824489795918367</li>\n",
       "\t<li>0.0383333333333333</li>\n",
       "\t<li>0.603874657746505</li>\n",
       "\t<li>0.648163117389417</li>\n",
       "\t<li>0.366674876626466</li>\n",
       "\t<li>0.0546925925925926</li>\n",
       "\t<li>0.490030525302997</li>\n",
       "\t<li>0.741960171940443</li>\n",
       "\t<li>0.200962947992688</li>\n",
       "\t<li>0.144319152112256</li>\n",
       "\t<li>0.141622799128338</li>\n",
       "\t<li>0.465051818921838</li>\n",
       "\t<li>0.190328571428571</li>\n",
       "\t<li>0.0465833333333333</li>\n",
       "\t<li>0.317557253604386</li>\n",
       "\t<li>0.478611274427798</li>\n",
       "\t<li>0.268190451176945</li>\n",
       "\t<li>0.0878809523809524</li>\n",
       "\t<li>0.0964445887445887</li>\n",
       "\t<li>0.060128140486964</li>\n",
       "\t<li>0.0906093514328808</li>\n",
       "\t<li>0.267174812202963</li>\n",
       "\t<li>0.188604597701149</li>\n",
       "\t<li>0.239579927702157</li>\n",
       "\t<li>0.199477994063257</li>\n",
       "\t<li>0.0481555555555555</li>\n",
       "\t<li>0.280759291894501</li>\n",
       "\t<li>0.307182655465532</li>\n",
       "\t<li>0.0674291316526611</li>\n",
       "\t<li>0.350158234524836</li>\n",
       "\t<li>0.165481001203937</li>\n",
       "\t<li>0.222794253780082</li>\n",
       "\t<li>0.0572920060331825</li>\n",
       "\t<li>0.190103982016468</li>\n",
       "\t<li>0.102093410818928</li>\n",
       "\t<li>0.52706734155081</li>\n",
       "\t<li>0.216547063522906</li>\n",
       "\t<li>0.0828364145658263</li>\n",
       "\t<li>0.428922504024337</li>\n",
       "\t<li>0.57723485821564</li>\n",
       "\t<li>0.291559191394476</li>\n",
       "\t<li>0.405754794645023</li>\n",
       "\t<li>0.0627333333333333</li>\n",
       "\t<li>0.295443444275023</li>\n",
       "\t<li>0.341839561683823</li>\n",
       "\t<li>0.348153534318752</li>\n",
       "\t<li>0.803763277603732</li>\n",
       "\t<li>0.538657546415613</li>\n",
       "\t<li>0.155703567421288</li>\n",
       "\t<li>0.0284</li>\n",
       "\t<li>0.0653951825951826</li>\n",
       "\t<li>0.681006936820214</li>\n",
       "\t<li>0.0250909090909091</li>\n",
       "\t<li>0.0407265010351967</li>\n",
       "\t<li>0.000662337662337662</li>\n",
       "\t<li>0.0378315298736351</li>\n",
       "\t<li>0.0378666666666667</li>\n",
       "\t<li>0.119664280225878</li>\n",
       "\t<li>0.253418844823313</li>\n",
       "\t<li>0.0625019607843137</li>\n",
       "\t<li>0.00442424242424242</li>\n",
       "\t<li>0.11971220043573</li>\n",
       "\t<li>0.348656986068479</li>\n",
       "\t<li>0.415787282375247</li>\n",
       "\t<li>0.0486095238095238</li>\n",
       "\t<li>0.039</li>\n",
       "\t<li>0.596140883735525</li>\n",
       "\t<li>0.265724630671763</li>\n",
       "\t<li>0.172893900808187</li>\n",
       "\t<li>0.457941012983484</li>\n",
       "\t<li>0.392812214827628</li>\n",
       "\t<li>0.0157106674420107</li>\n",
       "\t<li>0.135243434343434</li>\n",
       "\t<li>0.0194</li>\n",
       "\t<li>0.220547340632649</li>\n",
       "\t<li>0.187424999411323</li>\n",
       "\t<li>0.447072904356384</li>\n",
       "\t<li>0.387715328246304</li>\n",
       "\t<li>0.454045868945869</li>\n",
       "\t<li>0.0378849539815926</li>\n",
       "\t<li>0.398228772866494</li>\n",
       "\t<li>0.390477508204148</li>\n",
       "\t<li>0.152133478471214</li>\n",
       "\t<li>0.58755787726083</li>\n",
       "\t<li>0.428309944239527</li>\n",
       "\t<li>0.744634570602112</li>\n",
       "\t<li>0.190821217682475</li>\n",
       "\t<li>0.455303171132771</li>\n",
       "\t<li>0.640504928814659</li>\n",
       "\t<li>0.0156773341086774</li>\n",
       "\t<li>0.144972089314195</li>\n",
       "\t<li>0.0595555555555556</li>\n",
       "\t<li>0.150691562003639</li>\n",
       "\t<li>0.0750747474747475</li>\n",
       "\t<li>0.310302594492603</li>\n",
       "\t<li>0.304238847453225</li>\n",
       "\t<li>0.032149855877015</li>\n",
       "\t<li>0.0784596684017737</li>\n",
       "\t<li>0.301930155269706</li>\n",
       "\t<li>0.0478380952380952</li>\n",
       "\t<li>0.0727528138528139</li>\n",
       "\t<li>0.0867385281385281</li>\n",
       "\t<li>0.0228714285714286</li>\n",
       "\t<li>0.150608940453403</li>\n",
       "\t<li>0.62865894012919</li>\n",
       "\t<li>0.100176750378682</li>\n",
       "\t<li>0.277604366968881</li>\n",
       "\t<li>0.676867492779548</li>\n",
       "\t<li>0.168737254901961</li>\n",
       "\t<li>0.0787162297397592</li>\n",
       "\t<li>0.148563647801376</li>\n",
       "\t<li>0.0766138645315855</li>\n",
       "\t<li>0.0373333333333333</li>\n",
       "\t<li>0.110430241941356</li>\n",
       "\t<li>0.0932695970695971</li>\n",
       "\t<li>0.389445683546134</li>\n",
       "\t<li>0.273769264069264</li>\n",
       "\t<li>0.552587839483542</li>\n",
       "\t<li>0.0196666666666667</li>\n",
       "\t<li>0.0512603174603175</li>\n",
       "\t<li>0.0923436637436637</li>\n",
       "\t<li>0.612239563359861</li>\n",
       "\t<li>0.0641617464283838</li>\n",
       "\t<li>0.0096734693877551</li>\n",
       "\t<li>0.0711967218771567</li>\n",
       "\t<li>0.105190565756083</li>\n",
       "\t<li>0.629535813059332</li>\n",
       "\t<li>0.1719504254532</li>\n",
       "\t<li>0.0893333333333333</li>\n",
       "\t<li>0.00613333333333333</li>\n",
       "\t<li>0.282449427904003</li>\n",
       "\t<li>0.190488111815043</li>\n",
       "\t<li>0.372815155640314</li>\n",
       "\t<li>0.0424769841269841</li>\n",
       "\t<li>0.536972732218383</li>\n",
       "\t<li>0.157903174603175</li>\n",
       "\t<li>0.0797116138763197</li>\n",
       "\t<li>0.00526666666666667</li>\n",
       "\t<li>0.360509642846561</li>\n",
       "\t<li>0.0704133889099406</li>\n",
       "\t<li>0.27020156324479</li>\n",
       "\t<li>0.011979797979798</li>\n",
       "\t<li>0.111183679466106</li>\n",
       "\t<li>0.615802007704841</li>\n",
       "\t<li>0.199248881974001</li>\n",
       "\t<li>0.0918197064989517</li>\n",
       "\t<li>0.385197457521753</li>\n",
       "\t<li>0.0917285159285159</li>\n",
       "\t<li>0.0815746031746032</li>\n",
       "\t<li>0.0373609883854351</li>\n",
       "\t<li>0.0331238095238095</li>\n",
       "\t<li>0.105266666666667</li>\n",
       "\t<li>0.717849442812553</li>\n",
       "\t<li>0.150977777777778</li>\n",
       "\t<li>0.709632212560175</li>\n",
       "\t<li>0.0595047619047619</li>\n",
       "\t<li>0.325248575518024</li>\n",
       "\t<li>0.100281481481481</li>\n",
       "\t<li>0.328013947814903</li>\n",
       "\t<li>0.6880333793035</li>\n",
       "\t<li>0.0663131169709263</li>\n",
       "\t<li>0.227163834795735</li>\n",
       "\t<li>0.0328</li>\n",
       "\t<li>0.393872574704721</li>\n",
       "\t<li>0.0248666666666667</li>\n",
       "\t<li>0.405740652973506</li>\n",
       "\t<li>0.438756417032222</li>\n",
       "\t<li>0.254374236623581</li>\n",
       "\t<li>0.698981838075479</li>\n",
       "\t<li>0.39408582741861</li>\n",
       "\t<li>0.495920752085532</li>\n",
       "\t<li>0.324486990786991</li>\n",
       "\t<li>0.510677126966246</li>\n",
       "\t<li>0.0100571428571429</li>\n",
       "\t<li>0.0342899930986887</li>\n",
       "\t<li>0.154558033299447</li>\n",
       "\t<li>0.411068151456582</li>\n",
       "\t<li>0.122001678329519</li>\n",
       "\t<li>0.308214343854729</li>\n",
       "\t<li>0.164942251950948</li>\n",
       "\t<li>0.360687897186617</li>\n",
       "\t<li>0.483508579166423</li>\n",
       "\t<li>0.0729142857142857</li>\n",
       "\t<li>0.395253367660061</li>\n",
       "\t<li>0.263278816817658</li>\n",
       "\t<li>0.422276190476191</li>\n",
       "\t<li>0.229750385846038</li>\n",
       "\t<li>0.0108047619047619</li>\n",
       "\t<li>0.176508485316802</li>\n",
       "\t<li>0.474306200535843</li>\n",
       "\t<li>0.276660728826316</li>\n",
       "\t<li>0.0947634920634921</li>\n",
       "\t<li>0.105395238095238</li>\n",
       "\t<li>0.112504761904762</li>\n",
       "\t<li>0.806234385012738</li>\n",
       "\t<li>0.136431402244281</li>\n",
       "\t<li>0.435833333333333</li>\n",
       "\t<li>0.009</li>\n",
       "\t<li>0.0114545454545455</li>\n",
       "\t<li>0.482571392629974</li>\n",
       "\t<li>0.0602242424242424</li>\n",
       "\t<li>0.431337554020286</li>\n",
       "\t<li>0.27681027662206</li>\n",
       "\t<li>0.0367380952380952</li>\n",
       "\t<li>0.62318882370584</li>\n",
       "\t<li>0.209622273151685</li>\n",
       "\t<li>0.0078</li>\n",
       "\t<li>0.289953713232959</li>\n",
       "\t<li>0.315235664322403</li>\n",
       "\t<li>0.65275649266471</li>\n",
       "\t<li>0.575027900354182</li>\n",
       "\t<li>0.107434295029988</li>\n",
       "\t<li>0.0997706959706959</li>\n",
       "\t<li>0.213135012951385</li>\n",
       "\t<li>0.743771433091391</li>\n",
       "\t<li>0.0764550724637681</li>\n",
       "\t<li>0.318349534459089</li>\n",
       "\t<li>0.140746664609543</li>\n",
       "\t<li>0.0353240413406742</li>\n",
       "\t<li>0.0570884057971014</li>\n",
       "\t<li>0.538681238884521</li>\n",
       "\t<li>0.363733099135034</li>\n",
       "\t<li>0.619358197014435</li>\n",
       "\t<li>0.111166666666667</li>\n",
       "\t<li>0.0520356840080327</li>\n",
       "\t<li>0.516450076827099</li>\n",
       "\t<li>0.0251210884353741</li>\n",
       "\t<li>0.294442302470784</li>\n",
       "\t<li>0.672557520676255</li>\n",
       "\t<li>0.240112825653878</li>\n",
       "\t<li>0.0230758620689655</li>\n",
       "\t<li>0.0054</li>\n",
       "\t<li>0.139863492063492</li>\n",
       "\t<li>0.66436611124831</li>\n",
       "\t<li>0.0980222222222222</li>\n",
       "\t<li>0.0244666666666667</li>\n",
       "\t<li>0.104156228956229</li>\n",
       "\t<li>0.550629126047257</li>\n",
       "\t<li>0.0130447438918766</li>\n",
       "\t<li>0.338244206903106</li>\n",
       "\t<li>0.0842</li>\n",
       "\t<li>0.541711652052044</li>\n",
       "\t<li>0.349881345415894</li>\n",
       "\t<li>0.531041458306449</li>\n",
       "\t<li>0.625788822549875</li>\n",
       "\t<li>0.047374358974359</li>\n",
       "\t<li>0.0038</li>\n",
       "\t<li>0.676460203701692</li>\n",
       "\t<li>0.00993333333333333</li>\n",
       "\t<li>0.677239550859892</li>\n",
       "\t<li>0.106911111111111</li>\n",
       "\t<li>0.145854197654198</li>\n",
       "\t<li>0.144922518680328</li>\n",
       "\t<li>0.00817391304347826</li>\n",
       "\t<li>0.0178575757575758</li>\n",
       "\t<li>0.12160293040293</li>\n",
       "\t<li>0.017</li>\n",
       "\t<li>0.357125539177879</li>\n",
       "\t<li>0.338516133398352</li>\n",
       "\t<li>0.113944444444444</li>\n",
       "\t<li>0.0374086834733894</li>\n",
       "\t<li>0.3655002859727</li>\n",
       "\t<li>0.294799301435585</li>\n",
       "\t<li>0.0244242424242424</li>\n",
       "\t<li>0.0357666666666667</li>\n",
       "\t<li>0.0600285714285714</li>\n",
       "\t<li>0.233565641098458</li>\n",
       "\t<li>0.0560986717216531</li>\n",
       "\t<li>0.102466666666667</li>\n",
       "\t<li>0.53278700277892</li>\n",
       "\t<li>0.429590465914747</li>\n",
       "\t<li>0.00677435897435897</li>\n",
       "\t<li>0.0343099415204678</li>\n",
       "\t<li>0.452730246566324</li>\n",
       "\t<li>0.0253333333333333</li>\n",
       "\t<li>0.202371600076164</li>\n",
       "\t<li>0.289645265148295</li>\n",
       "\t<li>0.0232778566970787</li>\n",
       "\t<li>0.626740403927503</li>\n",
       "\t<li>0.198095894271918</li>\n",
       "\t<li>0.00922424242424242</li>\n",
       "\t<li>0.245906109155619</li>\n",
       "\t<li>0.0288666666666667</li>\n",
       "\t<li>0.361919314443584</li>\n",
       "\t<li>0.534399308386028</li>\n",
       "\t<li>0.31039062049062</li>\n",
       "\t<li>0.00497309479231676</li>\n",
       "\t<li>0.838908268033228</li>\n",
       "\t<li>0.640486206224888</li>\n",
       "\t<li>0.311127300973258</li>\n",
       "\t<li>0.204467377398721</li>\n",
       "\t<li>0.149169358178054</li>\n",
       "\t<li>0.778886613952768</li>\n",
       "\t<li>0.0356888888888889</li>\n",
       "\t<li>0.151863942822034</li>\n",
       "\t<li>0.109799652199652</li>\n",
       "\t<li>0.0533871794871795</li>\n",
       "\t<li>0.519950469788172</li>\n",
       "\t<li>0.639161009661332</li>\n",
       "\t<li>0.0181238095238095</li>\n",
       "\t<li>0.535326076236431</li>\n",
       "\t<li>0.0178448979591837</li>\n",
       "\t<li>0.0374705882352941</li>\n",
       "\t<li>0.0935802197802198</li>\n",
       "\t<li>0.706244476520718</li>\n",
       "\t<li>0.421852941176471</li>\n",
       "\t<li>0.289691481982747</li>\n",
       "\t<li>0.103676039646628</li>\n",
       "\t<li>0.0519054726368159</li>\n",
       "\t<li>0.0708282717282717</li>\n",
       "\t<li>0.322005043614739</li>\n",
       "\t<li>0.0589529411764706</li>\n",
       "\t<li>0.0485163265306122</li>\n",
       "\t<li>0.0538380952380952</li>\n",
       "\t<li>0.214603949392259</li>\n",
       "\t<li>0.177011039617733</li>\n",
       "\t<li>0.0306333333333333</li>\n",
       "\t<li>0.376446677990467</li>\n",
       "\t<li>0.0774028886655573</li>\n",
       "\t<li>0.575840860387835</li>\n",
       "\t<li>0.257164879195585</li>\n",
       "\t<li>0.0392948306595365</li>\n",
       "\t<li>0.208189830090342</li>\n",
       "\t<li>0.226569102144005</li>\n",
       "\t<li>0.340363301614579</li>\n",
       "\t<li>0.302163923844582</li>\n",
       "\t<li>0.419854541732668</li>\n",
       "\t<li>0.468705948432882</li>\n",
       "\t<li>0.239743221162657</li>\n",
       "\t<li>0.0556585266585267</li>\n",
       "\t<li>0.0598242424242424</li>\n",
       "\t<li>0.0223986013986014</li>\n",
       "\t<li>0.0138555555555556</li>\n",
       "\t<li>0.331105283357457</li>\n",
       "\t<li>0.466434140198813</li>\n",
       "\t<li>0.260565651018484</li>\n",
       "\t<li>0.459321698842957</li>\n",
       "\t<li>0.0558666666666667</li>\n",
       "\t<li>0.141293395195313</li>\n",
       "\t<li>0.242394352190637</li>\n",
       "\t<li>0.039630303030303</li>\n",
       "\t<li>0.403255845886581</li>\n",
       "\t<li>0.189922408963585</li>\n",
       "\t<li>0.0342166666666667</li>\n",
       "\t<li>0.0112</li>\n",
       "\t<li>0.024988332302618</li>\n",
       "\t<li>0.481976695508585</li>\n",
       "\t<li>0.0444</li>\n",
       "\t<li>0.0191777777777778</li>\n",
       "\t<li>0.0296380952380952</li>\n",
       "\t<li>0.367306685423997</li>\n",
       "\t<li>0.0778380952380952</li>\n",
       "\t<li>0.289312944745478</li>\n",
       "\t<li>0.559175706432625</li>\n",
       "\t<li>0.0308967032967033</li>\n",
       "\t<li>0.101347619047619</li>\n",
       "\t<li>0.293640599268311</li>\n",
       "\t<li>0.798060181418768</li>\n",
       "\t<li>0.134602661064426</li>\n",
       "\t<li>0.099288198757764</li>\n",
       "\t<li>0.0940862745098039</li>\n",
       "\t<li>0.218115708812261</li>\n",
       "\t<li>0.466214721183192</li>\n",
       "\t<li>0.565577330470973</li>\n",
       "\t<li>0.15545166495444</li>\n",
       "\t<li>0.0932862745098039</li>\n",
       "\t<li>0.514839955371036</li>\n",
       "\t<li>0.0177645962732919</li>\n",
       "\t<li>0.289443145743146</li>\n",
       "\t<li>0.197659414659415</li>\n",
       "\t<li>0.813799249039808</li>\n",
       "\t<li>0.228247875815897</li>\n",
       "\t<li>0.536628586862503</li>\n",
       "\t<li>0.693993902066963</li>\n",
       "\t<li>0.467215482556659</li>\n",
       "\t<li>0.262524816893888</li>\n",
       "\t<li>0.193785871798834</li>\n",
       "\t<li>0.153193650793651</li>\n",
       "\t<li>0.0451210884353742</li>\n",
       "\t<li>0.197006647324272</li>\n",
       "\t<li>0.0701230769230769</li>\n",
       "\t<li>0.0704074534161491</li>\n",
       "\t<li>0.116060029294737</li>\n",
       "\t<li>0.509875081675082</li>\n",
       "\t<li>0.240945731135154</li>\n",
       "\t<li>0.214629835415337</li>\n",
       "\t<li>0.00522816399286987</li>\n",
       "\t<li>0.312126499687464</li>\n",
       "\t<li>0.243039882810303</li>\n",
       "\t<li>0.0422291316526611</li>\n",
       "\t<li>0.120124239317968</li>\n",
       "\t<li>0.127603242816995</li>\n",
       "\t<li>0.0698856065503124</li>\n",
       "\t<li>0.0162931677018634</li>\n",
       "\t<li>0.176088888888889</li>\n",
       "\t<li>0.114416246498599</li>\n",
       "\t<li>0.0174888888888889</li>\n",
       "\t<li>0.123037458837459</li>\n",
       "\t<li>0.111888888888889</li>\n",
       "\t<li>0.366326102952682</li>\n",
       "\t<li>0.243043587613955</li>\n",
       "\t<li>0.137607246376812</li>\n",
       "\t<li>0.0491333333333333</li>\n",
       "\t<li>0.466450994433474</li>\n",
       "\t<li>0.366124988474531</li>\n",
       "\t<li>0.262872307543707</li>\n",
       "\t<li>0.169280866503436</li>\n",
       "\t<li>0.630904319669879</li>\n",
       "\t<li>0.505980371336913</li>\n",
       "\t<li>0.564482970921137</li>\n",
       "\t<li>0.252544876044818</li>\n",
       "\t<li>0.308024242424242</li>\n",
       "\t<li>0.0314147186147186</li>\n",
       "\t<li>0.0708261437908497</li>\n",
       "\t<li>0.564847425424129</li>\n",
       "\t<li>0.314746867051548</li>\n",
       "\t<li>0.148449940752894</li>\n",
       "\t<li>0.443782443770596</li>\n",
       "\t<li>0.0527897435897436</li>\n",
       "\t<li>0.00142424242424242</li>\n",
       "\t<li>0.572227885200541</li>\n",
       "\t<li>0.474441772961746</li>\n",
       "\t<li>0.0553878787878788</li>\n",
       "\t<li>9.09090909090909e-05</li>\n",
       "\t<li>0.502523426944876</li>\n",
       "\t<li>0.415692500564034</li>\n",
       "\t<li>0.513565347639667</li>\n",
       "\t<li>0.0237333333333333</li>\n",
       "\t<li>0.394099602454177</li>\n",
       "\t<li>0.342877724748259</li>\n",
       "\t<li>0.212610875831639</li>\n",
       "\t<li>0.0403761904761905</li>\n",
       "\t<li>0.763012831118629</li>\n",
       "\t<li>0.175763636363636</li>\n",
       "\t<li>0.186389016978732</li>\n",
       "\t<li>0.148859383431903</li>\n",
       "\t<li>0.525763076000812</li>\n",
       "\t<li>0.298697784291618</li>\n",
       "\t<li>0.2990466729727</li>\n",
       "\t<li>0.137109195402299</li>\n",
       "\t<li>0.225788481176847</li>\n",
       "\t<li>0.0500863731656185</li>\n",
       "\t<li>0.099187031408308</li>\n",
       "\t<li>0.390662444166576</li>\n",
       "\t<li>0.351110446598191</li>\n",
       "\t<li>0.0279019607843137</li>\n",
       "\t<li>0.309185570434518</li>\n",
       "\t<li>0.0349238095238095</li>\n",
       "\t<li>0.0476739130434783</li>\n",
       "\t<li>0.0385333333333333</li>\n",
       "\t<li>0.245717893335458</li>\n",
       "\t<li>0.0527222222222222</li>\n",
       "\t<li>0.284787878787879</li>\n",
       "\t<li>0.627641641703345</li>\n",
       "\t<li>0.21879732640771</li>\n",
       "\t<li>0.469045016904163</li>\n",
       "\t<li>0.594151958049773</li>\n",
       "\t<li>0.373316017316017</li>\n",
       "\t<li>0.0264714285714286</li>\n",
       "\t<li>0.254669573718702</li>\n",
       "\t<li>0.041559589830252</li>\n",
       "\t<li>0.311598036268764</li>\n",
       "\t<li>0.841786907647478</li>\n",
       "\t<li>0.0159636363636364</li>\n",
       "\t<li>0.0312666666666667</li>\n",
       "\t<li>0.279655611653983</li>\n",
       "\t<li>0.412263653821177</li>\n",
       "\t<li>0.379312290957525</li>\n",
       "\t<li>0.439653944513374</li>\n",
       "\t<li>0.784785885812251</li>\n",
       "\t<li>0.0545271708683473</li>\n",
       "\t<li>0.188017638927227</li>\n",
       "\t<li>0.344273470384587</li>\n",
       "\t<li>0.562868361213509</li>\n",
       "\t<li>0.0802</li>\n",
       "\t<li>0.062573544973545</li>\n",
       "\t<li>0.0359350649350649</li>\n",
       "\t<li>0.391175023359331</li>\n",
       "\t<li>0.408658303032619</li>\n",
       "\t<li>0.398098078280914</li>\n",
       "\t<li>0.0789806239737274</li>\n",
       "\t<li>0.271022290241679</li>\n",
       "\t<li>0.551962166207989</li>\n",
       "\t<li>0.212661741946708</li>\n",
       "\t<li>0.0751591431556949</li>\n",
       "\t<li>0.00578840579710145</li>\n",
       "\t<li>0.0592</li>\n",
       "\t<li>0.0317575757575758</li>\n",
       "\t<li>0.400923240463152</li>\n",
       "\t<li>0.0474380952380952</li>\n",
       "\t<li>0.0545703703703704</li>\n",
       "\t<li>0.00231632653061224</li>\n",
       "\t<li>0.105234798534799</li>\n",
       "\t<li>0.587191562464047</li>\n",
       "\t<li>0.0900055555555555</li>\n",
       "\t<li>0.683906811495236</li>\n",
       "\t<li>0.132463157894737</li>\n",
       "\t<li>0.481602978553558</li>\n",
       "\t<li>0.495182859851671</li>\n",
       "\t<li>0.0279650793650794</li>\n",
       "\t<li>0.252402575687401</li>\n",
       "\t<li>0.018</li>\n",
       "\t<li>0.0356</li>\n",
       "\t<li>0.174758494177985</li>\n",
       "\t<li>0.153344139434442</li>\n",
       "\t<li>0.0726857142857143</li>\n",
       "\t<li>0.683499165226029</li>\n",
       "\t<li>0.153202628398228</li>\n",
       "\t<li>0.0596564102564103</li>\n",
       "\t<li>0.0799514105585433</li>\n",
       "\t<li>0.0164047619047619</li>\n",
       "\t<li>0.263091465594213</li>\n",
       "\t<li>0.474782800125594</li>\n",
       "\t<li>0.0670761904761905</li>\n",
       "\t<li>0.280788345367725</li>\n",
       "\t<li>0.704240171759427</li>\n",
       "\t<li>0.102052052545156</li>\n",
       "\t<li>0.00213333333333333</li>\n",
       "\t<li>0.0387119975262832</li>\n",
       "\t<li>0.425337059203743</li>\n",
       "\t<li>0.331278391850968</li>\n",
       "\t<li>0.0281194805194805</li>\n",
       "\t<li>0.0349555555555556</li>\n",
       "\t<li>0.475729114435763</li>\n",
       "\t<li>0.676810280270968</li>\n",
       "\t<li>0.523089841479637</li>\n",
       "\t<li>0.139011388611389</li>\n",
       "\t<li>0.779318549866474</li>\n",
       "\t<li>0.367001849785385</li>\n",
       "\t<li>0.0572705882352941</li>\n",
       "\t<li>0.314250992428384</li>\n",
       "\t<li>0.161834335058676</li>\n",
       "\t<li>0.00213333333333333</li>\n",
       "\t<li>0.412955823826079</li>\n",
       "\t<li>0.0199333333333333</li>\n",
       "\t<li>0.0458928329888822</li>\n",
       "\t<li>0.319390905296081</li>\n",
       "\t<li>0.420037351306949</li>\n",
       "\t<li>0.364102566004655</li>\n",
       "\t<li>0.146947619047619</li>\n",
       "\t<li>0.301476381253682</li>\n",
       "\t<li>0.00517391304347826</li>\n",
       "\t<li>0.205660606060606</li>\n",
       "\t<li>0.61367634871572</li>\n",
       "\t<li>0.0319714285714286</li>\n",
       "\t<li>0.150427973512282</li>\n",
       "\t<li>0.0271909090909091</li>\n",
       "\t<li>0.0048</li>\n",
       "\t<li>0.0260666666666667</li>\n",
       "\t<li>0.0725269841269841</li>\n",
       "\t<li>0.374685717160095</li>\n",
       "\t<li>0.150709401709402</li>\n",
       "\t<li>0.0235884057971015</li>\n",
       "\t<li>0.308170014728704</li>\n",
       "\t<li>0.211549318295058</li>\n",
       "\t<li>0.32041062603476</li>\n",
       "\t<li>0.082</li>\n",
       "\t<li>0.418124033184996</li>\n",
       "\t<li>0.0607015873015873</li>\n",
       "\t<li>0.0659246443211961</li>\n",
       "\t<li>0.535017418519619</li>\n",
       "\t<li>0.354019973464449</li>\n",
       "\t<li>0.102384283076097</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.041809977324263</li>\n",
       "\t<li>0.279769062472443</li>\n",
       "\t<li>0.353489633400662</li>\n",
       "\t<li>0.207294912736624</li>\n",
       "\t<li>0.376647132086538</li>\n",
       "\t<li>0.415349870433962</li>\n",
       "\t<li>0.0468809523809524</li>\n",
       "\t<li>0.157501944025473</li>\n",
       "\t<li>0.0733975130133025</li>\n",
       "\t<li>0.595211111111111</li>\n",
       "\t<li>0.445251352382422</li>\n",
       "\t<li>0.0661666666666667</li>\n",
       "\t<li>0.0519172161172161</li>\n",
       "\t<li>0.480804215675408</li>\n",
       "\t<li>0.0038</li>\n",
       "\t<li>0.195854665844033</li>\n",
       "\t<li>0.439200500428439</li>\n",
       "\t<li>0.0630340440653873</li>\n",
       "\t<li>0.0118</li>\n",
       "\t<li>0.516250192853268</li>\n",
       "\t<li>0.498544916984795</li>\n",
       "\t<li>0.760591175551091</li>\n",
       "\t<li>0.365394422263113</li>\n",
       "\t<li>0.119327103413597</li>\n",
       "\t<li>0.412778176927255</li>\n",
       "\t<li>0.0782047619047619</li>\n",
       "\t<li>0.591030502491688</li>\n",
       "\t<li>0.27405786440186</li>\n",
       "\t<li>0.63467417689523</li>\n",
       "\t<li>0.0106</li>\n",
       "\t<li>0.632594316947072</li>\n",
       "\t<li>0.57018670495502</li>\n",
       "\t<li>0.542914132310845</li>\n",
       "\t<li>0.0448222222222222</li>\n",
       "\t<li>0.100646464646465</li>\n",
       "\t<li>0.0688333333333333</li>\n",
       "\t<li>0.391323809523809</li>\n",
       "\t<li>0.134269378091727</li>\n",
       "\t<li>0.358933333333333</li>\n",
       "\t<li>0.569480904485631</li>\n",
       "\t<li>0.0074</li>\n",
       "\t<li>0.584502109134811</li>\n",
       "\t<li>0.210745425641771</li>\n",
       "\t<li>0.0312941798941799</li>\n",
       "\t<li>0.0048</li>\n",
       "\t<li>0.0028</li>\n",
       "\t<li>0.501271491786265</li>\n",
       "\t<li>0.0104</li>\n",
       "\t<li>0.0524666666666667</li>\n",
       "\t<li>0.0858979296066253</li>\n",
       "\t<li>0.289262426194988</li>\n",
       "\t<li>0.424623370845067</li>\n",
       "\t<li>0.072231746031746</li>\n",
       "\t<li>0.135167303685025</li>\n",
       "\t<li>0.369611016635817</li>\n",
       "\t<li>0.275685940420367</li>\n",
       "\t<li>0.0731760683760684</li>\n",
       "\t<li>0.0586952380952381</li>\n",
       "\t<li>0.0121575757575758</li>\n",
       "\t<li>0.00907142857142857</li>\n",
       "\t<li>0.230981594733769</li>\n",
       "\t<li>0.253574848576014</li>\n",
       "\t<li>0.485489586741761</li>\n",
       "\t<li>0.126477845234978</li>\n",
       "\t<li>0.252266854242723</li>\n",
       "\t<li>0.461561185201059</li>\n",
       "\t<li>0.714396163342505</li>\n",
       "\t<li>0.888072725437863</li>\n",
       "\t<li>0.38059559683715</li>\n",
       "\t<li>0.300584668691521</li>\n",
       "\t<li>0.186609146156279</li>\n",
       "\t<li>0.0910128978864273</li>\n",
       "\t<li>0.0950509379509379</li>\n",
       "\t<li>0.0445714285714286</li>\n",
       "\t<li>0.733074423960969</li>\n",
       "\t<li>0.313762255278213</li>\n",
       "\t<li>0.391056897334929</li>\n",
       "\t<li>0.537918660469894</li>\n",
       "\t<li>0.545035145836005</li>\n",
       "\t<li>0.183944370730748</li>\n",
       "\t<li>0.191006626546944</li>\n",
       "\t<li>0.229416466490255</li>\n",
       "\t<li>0.14783951833607</li>\n",
       "\t<li>0.175466380362932</li>\n",
       "\t<li>0.300386921867483</li>\n",
       "\t<li>0.119599076512164</li>\n",
       "\t<li>0.189289243351157</li>\n",
       "\t<li>0.139527909784603</li>\n",
       "\t<li>0.509938308558842</li>\n",
       "\t<li>0.00901062801932367</li>\n",
       "\t<li>0.316608146554032</li>\n",
       "\t<li>0.573690208720772</li>\n",
       "\t<li>0.0828583241320083</li>\n",
       "\t<li>0.115489599620943</li>\n",
       "\t<li>0.0204444444444444</li>\n",
       "\t<li>0.344281594427353</li>\n",
       "\t<li>0.502059748478351</li>\n",
       "\t<li>0.0368265010351967</li>\n",
       "\t<li>0.639156377249408</li>\n",
       "\t<li>0.605557813580397</li>\n",
       "\t<li>0.355263089775196</li>\n",
       "\t<li>0.060209653092006</li>\n",
       "\t<li>0.0283333333333333</li>\n",
       "\t<li>0.465417845947928</li>\n",
       "\t<li>0.383771867073044</li>\n",
       "\t<li>0.609687775060824</li>\n",
       "\t<li>0.104822222222222</li>\n",
       "\t<li>0.0186</li>\n",
       "\t<li>0.660421304876933</li>\n",
       "\t<li>0.481409203757834</li>\n",
       "\t<li>0.809747766725594</li>\n",
       "\t<li>0.0787714795008913</li>\n",
       "\t<li>0.017379797979798</li>\n",
       "\t<li>0.314007293648733</li>\n",
       "\t<li>0.209001180072758</li>\n",
       "\t<li>0.305927732644714</li>\n",
       "\t<li>0.525786629452419</li>\n",
       "\t<li>0.862981592950686</li>\n",
       "\t<li>0.0898273769450979</li>\n",
       "\t<li>0.107311111111111</li>\n",
       "\t<li>0.688490254374722</li>\n",
       "\t<li>0.178830976430976</li>\n",
       "\t<li>0.110597198879552</li>\n",
       "\t<li>0.441491346096358</li>\n",
       "\t<li>0.0665282106782107</li>\n",
       "\t<li>0.0869333333333333</li>\n",
       "\t<li>0.281417149517143</li>\n",
       "\t<li>0.319920834356719</li>\n",
       "\t<li>0.441589346278222</li>\n",
       "\t<li>0.312142531467055</li>\n",
       "\t<li>0.00666666666666667</li>\n",
       "\t<li>0.182681203746909</li>\n",
       "\t<li>0.0879587301587302</li>\n",
       "\t<li>0.0939294190086643</li>\n",
       "\t<li>0.848838044240041</li>\n",
       "\t<li>0.679419157351322</li>\n",
       "\t<li>0.695131142211447</li>\n",
       "\t<li>0.277875562262769</li>\n",
       "\t<li>0.290326504867681</li>\n",
       "\t<li>0.130597198879552</li>\n",
       "\t<li>0.7560411117244</li>\n",
       "\t<li>0.154198364850985</li>\n",
       "\t<li>0.357935797873526</li>\n",
       "\t<li>0.0274550724637681</li>\n",
       "\t<li>0.0098</li>\n",
       "\t<li>0.515103801446009</li>\n",
       "\t<li>0.0518568637711495</li>\n",
       "\t<li>0.0602037037037037</li>\n",
       "\t<li>0.212746951374304</li>\n",
       "\t<li>0.0082</li>\n",
       "\t<li>0.104533333333333</li>\n",
       "\t<li>0.0296</li>\n",
       "\t<li>0.360223821220606</li>\n",
       "\t<li>0.216810084033613</li>\n",
       "\t<li>0.078130303030303</li>\n",
       "\t<li>0.418873876001768</li>\n",
       "\t<li>0.552865475261585</li>\n",
       "\t<li>0.0629212666467276</li>\n",
       "\t<li>0.318102412502822</li>\n",
       "\t<li>0.55304407831155</li>\n",
       "\t<li>0.422692927377851</li>\n",
       "\t<li>0.163141247394718</li>\n",
       "\t<li>0.0651428571428571</li>\n",
       "\t<li>0.807349226711289</li>\n",
       "\t<li>0.308089905490167</li>\n",
       "\t<li>0.27977538174277</li>\n",
       "\t<li>0.594811542869982</li>\n",
       "\t<li>0.782831426708755</li>\n",
       "\t<li>0.592180283430288</li>\n",
       "\t<li>0.304757152840505</li>\n",
       "\t<li>0.513852942636185</li>\n",
       "\t<li>0.598395093538502</li>\n",
       "\t<li>0.367660878040489</li>\n",
       "\t<li>0.208479783050437</li>\n",
       "\t<li>0.387837330440576</li>\n",
       "\t<li>0.337120928477463</li>\n",
       "\t<li>0.00942424242424242</li>\n",
       "\t<li>0.044831746031746</li>\n",
       "\t<li>0.0331037037037037</li>\n",
       "\t<li>0.083404662004662</li>\n",
       "\t<li>0.265355856123919</li>\n",
       "\t<li>0.0668297996121526</li>\n",
       "\t<li>0.739165205095001</li>\n",
       "\t<li>0.379921036381695</li>\n",
       "\t<li>0.0158047619047619</li>\n",
       "\t<li>0.00509090909090909</li>\n",
       "\t<li>0.540346084545982</li>\n",
       "\t<li>0.051837037037037</li>\n",
       "\t<li>0.3089820785677</li>\n",
       "\t<li>0.367711224104156</li>\n",
       "\t<li>0.307965492429222</li>\n",
       "\t<li>0.238451413027758</li>\n",
       "\t<li>0.77765227334506</li>\n",
       "\t<li>0.575820802620802</li>\n",
       "\t<li>0.0733235294117647</li>\n",
       "\t<li>0.0016</li>\n",
       "\t<li>0.0384962370962371</li>\n",
       "\t<li>0.299365504309043</li>\n",
       "\t<li>0.155018217260323</li>\n",
       "\t<li>0.0672619047619048</li>\n",
       "\t<li>0.46806110634685</li>\n",
       "\t<li>0.114472421695951</li>\n",
       "\t<li>0.620615416911482</li>\n",
       "\t<li>0.200309037855656</li>\n",
       "\t<li>0.270726019802373</li>\n",
       "\t<li>0.174452762703832</li>\n",
       "\t<li>0.587592527390301</li>\n",
       "\t<li>0.199300398780202</li>\n",
       "\t<li>0.0102448979591837</li>\n",
       "\t<li>0.0397285714285714</li>\n",
       "\t<li>0.0162666666666667</li>\n",
       "\t<li>0.0114666666666667</li>\n",
       "\t<li>0.195693463730689</li>\n",
       "\t<li>0.201188496217982</li>\n",
       "\t<li>0.393090790408653</li>\n",
       "\t<li>0.310266666666667</li>\n",
       "\t<li>0.0288448979591837</li>\n",
       "\t<li>0.109350793650794</li>\n",
       "\t<li>0.526247670438228</li>\n",
       "\t<li>0.711136294459992</li>\n",
       "\t<li>0.377678952147725</li>\n",
       "\t<li>0.425689399257037</li>\n",
       "\t<li>0.0698122028914482</li>\n",
       "\t<li>0.0600952380952381</li>\n",
       "\t<li>0.473020623873165</li>\n",
       "\t<li>0.0322267806267806</li>\n",
       "\t<li>0.0808721393034826</li>\n",
       "\t<li>0.784359690569302</li>\n",
       "\t<li>0.437332651057119</li>\n",
       "\t<li>0.652655224508978</li>\n",
       "\t<li>0.0578666666666667</li>\n",
       "\t<li>0.03442030651341</li>\n",
       "\t<li>0.0703958041958042</li>\n",
       "\t<li>0.21453961981213</li>\n",
       "\t<li>0.100506293706294</li>\n",
       "\t<li>0.679990166363317</li>\n",
       "\t<li>0.251027323722086</li>\n",
       "\t<li>0.409398757763975</li>\n",
       "\t<li>0.0698380952380952</li>\n",
       "\t<li>0.250403572359249</li>\n",
       "\t<li>0.0121380952380952</li>\n",
       "\t<li>0.0570124015418133</li>\n",
       "\t<li>0.368781249022565</li>\n",
       "\t<li>0.00433333333333333</li>\n",
       "\t<li>0.00822222222222222</li>\n",
       "\t<li>0.294180077417447</li>\n",
       "\t<li>0.0293575757575758</li>\n",
       "\t<li>0.00322424242424242</li>\n",
       "\t<li>0.0991636363636363</li>\n",
       "\t<li>0.351744250426449</li>\n",
       "\t<li>0.257699518506827</li>\n",
       "\t<li>0.255360956057483</li>\n",
       "\t<li>0.21061965666277</li>\n",
       "\t<li>0.342812896325479</li>\n",
       "\t<li>0.0142909090909091</li>\n",
       "\t<li>0.42633098475994</li>\n",
       "\t<li>0.73804566827312</li>\n",
       "\t<li>0.229479614256193</li>\n",
       "\t<li>0.593167357105254</li>\n",
       "\t<li>0.572870918258768</li>\n",
       "\t<li>0.245210611004808</li>\n",
       "\t<li>0.114496825396825</li>\n",
       "\t<li>0.0745897435897436</li>\n",
       "\t<li>0.241977230232595</li>\n",
       "\t<li>0.0665761904761905</li>\n",
       "\t<li>0.356498184074626</li>\n",
       "\t<li>0.541662314184965</li>\n",
       "\t<li>0.537599046032529</li>\n",
       "\t<li>0.054593837535014</li>\n",
       "\t<li>0.0462564102564103</li>\n",
       "\t<li>0.415336751592259</li>\n",
       "\t<li>0.524927851005866</li>\n",
       "\t<li>0.00467777777777778</li>\n",
       "\t<li>0.475826385918094</li>\n",
       "\t<li>0.298654006808713</li>\n",
       "\t<li>0.855227625357486</li>\n",
       "\t<li>0.156878479375031</li>\n",
       "\t<li>0.526695958421396</li>\n",
       "\t<li>0.633782167395233</li>\n",
       "\t<li>0.658225612556666</li>\n",
       "\t<li>0.0858749979905152</li>\n",
       "\t<li>0.278734372594227</li>\n",
       "\t<li>0.194383150874964</li>\n",
       "\t<li>0.244549500497125</li>\n",
       "\t<li>0.448434606708141</li>\n",
       "\t<li>0.0427064935064935</li>\n",
       "\t<li>0.809052842494791</li>\n",
       "\t<li>0.232185414585415</li>\n",
       "\t<li>0.036</li>\n",
       "\t<li>0.194143083738472</li>\n",
       "\t<li>0.477569492304233</li>\n",
       "\t<li>0.147252756393933</li>\n",
       "\t<li>0.21487685482351</li>\n",
       "\t<li>0.372432718280477</li>\n",
       "\t<li>0.184978190739358</li>\n",
       "\t<li>0.1006</li>\n",
       "\t<li>0.513453282164648</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.162217543859649</li>\n",
       "\t<li>0.208779351650147</li>\n",
       "\t<li>0.237044627023114</li>\n",
       "\t<li>0.474138448593901</li>\n",
       "\t<li>0.29975804069693</li>\n",
       "\t<li>0.604209224874742</li>\n",
       "\t<li>0.122096914700544</li>\n",
       "\t<li>0.466259836896186</li>\n",
       "\t<li>0.0594948306595365</li>\n",
       "\t<li>0.317124363814219</li>\n",
       "\t<li>0.220421340446081</li>\n",
       "\t<li>0.0895954699121028</li>\n",
       "\t<li>0.692376534300207</li>\n",
       "\t<li>0.437216273156773</li>\n",
       "\t<li>0.239452729415019</li>\n",
       "\t<li>0.00766666666666667</li>\n",
       "\t<li>0.0688725274725275</li>\n",
       "\t<li>0.0409777777777778</li>\n",
       "\t<li>0.051658608058608</li>\n",
       "\t<li>0.827669993834232</li>\n",
       "\t<li>0.256817988778487</li>\n",
       "\t<li>0.135108371040724</li>\n",
       "\t<li>0.147487914230019</li>\n",
       "\t<li>0.0393908496732026</li>\n",
       "\t<li>0.191258657995136</li>\n",
       "\t<li>0.170636590222294</li>\n",
       "\t<li>0.0640787620924583</li>\n",
       "\t<li>0.304324378077303</li>\n",
       "\t<li>0.0631428571428572</li>\n",
       "\t<li>0.422405163006667</li>\n",
       "\t<li>0.0111333333333333</li>\n",
       "\t<li>0.482947030955207</li>\n",
       "\t<li>0.108928515928516</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.582924486417423</li>\n",
       "\t<li>0.669187156890966</li>\n",
       "\t<li>0.195216707870222</li>\n",
       "\t<li>0.119292722371968</li>\n",
       "\t<li>0.0868337068160597</li>\n",
       "\t<li>0.0808148459383754</li>\n",
       "\t<li>0.373955251864044</li>\n",
       "\t<li>0.0430459096459096</li>\n",
       "\t<li>0.367033420004021</li>\n",
       "\t<li>0.115671100164204</li>\n",
       "\t<li>0.0130147186147186</li>\n",
       "\t<li>0.161323099164703</li>\n",
       "\t<li>0.431120396557889</li>\n",
       "\t<li>0.107273400673401</li>\n",
       "\t<li>0.56055297739209</li>\n",
       "\t<li>0.625714444208014</li>\n",
       "\t<li>0.251417791468039</li>\n",
       "\t<li>0.551041303289911</li>\n",
       "\t<li>0.248545818387376</li>\n",
       "\t<li>0.3295071523647</li>\n",
       "\t<li>0.572191952414778</li>\n",
       "\t<li>0.0928666666666667</li>\n",
       "\t<li>0.443179929067408</li>\n",
       "\t<li>0.290382491582492</li>\n",
       "\t<li>0.197602196406712</li>\n",
       "\t<li>0.145585847485847</li>\n",
       "\t<li>0.495879747696854</li>\n",
       "\t<li>0.568835176867885</li>\n",
       "\t<li>0.178825097125097</li>\n",
       "\t<li>0.263006722689076</li>\n",
       "\t<li>0.0122666666666667</li>\n",
       "\t<li>0.204530764277533</li>\n",
       "\t<li>0.276815621836675</li>\n",
       "\t<li>0.0919473193473193</li>\n",
       "\t<li>0.285776930746126</li>\n",
       "\t<li>0.0861026915113871</li>\n",
       "\t<li>0.365720549442655</li>\n",
       "\t<li>0.0246666666666667</li>\n",
       "\t<li>0.341597836374044</li>\n",
       "\t<li>0.0703703703703704</li>\n",
       "\t<li>0.226417106965196</li>\n",
       "\t<li>0.145939853626176</li>\n",
       "\t<li>0.0578666666666667</li>\n",
       "\t<li>0.0203358070500928</li>\n",
       "\t<li>0.102344897959184</li>\n",
       "\t<li>0.257191002355077</li>\n",
       "\t<li>0.380270430813124</li>\n",
       "\t<li>0.22801255729577</li>\n",
       "\t<li>0.0496</li>\n",
       "\t<li>0.0243550524687487</li>\n",
       "\t<li>0.0796714285714286</li>\n",
       "\t<li>0.30290950428222</li>\n",
       "\t<li>0.089566905363544</li>\n",
       "\t<li>0.157804761904762</li>\n",
       "\t<li>0.193304568404978</li>\n",
       "\t<li>0.327884993163203</li>\n",
       "\t<li>0.188250444061209</li>\n",
       "\t<li>0.285145124113595</li>\n",
       "\t<li>0.100728571428571</li>\n",
       "\t<li>0.160782740375844</li>\n",
       "\t<li>0.0167884057971014</li>\n",
       "\t<li>0.231235686066859</li>\n",
       "\t<li>0.0508666666666667</li>\n",
       "\t<li>0.0850083550661644</li>\n",
       "\t<li>0.0765111111111111</li>\n",
       "\t<li>0.0360731601731602</li>\n",
       "\t<li>0.419970039631746</li>\n",
       "\t<li>0.0469543988739774</li>\n",
       "\t<li>0.0241040293040293</li>\n",
       "\t<li>0.67258345763243</li>\n",
       "\t<li>0.526772102040275</li>\n",
       "\t<li>0.633922369109906</li>\n",
       "\t<li>0.122292661306947</li>\n",
       "\t<li>0.170855978803628</li>\n",
       "\t<li>0.130204650852884</li>\n",
       "\t<li>0.264960078116749</li>\n",
       "\t<li>0.00929090909090909</li>\n",
       "\t<li>0.0296571428571429</li>\n",
       "\t<li>0.27537588885422</li>\n",
       "\t<li>0.0365619047619048</li>\n",
       "\t<li>0.0838814814814815</li>\n",
       "\t<li>0.4780178032596</li>\n",
       "\t<li>0.40745326915483</li>\n",
       "\t<li>0.315968944272752</li>\n",
       "\t<li>0.0460666666666667</li>\n",
       "\t<li>0.0749979171771625</li>\n",
       "\t<li>0.643386147970091</li>\n",
       "\t<li>0.503910814401044</li>\n",
       "\t<li>0.0305651662063427</li>\n",
       "\t<li>0.128120613051956</li>\n",
       "\t<li>0.239764250808907</li>\n",
       "\t<li>0.2464965057763</li>\n",
       "\t<li>0.808017646322295</li>\n",
       "\t<li>0.712779408041</li>\n",
       "\t<li>0.335160432306445</li>\n",
       "\t<li>0.218738630654148</li>\n",
       "\t<li>0.00213333333333333</li>\n",
       "\t<li>0.0358</li>\n",
       "\t<li>0.502336046085691</li>\n",
       "\t<li>0.283245683728037</li>\n",
       "\t<li>0.0552761686075119</li>\n",
       "\t<li>0.422017850833885</li>\n",
       "\t<li>0.02720853432282</li>\n",
       "\t<li>0.109115909918344</li>\n",
       "\t<li>0.330525630675507</li>\n",
       "\t<li>0.43256635663063</li>\n",
       "\t<li>0.79601703297908</li>\n",
       "\t<li>0.358075324599865</li>\n",
       "\t<li>0.35064557697692</li>\n",
       "\t<li>0.139632051473871</li>\n",
       "\t<li>0.819280051850075</li>\n",
       "\t<li>0.0324</li>\n",
       "\t<li>0.0297039215686275</li>\n",
       "\t<li>0.351741122153351</li>\n",
       "\t<li>0.0433333333333333</li>\n",
       "\t<li>0.225181928350018</li>\n",
       "\t<li>0.308127300339668</li>\n",
       "\t<li>0.0231741076533529</li>\n",
       "\t<li>0.0617448801742919</li>\n",
       "\t<li>0.390556047016344</li>\n",
       "\t<li>0.0494229691876751</li>\n",
       "\t<li>0.102529310344828</li>\n",
       "\t<li>0.385699812602209</li>\n",
       "\t<li>0.0902761904761905</li>\n",
       "\t<li>0.422158145200042</li>\n",
       "\t<li>0.0518428571428571</li>\n",
       "\t<li>0.0448</li>\n",
       "\t<li>0.427318604533966</li>\n",
       "\t<li>0.0626666666666667</li>\n",
       "\t<li>0.300187172971319</li>\n",
       "\t<li>0.590423419714938</li>\n",
       "\t<li>0.337406643222685</li>\n",
       "\t<li>0.251815816530413</li>\n",
       "\t<li>0.0184242424242424</li>\n",
       "\t<li>0.040888198757764</li>\n",
       "\t<li>0.0682119174942704</li>\n",
       "\t<li>0.446553384055558</li>\n",
       "\t<li>0.0181931677018634</li>\n",
       "\t<li>0.0421333333333333</li>\n",
       "\t<li>0.0714842572497745</li>\n",
       "\t<li>0.00566666666666667</li>\n",
       "\t<li>0.372760666433911</li>\n",
       "\t<li>0.0704874458874459</li>\n",
       "\t<li>0.546724902835585</li>\n",
       "\t<li>0.154972089314195</li>\n",
       "\t<li>0.21515633364454</li>\n",
       "\t<li>0.731486138052025</li>\n",
       "\t<li>0.303174705009781</li>\n",
       "\t<li>0.556912440704805</li>\n",
       "\t<li>0.02880853432282</li>\n",
       "\t<li>0.68762645898416</li>\n",
       "\t<li>0.631088747276892</li>\n",
       "\t<li>0.291282484948002</li>\n",
       "\t<li>0.0409713879681124</li>\n",
       "\t<li>0.226215568600665</li>\n",
       "\t<li>0.38698600377361</li>\n",
       "\t<li>0.00946666666666667</li>\n",
       "\t<li>0.270738610408377</li>\n",
       "\t<li>0.48055685617084</li>\n",
       "\t<li>0.264662890731995</li>\n",
       "\t<li>0.544753775571682</li>\n",
       "\t<li>0.0512095238095238</li>\n",
       "\t<li>0.0368659768384966</li>\n",
       "\t<li>0.589843703943426</li>\n",
       "\t<li>0.00633333333333333</li>\n",
       "\t<li>0.149412822175022</li>\n",
       "\t<li>0.0962490028490029</li>\n",
       "\t<li>0.175590482506</li>\n",
       "\t<li>0.0204595365418895</li>\n",
       "\t<li>0.330935438341927</li>\n",
       "\t<li>0.36837963138542</li>\n",
       "\t<li>0.00209090909090909</li>\n",
       "\t<li>0.196189743589744</li>\n",
       "\t<li>0.37747921222951</li>\n",
       "\t<li>0.674150171699797</li>\n",
       "\t<li>0.588513467924093</li>\n",
       "\t<li>0.17639066039779</li>\n",
       "\t<li>0.269172343714798</li>\n",
       "\t<li>0.213655305949585</li>\n",
       "\t<li>0.0186095238095238</li>\n",
       "\t<li>0.545525636802509</li>\n",
       "\t<li>0.158670588235294</li>\n",
       "\t<li>0.00466666666666667</li>\n",
       "\t<li>0.237250675472075</li>\n",
       "\t<li>0.0629230769230769</li>\n",
       "\t<li>0.0241676767676768</li>\n",
       "\t<li>0.230720018354539</li>\n",
       "\t<li>0.486711145925037</li>\n",
       "\t<li>0.223129877036028</li>\n",
       "\t<li>0.151466005022699</li>\n",
       "\t<li>0.669874251341531</li>\n",
       "\t<li>0.418571734587252</li>\n",
       "\t<li>0.0190091954022989</li>\n",
       "\t<li>0.00943330375628512</li>\n",
       "\t<li>0.1595184278895</li>\n",
       "\t<li>0.112942857142857</li>\n",
       "\t<li>0.236210827746021</li>\n",
       "\t<li>0.010578231292517</li>\n",
       "\t<li>0.193046918172156</li>\n",
       "\t<li>0.363330145081496</li>\n",
       "\t<li>0.233797144352314</li>\n",
       "\t<li>8e-04</li>\n",
       "\t<li>0.38461456012582</li>\n",
       "\t<li>0.0407333333333333</li>\n",
       "\t<li>0.167713804713805</li>\n",
       "\t<li>0.335765623012756</li>\n",
       "\t<li>0.0341247794811705</li>\n",
       "\t<li>0.058774358974359</li>\n",
       "\t<li>0.29656016885274</li>\n",
       "\t<li>0.0757377084167</li>\n",
       "\t<li>0.661128647775432</li>\n",
       "\t<li>0.486335239752585</li>\n",
       "\t<li>0.274125876299378</li>\n",
       "\t<li>0.398757004765239</li>\n",
       "\t<li>0.312901092470678</li>\n",
       "\t<li>0.0167878787878788</li>\n",
       "\t<li>0.49212230802782</li>\n",
       "\t<li>0.417712580646714</li>\n",
       "\t<li>0.368876819281209</li>\n",
       "\t<li>0.399285034115656</li>\n",
       "\t<li>0.358669524090498</li>\n",
       "\t<li>0.0128</li>\n",
       "\t<li>0.317530757368029</li>\n",
       "\t<li>0.198690318213617</li>\n",
       "\t<li>0.0969715728715729</li>\n",
       "\t<li>0.314874138665033</li>\n",
       "\t<li>0.210215561198177</li>\n",
       "\t<li>0.547314875872405</li>\n",
       "\t<li>0.16773030602238</li>\n",
       "\t<li>0.043155587228107</li>\n",
       "\t<li>0.0465238095238095</li>\n",
       "\t<li>0.302429760258227</li>\n",
       "\t<li>0.0102242424242424</li>\n",
       "\t<li>0.783340565198175</li>\n",
       "\t<li>0.187846866202753</li>\n",
       "\t<li>0.345716502198186</li>\n",
       "\t<li>0.259585062890326</li>\n",
       "\t<li>0.213997280585225</li>\n",
       "\t<li>0.507869077340633</li>\n",
       "\t<li>0.175453119868637</li>\n",
       "\t<li>0.414077009493443</li>\n",
       "\t<li>0.316329633371636</li>\n",
       "\t<li>0.384129505114509</li>\n",
       "\t<li>0.33053247964253</li>\n",
       "\t<li>0.256483765463015</li>\n",
       "\t<li>0.116038211146528</li>\n",
       "\t<li>0.035851355661882</li>\n",
       "\t<li>0.0248974232117089</li>\n",
       "\t<li>0.0848666666666666</li>\n",
       "\t<li>0.207570096225025</li>\n",
       "\t<li>0.268501287085481</li>\n",
       "\t<li>0.0632095238095238</li>\n",
       "\t<li>0.25781656063235</li>\n",
       "\t<li>0.0502459096459096</li>\n",
       "\t<li>0.0831904761904762</li>\n",
       "\t<li>0.0328032924190819</li>\n",
       "\t<li>0.242366292328361</li>\n",
       "\t<li>0.454527494236908</li>\n",
       "\t<li>0.0289380952380952</li>\n",
       "\t<li>0.427383136581075</li>\n",
       "\t<li>0.785156169090612</li>\n",
       "\t<li>0.0652</li>\n",
       "\t<li>0.447523396573703</li>\n",
       "\t<li>0.189641133579641</li>\n",
       "\t<li>0.0413380952380952</li>\n",
       "\t<li>0.146168975468975</li>\n",
       "\t<li>0.511608303418371</li>\n",
       "\t<li>0.0105575757575758</li>\n",
       "\t<li>0.684172588569353</li>\n",
       "\t<li>0.142118803418803</li>\n",
       "\t<li>0.0595370515841843</li>\n",
       "\t<li>0.656540648053628</li>\n",
       "\t<li>0.668990329836979</li>\n",
       "\t<li>0.108187426694816</li>\n",
       "\t<li>0.0119358070500928</li>\n",
       "\t<li>0.392743430515844</li>\n",
       "\t<li>0.223110197439574</li>\n",
       "\t<li>0.543278103035985</li>\n",
       "\t<li>0.277935719686639</li>\n",
       "\t<li>0.389467782217782</li>\n",
       "\t<li>0.39109015811024</li>\n",
       "\t<li>0.368649629375681</li>\n",
       "\t<li>0.231261163081585</li>\n",
       "\t<li>0.031578231292517</li>\n",
       "\t<li>0.661937946552801</li>\n",
       "\t<li>0.0328666666666667</li>\n",
       "\t<li>0.291528407340625</li>\n",
       "\t<li>0.0371926406926407</li>\n",
       "\t<li>0.632235050781604</li>\n",
       "\t<li>0.0629591836734694</li>\n",
       "\t<li>0.209477182680085</li>\n",
       "\t<li>0.20159488046791</li>\n",
       "\t<li>0.0161333333333333</li>\n",
       "\t<li>0.318255149005995</li>\n",
       "\t<li>0.517569014595609</li>\n",
       "\t<li>0.0742421245421245</li>\n",
       "\t<li>0.020978231292517</li>\n",
       "\t<li>0.621984153615674</li>\n",
       "\t<li>0.134348891459418</li>\n",
       "\t<li>0.211825940473073</li>\n",
       "\t<li>0.514845440695551</li>\n",
       "\t<li>0.0434428571428571</li>\n",
       "\t<li>0.22194794062147</li>\n",
       "\t<li>0.372580601195712</li>\n",
       "\t<li>0.0650410256410256</li>\n",
       "\t<li>0.0517</li>\n",
       "\t<li>0.23162295662591</li>\n",
       "\t<li>0.324765915803077</li>\n",
       "\t<li>0.150927674624226</li>\n",
       "\t<li>0.596247355676311</li>\n",
       "\t<li>0.368936985923394</li>\n",
       "\t<li>0.305086965500248</li>\n",
       "\t<li>0.00426233766233766</li>\n",
       "\t<li>0.264110301120331</li>\n",
       "\t<li>0.424242617232572</li>\n",
       "\t<li>0.00784489795918367</li>\n",
       "\t<li>0.475891914981982</li>\n",
       "\t<li>0.382656273186503</li>\n",
       "\t<li>0.662802837499407</li>\n",
       "\t<li>0.150482051282051</li>\n",
       "\t<li>0.682377327830452</li>\n",
       "\t<li>0.0924564102564103</li>\n",
       "\t<li>0.045995670995671</li>\n",
       "\t<li>0.0108</li>\n",
       "\t<li>0.702087492624913</li>\n",
       "\t<li>0.379392145539237</li>\n",
       "\t<li>0.001</li>\n",
       "\t<li>0.0559300366300366</li>\n",
       "\t<li>0.0210909090909091</li>\n",
       "\t<li>0.0730904761904762</li>\n",
       "\t<li>0.120644289044289</li>\n",
       "\t<li>0.382560243451559</li>\n",
       "\t<li>0.428814084119177</li>\n",
       "\t<li>0.146867040149393</li>\n",
       "\t<li>0.827661620155868</li>\n",
       "\t<li>0.0658507936507936</li>\n",
       "\t<li>0.186260212093377</li>\n",
       "\t<li>0.748145069080872</li>\n",
       "\t<li>0.00766666666666667</li>\n",
       "\t<li>0.144147362499376</li>\n",
       "\t<li>0.367816674617236</li>\n",
       "\t<li>0.249503010399562</li>\n",
       "\t<li>0.270551945145766</li>\n",
       "\t<li>0.232542773974768</li>\n",
       "\t<li>0.494969044686016</li>\n",
       "\t<li>0.730202050551766</li>\n",
       "\t<li>0.588403663316362</li>\n",
       "\t<li>0.371865109186483</li>\n",
       "\t<li>0.208005259239406</li>\n",
       "\t<li>0.195637169043863</li>\n",
       "\t<li>0.160361701261701</li>\n",
       "\t<li>0.0193714285714286</li>\n",
       "\t<li>0.0158666666666667</li>\n",
       "\t<li>0.0280538461538462</li>\n",
       "\t<li>0.729376768112002</li>\n",
       "\t<li>0.244041215973569</li>\n",
       "\t<li>0.118790849673203</li>\n",
       "\t<li>0.106571717171717</li>\n",
       "\t<li>0.433991872100007</li>\n",
       "\t<li>0.0608857142857143</li>\n",
       "\t<li>0.0691333333333333</li>\n",
       "\t<li>0.65000191022047</li>\n",
       "\t<li>0.651639088112206</li>\n",
       "\t<li>0.439207165959861</li>\n",
       "\t<li>0.346891056032109</li>\n",
       "\t<li>0.29402385335275</li>\n",
       "\t<li>0.00659815546772069</li>\n",
       "\t<li>0.0982682431995865</li>\n",
       "\t<li>0.0739904761904762</li>\n",
       "\t<li>0.761544699197131</li>\n",
       "\t<li>0.00649090909090909</li>\n",
       "\t<li>0.107051307847005</li>\n",
       "\t<li>0.349951635559961</li>\n",
       "\t<li>0.433959200461904</li>\n",
       "\t<li>0.0274949096880131</li>\n",
       "\t<li>0.0141333333333333</li>\n",
       "\t<li>0.235675316496369</li>\n",
       "\t<li>0.474336717738204</li>\n",
       "\t<li>0.709188648804476</li>\n",
       "\t<li>0.327763742506913</li>\n",
       "\t<li>0.0388714285714286</li>\n",
       "\t<li>0.2240654584846</li>\n",
       "\t<li>0.346826801367435</li>\n",
       "\t<li>0.757722394429666</li>\n",
       "\t<li>0.0300666666666667</li>\n",
       "\t<li>0.199370531767083</li>\n",
       "\t<li>0.600581905761181</li>\n",
       "\t<li>0.368189963898077</li>\n",
       "\t<li>0.201154013491749</li>\n",
       "\t<li>0.668778506396282</li>\n",
       "\t<li>0.0204</li>\n",
       "\t<li>0.154136507936508</li>\n",
       "\t<li>0.672268360737523</li>\n",
       "\t<li>0.157450273855882</li>\n",
       "\t<li>0.000235294117647059</li>\n",
       "\t<li>0.302083590675208</li>\n",
       "\t<li>0.140525925925926</li>\n",
       "\t<li>0.0714481792717087</li>\n",
       "\t<li>0.608711319649256</li>\n",
       "\t<li>0.112314285714286</li>\n",
       "\t<li>0.333080615800697</li>\n",
       "\t<li>0.00973333333333333</li>\n",
       "\t<li>0.263663806950509</li>\n",
       "\t<li>0.006</li>\n",
       "\t<li>0.0402598219390672</li>\n",
       "\t<li>0.0670656990068755</li>\n",
       "\t<li>0.106415586712138</li>\n",
       "\t<li>0.0468166666666667</li>\n",
       "\t<li>0.0395619047619048</li>\n",
       "\t<li>0.0919690476190476</li>\n",
       "\t<li>0.0633897435897436</li>\n",
       "\t<li>0.0445566923497958</li>\n",
       "\t<li>0.0189047619047619</li>\n",
       "\t<li>0.115769140383426</li>\n",
       "\t<li>0.0618047619047619</li>\n",
       "\t<li>0.0203686274509804</li>\n",
       "\t<li>0.00142424242424242</li>\n",
       "\t<li>0.38342849197586</li>\n",
       "\t<li>0.852856601307143</li>\n",
       "\t<li>0.260003782953814</li>\n",
       "\t<li>0.0663207600281492</li>\n",
       "\t<li>0.0197575757575758</li>\n",
       "\t<li>0.499076422777154</li>\n",
       "\t<li>0.227236408036408</li>\n",
       "\t<li>0.128069024840808</li>\n",
       "\t<li>0.604572949178595</li>\n",
       "\t<li>0.6104474259193</li>\n",
       "\t<li>0.0547285714285714</li>\n",
       "\t<li>0.538792271155307</li>\n",
       "\t<li>0.0779490842490843</li>\n",
       "\t<li>0.0194666666666667</li>\n",
       "\t<li>0.687524851748526</li>\n",
       "\t<li>0.248955861279166</li>\n",
       "\t<li>0.425913216230292</li>\n",
       "\t<li>0.261883747789335</li>\n",
       "\t<li>0.301614519580037</li>\n",
       "\t<li>0.227101960784314</li>\n",
       "\t<li>0.0228</li>\n",
       "\t<li>0.458696448784663</li>\n",
       "\t<li>0.0623428571428571</li>\n",
       "\t<li>0.0670854700854701</li>\n",
       "\t<li>0.329848336813399</li>\n",
       "\t<li>0.374627178391022</li>\n",
       "\t<li>0.42265869101895</li>\n",
       "\t<li>0.117145064086241</li>\n",
       "\t<li>0.670749969824215</li>\n",
       "\t<li>0.379973106276359</li>\n",
       "\t<li>0.220000872163182</li>\n",
       "\t<li>0.148253374076904</li>\n",
       "\t<li>0.0697224089635854</li>\n",
       "\t<li>0.391243062271746</li>\n",
       "\t<li>0.0974380952380952</li>\n",
       "\t<li>0.00289090909090909</li>\n",
       "\t<li>0.447340962760438</li>\n",
       "\t<li>0.352059064356158</li>\n",
       "\t<li>0.328763576174391</li>\n",
       "\t<li>0.00995757575757576</li>\n",
       "\t<li>0.0564047619047619</li>\n",
       "\t<li>0.499517306054529</li>\n",
       "\t<li>0.264267144492313</li>\n",
       "\t<li>0.648435212339</li>\n",
       "\t<li>0.0607229524288348</li>\n",
       "\t<li>0.459917647058823</li>\n",
       "\t<li>0.0934151882975412</li>\n",
       "\t<li>0.814013599147317</li>\n",
       "\t<li>0.00746666666666667</li>\n",
       "\t<li>0.027237037037037</li>\n",
       "\t<li>0.278787963272301</li>\n",
       "\t<li>0.644552081252081</li>\n",
       "\t<li>0.706449246404825</li>\n",
       "\t<li>0.0048</li>\n",
       "\t<li>0.0580745098039216</li>\n",
       "\t<li>0.0184047619047619</li>\n",
       "\t<li>0.219033211233211</li>\n",
       "\t<li>0.362470971727596</li>\n",
       "\t<li>0.00954285714285714</li>\n",
       "\t<li>0.865554624940664</li>\n",
       "\t<li>0.604889026290867</li>\n",
       "\t<li>0.45710416171468</li>\n",
       "\t<li>0.321347386013069</li>\n",
       "\t<li>0.556274691057301</li>\n",
       "\t<li>0.144271763175016</li>\n",
       "\t<li>0.536879351917059</li>\n",
       "\t<li>0.233402319335823</li>\n",
       "\t<li>0.10163894993895</li>\n",
       "\t<li>0.212447939519923</li>\n",
       "\t<li>0.230654565473356</li>\n",
       "\t<li>0.413968897690306</li>\n",
       "\t<li>0.19102449294001</li>\n",
       "\t<li>0.319777827286465</li>\n",
       "\t<li>0.291493981579849</li>\n",
       "\t<li>0.00809090909090909</li>\n",
       "\t<li>0.148263362156457</li>\n",
       "\t<li>0.002</li>\n",
       "\t<li>0.474988858590471</li>\n",
       "\t<li>0.293499554269232</li>\n",
       "\t<li>0.133104761904762</li>\n",
       "\t<li>0.4678877201779</li>\n",
       "\t<li>0.0895974842767296</li>\n",
       "\t<li>0.174179310344828</li>\n",
       "\t<li>0.301087263839736</li>\n",
       "\t<li>0.0460444444444444</li>\n",
       "\t<li>0.267754674263379</li>\n",
       "\t<li>0.12642614379085</li>\n",
       "\t<li>0.0196242424242424</li>\n",
       "\t<li>0.537195793473164</li>\n",
       "\t<li>0.0935333333333333</li>\n",
       "\t<li>0.0628047619047619</li>\n",
       "\t<li>0.0784666666666667</li>\n",
       "\t<li>0.347248441551687</li>\n",
       "\t<li>0.243234312177997</li>\n",
       "\t<li>0.0110024737167594</li>\n",
       "\t<li>0.389838466243758</li>\n",
       "\t<li>0.492113499292125</li>\n",
       "\t<li>0.027778231292517</li>\n",
       "\t<li>0.514638372923188</li>\n",
       "\t<li>0.126111111111111</li>\n",
       "\t<li>0.0444206349206349</li>\n",
       "\t<li>0.252349997713857</li>\n",
       "\t<li>0.280270562056559</li>\n",
       "\t<li>0.319795079305615</li>\n",
       "\t<li>0.0180242424242424</li>\n",
       "\t<li>0.183724018838305</li>\n",
       "\t<li>0.0775756187783935</li>\n",
       "\t<li>0.383244588744589</li>\n",
       "\t<li>0.175342857142857</li>\n",
       "\t<li>0.0135931677018634</li>\n",
       "\t<li>0.0409020408163265</li>\n",
       "\t<li>0.335437364471853</li>\n",
       "\t<li>0.454953809948352</li>\n",
       "\t<li>0.810441547633601</li>\n",
       "\t<li>0.0352747474747475</li>\n",
       "\t<li>0.211952941176471</li>\n",
       "\t<li>0.0235285714285714</li>\n",
       "\t<li>0.457761027950668</li>\n",
       "\t<li>0.212335246282094</li>\n",
       "\t<li>0.160738095238095</li>\n",
       "\t<li>0.0646730158730159</li>\n",
       "\t<li>0.399322159631053</li>\n",
       "\t<li>0.171839071603778</li>\n",
       "\t<li>0.052356862745098</li>\n",
       "\t<li>0.0319560090702948</li>\n",
       "\t<li>0.335196471069994</li>\n",
       "\t<li>0.190525165001189</li>\n",
       "\t<li>0.00909090909090909</li>\n",
       "\t<li>0.0393619047619048</li>\n",
       "\t<li>0.432405458834251</li>\n",
       "\t<li>0.580810988130229</li>\n",
       "\t<li>0.143869741369741</li>\n",
       "\t<li>0.0404943504256937</li>\n",
       "\t<li>9.09090909090909e-05</li>\n",
       "\t<li>0.41961652229829</li>\n",
       "\t<li>0.0870783103254431</li>\n",
       "\t<li>0.208521699193482</li>\n",
       "\t<li>0.0914508899709711</li>\n",
       "\t<li>0.507317141920146</li>\n",
       "\t<li>0.0702642857142857</li>\n",
       "\t<li>0.230625986019151</li>\n",
       "\t<li>0.123316172463305</li>\n",
       "\t<li>0.486371238702818</li>\n",
       "\t<li>0.127573405211141</li>\n",
       "\t<li>0.0808883933676387</li>\n",
       "\t<li>0.655226094955746</li>\n",
       "\t<li>0.640896748726266</li>\n",
       "\t<li>0.120290708007341</li>\n",
       "\t<li>0.418326940814628</li>\n",
       "\t<li>0.174287601034734</li>\n",
       "\t<li>0.0178337662337662</li>\n",
       "\t<li>0.0485200241671569</li>\n",
       "\t<li>0.166570586584339</li>\n",
       "\t<li>0.223664595976089</li>\n",
       "\t<li>0.360729834841786</li>\n",
       "\t<li>0.22123163725692</li>\n",
       "\t<li>0.483856692740181</li>\n",
       "\t<li>0.0284039215686275</li>\n",
       "\t<li>0.217811970217412</li>\n",
       "\t<li>0.637398736824343</li>\n",
       "\t<li>0.775120870356815</li>\n",
       "\t<li>0.121545982906221</li>\n",
       "\t<li>0.0895825396825397</li>\n",
       "\t<li>0.00440952380952381</li>\n",
       "\t<li>0.378986217358631</li>\n",
       "\t<li>0.0328405797101449</li>\n",
       "\t<li>0.0235205128205128</li>\n",
       "\t<li>0.663065234842642</li>\n",
       "\t<li>0.1168</li>\n",
       "\t<li>0.185046405228758</li>\n",
       "\t<li>0.33281228668113</li>\n",
       "\t<li>0.729494346308559</li>\n",
       "\t<li>0.0794250377073906</li>\n",
       "\t<li>0.0691716603882932</li>\n",
       "\t<li>0.175877432869996</li>\n",
       "\t<li>0.0268047619047619</li>\n",
       "\t<li>0.421053451095202</li>\n",
       "\t<li>0.487261947186317</li>\n",
       "\t<li>0.640005404166208</li>\n",
       "\t<li>0.248536716178094</li>\n",
       "\t<li>0.122024242424242</li>\n",
       "\t<li>0.0637686274509804</li>\n",
       "\t<li>0.0780758907758907</li>\n",
       "\t<li>0.0116666666666667</li>\n",
       "\t<li>0.547958892884658</li>\n",
       "\t<li>0.421848057427041</li>\n",
       "\t<li>0.0413247225205872</li>\n",
       "\t<li>0.36810381275491</li>\n",
       "\t<li>0.0498666666666667</li>\n",
       "\t<li>0.677631688827879</li>\n",
       "\t<li>0.249111493724492</li>\n",
       "\t<li>0.735682519824392</li>\n",
       "\t<li>0.0276269841269841</li>\n",
       "\t<li>0.591329460068238</li>\n",
       "\t<li>0.440550995239788</li>\n",
       "\t<li>0.154649595687332</li>\n",
       "\t<li>0.0480047619047619</li>\n",
       "\t<li>0.235115653936707</li>\n",
       "\t<li>0.0869333333333333</li>\n",
       "\t<li>0.132469841269841</li>\n",
       "\t<li>0.507676108528792</li>\n",
       "\t<li>0.237856732801472</li>\n",
       "\t<li>0.00933333333333333</li>\n",
       "\t<li>0.250326537118351</li>\n",
       "\t<li>0.237531614062577</li>\n",
       "\t<li>0.0612571428571429</li>\n",
       "\t<li>0.0281230769230769</li>\n",
       "\t<li>0.543370069951374</li>\n",
       "\t<li>0.295349594632302</li>\n",
       "\t<li>0.195602748441943</li>\n",
       "\t<li>0.0141333333333333</li>\n",
       "\t<li>0.282436814327739</li>\n",
       "\t<li>0.214038128809445</li>\n",
       "\t<li>0.355551123924004</li>\n",
       "\t<li>0.199634717337053</li>\n",
       "\t<li>0.0886120082815735</li>\n",
       "\t<li>0.476181227928559</li>\n",
       "\t<li>0.313311821803927</li>\n",
       "\t<li>0.655482090457493</li>\n",
       "\t<li>0.0180909090909091</li>\n",
       "\t<li>0.188301565432909</li>\n",
       "\t<li>0.0731425287356322</li>\n",
       "\t<li>0.0162879869189055</li>\n",
       "\t<li>0.0262969696969697</li>\n",
       "\t<li>0.114137254580363</li>\n",
       "\t<li>0.195256132756133</li>\n",
       "\t<li>0.645082171530284</li>\n",
       "\t<li>0.0866</li>\n",
       "\t<li>0.173414285714286</li>\n",
       "\t<li>0.04620853432282</li>\n",
       "\t<li>0.729948878366983</li>\n",
       "\t<li>0.0715447293447293</li>\n",
       "\t<li>0.595765895398517</li>\n",
       "\t<li>0.0353555555555556</li>\n",
       "\t<li>0.0638862745098039</li>\n",
       "\t<li>0.155683641307982</li>\n",
       "\t<li>0.60683780633572</li>\n",
       "\t<li>0.524395606518497</li>\n",
       "\t<li>0.418284042284305</li>\n",
       "\t<li>0.646343786709744</li>\n",
       "\t<li>0.0231333333333333</li>\n",
       "\t<li>0.351036757660706</li>\n",
       "\t<li>0.584150542615313</li>\n",
       "\t<li>0.115877922077922</li>\n",
       "\t<li>0.0738506879478207</li>\n",
       "\t<li>0.638429688940546</li>\n",
       "\t<li>0.0088</li>\n",
       "\t<li>0.218502071516578</li>\n",
       "\t<li>0.236234709396882</li>\n",
       "\t<li>0.0561294264607698</li>\n",
       "\t<li>0.386173278994332</li>\n",
       "\t<li>0.122357142857143</li>\n",
       "\t<li>0.270787372063464</li>\n",
       "\t<li>0.512690389846322</li>\n",
       "\t<li>0.0201197583511016</li>\n",
       "\t<li>0.538740727771748</li>\n",
       "\t<li>0.0373230769230769</li>\n",
       "\t<li>0.557972159663091</li>\n",
       "\t<li>0.218904509525562</li>\n",
       "\t<li>0.430485959095843</li>\n",
       "\t<li>0.6906847732655</li>\n",
       "\t<li>0.0185422463893792</li>\n",
       "\t<li>0.0116913626056483</li>\n",
       "\t<li>0.0365115646258503</li>\n",
       "\t<li>0.697792762657136</li>\n",
       "\t<li>0.785361399116238</li>\n",
       "\t<li>0.0461333333333333</li>\n",
       "\t<li>0.006</li>\n",
       "\t<li>0.0286761904761905</li>\n",
       "\t<li>0.10196790986791</li>\n",
       "\t<li>0.0511050583628677</li>\n",
       "\t<li>0.0375675213675214</li>\n",
       "\t<li>0.0632798316111749</li>\n",
       "\t<li>0.643925769277783</li>\n",
       "\t<li>0.143593310657596</li>\n",
       "\t<li>0.0197217391304348</li>\n",
       "\t<li>0.509039841027984</li>\n",
       "\t<li>0.631491229789319</li>\n",
       "\t<li>0.15473932016072</li>\n",
       "\t<li>0.0619948868741321</li>\n",
       "\t<li>0.631510040245023</li>\n",
       "\t<li>0.0200285714285714</li>\n",
       "\t<li>0.40157776900883</li>\n",
       "\t<li>0.0524243705985835</li>\n",
       "\t<li>0.175101887372963</li>\n",
       "\t<li>0.00566666666666667</li>\n",
       "\t<li>0.243075635964627</li>\n",
       "\t<li>0.420006484119292</li>\n",
       "\t<li>0.109966666666667</li>\n",
       "\t<li>0.220755648745138</li>\n",
       "\t<li>0.0024</li>\n",
       "\t<li>0.513641429945596</li>\n",
       "\t<li>0.17123538716673</li>\n",
       "\t<li>0.00209090909090909</li>\n",
       "\t<li>0.557749673749674</li>\n",
       "\t<li>0.0597333333333333</li>\n",
       "\t<li>0.439517909276698</li>\n",
       "\t<li>0.454787856259751</li>\n",
       "\t<li>0.248552413254401</li>\n",
       "\t<li>0.134250549450549</li>\n",
       "\t<li>0.657388560901124</li>\n",
       "\t<li>0.111611111111111</li>\n",
       "\t<li>0.106639956122309</li>\n",
       "\t<li>0.345859708561882</li>\n",
       "\t<li>0.0540047619047619</li>\n",
       "\t<li>0.367525596681185</li>\n",
       "\t<li>0.593266379342398</li>\n",
       "\t<li>0.176785411067838</li>\n",
       "\t<li>0.0188</li>\n",
       "\t<li>0.290231616643269</li>\n",
       "\t<li>0.0480683982683983</li>\n",
       "\t<li>0.176752496860813</li>\n",
       "\t<li>0.00733333333333333</li>\n",
       "\t<li>0.50031639077822</li>\n",
       "\t<li>0.412294262644807</li>\n",
       "\t<li>0.518579294848777</li>\n",
       "\t<li>0.0261326007326007</li>\n",
       "\t<li>0.01905983436853</li>\n",
       "\t<li>0.0651991231275119</li>\n",
       "\t<li>0.72489130706926</li>\n",
       "\t<li>0.0268290043290043</li>\n",
       "\t<li>0.404482163427366</li>\n",
       "\t<li>0.0928116447493806</li>\n",
       "\t<li>0.0546692677070828</li>\n",
       "\t<li>0.140716596247331</li>\n",
       "\t<li>0.00679763177998472</li>\n",
       "\t<li>0.358955824179798</li>\n",
       "\t<li>0.0767428571428571</li>\n",
       "\t<li>0.532318555576708</li>\n",
       "\t<li>0.504254436513197</li>\n",
       "\t<li>0.518370962383905</li>\n",
       "\t<li>0.0338566473655403</li>\n",
       "\t<li>0.0302464646464646</li>\n",
       "\t<li>0.251135388545539</li>\n",
       "\t<li>0.00133333333333333</li>\n",
       "\t<li>0.323852168737935</li>\n",
       "\t<li>0.0388</li>\n",
       "\t<li>0.730835524248414</li>\n",
       "\t<li>0.620188874874476</li>\n",
       "\t<li>0.630244244419824</li>\n",
       "\t<li>0.0721142638456071</li>\n",
       "\t<li>0.12166031812939</li>\n",
       "\t<li>0.305540265889635</li>\n",
       "\t<li>0.245012615605485</li>\n",
       "\t<li>0.490300430941607</li>\n",
       "\t<li>0.0034</li>\n",
       "\t<li>0.0107575757575758</li>\n",
       "\t<li>0.882408533589661</li>\n",
       "\t<li>0.126232492997199</li>\n",
       "\t<li>0.636695134235557</li>\n",
       "\t<li>0.615489800748144</li>\n",
       "\t<li>0.0198761904761905</li>\n",
       "\t<li>0.468966866621097</li>\n",
       "\t<li>0.0903596273291925</li>\n",
       "\t<li>0.3519291679354</li>\n",
       "\t<li>0.388504130427536</li>\n",
       "\t<li>0.251495953222723</li>\n",
       "\t<li>0.170214946457052</li>\n",
       "\t<li>0.341623667918618</li>\n",
       "\t<li>0.299819894697688</li>\n",
       "\t<li>0.0504290043290043</li>\n",
       "\t<li>0.0949139573070608</li>\n",
       "\t<li>0.549730149364976</li>\n",
       "\t<li>0.416010308146513</li>\n",
       "\t<li>0.331928693275643</li>\n",
       "\t<li>0.150220757352101</li>\n",
       "\t<li>0.156840427251817</li>\n",
       "\t<li>0.537939348869069</li>\n",
       "\t<li>0.314179757321879</li>\n",
       "\t<li>0.560541568132123</li>\n",
       "\t<li>0.25118514292517</li>\n",
       "\t<li>0.288552866902346</li>\n",
       "\t<li>0.407984128813522</li>\n",
       "\t<li>0.176759706959707</li>\n",
       "\t<li>0.0212</li>\n",
       "\t<li>0.641934489245435</li>\n",
       "\t<li>0.0203396825396825</li>\n",
       "\t<li>0.0052</li>\n",
       "\t<li>0.401647385027601</li>\n",
       "\t<li>0.511356057862999</li>\n",
       "\t<li>0.00623809523809524</li>\n",
       "\t<li>0.0592004329004329</li>\n",
       "\t<li>0.237141352014374</li>\n",
       "\t<li>0.085396392243525</li>\n",
       "\t<li>0.788862512826833</li>\n",
       "\t<li>0.0808649572649573</li>\n",
       "\t<li>0.473144649730227</li>\n",
       "\t<li>0.0228</li>\n",
       "\t<li>0.136904761904762</li>\n",
       "\t<li>0.140655555555556</li>\n",
       "\t<li>0.765634914227787</li>\n",
       "\t<li>0.6158</li>\n",
       "\t<li>0.539687413737704</li>\n",
       "\t<li>0.0686062027229007</li>\n",
       "\t<li>0.483012053932509</li>\n",
       "\t<li>0.0733139573070608</li>\n",
       "\t<li>0.215238805970149</li>\n",
       "\t<li>0.264931591323405</li>\n",
       "\t<li>0.318030064563293</li>\n",
       "\t<li>0.631477491312862</li>\n",
       "\t<li>0.407353968045321</li>\n",
       "\t<li>0.00818468899521531</li>\n",
       "\t<li>0.186946111375077</li>\n",
       "\t<li>0.086162981352753</li>\n",
       "\t<li>0.215811473127263</li>\n",
       "\t<li>0.0303543436955202</li>\n",
       "\t<li>0.604514698161444</li>\n",
       "\t<li>0.723084875091197</li>\n",
       "\t<li>0.289415592624221</li>\n",
       "\t<li>0.372545743145743</li>\n",
       "\t<li>0.336713213980879</li>\n",
       "\t<li>0.039318315018315</li>\n",
       "\t<li>0.309967764625572</li>\n",
       "\t<li>0.0734321043249836</li>\n",
       "\t<li>0.201138644688645</li>\n",
       "\t<li>0.256224259866968</li>\n",
       "\t<li>0.314791419962736</li>\n",
       "\t<li>0.140282184809471</li>\n",
       "\t<li>0.099782683982684</li>\n",
       "\t<li>0.704344831895001</li>\n",
       "\t<li>0.201224498525836</li>\n",
       "\t<li>0.0415333333333333</li>\n",
       "\t<li>0.0519019607843137</li>\n",
       "\t<li>0.694303316604856</li>\n",
       "\t<li>0.282422205186471</li>\n",
       "\t<li>0.120420660603014</li>\n",
       "\t<li>0.172652940854872</li>\n",
       "\t<li>0.304401309154514</li>\n",
       "\t<li>0.05003186537773</li>\n",
       "\t<li>0.508016796690953</li>\n",
       "\t<li>0.70500700439963</li>\n",
       "\t<li>0.504013885833157</li>\n",
       "\t<li>0.343224242424242</li>\n",
       "\t<li>0.0454884057971014</li>\n",
       "\t<li>0.570207676366393</li>\n",
       "\t<li>0.125894983688087</li>\n",
       "\t<li>0.133616987610091</li>\n",
       "\t<li>0.545872231676956</li>\n",
       "\t<li>0.209753091684435</li>\n",
       "\t<li>0.255642772443036</li>\n",
       "\t<li>0.00151428571428571</li>\n",
       "\t<li>0.0741897435897436</li>\n",
       "\t<li>0.465913523165361</li>\n",
       "\t<li>0.166758862165556</li>\n",
       "\t<li>0.16904126984127</li>\n",
       "\t<li>0.333526921480466</li>\n",
       "\t<li>0.200735817805383</li>\n",
       "\t<li>0.0346285714285714</li>\n",
       "\t<li>0.299428793348988</li>\n",
       "\t<li>0.284971170474937</li>\n",
       "\t<li>0.244132627546813</li>\n",
       "\t<li>0.311459815778714</li>\n",
       "\t<li>0.314835655038608</li>\n",
       "\t<li>0.288097508742712</li>\n",
       "\t<li>0.00133333333333333</li>\n",
       "\t<li>0.686166787728018</li>\n",
       "\t<li>0.282616631571023</li>\n",
       "\t<li>0.3848515984014</li>\n",
       "\t<li>0.129150549450549</li>\n",
       "\t<li>0.415681447726581</li>\n",
       "\t<li>0.223621388443156</li>\n",
       "\t<li>0.469968886032506</li>\n",
       "\t<li>0.154807008086253</li>\n",
       "\t<li>0.299682621888885</li>\n",
       "\t<li>0.299367211511165</li>\n",
       "\t<li>0.601434676318298</li>\n",
       "\t<li>0.610963313102379</li>\n",
       "\t<li>0.272414718614719</li>\n",
       "\t<li>0.0222</li>\n",
       "\t<li>0.118647619047619</li>\n",
       "\t<li>0.659677105017832</li>\n",
       "\t<li>0.207591088833194</li>\n",
       "\t<li>0.006</li>\n",
       "\t<li>0.0170242424242424</li>\n",
       "\t<li>0.247568264688533</li>\n",
       "\t<li>0.0280666666666667</li>\n",
       "\t<li>0.290326480640085</li>\n",
       "\t<li>0.32214621413716</li>\n",
       "\t<li>0.0250448979591837</li>\n",
       "\t<li>0.0167897435897436</li>\n",
       "\t<li>0.0112666666666667</li>\n",
       "\t<li>0.075847619047619</li>\n",
       "\t<li>0.102975200989487</li>\n",
       "\t<li>0.0895166666666667</li>\n",
       "\t<li>0.21335544210567</li>\n",
       "\t<li>0.0940847880914818</li>\n",
       "\t<li>0.294917309481363</li>\n",
       "\t<li>0.031970695970696</li>\n",
       "\t<li>0.0320560846560847</li>\n",
       "\t<li>0.316107836913662</li>\n",
       "\t<li>0.0400818070818071</li>\n",
       "\t<li>0.183807070707071</li>\n",
       "\t<li>0.648082372111897</li>\n",
       "\t<li>0.266412670232578</li>\n",
       "\t<li>0.60368545869941</li>\n",
       "\t<li>0.178455544164321</li>\n",
       "\t<li>0.0357380952380952</li>\n",
       "\t<li>0.0612242424242424</li>\n",
       "\t<li>0.22099898989899</li>\n",
       "\t<li>0.0977333333333333</li>\n",
       "\t<li>0.0270196078431373</li>\n",
       "\t<li>0.263916995610636</li>\n",
       "\t<li>0.034178231292517</li>\n",
       "\t<li>0.650701455301455</li>\n",
       "\t<li>0.00899567099567099</li>\n",
       "\t<li>0.179704777489037</li>\n",
       "\t<li>0.247198619248563</li>\n",
       "\t<li>0.0494666666666667</li>\n",
       "\t<li>0.242614081012061</li>\n",
       "\t<li>0.0611890756302521</li>\n",
       "\t<li>0.161042148813548</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.0386666666666667\n",
       "\\item 0.284307793173011\n",
       "\\item 0.357987802849658\n",
       "\\item 0.0494333333333333\n",
       "\\item 0.589378795671079\n",
       "\\item 0.111455237641615\n",
       "\\item 0.323146097272539\n",
       "\\item 0.323419225306165\n",
       "\\item 0.0732036075036075\n",
       "\\item 9.09090909090909e-05\n",
       "\\item 0.371806204993215\n",
       "\\item 0.253524318826233\n",
       "\\item 0.0327115646258503\n",
       "\\item 0.768577930732082\n",
       "\\item 0.118376479076479\n",
       "\\item 0.0389967320261438\n",
       "\\item 0.030564267866067\n",
       "\\item 0.153095203946263\n",
       "\\item 0.202483166814215\n",
       "\\item 0.0114024737167594\n",
       "\\item 0.106046405228758\n",
       "\\item 0.29346375424886\n",
       "\\item 0.799570000538655\n",
       "\\item 0.00573333333333333\n",
       "\\item 0.5085797649002\n",
       "\\item 0.0682929499072356\n",
       "\\item 0.287933901925218\n",
       "\\item 0.418726586129839\n",
       "\\item 0.426580377603335\n",
       "\\item 0.314469341223495\n",
       "\\item 0.292820313987962\n",
       "\\item 0.657049667521002\n",
       "\\item 0.0256076371389804\n",
       "\\item 0.351996273468985\n",
       "\\item 0.566468441782719\n",
       "\\item 0.268638978801203\n",
       "\\item 0.673271758263628\n",
       "\\item 0.639288675661089\n",
       "\\item 0.787783287780627\n",
       "\\item 0.0233333333333333\n",
       "\\item 0.171500477300477\n",
       "\\item 0.119285409016808\n",
       "\\item 0.0653362745098039\n",
       "\\item 0.627611595913476\n",
       "\\item 0.259306722689076\n",
       "\\item 0.657464618843982\n",
       "\\item 0.223395785757563\n",
       "\\item 0.0201636363636364\n",
       "\\item 0.216319848984649\n",
       "\\item 0.131109351432881\n",
       "\\item 0.0198444444444444\n",
       "\\item 0.0243603318250377\n",
       "\\item 0.204613680522376\n",
       "\\item 0.232045146520104\n",
       "\\item 0.779745979852686\n",
       "\\item 0.375555555555556\n",
       "\\item 0.575470191632187\n",
       "\\item 0.444164448582921\n",
       "\\item 0.123793530543531\n",
       "\\item 0.394624221962658\n",
       "\\item 0.787017562218569\n",
       "\\item 0.0126242424242424\n",
       "\\item 0.0957952380952381\n",
       "\\item 0.143419180819181\n",
       "\\item 0.0484333333333333\n",
       "\\item 0.0992571428571428\n",
       "\\item 0.00917142857142857\n",
       "\\item 0.0188188110026619\n",
       "\\item 0.544796632996633\n",
       "\\item 0.125974548440066\n",
       "\\item 0.481474910143135\n",
       "\\item 0.0384257703081232\n",
       "\\item 0.0466444444444444\n",
       "\\item 0.503750648882545\n",
       "\\item 0.0580952380952381\n",
       "\\item 0.203208080808081\n",
       "\\item 0.530479579137357\n",
       "\\item 0.0368943977591036\n",
       "\\item 0.25924207414433\n",
       "\\item 0.0973109557109557\n",
       "\\item 0.794891299403227\n",
       "\\item 0.0474490028490029\n",
       "\\item 0.0257497326203209\n",
       "\\item 0.54952819565919\n",
       "\\item 0.233513556115693\n",
       "\\item 0.220663402613324\n",
       "\\item 0.00313333333333333\n",
       "\\item 0.0868405797101449\n",
       "\\item 0.710100044877115\n",
       "\\item 0.2369\n",
       "\\item 0.0212975723622782\n",
       "\\item 0.0691290043290043\n",
       "\\item 0.0969240468804379\n",
       "\\item 0.663681517942318\n",
       "\\item 0.142473367487064\n",
       "\\item 0.292054991818389\n",
       "\\item 0.0947949096880131\n",
       "\\item 0.064574358974359\n",
       "\\item 0.332497741192073\n",
       "\\item 0.333328000196019\n",
       "\\item 0.525190712488606\n",
       "\\item 0.0287575757575758\n",
       "\\item 0.489178923361334\n",
       "\\item 0.242720091605708\n",
       "\\item 0.386952646541243\n",
       "\\item 0.342856513695812\n",
       "\\item 0.032830303030303\n",
       "\\item 0.189535353535354\n",
       "\\item 0.210482655945651\n",
       "\\item 0.0498095238095238\n",
       "\\item 0.0331333333333333\n",
       "\\item 0.644927177881025\n",
       "\\item 0.141847619047619\n",
       "\\item 0.583307089324737\n",
       "\\item 0.213456439993848\n",
       "\\item 0.0228242424242424\n",
       "\\item 0.0101480519480519\n",
       "\\item 0.30572274359553\n",
       "\\item 0.040411167945439\n",
       "\\item 0.0219428571428571\n",
       "\\item 0.0237333333333333\n",
       "\\item 0.0600539542194715\n",
       "\\item 0.0337307189542484\n",
       "\\item 0.171074208304011\n",
       "\\item 0.181488568614938\n",
       "\\item 0.0301389978213508\n",
       "\\item 0.0276758620689655\n",
       "\\item 0.294828571428571\n",
       "\\item 0.629823419909178\n",
       "\\item 0.0543151515151515\n",
       "\\item 0.59868781383623\n",
       "\\item 0.256419523585888\n",
       "\\item 0.306866698214982\n",
       "\\item 0.282727478564128\n",
       "\\item 0.340124195763443\n",
       "\\item 0.50100325846369\n",
       "\\item 0.0508666666666667\n",
       "\\item 0.469653497438885\n",
       "\\item 0.118086150015562\n",
       "\\item 0.572841387756388\n",
       "\\item 0.314598026815766\n",
       "\\item 0.172211644749381\n",
       "\\item 0.312275030741055\n",
       "\\item 0.428704688112038\n",
       "\\item 0.316485521866962\n",
       "\\item 0.218884753213629\n",
       "\\item 0.538241060137602\n",
       "\\item 0.212618794480247\n",
       "\\item 0.271734902537344\n",
       "\\item 0.0656476190476191\n",
       "\\item 0.0681892849447471\n",
       "\\item 0.270012535356496\n",
       "\\item 0.219599945265462\n",
       "\\item 0.127282352941176\n",
       "\\item 0.0117241830065359\n",
       "\\item 0.129743353682011\n",
       "\\item 0.0368705882352941\n",
       "\\item 0.425313918564177\n",
       "\\item 0.250565328386548\n",
       "\\item 0.811754268262775\n",
       "\\item 0.10218750106103\n",
       "\\item 0.118322222222222\n",
       "\\item 0.0282564102564103\n",
       "\\item 0.0904044334975369\n",
       "\\item 0.0226837789661319\n",
       "\\item 0.441826362146999\n",
       "\\item 0.0640064935064935\n",
       "\\item 0.371058461660636\n",
       "\\item 0.0340666666666667\n",
       "\\item 0.137093351424695\n",
       "\\item 0.621160071352579\n",
       "\\item 0.240783019252167\n",
       "\\item 0.358916487664258\n",
       "\\item 0.0433751322751323\n",
       "\\item 0.299365270831587\n",
       "\\item 0.0096\n",
       "\\item 0.245954102884316\n",
       "\\item 0.253102089913986\n",
       "\\item 0.578974293628385\n",
       "\\item 0.775791193067877\n",
       "\\item 0.733348996767748\n",
       "\\item 0.64141104881558\n",
       "\\item 0.437606539316601\n",
       "\\item 0.00446666666666667\n",
       "\\item 0.693518958187248\n",
       "\\item 0.0239629465354663\n",
       "\\item 0.001\n",
       "\\item 0.134897917177162\n",
       "\\item 0.226644142647396\n",
       "\\item 0.159959516856069\n",
       "\\item 0.000244897959183673\n",
       "\\item 0.359045390437543\n",
       "\\item 0.477865833621991\n",
       "\\item 0.214059271331079\n",
       "\\item 0.00755555555555555\n",
       "\\item 0.000333333333333333\n",
       "\\item 0.0472571428571429\n",
       "\\item 0.0151666666666667\n",
       "\\item 0.0358242424242424\n",
       "\\item 0.637158012728281\n",
       "\\item 0.482731520614185\n",
       "\\item 0.111332323232323\n",
       "\\item 0.152250127437984\n",
       "\\item 0.497167088916823\n",
       "\\item 0.271479273781657\n",
       "\\item 0.455203818836828\n",
       "\\item 0.0448757575757576\n",
       "\\item 0.763935244230151\n",
       "\\item 0.105061904761905\n",
       "\\item 0.606830139245331\n",
       "\\item 0.0266\n",
       "\\item 8e-04\n",
       "\\item 0.0942739541160594\n",
       "\\item 0.438700079703044\n",
       "\\item 0.243512348779888\n",
       "\\item 0.503079037870049\n",
       "\\item 0.133569126842656\n",
       "\\item 0.0412037037037037\n",
       "\\item 0.0102036923860453\n",
       "\\item 0.0580067873303167\n",
       "\\item 0.0811474842767296\n",
       "\\item 0.00489090909090909\n",
       "\\item 0.330810339039293\n",
       "\\item 0.464420454461405\n",
       "\\item 0.0700666666666667\n",
       "\\item 0.0973904761904762\n",
       "\\item 0.163766416955655\n",
       "\\item 0.523274217931655\n",
       "\\item 0.199467024929094\n",
       "\\item 0.00769090909090909\n",
       "\\item 0.677031551758182\n",
       "\\item 0.0696396825396825\n",
       "\\item 0.413304805937685\n",
       "\\item 0.0611194805194805\n",
       "\\item 0.0169884057971014\n",
       "\\item 0.0607285714285714\n",
       "\\item 0.43492685720077\n",
       "\\item 0.216448023096463\n",
       "\\item 0.355853453920927\n",
       "\\item 0.527556616736744\n",
       "\\item 0.0530505494505495\n",
       "\\item 0.0183686274509804\n",
       "\\item 0.0113011879804333\n",
       "\\item 0.0813967679379444\n",
       "\\item 0.0760971448228414\n",
       "\\item 0.0506560846560846\n",
       "\\item 0.400735353535354\n",
       "\\item 0.515018519229181\n",
       "\\item 0.360409195402299\n",
       "\\item 0.00824489795918367\n",
       "\\item 0.0383333333333333\n",
       "\\item 0.603874657746505\n",
       "\\item 0.648163117389417\n",
       "\\item 0.366674876626466\n",
       "\\item 0.0546925925925926\n",
       "\\item 0.490030525302997\n",
       "\\item 0.741960171940443\n",
       "\\item 0.200962947992688\n",
       "\\item 0.144319152112256\n",
       "\\item 0.141622799128338\n",
       "\\item 0.465051818921838\n",
       "\\item 0.190328571428571\n",
       "\\item 0.0465833333333333\n",
       "\\item 0.317557253604386\n",
       "\\item 0.478611274427798\n",
       "\\item 0.268190451176945\n",
       "\\item 0.0878809523809524\n",
       "\\item 0.0964445887445887\n",
       "\\item 0.060128140486964\n",
       "\\item 0.0906093514328808\n",
       "\\item 0.267174812202963\n",
       "\\item 0.188604597701149\n",
       "\\item 0.239579927702157\n",
       "\\item 0.199477994063257\n",
       "\\item 0.0481555555555555\n",
       "\\item 0.280759291894501\n",
       "\\item 0.307182655465532\n",
       "\\item 0.0674291316526611\n",
       "\\item 0.350158234524836\n",
       "\\item 0.165481001203937\n",
       "\\item 0.222794253780082\n",
       "\\item 0.0572920060331825\n",
       "\\item 0.190103982016468\n",
       "\\item 0.102093410818928\n",
       "\\item 0.52706734155081\n",
       "\\item 0.216547063522906\n",
       "\\item 0.0828364145658263\n",
       "\\item 0.428922504024337\n",
       "\\item 0.57723485821564\n",
       "\\item 0.291559191394476\n",
       "\\item 0.405754794645023\n",
       "\\item 0.0627333333333333\n",
       "\\item 0.295443444275023\n",
       "\\item 0.341839561683823\n",
       "\\item 0.348153534318752\n",
       "\\item 0.803763277603732\n",
       "\\item 0.538657546415613\n",
       "\\item 0.155703567421288\n",
       "\\item 0.0284\n",
       "\\item 0.0653951825951826\n",
       "\\item 0.681006936820214\n",
       "\\item 0.0250909090909091\n",
       "\\item 0.0407265010351967\n",
       "\\item 0.000662337662337662\n",
       "\\item 0.0378315298736351\n",
       "\\item 0.0378666666666667\n",
       "\\item 0.119664280225878\n",
       "\\item 0.253418844823313\n",
       "\\item 0.0625019607843137\n",
       "\\item 0.00442424242424242\n",
       "\\item 0.11971220043573\n",
       "\\item 0.348656986068479\n",
       "\\item 0.415787282375247\n",
       "\\item 0.0486095238095238\n",
       "\\item 0.039\n",
       "\\item 0.596140883735525\n",
       "\\item 0.265724630671763\n",
       "\\item 0.172893900808187\n",
       "\\item 0.457941012983484\n",
       "\\item 0.392812214827628\n",
       "\\item 0.0157106674420107\n",
       "\\item 0.135243434343434\n",
       "\\item 0.0194\n",
       "\\item 0.220547340632649\n",
       "\\item 0.187424999411323\n",
       "\\item 0.447072904356384\n",
       "\\item 0.387715328246304\n",
       "\\item 0.454045868945869\n",
       "\\item 0.0378849539815926\n",
       "\\item 0.398228772866494\n",
       "\\item 0.390477508204148\n",
       "\\item 0.152133478471214\n",
       "\\item 0.58755787726083\n",
       "\\item 0.428309944239527\n",
       "\\item 0.744634570602112\n",
       "\\item 0.190821217682475\n",
       "\\item 0.455303171132771\n",
       "\\item 0.640504928814659\n",
       "\\item 0.0156773341086774\n",
       "\\item 0.144972089314195\n",
       "\\item 0.0595555555555556\n",
       "\\item 0.150691562003639\n",
       "\\item 0.0750747474747475\n",
       "\\item 0.310302594492603\n",
       "\\item 0.304238847453225\n",
       "\\item 0.032149855877015\n",
       "\\item 0.0784596684017737\n",
       "\\item 0.301930155269706\n",
       "\\item 0.0478380952380952\n",
       "\\item 0.0727528138528139\n",
       "\\item 0.0867385281385281\n",
       "\\item 0.0228714285714286\n",
       "\\item 0.150608940453403\n",
       "\\item 0.62865894012919\n",
       "\\item 0.100176750378682\n",
       "\\item 0.277604366968881\n",
       "\\item 0.676867492779548\n",
       "\\item 0.168737254901961\n",
       "\\item 0.0787162297397592\n",
       "\\item 0.148563647801376\n",
       "\\item 0.0766138645315855\n",
       "\\item 0.0373333333333333\n",
       "\\item 0.110430241941356\n",
       "\\item 0.0932695970695971\n",
       "\\item 0.389445683546134\n",
       "\\item 0.273769264069264\n",
       "\\item 0.552587839483542\n",
       "\\item 0.0196666666666667\n",
       "\\item 0.0512603174603175\n",
       "\\item 0.0923436637436637\n",
       "\\item 0.612239563359861\n",
       "\\item 0.0641617464283838\n",
       "\\item 0.0096734693877551\n",
       "\\item 0.0711967218771567\n",
       "\\item 0.105190565756083\n",
       "\\item 0.629535813059332\n",
       "\\item 0.1719504254532\n",
       "\\item 0.0893333333333333\n",
       "\\item 0.00613333333333333\n",
       "\\item 0.282449427904003\n",
       "\\item 0.190488111815043\n",
       "\\item 0.372815155640314\n",
       "\\item 0.0424769841269841\n",
       "\\item 0.536972732218383\n",
       "\\item 0.157903174603175\n",
       "\\item 0.0797116138763197\n",
       "\\item 0.00526666666666667\n",
       "\\item 0.360509642846561\n",
       "\\item 0.0704133889099406\n",
       "\\item 0.27020156324479\n",
       "\\item 0.011979797979798\n",
       "\\item 0.111183679466106\n",
       "\\item 0.615802007704841\n",
       "\\item 0.199248881974001\n",
       "\\item 0.0918197064989517\n",
       "\\item 0.385197457521753\n",
       "\\item 0.0917285159285159\n",
       "\\item 0.0815746031746032\n",
       "\\item 0.0373609883854351\n",
       "\\item 0.0331238095238095\n",
       "\\item 0.105266666666667\n",
       "\\item 0.717849442812553\n",
       "\\item 0.150977777777778\n",
       "\\item 0.709632212560175\n",
       "\\item 0.0595047619047619\n",
       "\\item 0.325248575518024\n",
       "\\item 0.100281481481481\n",
       "\\item 0.328013947814903\n",
       "\\item 0.6880333793035\n",
       "\\item 0.0663131169709263\n",
       "\\item 0.227163834795735\n",
       "\\item 0.0328\n",
       "\\item 0.393872574704721\n",
       "\\item 0.0248666666666667\n",
       "\\item 0.405740652973506\n",
       "\\item 0.438756417032222\n",
       "\\item 0.254374236623581\n",
       "\\item 0.698981838075479\n",
       "\\item 0.39408582741861\n",
       "\\item 0.495920752085532\n",
       "\\item 0.324486990786991\n",
       "\\item 0.510677126966246\n",
       "\\item 0.0100571428571429\n",
       "\\item 0.0342899930986887\n",
       "\\item 0.154558033299447\n",
       "\\item 0.411068151456582\n",
       "\\item 0.122001678329519\n",
       "\\item 0.308214343854729\n",
       "\\item 0.164942251950948\n",
       "\\item 0.360687897186617\n",
       "\\item 0.483508579166423\n",
       "\\item 0.0729142857142857\n",
       "\\item 0.395253367660061\n",
       "\\item 0.263278816817658\n",
       "\\item 0.422276190476191\n",
       "\\item 0.229750385846038\n",
       "\\item 0.0108047619047619\n",
       "\\item 0.176508485316802\n",
       "\\item 0.474306200535843\n",
       "\\item 0.276660728826316\n",
       "\\item 0.0947634920634921\n",
       "\\item 0.105395238095238\n",
       "\\item 0.112504761904762\n",
       "\\item 0.806234385012738\n",
       "\\item 0.136431402244281\n",
       "\\item 0.435833333333333\n",
       "\\item 0.009\n",
       "\\item 0.0114545454545455\n",
       "\\item 0.482571392629974\n",
       "\\item 0.0602242424242424\n",
       "\\item 0.431337554020286\n",
       "\\item 0.27681027662206\n",
       "\\item 0.0367380952380952\n",
       "\\item 0.62318882370584\n",
       "\\item 0.209622273151685\n",
       "\\item 0.0078\n",
       "\\item 0.289953713232959\n",
       "\\item 0.315235664322403\n",
       "\\item 0.65275649266471\n",
       "\\item 0.575027900354182\n",
       "\\item 0.107434295029988\n",
       "\\item 0.0997706959706959\n",
       "\\item 0.213135012951385\n",
       "\\item 0.743771433091391\n",
       "\\item 0.0764550724637681\n",
       "\\item 0.318349534459089\n",
       "\\item 0.140746664609543\n",
       "\\item 0.0353240413406742\n",
       "\\item 0.0570884057971014\n",
       "\\item 0.538681238884521\n",
       "\\item 0.363733099135034\n",
       "\\item 0.619358197014435\n",
       "\\item 0.111166666666667\n",
       "\\item 0.0520356840080327\n",
       "\\item 0.516450076827099\n",
       "\\item 0.0251210884353741\n",
       "\\item 0.294442302470784\n",
       "\\item 0.672557520676255\n",
       "\\item 0.240112825653878\n",
       "\\item 0.0230758620689655\n",
       "\\item 0.0054\n",
       "\\item 0.139863492063492\n",
       "\\item 0.66436611124831\n",
       "\\item 0.0980222222222222\n",
       "\\item 0.0244666666666667\n",
       "\\item 0.104156228956229\n",
       "\\item 0.550629126047257\n",
       "\\item 0.0130447438918766\n",
       "\\item 0.338244206903106\n",
       "\\item 0.0842\n",
       "\\item 0.541711652052044\n",
       "\\item 0.349881345415894\n",
       "\\item 0.531041458306449\n",
       "\\item 0.625788822549875\n",
       "\\item 0.047374358974359\n",
       "\\item 0.0038\n",
       "\\item 0.676460203701692\n",
       "\\item 0.00993333333333333\n",
       "\\item 0.677239550859892\n",
       "\\item 0.106911111111111\n",
       "\\item 0.145854197654198\n",
       "\\item 0.144922518680328\n",
       "\\item 0.00817391304347826\n",
       "\\item 0.0178575757575758\n",
       "\\item 0.12160293040293\n",
       "\\item 0.017\n",
       "\\item 0.357125539177879\n",
       "\\item 0.338516133398352\n",
       "\\item 0.113944444444444\n",
       "\\item 0.0374086834733894\n",
       "\\item 0.3655002859727\n",
       "\\item 0.294799301435585\n",
       "\\item 0.0244242424242424\n",
       "\\item 0.0357666666666667\n",
       "\\item 0.0600285714285714\n",
       "\\item 0.233565641098458\n",
       "\\item 0.0560986717216531\n",
       "\\item 0.102466666666667\n",
       "\\item 0.53278700277892\n",
       "\\item 0.429590465914747\n",
       "\\item 0.00677435897435897\n",
       "\\item 0.0343099415204678\n",
       "\\item 0.452730246566324\n",
       "\\item 0.0253333333333333\n",
       "\\item 0.202371600076164\n",
       "\\item 0.289645265148295\n",
       "\\item 0.0232778566970787\n",
       "\\item 0.626740403927503\n",
       "\\item 0.198095894271918\n",
       "\\item 0.00922424242424242\n",
       "\\item 0.245906109155619\n",
       "\\item 0.0288666666666667\n",
       "\\item 0.361919314443584\n",
       "\\item 0.534399308386028\n",
       "\\item 0.31039062049062\n",
       "\\item 0.00497309479231676\n",
       "\\item 0.838908268033228\n",
       "\\item 0.640486206224888\n",
       "\\item 0.311127300973258\n",
       "\\item 0.204467377398721\n",
       "\\item 0.149169358178054\n",
       "\\item 0.778886613952768\n",
       "\\item 0.0356888888888889\n",
       "\\item 0.151863942822034\n",
       "\\item 0.109799652199652\n",
       "\\item 0.0533871794871795\n",
       "\\item 0.519950469788172\n",
       "\\item 0.639161009661332\n",
       "\\item 0.0181238095238095\n",
       "\\item 0.535326076236431\n",
       "\\item 0.0178448979591837\n",
       "\\item 0.0374705882352941\n",
       "\\item 0.0935802197802198\n",
       "\\item 0.706244476520718\n",
       "\\item 0.421852941176471\n",
       "\\item 0.289691481982747\n",
       "\\item 0.103676039646628\n",
       "\\item 0.0519054726368159\n",
       "\\item 0.0708282717282717\n",
       "\\item 0.322005043614739\n",
       "\\item 0.0589529411764706\n",
       "\\item 0.0485163265306122\n",
       "\\item 0.0538380952380952\n",
       "\\item 0.214603949392259\n",
       "\\item 0.177011039617733\n",
       "\\item 0.0306333333333333\n",
       "\\item 0.376446677990467\n",
       "\\item 0.0774028886655573\n",
       "\\item 0.575840860387835\n",
       "\\item 0.257164879195585\n",
       "\\item 0.0392948306595365\n",
       "\\item 0.208189830090342\n",
       "\\item 0.226569102144005\n",
       "\\item 0.340363301614579\n",
       "\\item 0.302163923844582\n",
       "\\item 0.419854541732668\n",
       "\\item 0.468705948432882\n",
       "\\item 0.239743221162657\n",
       "\\item 0.0556585266585267\n",
       "\\item 0.0598242424242424\n",
       "\\item 0.0223986013986014\n",
       "\\item 0.0138555555555556\n",
       "\\item 0.331105283357457\n",
       "\\item 0.466434140198813\n",
       "\\item 0.260565651018484\n",
       "\\item 0.459321698842957\n",
       "\\item 0.0558666666666667\n",
       "\\item 0.141293395195313\n",
       "\\item 0.242394352190637\n",
       "\\item 0.039630303030303\n",
       "\\item 0.403255845886581\n",
       "\\item 0.189922408963585\n",
       "\\item 0.0342166666666667\n",
       "\\item 0.0112\n",
       "\\item 0.024988332302618\n",
       "\\item 0.481976695508585\n",
       "\\item 0.0444\n",
       "\\item 0.0191777777777778\n",
       "\\item 0.0296380952380952\n",
       "\\item 0.367306685423997\n",
       "\\item 0.0778380952380952\n",
       "\\item 0.289312944745478\n",
       "\\item 0.559175706432625\n",
       "\\item 0.0308967032967033\n",
       "\\item 0.101347619047619\n",
       "\\item 0.293640599268311\n",
       "\\item 0.798060181418768\n",
       "\\item 0.134602661064426\n",
       "\\item 0.099288198757764\n",
       "\\item 0.0940862745098039\n",
       "\\item 0.218115708812261\n",
       "\\item 0.466214721183192\n",
       "\\item 0.565577330470973\n",
       "\\item 0.15545166495444\n",
       "\\item 0.0932862745098039\n",
       "\\item 0.514839955371036\n",
       "\\item 0.0177645962732919\n",
       "\\item 0.289443145743146\n",
       "\\item 0.197659414659415\n",
       "\\item 0.813799249039808\n",
       "\\item 0.228247875815897\n",
       "\\item 0.536628586862503\n",
       "\\item 0.693993902066963\n",
       "\\item 0.467215482556659\n",
       "\\item 0.262524816893888\n",
       "\\item 0.193785871798834\n",
       "\\item 0.153193650793651\n",
       "\\item 0.0451210884353742\n",
       "\\item 0.197006647324272\n",
       "\\item 0.0701230769230769\n",
       "\\item 0.0704074534161491\n",
       "\\item 0.116060029294737\n",
       "\\item 0.509875081675082\n",
       "\\item 0.240945731135154\n",
       "\\item 0.214629835415337\n",
       "\\item 0.00522816399286987\n",
       "\\item 0.312126499687464\n",
       "\\item 0.243039882810303\n",
       "\\item 0.0422291316526611\n",
       "\\item 0.120124239317968\n",
       "\\item 0.127603242816995\n",
       "\\item 0.0698856065503124\n",
       "\\item 0.0162931677018634\n",
       "\\item 0.176088888888889\n",
       "\\item 0.114416246498599\n",
       "\\item 0.0174888888888889\n",
       "\\item 0.123037458837459\n",
       "\\item 0.111888888888889\n",
       "\\item 0.366326102952682\n",
       "\\item 0.243043587613955\n",
       "\\item 0.137607246376812\n",
       "\\item 0.0491333333333333\n",
       "\\item 0.466450994433474\n",
       "\\item 0.366124988474531\n",
       "\\item 0.262872307543707\n",
       "\\item 0.169280866503436\n",
       "\\item 0.630904319669879\n",
       "\\item 0.505980371336913\n",
       "\\item 0.564482970921137\n",
       "\\item 0.252544876044818\n",
       "\\item 0.308024242424242\n",
       "\\item 0.0314147186147186\n",
       "\\item 0.0708261437908497\n",
       "\\item 0.564847425424129\n",
       "\\item 0.314746867051548\n",
       "\\item 0.148449940752894\n",
       "\\item 0.443782443770596\n",
       "\\item 0.0527897435897436\n",
       "\\item 0.00142424242424242\n",
       "\\item 0.572227885200541\n",
       "\\item 0.474441772961746\n",
       "\\item 0.0553878787878788\n",
       "\\item 9.09090909090909e-05\n",
       "\\item 0.502523426944876\n",
       "\\item 0.415692500564034\n",
       "\\item 0.513565347639667\n",
       "\\item 0.0237333333333333\n",
       "\\item 0.394099602454177\n",
       "\\item 0.342877724748259\n",
       "\\item 0.212610875831639\n",
       "\\item 0.0403761904761905\n",
       "\\item 0.763012831118629\n",
       "\\item 0.175763636363636\n",
       "\\item 0.186389016978732\n",
       "\\item 0.148859383431903\n",
       "\\item 0.525763076000812\n",
       "\\item 0.298697784291618\n",
       "\\item 0.2990466729727\n",
       "\\item 0.137109195402299\n",
       "\\item 0.225788481176847\n",
       "\\item 0.0500863731656185\n",
       "\\item 0.099187031408308\n",
       "\\item 0.390662444166576\n",
       "\\item 0.351110446598191\n",
       "\\item 0.0279019607843137\n",
       "\\item 0.309185570434518\n",
       "\\item 0.0349238095238095\n",
       "\\item 0.0476739130434783\n",
       "\\item 0.0385333333333333\n",
       "\\item 0.245717893335458\n",
       "\\item 0.0527222222222222\n",
       "\\item 0.284787878787879\n",
       "\\item 0.627641641703345\n",
       "\\item 0.21879732640771\n",
       "\\item 0.469045016904163\n",
       "\\item 0.594151958049773\n",
       "\\item 0.373316017316017\n",
       "\\item 0.0264714285714286\n",
       "\\item 0.254669573718702\n",
       "\\item 0.041559589830252\n",
       "\\item 0.311598036268764\n",
       "\\item 0.841786907647478\n",
       "\\item 0.0159636363636364\n",
       "\\item 0.0312666666666667\n",
       "\\item 0.279655611653983\n",
       "\\item 0.412263653821177\n",
       "\\item 0.379312290957525\n",
       "\\item 0.439653944513374\n",
       "\\item 0.784785885812251\n",
       "\\item 0.0545271708683473\n",
       "\\item 0.188017638927227\n",
       "\\item 0.344273470384587\n",
       "\\item 0.562868361213509\n",
       "\\item 0.0802\n",
       "\\item 0.062573544973545\n",
       "\\item 0.0359350649350649\n",
       "\\item 0.391175023359331\n",
       "\\item 0.408658303032619\n",
       "\\item 0.398098078280914\n",
       "\\item 0.0789806239737274\n",
       "\\item 0.271022290241679\n",
       "\\item 0.551962166207989\n",
       "\\item 0.212661741946708\n",
       "\\item 0.0751591431556949\n",
       "\\item 0.00578840579710145\n",
       "\\item 0.0592\n",
       "\\item 0.0317575757575758\n",
       "\\item 0.400923240463152\n",
       "\\item 0.0474380952380952\n",
       "\\item 0.0545703703703704\n",
       "\\item 0.00231632653061224\n",
       "\\item 0.105234798534799\n",
       "\\item 0.587191562464047\n",
       "\\item 0.0900055555555555\n",
       "\\item 0.683906811495236\n",
       "\\item 0.132463157894737\n",
       "\\item 0.481602978553558\n",
       "\\item 0.495182859851671\n",
       "\\item 0.0279650793650794\n",
       "\\item 0.252402575687401\n",
       "\\item 0.018\n",
       "\\item 0.0356\n",
       "\\item 0.174758494177985\n",
       "\\item 0.153344139434442\n",
       "\\item 0.0726857142857143\n",
       "\\item 0.683499165226029\n",
       "\\item 0.153202628398228\n",
       "\\item 0.0596564102564103\n",
       "\\item 0.0799514105585433\n",
       "\\item 0.0164047619047619\n",
       "\\item 0.263091465594213\n",
       "\\item 0.474782800125594\n",
       "\\item 0.0670761904761905\n",
       "\\item 0.280788345367725\n",
       "\\item 0.704240171759427\n",
       "\\item 0.102052052545156\n",
       "\\item 0.00213333333333333\n",
       "\\item 0.0387119975262832\n",
       "\\item 0.425337059203743\n",
       "\\item 0.331278391850968\n",
       "\\item 0.0281194805194805\n",
       "\\item 0.0349555555555556\n",
       "\\item 0.475729114435763\n",
       "\\item 0.676810280270968\n",
       "\\item 0.523089841479637\n",
       "\\item 0.139011388611389\n",
       "\\item 0.779318549866474\n",
       "\\item 0.367001849785385\n",
       "\\item 0.0572705882352941\n",
       "\\item 0.314250992428384\n",
       "\\item 0.161834335058676\n",
       "\\item 0.00213333333333333\n",
       "\\item 0.412955823826079\n",
       "\\item 0.0199333333333333\n",
       "\\item 0.0458928329888822\n",
       "\\item 0.319390905296081\n",
       "\\item 0.420037351306949\n",
       "\\item 0.364102566004655\n",
       "\\item 0.146947619047619\n",
       "\\item 0.301476381253682\n",
       "\\item 0.00517391304347826\n",
       "\\item 0.205660606060606\n",
       "\\item 0.61367634871572\n",
       "\\item 0.0319714285714286\n",
       "\\item 0.150427973512282\n",
       "\\item 0.0271909090909091\n",
       "\\item 0.0048\n",
       "\\item 0.0260666666666667\n",
       "\\item 0.0725269841269841\n",
       "\\item 0.374685717160095\n",
       "\\item 0.150709401709402\n",
       "\\item 0.0235884057971015\n",
       "\\item 0.308170014728704\n",
       "\\item 0.211549318295058\n",
       "\\item 0.32041062603476\n",
       "\\item 0.082\n",
       "\\item 0.418124033184996\n",
       "\\item 0.0607015873015873\n",
       "\\item 0.0659246443211961\n",
       "\\item 0.535017418519619\n",
       "\\item 0.354019973464449\n",
       "\\item 0.102384283076097\n",
       "\\item 0.012\n",
       "\\item 0.041809977324263\n",
       "\\item 0.279769062472443\n",
       "\\item 0.353489633400662\n",
       "\\item 0.207294912736624\n",
       "\\item 0.376647132086538\n",
       "\\item 0.415349870433962\n",
       "\\item 0.0468809523809524\n",
       "\\item 0.157501944025473\n",
       "\\item 0.0733975130133025\n",
       "\\item 0.595211111111111\n",
       "\\item 0.445251352382422\n",
       "\\item 0.0661666666666667\n",
       "\\item 0.0519172161172161\n",
       "\\item 0.480804215675408\n",
       "\\item 0.0038\n",
       "\\item 0.195854665844033\n",
       "\\item 0.439200500428439\n",
       "\\item 0.0630340440653873\n",
       "\\item 0.0118\n",
       "\\item 0.516250192853268\n",
       "\\item 0.498544916984795\n",
       "\\item 0.760591175551091\n",
       "\\item 0.365394422263113\n",
       "\\item 0.119327103413597\n",
       "\\item 0.412778176927255\n",
       "\\item 0.0782047619047619\n",
       "\\item 0.591030502491688\n",
       "\\item 0.27405786440186\n",
       "\\item 0.63467417689523\n",
       "\\item 0.0106\n",
       "\\item 0.632594316947072\n",
       "\\item 0.57018670495502\n",
       "\\item 0.542914132310845\n",
       "\\item 0.0448222222222222\n",
       "\\item 0.100646464646465\n",
       "\\item 0.0688333333333333\n",
       "\\item 0.391323809523809\n",
       "\\item 0.134269378091727\n",
       "\\item 0.358933333333333\n",
       "\\item 0.569480904485631\n",
       "\\item 0.0074\n",
       "\\item 0.584502109134811\n",
       "\\item 0.210745425641771\n",
       "\\item 0.0312941798941799\n",
       "\\item 0.0048\n",
       "\\item 0.0028\n",
       "\\item 0.501271491786265\n",
       "\\item 0.0104\n",
       "\\item 0.0524666666666667\n",
       "\\item 0.0858979296066253\n",
       "\\item 0.289262426194988\n",
       "\\item 0.424623370845067\n",
       "\\item 0.072231746031746\n",
       "\\item 0.135167303685025\n",
       "\\item 0.369611016635817\n",
       "\\item 0.275685940420367\n",
       "\\item 0.0731760683760684\n",
       "\\item 0.0586952380952381\n",
       "\\item 0.0121575757575758\n",
       "\\item 0.00907142857142857\n",
       "\\item 0.230981594733769\n",
       "\\item 0.253574848576014\n",
       "\\item 0.485489586741761\n",
       "\\item 0.126477845234978\n",
       "\\item 0.252266854242723\n",
       "\\item 0.461561185201059\n",
       "\\item 0.714396163342505\n",
       "\\item 0.888072725437863\n",
       "\\item 0.38059559683715\n",
       "\\item 0.300584668691521\n",
       "\\item 0.186609146156279\n",
       "\\item 0.0910128978864273\n",
       "\\item 0.0950509379509379\n",
       "\\item 0.0445714285714286\n",
       "\\item 0.733074423960969\n",
       "\\item 0.313762255278213\n",
       "\\item 0.391056897334929\n",
       "\\item 0.537918660469894\n",
       "\\item 0.545035145836005\n",
       "\\item 0.183944370730748\n",
       "\\item 0.191006626546944\n",
       "\\item 0.229416466490255\n",
       "\\item 0.14783951833607\n",
       "\\item 0.175466380362932\n",
       "\\item 0.300386921867483\n",
       "\\item 0.119599076512164\n",
       "\\item 0.189289243351157\n",
       "\\item 0.139527909784603\n",
       "\\item 0.509938308558842\n",
       "\\item 0.00901062801932367\n",
       "\\item 0.316608146554032\n",
       "\\item 0.573690208720772\n",
       "\\item 0.0828583241320083\n",
       "\\item 0.115489599620943\n",
       "\\item 0.0204444444444444\n",
       "\\item 0.344281594427353\n",
       "\\item 0.502059748478351\n",
       "\\item 0.0368265010351967\n",
       "\\item 0.639156377249408\n",
       "\\item 0.605557813580397\n",
       "\\item 0.355263089775196\n",
       "\\item 0.060209653092006\n",
       "\\item 0.0283333333333333\n",
       "\\item 0.465417845947928\n",
       "\\item 0.383771867073044\n",
       "\\item 0.609687775060824\n",
       "\\item 0.104822222222222\n",
       "\\item 0.0186\n",
       "\\item 0.660421304876933\n",
       "\\item 0.481409203757834\n",
       "\\item 0.809747766725594\n",
       "\\item 0.0787714795008913\n",
       "\\item 0.017379797979798\n",
       "\\item 0.314007293648733\n",
       "\\item 0.209001180072758\n",
       "\\item 0.305927732644714\n",
       "\\item 0.525786629452419\n",
       "\\item 0.862981592950686\n",
       "\\item 0.0898273769450979\n",
       "\\item 0.107311111111111\n",
       "\\item 0.688490254374722\n",
       "\\item 0.178830976430976\n",
       "\\item 0.110597198879552\n",
       "\\item 0.441491346096358\n",
       "\\item 0.0665282106782107\n",
       "\\item 0.0869333333333333\n",
       "\\item 0.281417149517143\n",
       "\\item 0.319920834356719\n",
       "\\item 0.441589346278222\n",
       "\\item 0.312142531467055\n",
       "\\item 0.00666666666666667\n",
       "\\item 0.182681203746909\n",
       "\\item 0.0879587301587302\n",
       "\\item 0.0939294190086643\n",
       "\\item 0.848838044240041\n",
       "\\item 0.679419157351322\n",
       "\\item 0.695131142211447\n",
       "\\item 0.277875562262769\n",
       "\\item 0.290326504867681\n",
       "\\item 0.130597198879552\n",
       "\\item 0.7560411117244\n",
       "\\item 0.154198364850985\n",
       "\\item 0.357935797873526\n",
       "\\item 0.0274550724637681\n",
       "\\item 0.0098\n",
       "\\item 0.515103801446009\n",
       "\\item 0.0518568637711495\n",
       "\\item 0.0602037037037037\n",
       "\\item 0.212746951374304\n",
       "\\item 0.0082\n",
       "\\item 0.104533333333333\n",
       "\\item 0.0296\n",
       "\\item 0.360223821220606\n",
       "\\item 0.216810084033613\n",
       "\\item 0.078130303030303\n",
       "\\item 0.418873876001768\n",
       "\\item 0.552865475261585\n",
       "\\item 0.0629212666467276\n",
       "\\item 0.318102412502822\n",
       "\\item 0.55304407831155\n",
       "\\item 0.422692927377851\n",
       "\\item 0.163141247394718\n",
       "\\item 0.0651428571428571\n",
       "\\item 0.807349226711289\n",
       "\\item 0.308089905490167\n",
       "\\item 0.27977538174277\n",
       "\\item 0.594811542869982\n",
       "\\item 0.782831426708755\n",
       "\\item 0.592180283430288\n",
       "\\item 0.304757152840505\n",
       "\\item 0.513852942636185\n",
       "\\item 0.598395093538502\n",
       "\\item 0.367660878040489\n",
       "\\item 0.208479783050437\n",
       "\\item 0.387837330440576\n",
       "\\item 0.337120928477463\n",
       "\\item 0.00942424242424242\n",
       "\\item 0.044831746031746\n",
       "\\item 0.0331037037037037\n",
       "\\item 0.083404662004662\n",
       "\\item 0.265355856123919\n",
       "\\item 0.0668297996121526\n",
       "\\item 0.739165205095001\n",
       "\\item 0.379921036381695\n",
       "\\item 0.0158047619047619\n",
       "\\item 0.00509090909090909\n",
       "\\item 0.540346084545982\n",
       "\\item 0.051837037037037\n",
       "\\item 0.3089820785677\n",
       "\\item 0.367711224104156\n",
       "\\item 0.307965492429222\n",
       "\\item 0.238451413027758\n",
       "\\item 0.77765227334506\n",
       "\\item 0.575820802620802\n",
       "\\item 0.0733235294117647\n",
       "\\item 0.0016\n",
       "\\item 0.0384962370962371\n",
       "\\item 0.299365504309043\n",
       "\\item 0.155018217260323\n",
       "\\item 0.0672619047619048\n",
       "\\item 0.46806110634685\n",
       "\\item 0.114472421695951\n",
       "\\item 0.620615416911482\n",
       "\\item 0.200309037855656\n",
       "\\item 0.270726019802373\n",
       "\\item 0.174452762703832\n",
       "\\item 0.587592527390301\n",
       "\\item 0.199300398780202\n",
       "\\item 0.0102448979591837\n",
       "\\item 0.0397285714285714\n",
       "\\item 0.0162666666666667\n",
       "\\item 0.0114666666666667\n",
       "\\item 0.195693463730689\n",
       "\\item 0.201188496217982\n",
       "\\item 0.393090790408653\n",
       "\\item 0.310266666666667\n",
       "\\item 0.0288448979591837\n",
       "\\item 0.109350793650794\n",
       "\\item 0.526247670438228\n",
       "\\item 0.711136294459992\n",
       "\\item 0.377678952147725\n",
       "\\item 0.425689399257037\n",
       "\\item 0.0698122028914482\n",
       "\\item 0.0600952380952381\n",
       "\\item 0.473020623873165\n",
       "\\item 0.0322267806267806\n",
       "\\item 0.0808721393034826\n",
       "\\item 0.784359690569302\n",
       "\\item 0.437332651057119\n",
       "\\item 0.652655224508978\n",
       "\\item 0.0578666666666667\n",
       "\\item 0.03442030651341\n",
       "\\item 0.0703958041958042\n",
       "\\item 0.21453961981213\n",
       "\\item 0.100506293706294\n",
       "\\item 0.679990166363317\n",
       "\\item 0.251027323722086\n",
       "\\item 0.409398757763975\n",
       "\\item 0.0698380952380952\n",
       "\\item 0.250403572359249\n",
       "\\item 0.0121380952380952\n",
       "\\item 0.0570124015418133\n",
       "\\item 0.368781249022565\n",
       "\\item 0.00433333333333333\n",
       "\\item 0.00822222222222222\n",
       "\\item 0.294180077417447\n",
       "\\item 0.0293575757575758\n",
       "\\item 0.00322424242424242\n",
       "\\item 0.0991636363636363\n",
       "\\item 0.351744250426449\n",
       "\\item 0.257699518506827\n",
       "\\item 0.255360956057483\n",
       "\\item 0.21061965666277\n",
       "\\item 0.342812896325479\n",
       "\\item 0.0142909090909091\n",
       "\\item 0.42633098475994\n",
       "\\item 0.73804566827312\n",
       "\\item 0.229479614256193\n",
       "\\item 0.593167357105254\n",
       "\\item 0.572870918258768\n",
       "\\item 0.245210611004808\n",
       "\\item 0.114496825396825\n",
       "\\item 0.0745897435897436\n",
       "\\item 0.241977230232595\n",
       "\\item 0.0665761904761905\n",
       "\\item 0.356498184074626\n",
       "\\item 0.541662314184965\n",
       "\\item 0.537599046032529\n",
       "\\item 0.054593837535014\n",
       "\\item 0.0462564102564103\n",
       "\\item 0.415336751592259\n",
       "\\item 0.524927851005866\n",
       "\\item 0.00467777777777778\n",
       "\\item 0.475826385918094\n",
       "\\item 0.298654006808713\n",
       "\\item 0.855227625357486\n",
       "\\item 0.156878479375031\n",
       "\\item 0.526695958421396\n",
       "\\item 0.633782167395233\n",
       "\\item 0.658225612556666\n",
       "\\item 0.0858749979905152\n",
       "\\item 0.278734372594227\n",
       "\\item 0.194383150874964\n",
       "\\item 0.244549500497125\n",
       "\\item 0.448434606708141\n",
       "\\item 0.0427064935064935\n",
       "\\item 0.809052842494791\n",
       "\\item 0.232185414585415\n",
       "\\item 0.036\n",
       "\\item 0.194143083738472\n",
       "\\item 0.477569492304233\n",
       "\\item 0.147252756393933\n",
       "\\item 0.21487685482351\n",
       "\\item 0.372432718280477\n",
       "\\item 0.184978190739358\n",
       "\\item 0.1006\n",
       "\\item 0.513453282164648\n",
       "\\item 0.004\n",
       "\\item 0.162217543859649\n",
       "\\item 0.208779351650147\n",
       "\\item 0.237044627023114\n",
       "\\item 0.474138448593901\n",
       "\\item 0.29975804069693\n",
       "\\item 0.604209224874742\n",
       "\\item 0.122096914700544\n",
       "\\item 0.466259836896186\n",
       "\\item 0.0594948306595365\n",
       "\\item 0.317124363814219\n",
       "\\item 0.220421340446081\n",
       "\\item 0.0895954699121028\n",
       "\\item 0.692376534300207\n",
       "\\item 0.437216273156773\n",
       "\\item 0.239452729415019\n",
       "\\item 0.00766666666666667\n",
       "\\item 0.0688725274725275\n",
       "\\item 0.0409777777777778\n",
       "\\item 0.051658608058608\n",
       "\\item 0.827669993834232\n",
       "\\item 0.256817988778487\n",
       "\\item 0.135108371040724\n",
       "\\item 0.147487914230019\n",
       "\\item 0.0393908496732026\n",
       "\\item 0.191258657995136\n",
       "\\item 0.170636590222294\n",
       "\\item 0.0640787620924583\n",
       "\\item 0.304324378077303\n",
       "\\item 0.0631428571428572\n",
       "\\item 0.422405163006667\n",
       "\\item 0.0111333333333333\n",
       "\\item 0.482947030955207\n",
       "\\item 0.108928515928516\n",
       "\\item 0.012\n",
       "\\item 0.582924486417423\n",
       "\\item 0.669187156890966\n",
       "\\item 0.195216707870222\n",
       "\\item 0.119292722371968\n",
       "\\item 0.0868337068160597\n",
       "\\item 0.0808148459383754\n",
       "\\item 0.373955251864044\n",
       "\\item 0.0430459096459096\n",
       "\\item 0.367033420004021\n",
       "\\item 0.115671100164204\n",
       "\\item 0.0130147186147186\n",
       "\\item 0.161323099164703\n",
       "\\item 0.431120396557889\n",
       "\\item 0.107273400673401\n",
       "\\item 0.56055297739209\n",
       "\\item 0.625714444208014\n",
       "\\item 0.251417791468039\n",
       "\\item 0.551041303289911\n",
       "\\item 0.248545818387376\n",
       "\\item 0.3295071523647\n",
       "\\item 0.572191952414778\n",
       "\\item 0.0928666666666667\n",
       "\\item 0.443179929067408\n",
       "\\item 0.290382491582492\n",
       "\\item 0.197602196406712\n",
       "\\item 0.145585847485847\n",
       "\\item 0.495879747696854\n",
       "\\item 0.568835176867885\n",
       "\\item 0.178825097125097\n",
       "\\item 0.263006722689076\n",
       "\\item 0.0122666666666667\n",
       "\\item 0.204530764277533\n",
       "\\item 0.276815621836675\n",
       "\\item 0.0919473193473193\n",
       "\\item 0.285776930746126\n",
       "\\item 0.0861026915113871\n",
       "\\item 0.365720549442655\n",
       "\\item 0.0246666666666667\n",
       "\\item 0.341597836374044\n",
       "\\item 0.0703703703703704\n",
       "\\item 0.226417106965196\n",
       "\\item 0.145939853626176\n",
       "\\item 0.0578666666666667\n",
       "\\item 0.0203358070500928\n",
       "\\item 0.102344897959184\n",
       "\\item 0.257191002355077\n",
       "\\item 0.380270430813124\n",
       "\\item 0.22801255729577\n",
       "\\item 0.0496\n",
       "\\item 0.0243550524687487\n",
       "\\item 0.0796714285714286\n",
       "\\item 0.30290950428222\n",
       "\\item 0.089566905363544\n",
       "\\item 0.157804761904762\n",
       "\\item 0.193304568404978\n",
       "\\item 0.327884993163203\n",
       "\\item 0.188250444061209\n",
       "\\item 0.285145124113595\n",
       "\\item 0.100728571428571\n",
       "\\item 0.160782740375844\n",
       "\\item 0.0167884057971014\n",
       "\\item 0.231235686066859\n",
       "\\item 0.0508666666666667\n",
       "\\item 0.0850083550661644\n",
       "\\item 0.0765111111111111\n",
       "\\item 0.0360731601731602\n",
       "\\item 0.419970039631746\n",
       "\\item 0.0469543988739774\n",
       "\\item 0.0241040293040293\n",
       "\\item 0.67258345763243\n",
       "\\item 0.526772102040275\n",
       "\\item 0.633922369109906\n",
       "\\item 0.122292661306947\n",
       "\\item 0.170855978803628\n",
       "\\item 0.130204650852884\n",
       "\\item 0.264960078116749\n",
       "\\item 0.00929090909090909\n",
       "\\item 0.0296571428571429\n",
       "\\item 0.27537588885422\n",
       "\\item 0.0365619047619048\n",
       "\\item 0.0838814814814815\n",
       "\\item 0.4780178032596\n",
       "\\item 0.40745326915483\n",
       "\\item 0.315968944272752\n",
       "\\item 0.0460666666666667\n",
       "\\item 0.0749979171771625\n",
       "\\item 0.643386147970091\n",
       "\\item 0.503910814401044\n",
       "\\item 0.0305651662063427\n",
       "\\item 0.128120613051956\n",
       "\\item 0.239764250808907\n",
       "\\item 0.2464965057763\n",
       "\\item 0.808017646322295\n",
       "\\item 0.712779408041\n",
       "\\item 0.335160432306445\n",
       "\\item 0.218738630654148\n",
       "\\item 0.00213333333333333\n",
       "\\item 0.0358\n",
       "\\item 0.502336046085691\n",
       "\\item 0.283245683728037\n",
       "\\item 0.0552761686075119\n",
       "\\item 0.422017850833885\n",
       "\\item 0.02720853432282\n",
       "\\item 0.109115909918344\n",
       "\\item 0.330525630675507\n",
       "\\item 0.43256635663063\n",
       "\\item 0.79601703297908\n",
       "\\item 0.358075324599865\n",
       "\\item 0.35064557697692\n",
       "\\item 0.139632051473871\n",
       "\\item 0.819280051850075\n",
       "\\item 0.0324\n",
       "\\item 0.0297039215686275\n",
       "\\item 0.351741122153351\n",
       "\\item 0.0433333333333333\n",
       "\\item 0.225181928350018\n",
       "\\item 0.308127300339668\n",
       "\\item 0.0231741076533529\n",
       "\\item 0.0617448801742919\n",
       "\\item 0.390556047016344\n",
       "\\item 0.0494229691876751\n",
       "\\item 0.102529310344828\n",
       "\\item 0.385699812602209\n",
       "\\item 0.0902761904761905\n",
       "\\item 0.422158145200042\n",
       "\\item 0.0518428571428571\n",
       "\\item 0.0448\n",
       "\\item 0.427318604533966\n",
       "\\item 0.0626666666666667\n",
       "\\item 0.300187172971319\n",
       "\\item 0.590423419714938\n",
       "\\item 0.337406643222685\n",
       "\\item 0.251815816530413\n",
       "\\item 0.0184242424242424\n",
       "\\item 0.040888198757764\n",
       "\\item 0.0682119174942704\n",
       "\\item 0.446553384055558\n",
       "\\item 0.0181931677018634\n",
       "\\item 0.0421333333333333\n",
       "\\item 0.0714842572497745\n",
       "\\item 0.00566666666666667\n",
       "\\item 0.372760666433911\n",
       "\\item 0.0704874458874459\n",
       "\\item 0.546724902835585\n",
       "\\item 0.154972089314195\n",
       "\\item 0.21515633364454\n",
       "\\item 0.731486138052025\n",
       "\\item 0.303174705009781\n",
       "\\item 0.556912440704805\n",
       "\\item 0.02880853432282\n",
       "\\item 0.68762645898416\n",
       "\\item 0.631088747276892\n",
       "\\item 0.291282484948002\n",
       "\\item 0.0409713879681124\n",
       "\\item 0.226215568600665\n",
       "\\item 0.38698600377361\n",
       "\\item 0.00946666666666667\n",
       "\\item 0.270738610408377\n",
       "\\item 0.48055685617084\n",
       "\\item 0.264662890731995\n",
       "\\item 0.544753775571682\n",
       "\\item 0.0512095238095238\n",
       "\\item 0.0368659768384966\n",
       "\\item 0.589843703943426\n",
       "\\item 0.00633333333333333\n",
       "\\item 0.149412822175022\n",
       "\\item 0.0962490028490029\n",
       "\\item 0.175590482506\n",
       "\\item 0.0204595365418895\n",
       "\\item 0.330935438341927\n",
       "\\item 0.36837963138542\n",
       "\\item 0.00209090909090909\n",
       "\\item 0.196189743589744\n",
       "\\item 0.37747921222951\n",
       "\\item 0.674150171699797\n",
       "\\item 0.588513467924093\n",
       "\\item 0.17639066039779\n",
       "\\item 0.269172343714798\n",
       "\\item 0.213655305949585\n",
       "\\item 0.0186095238095238\n",
       "\\item 0.545525636802509\n",
       "\\item 0.158670588235294\n",
       "\\item 0.00466666666666667\n",
       "\\item 0.237250675472075\n",
       "\\item 0.0629230769230769\n",
       "\\item 0.0241676767676768\n",
       "\\item 0.230720018354539\n",
       "\\item 0.486711145925037\n",
       "\\item 0.223129877036028\n",
       "\\item 0.151466005022699\n",
       "\\item 0.669874251341531\n",
       "\\item 0.418571734587252\n",
       "\\item 0.0190091954022989\n",
       "\\item 0.00943330375628512\n",
       "\\item 0.1595184278895\n",
       "\\item 0.112942857142857\n",
       "\\item 0.236210827746021\n",
       "\\item 0.010578231292517\n",
       "\\item 0.193046918172156\n",
       "\\item 0.363330145081496\n",
       "\\item 0.233797144352314\n",
       "\\item 8e-04\n",
       "\\item 0.38461456012582\n",
       "\\item 0.0407333333333333\n",
       "\\item 0.167713804713805\n",
       "\\item 0.335765623012756\n",
       "\\item 0.0341247794811705\n",
       "\\item 0.058774358974359\n",
       "\\item 0.29656016885274\n",
       "\\item 0.0757377084167\n",
       "\\item 0.661128647775432\n",
       "\\item 0.486335239752585\n",
       "\\item 0.274125876299378\n",
       "\\item 0.398757004765239\n",
       "\\item 0.312901092470678\n",
       "\\item 0.0167878787878788\n",
       "\\item 0.49212230802782\n",
       "\\item 0.417712580646714\n",
       "\\item 0.368876819281209\n",
       "\\item 0.399285034115656\n",
       "\\item 0.358669524090498\n",
       "\\item 0.0128\n",
       "\\item 0.317530757368029\n",
       "\\item 0.198690318213617\n",
       "\\item 0.0969715728715729\n",
       "\\item 0.314874138665033\n",
       "\\item 0.210215561198177\n",
       "\\item 0.547314875872405\n",
       "\\item 0.16773030602238\n",
       "\\item 0.043155587228107\n",
       "\\item 0.0465238095238095\n",
       "\\item 0.302429760258227\n",
       "\\item 0.0102242424242424\n",
       "\\item 0.783340565198175\n",
       "\\item 0.187846866202753\n",
       "\\item 0.345716502198186\n",
       "\\item 0.259585062890326\n",
       "\\item 0.213997280585225\n",
       "\\item 0.507869077340633\n",
       "\\item 0.175453119868637\n",
       "\\item 0.414077009493443\n",
       "\\item 0.316329633371636\n",
       "\\item 0.384129505114509\n",
       "\\item 0.33053247964253\n",
       "\\item 0.256483765463015\n",
       "\\item 0.116038211146528\n",
       "\\item 0.035851355661882\n",
       "\\item 0.0248974232117089\n",
       "\\item 0.0848666666666666\n",
       "\\item 0.207570096225025\n",
       "\\item 0.268501287085481\n",
       "\\item 0.0632095238095238\n",
       "\\item 0.25781656063235\n",
       "\\item 0.0502459096459096\n",
       "\\item 0.0831904761904762\n",
       "\\item 0.0328032924190819\n",
       "\\item 0.242366292328361\n",
       "\\item 0.454527494236908\n",
       "\\item 0.0289380952380952\n",
       "\\item 0.427383136581075\n",
       "\\item 0.785156169090612\n",
       "\\item 0.0652\n",
       "\\item 0.447523396573703\n",
       "\\item 0.189641133579641\n",
       "\\item 0.0413380952380952\n",
       "\\item 0.146168975468975\n",
       "\\item 0.511608303418371\n",
       "\\item 0.0105575757575758\n",
       "\\item 0.684172588569353\n",
       "\\item 0.142118803418803\n",
       "\\item 0.0595370515841843\n",
       "\\item 0.656540648053628\n",
       "\\item 0.668990329836979\n",
       "\\item 0.108187426694816\n",
       "\\item 0.0119358070500928\n",
       "\\item 0.392743430515844\n",
       "\\item 0.223110197439574\n",
       "\\item 0.543278103035985\n",
       "\\item 0.277935719686639\n",
       "\\item 0.389467782217782\n",
       "\\item 0.39109015811024\n",
       "\\item 0.368649629375681\n",
       "\\item 0.231261163081585\n",
       "\\item 0.031578231292517\n",
       "\\item 0.661937946552801\n",
       "\\item 0.0328666666666667\n",
       "\\item 0.291528407340625\n",
       "\\item 0.0371926406926407\n",
       "\\item 0.632235050781604\n",
       "\\item 0.0629591836734694\n",
       "\\item 0.209477182680085\n",
       "\\item 0.20159488046791\n",
       "\\item 0.0161333333333333\n",
       "\\item 0.318255149005995\n",
       "\\item 0.517569014595609\n",
       "\\item 0.0742421245421245\n",
       "\\item 0.020978231292517\n",
       "\\item 0.621984153615674\n",
       "\\item 0.134348891459418\n",
       "\\item 0.211825940473073\n",
       "\\item 0.514845440695551\n",
       "\\item 0.0434428571428571\n",
       "\\item 0.22194794062147\n",
       "\\item 0.372580601195712\n",
       "\\item 0.0650410256410256\n",
       "\\item 0.0517\n",
       "\\item 0.23162295662591\n",
       "\\item 0.324765915803077\n",
       "\\item 0.150927674624226\n",
       "\\item 0.596247355676311\n",
       "\\item 0.368936985923394\n",
       "\\item 0.305086965500248\n",
       "\\item 0.00426233766233766\n",
       "\\item 0.264110301120331\n",
       "\\item 0.424242617232572\n",
       "\\item 0.00784489795918367\n",
       "\\item 0.475891914981982\n",
       "\\item 0.382656273186503\n",
       "\\item 0.662802837499407\n",
       "\\item 0.150482051282051\n",
       "\\item 0.682377327830452\n",
       "\\item 0.0924564102564103\n",
       "\\item 0.045995670995671\n",
       "\\item 0.0108\n",
       "\\item 0.702087492624913\n",
       "\\item 0.379392145539237\n",
       "\\item 0.001\n",
       "\\item 0.0559300366300366\n",
       "\\item 0.0210909090909091\n",
       "\\item 0.0730904761904762\n",
       "\\item 0.120644289044289\n",
       "\\item 0.382560243451559\n",
       "\\item 0.428814084119177\n",
       "\\item 0.146867040149393\n",
       "\\item 0.827661620155868\n",
       "\\item 0.0658507936507936\n",
       "\\item 0.186260212093377\n",
       "\\item 0.748145069080872\n",
       "\\item 0.00766666666666667\n",
       "\\item 0.144147362499376\n",
       "\\item 0.367816674617236\n",
       "\\item 0.249503010399562\n",
       "\\item 0.270551945145766\n",
       "\\item 0.232542773974768\n",
       "\\item 0.494969044686016\n",
       "\\item 0.730202050551766\n",
       "\\item 0.588403663316362\n",
       "\\item 0.371865109186483\n",
       "\\item 0.208005259239406\n",
       "\\item 0.195637169043863\n",
       "\\item 0.160361701261701\n",
       "\\item 0.0193714285714286\n",
       "\\item 0.0158666666666667\n",
       "\\item 0.0280538461538462\n",
       "\\item 0.729376768112002\n",
       "\\item 0.244041215973569\n",
       "\\item 0.118790849673203\n",
       "\\item 0.106571717171717\n",
       "\\item 0.433991872100007\n",
       "\\item 0.0608857142857143\n",
       "\\item 0.0691333333333333\n",
       "\\item 0.65000191022047\n",
       "\\item 0.651639088112206\n",
       "\\item 0.439207165959861\n",
       "\\item 0.346891056032109\n",
       "\\item 0.29402385335275\n",
       "\\item 0.00659815546772069\n",
       "\\item 0.0982682431995865\n",
       "\\item 0.0739904761904762\n",
       "\\item 0.761544699197131\n",
       "\\item 0.00649090909090909\n",
       "\\item 0.107051307847005\n",
       "\\item 0.349951635559961\n",
       "\\item 0.433959200461904\n",
       "\\item 0.0274949096880131\n",
       "\\item 0.0141333333333333\n",
       "\\item 0.235675316496369\n",
       "\\item 0.474336717738204\n",
       "\\item 0.709188648804476\n",
       "\\item 0.327763742506913\n",
       "\\item 0.0388714285714286\n",
       "\\item 0.2240654584846\n",
       "\\item 0.346826801367435\n",
       "\\item 0.757722394429666\n",
       "\\item 0.0300666666666667\n",
       "\\item 0.199370531767083\n",
       "\\item 0.600581905761181\n",
       "\\item 0.368189963898077\n",
       "\\item 0.201154013491749\n",
       "\\item 0.668778506396282\n",
       "\\item 0.0204\n",
       "\\item 0.154136507936508\n",
       "\\item 0.672268360737523\n",
       "\\item 0.157450273855882\n",
       "\\item 0.000235294117647059\n",
       "\\item 0.302083590675208\n",
       "\\item 0.140525925925926\n",
       "\\item 0.0714481792717087\n",
       "\\item 0.608711319649256\n",
       "\\item 0.112314285714286\n",
       "\\item 0.333080615800697\n",
       "\\item 0.00973333333333333\n",
       "\\item 0.263663806950509\n",
       "\\item 0.006\n",
       "\\item 0.0402598219390672\n",
       "\\item 0.0670656990068755\n",
       "\\item 0.106415586712138\n",
       "\\item 0.0468166666666667\n",
       "\\item 0.0395619047619048\n",
       "\\item 0.0919690476190476\n",
       "\\item 0.0633897435897436\n",
       "\\item 0.0445566923497958\n",
       "\\item 0.0189047619047619\n",
       "\\item 0.115769140383426\n",
       "\\item 0.0618047619047619\n",
       "\\item 0.0203686274509804\n",
       "\\item 0.00142424242424242\n",
       "\\item 0.38342849197586\n",
       "\\item 0.852856601307143\n",
       "\\item 0.260003782953814\n",
       "\\item 0.0663207600281492\n",
       "\\item 0.0197575757575758\n",
       "\\item 0.499076422777154\n",
       "\\item 0.227236408036408\n",
       "\\item 0.128069024840808\n",
       "\\item 0.604572949178595\n",
       "\\item 0.6104474259193\n",
       "\\item 0.0547285714285714\n",
       "\\item 0.538792271155307\n",
       "\\item 0.0779490842490843\n",
       "\\item 0.0194666666666667\n",
       "\\item 0.687524851748526\n",
       "\\item 0.248955861279166\n",
       "\\item 0.425913216230292\n",
       "\\item 0.261883747789335\n",
       "\\item 0.301614519580037\n",
       "\\item 0.227101960784314\n",
       "\\item 0.0228\n",
       "\\item 0.458696448784663\n",
       "\\item 0.0623428571428571\n",
       "\\item 0.0670854700854701\n",
       "\\item 0.329848336813399\n",
       "\\item 0.374627178391022\n",
       "\\item 0.42265869101895\n",
       "\\item 0.117145064086241\n",
       "\\item 0.670749969824215\n",
       "\\item 0.379973106276359\n",
       "\\item 0.220000872163182\n",
       "\\item 0.148253374076904\n",
       "\\item 0.0697224089635854\n",
       "\\item 0.391243062271746\n",
       "\\item 0.0974380952380952\n",
       "\\item 0.00289090909090909\n",
       "\\item 0.447340962760438\n",
       "\\item 0.352059064356158\n",
       "\\item 0.328763576174391\n",
       "\\item 0.00995757575757576\n",
       "\\item 0.0564047619047619\n",
       "\\item 0.499517306054529\n",
       "\\item 0.264267144492313\n",
       "\\item 0.648435212339\n",
       "\\item 0.0607229524288348\n",
       "\\item 0.459917647058823\n",
       "\\item 0.0934151882975412\n",
       "\\item 0.814013599147317\n",
       "\\item 0.00746666666666667\n",
       "\\item 0.027237037037037\n",
       "\\item 0.278787963272301\n",
       "\\item 0.644552081252081\n",
       "\\item 0.706449246404825\n",
       "\\item 0.0048\n",
       "\\item 0.0580745098039216\n",
       "\\item 0.0184047619047619\n",
       "\\item 0.219033211233211\n",
       "\\item 0.362470971727596\n",
       "\\item 0.00954285714285714\n",
       "\\item 0.865554624940664\n",
       "\\item 0.604889026290867\n",
       "\\item 0.45710416171468\n",
       "\\item 0.321347386013069\n",
       "\\item 0.556274691057301\n",
       "\\item 0.144271763175016\n",
       "\\item 0.536879351917059\n",
       "\\item 0.233402319335823\n",
       "\\item 0.10163894993895\n",
       "\\item 0.212447939519923\n",
       "\\item 0.230654565473356\n",
       "\\item 0.413968897690306\n",
       "\\item 0.19102449294001\n",
       "\\item 0.319777827286465\n",
       "\\item 0.291493981579849\n",
       "\\item 0.00809090909090909\n",
       "\\item 0.148263362156457\n",
       "\\item 0.002\n",
       "\\item 0.474988858590471\n",
       "\\item 0.293499554269232\n",
       "\\item 0.133104761904762\n",
       "\\item 0.4678877201779\n",
       "\\item 0.0895974842767296\n",
       "\\item 0.174179310344828\n",
       "\\item 0.301087263839736\n",
       "\\item 0.0460444444444444\n",
       "\\item 0.267754674263379\n",
       "\\item 0.12642614379085\n",
       "\\item 0.0196242424242424\n",
       "\\item 0.537195793473164\n",
       "\\item 0.0935333333333333\n",
       "\\item 0.0628047619047619\n",
       "\\item 0.0784666666666667\n",
       "\\item 0.347248441551687\n",
       "\\item 0.243234312177997\n",
       "\\item 0.0110024737167594\n",
       "\\item 0.389838466243758\n",
       "\\item 0.492113499292125\n",
       "\\item 0.027778231292517\n",
       "\\item 0.514638372923188\n",
       "\\item 0.126111111111111\n",
       "\\item 0.0444206349206349\n",
       "\\item 0.252349997713857\n",
       "\\item 0.280270562056559\n",
       "\\item 0.319795079305615\n",
       "\\item 0.0180242424242424\n",
       "\\item 0.183724018838305\n",
       "\\item 0.0775756187783935\n",
       "\\item 0.383244588744589\n",
       "\\item 0.175342857142857\n",
       "\\item 0.0135931677018634\n",
       "\\item 0.0409020408163265\n",
       "\\item 0.335437364471853\n",
       "\\item 0.454953809948352\n",
       "\\item 0.810441547633601\n",
       "\\item 0.0352747474747475\n",
       "\\item 0.211952941176471\n",
       "\\item 0.0235285714285714\n",
       "\\item 0.457761027950668\n",
       "\\item 0.212335246282094\n",
       "\\item 0.160738095238095\n",
       "\\item 0.0646730158730159\n",
       "\\item 0.399322159631053\n",
       "\\item 0.171839071603778\n",
       "\\item 0.052356862745098\n",
       "\\item 0.0319560090702948\n",
       "\\item 0.335196471069994\n",
       "\\item 0.190525165001189\n",
       "\\item 0.00909090909090909\n",
       "\\item 0.0393619047619048\n",
       "\\item 0.432405458834251\n",
       "\\item 0.580810988130229\n",
       "\\item 0.143869741369741\n",
       "\\item 0.0404943504256937\n",
       "\\item 9.09090909090909e-05\n",
       "\\item 0.41961652229829\n",
       "\\item 0.0870783103254431\n",
       "\\item 0.208521699193482\n",
       "\\item 0.0914508899709711\n",
       "\\item 0.507317141920146\n",
       "\\item 0.0702642857142857\n",
       "\\item 0.230625986019151\n",
       "\\item 0.123316172463305\n",
       "\\item 0.486371238702818\n",
       "\\item 0.127573405211141\n",
       "\\item 0.0808883933676387\n",
       "\\item 0.655226094955746\n",
       "\\item 0.640896748726266\n",
       "\\item 0.120290708007341\n",
       "\\item 0.418326940814628\n",
       "\\item 0.174287601034734\n",
       "\\item 0.0178337662337662\n",
       "\\item 0.0485200241671569\n",
       "\\item 0.166570586584339\n",
       "\\item 0.223664595976089\n",
       "\\item 0.360729834841786\n",
       "\\item 0.22123163725692\n",
       "\\item 0.483856692740181\n",
       "\\item 0.0284039215686275\n",
       "\\item 0.217811970217412\n",
       "\\item 0.637398736824343\n",
       "\\item 0.775120870356815\n",
       "\\item 0.121545982906221\n",
       "\\item 0.0895825396825397\n",
       "\\item 0.00440952380952381\n",
       "\\item 0.378986217358631\n",
       "\\item 0.0328405797101449\n",
       "\\item 0.0235205128205128\n",
       "\\item 0.663065234842642\n",
       "\\item 0.1168\n",
       "\\item 0.185046405228758\n",
       "\\item 0.33281228668113\n",
       "\\item 0.729494346308559\n",
       "\\item 0.0794250377073906\n",
       "\\item 0.0691716603882932\n",
       "\\item 0.175877432869996\n",
       "\\item 0.0268047619047619\n",
       "\\item 0.421053451095202\n",
       "\\item 0.487261947186317\n",
       "\\item 0.640005404166208\n",
       "\\item 0.248536716178094\n",
       "\\item 0.122024242424242\n",
       "\\item 0.0637686274509804\n",
       "\\item 0.0780758907758907\n",
       "\\item 0.0116666666666667\n",
       "\\item 0.547958892884658\n",
       "\\item 0.421848057427041\n",
       "\\item 0.0413247225205872\n",
       "\\item 0.36810381275491\n",
       "\\item 0.0498666666666667\n",
       "\\item 0.677631688827879\n",
       "\\item 0.249111493724492\n",
       "\\item 0.735682519824392\n",
       "\\item 0.0276269841269841\n",
       "\\item 0.591329460068238\n",
       "\\item 0.440550995239788\n",
       "\\item 0.154649595687332\n",
       "\\item 0.0480047619047619\n",
       "\\item 0.235115653936707\n",
       "\\item 0.0869333333333333\n",
       "\\item 0.132469841269841\n",
       "\\item 0.507676108528792\n",
       "\\item 0.237856732801472\n",
       "\\item 0.00933333333333333\n",
       "\\item 0.250326537118351\n",
       "\\item 0.237531614062577\n",
       "\\item 0.0612571428571429\n",
       "\\item 0.0281230769230769\n",
       "\\item 0.543370069951374\n",
       "\\item 0.295349594632302\n",
       "\\item 0.195602748441943\n",
       "\\item 0.0141333333333333\n",
       "\\item 0.282436814327739\n",
       "\\item 0.214038128809445\n",
       "\\item 0.355551123924004\n",
       "\\item 0.199634717337053\n",
       "\\item 0.0886120082815735\n",
       "\\item 0.476181227928559\n",
       "\\item 0.313311821803927\n",
       "\\item 0.655482090457493\n",
       "\\item 0.0180909090909091\n",
       "\\item 0.188301565432909\n",
       "\\item 0.0731425287356322\n",
       "\\item 0.0162879869189055\n",
       "\\item 0.0262969696969697\n",
       "\\item 0.114137254580363\n",
       "\\item 0.195256132756133\n",
       "\\item 0.645082171530284\n",
       "\\item 0.0866\n",
       "\\item 0.173414285714286\n",
       "\\item 0.04620853432282\n",
       "\\item 0.729948878366983\n",
       "\\item 0.0715447293447293\n",
       "\\item 0.595765895398517\n",
       "\\item 0.0353555555555556\n",
       "\\item 0.0638862745098039\n",
       "\\item 0.155683641307982\n",
       "\\item 0.60683780633572\n",
       "\\item 0.524395606518497\n",
       "\\item 0.418284042284305\n",
       "\\item 0.646343786709744\n",
       "\\item 0.0231333333333333\n",
       "\\item 0.351036757660706\n",
       "\\item 0.584150542615313\n",
       "\\item 0.115877922077922\n",
       "\\item 0.0738506879478207\n",
       "\\item 0.638429688940546\n",
       "\\item 0.0088\n",
       "\\item 0.218502071516578\n",
       "\\item 0.236234709396882\n",
       "\\item 0.0561294264607698\n",
       "\\item 0.386173278994332\n",
       "\\item 0.122357142857143\n",
       "\\item 0.270787372063464\n",
       "\\item 0.512690389846322\n",
       "\\item 0.0201197583511016\n",
       "\\item 0.538740727771748\n",
       "\\item 0.0373230769230769\n",
       "\\item 0.557972159663091\n",
       "\\item 0.218904509525562\n",
       "\\item 0.430485959095843\n",
       "\\item 0.6906847732655\n",
       "\\item 0.0185422463893792\n",
       "\\item 0.0116913626056483\n",
       "\\item 0.0365115646258503\n",
       "\\item 0.697792762657136\n",
       "\\item 0.785361399116238\n",
       "\\item 0.0461333333333333\n",
       "\\item 0.006\n",
       "\\item 0.0286761904761905\n",
       "\\item 0.10196790986791\n",
       "\\item 0.0511050583628677\n",
       "\\item 0.0375675213675214\n",
       "\\item 0.0632798316111749\n",
       "\\item 0.643925769277783\n",
       "\\item 0.143593310657596\n",
       "\\item 0.0197217391304348\n",
       "\\item 0.509039841027984\n",
       "\\item 0.631491229789319\n",
       "\\item 0.15473932016072\n",
       "\\item 0.0619948868741321\n",
       "\\item 0.631510040245023\n",
       "\\item 0.0200285714285714\n",
       "\\item 0.40157776900883\n",
       "\\item 0.0524243705985835\n",
       "\\item 0.175101887372963\n",
       "\\item 0.00566666666666667\n",
       "\\item 0.243075635964627\n",
       "\\item 0.420006484119292\n",
       "\\item 0.109966666666667\n",
       "\\item 0.220755648745138\n",
       "\\item 0.0024\n",
       "\\item 0.513641429945596\n",
       "\\item 0.17123538716673\n",
       "\\item 0.00209090909090909\n",
       "\\item 0.557749673749674\n",
       "\\item 0.0597333333333333\n",
       "\\item 0.439517909276698\n",
       "\\item 0.454787856259751\n",
       "\\item 0.248552413254401\n",
       "\\item 0.134250549450549\n",
       "\\item 0.657388560901124\n",
       "\\item 0.111611111111111\n",
       "\\item 0.106639956122309\n",
       "\\item 0.345859708561882\n",
       "\\item 0.0540047619047619\n",
       "\\item 0.367525596681185\n",
       "\\item 0.593266379342398\n",
       "\\item 0.176785411067838\n",
       "\\item 0.0188\n",
       "\\item 0.290231616643269\n",
       "\\item 0.0480683982683983\n",
       "\\item 0.176752496860813\n",
       "\\item 0.00733333333333333\n",
       "\\item 0.50031639077822\n",
       "\\item 0.412294262644807\n",
       "\\item 0.518579294848777\n",
       "\\item 0.0261326007326007\n",
       "\\item 0.01905983436853\n",
       "\\item 0.0651991231275119\n",
       "\\item 0.72489130706926\n",
       "\\item 0.0268290043290043\n",
       "\\item 0.404482163427366\n",
       "\\item 0.0928116447493806\n",
       "\\item 0.0546692677070828\n",
       "\\item 0.140716596247331\n",
       "\\item 0.00679763177998472\n",
       "\\item 0.358955824179798\n",
       "\\item 0.0767428571428571\n",
       "\\item 0.532318555576708\n",
       "\\item 0.504254436513197\n",
       "\\item 0.518370962383905\n",
       "\\item 0.0338566473655403\n",
       "\\item 0.0302464646464646\n",
       "\\item 0.251135388545539\n",
       "\\item 0.00133333333333333\n",
       "\\item 0.323852168737935\n",
       "\\item 0.0388\n",
       "\\item 0.730835524248414\n",
       "\\item 0.620188874874476\n",
       "\\item 0.630244244419824\n",
       "\\item 0.0721142638456071\n",
       "\\item 0.12166031812939\n",
       "\\item 0.305540265889635\n",
       "\\item 0.245012615605485\n",
       "\\item 0.490300430941607\n",
       "\\item 0.0034\n",
       "\\item 0.0107575757575758\n",
       "\\item 0.882408533589661\n",
       "\\item 0.126232492997199\n",
       "\\item 0.636695134235557\n",
       "\\item 0.615489800748144\n",
       "\\item 0.0198761904761905\n",
       "\\item 0.468966866621097\n",
       "\\item 0.0903596273291925\n",
       "\\item 0.3519291679354\n",
       "\\item 0.388504130427536\n",
       "\\item 0.251495953222723\n",
       "\\item 0.170214946457052\n",
       "\\item 0.341623667918618\n",
       "\\item 0.299819894697688\n",
       "\\item 0.0504290043290043\n",
       "\\item 0.0949139573070608\n",
       "\\item 0.549730149364976\n",
       "\\item 0.416010308146513\n",
       "\\item 0.331928693275643\n",
       "\\item 0.150220757352101\n",
       "\\item 0.156840427251817\n",
       "\\item 0.537939348869069\n",
       "\\item 0.314179757321879\n",
       "\\item 0.560541568132123\n",
       "\\item 0.25118514292517\n",
       "\\item 0.288552866902346\n",
       "\\item 0.407984128813522\n",
       "\\item 0.176759706959707\n",
       "\\item 0.0212\n",
       "\\item 0.641934489245435\n",
       "\\item 0.0203396825396825\n",
       "\\item 0.0052\n",
       "\\item 0.401647385027601\n",
       "\\item 0.511356057862999\n",
       "\\item 0.00623809523809524\n",
       "\\item 0.0592004329004329\n",
       "\\item 0.237141352014374\n",
       "\\item 0.085396392243525\n",
       "\\item 0.788862512826833\n",
       "\\item 0.0808649572649573\n",
       "\\item 0.473144649730227\n",
       "\\item 0.0228\n",
       "\\item 0.136904761904762\n",
       "\\item 0.140655555555556\n",
       "\\item 0.765634914227787\n",
       "\\item 0.6158\n",
       "\\item 0.539687413737704\n",
       "\\item 0.0686062027229007\n",
       "\\item 0.483012053932509\n",
       "\\item 0.0733139573070608\n",
       "\\item 0.215238805970149\n",
       "\\item 0.264931591323405\n",
       "\\item 0.318030064563293\n",
       "\\item 0.631477491312862\n",
       "\\item 0.407353968045321\n",
       "\\item 0.00818468899521531\n",
       "\\item 0.186946111375077\n",
       "\\item 0.086162981352753\n",
       "\\item 0.215811473127263\n",
       "\\item 0.0303543436955202\n",
       "\\item 0.604514698161444\n",
       "\\item 0.723084875091197\n",
       "\\item 0.289415592624221\n",
       "\\item 0.372545743145743\n",
       "\\item 0.336713213980879\n",
       "\\item 0.039318315018315\n",
       "\\item 0.309967764625572\n",
       "\\item 0.0734321043249836\n",
       "\\item 0.201138644688645\n",
       "\\item 0.256224259866968\n",
       "\\item 0.314791419962736\n",
       "\\item 0.140282184809471\n",
       "\\item 0.099782683982684\n",
       "\\item 0.704344831895001\n",
       "\\item 0.201224498525836\n",
       "\\item 0.0415333333333333\n",
       "\\item 0.0519019607843137\n",
       "\\item 0.694303316604856\n",
       "\\item 0.282422205186471\n",
       "\\item 0.120420660603014\n",
       "\\item 0.172652940854872\n",
       "\\item 0.304401309154514\n",
       "\\item 0.05003186537773\n",
       "\\item 0.508016796690953\n",
       "\\item 0.70500700439963\n",
       "\\item 0.504013885833157\n",
       "\\item 0.343224242424242\n",
       "\\item 0.0454884057971014\n",
       "\\item 0.570207676366393\n",
       "\\item 0.125894983688087\n",
       "\\item 0.133616987610091\n",
       "\\item 0.545872231676956\n",
       "\\item 0.209753091684435\n",
       "\\item 0.255642772443036\n",
       "\\item 0.00151428571428571\n",
       "\\item 0.0741897435897436\n",
       "\\item 0.465913523165361\n",
       "\\item 0.166758862165556\n",
       "\\item 0.16904126984127\n",
       "\\item 0.333526921480466\n",
       "\\item 0.200735817805383\n",
       "\\item 0.0346285714285714\n",
       "\\item 0.299428793348988\n",
       "\\item 0.284971170474937\n",
       "\\item 0.244132627546813\n",
       "\\item 0.311459815778714\n",
       "\\item 0.314835655038608\n",
       "\\item 0.288097508742712\n",
       "\\item 0.00133333333333333\n",
       "\\item 0.686166787728018\n",
       "\\item 0.282616631571023\n",
       "\\item 0.3848515984014\n",
       "\\item 0.129150549450549\n",
       "\\item 0.415681447726581\n",
       "\\item 0.223621388443156\n",
       "\\item 0.469968886032506\n",
       "\\item 0.154807008086253\n",
       "\\item 0.299682621888885\n",
       "\\item 0.299367211511165\n",
       "\\item 0.601434676318298\n",
       "\\item 0.610963313102379\n",
       "\\item 0.272414718614719\n",
       "\\item 0.0222\n",
       "\\item 0.118647619047619\n",
       "\\item 0.659677105017832\n",
       "\\item 0.207591088833194\n",
       "\\item 0.006\n",
       "\\item 0.0170242424242424\n",
       "\\item 0.247568264688533\n",
       "\\item 0.0280666666666667\n",
       "\\item 0.290326480640085\n",
       "\\item 0.32214621413716\n",
       "\\item 0.0250448979591837\n",
       "\\item 0.0167897435897436\n",
       "\\item 0.0112666666666667\n",
       "\\item 0.075847619047619\n",
       "\\item 0.102975200989487\n",
       "\\item 0.0895166666666667\n",
       "\\item 0.21335544210567\n",
       "\\item 0.0940847880914818\n",
       "\\item 0.294917309481363\n",
       "\\item 0.031970695970696\n",
       "\\item 0.0320560846560847\n",
       "\\item 0.316107836913662\n",
       "\\item 0.0400818070818071\n",
       "\\item 0.183807070707071\n",
       "\\item 0.648082372111897\n",
       "\\item 0.266412670232578\n",
       "\\item 0.60368545869941\n",
       "\\item 0.178455544164321\n",
       "\\item 0.0357380952380952\n",
       "\\item 0.0612242424242424\n",
       "\\item 0.22099898989899\n",
       "\\item 0.0977333333333333\n",
       "\\item 0.0270196078431373\n",
       "\\item 0.263916995610636\n",
       "\\item 0.034178231292517\n",
       "\\item 0.650701455301455\n",
       "\\item 0.00899567099567099\n",
       "\\item 0.179704777489037\n",
       "\\item 0.247198619248563\n",
       "\\item 0.0494666666666667\n",
       "\\item 0.242614081012061\n",
       "\\item 0.0611890756302521\n",
       "\\item 0.161042148813548\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.0386666666666667\n",
       "2. 0.284307793173011\n",
       "3. 0.357987802849658\n",
       "4. 0.0494333333333333\n",
       "5. 0.589378795671079\n",
       "6. 0.111455237641615\n",
       "7. 0.323146097272539\n",
       "8. 0.323419225306165\n",
       "9. 0.0732036075036075\n",
       "10. 9.09090909090909e-05\n",
       "11. 0.371806204993215\n",
       "12. 0.253524318826233\n",
       "13. 0.0327115646258503\n",
       "14. 0.768577930732082\n",
       "15. 0.118376479076479\n",
       "16. 0.0389967320261438\n",
       "17. 0.030564267866067\n",
       "18. 0.153095203946263\n",
       "19. 0.202483166814215\n",
       "20. 0.0114024737167594\n",
       "21. 0.106046405228758\n",
       "22. 0.29346375424886\n",
       "23. 0.799570000538655\n",
       "24. 0.00573333333333333\n",
       "25. 0.5085797649002\n",
       "26. 0.0682929499072356\n",
       "27. 0.287933901925218\n",
       "28. 0.418726586129839\n",
       "29. 0.426580377603335\n",
       "30. 0.314469341223495\n",
       "31. 0.292820313987962\n",
       "32. 0.657049667521002\n",
       "33. 0.0256076371389804\n",
       "34. 0.351996273468985\n",
       "35. 0.566468441782719\n",
       "36. 0.268638978801203\n",
       "37. 0.673271758263628\n",
       "38. 0.639288675661089\n",
       "39. 0.787783287780627\n",
       "40. 0.0233333333333333\n",
       "41. 0.171500477300477\n",
       "42. 0.119285409016808\n",
       "43. 0.0653362745098039\n",
       "44. 0.627611595913476\n",
       "45. 0.259306722689076\n",
       "46. 0.657464618843982\n",
       "47. 0.223395785757563\n",
       "48. 0.0201636363636364\n",
       "49. 0.216319848984649\n",
       "50. 0.131109351432881\n",
       "51. 0.0198444444444444\n",
       "52. 0.0243603318250377\n",
       "53. 0.204613680522376\n",
       "54. 0.232045146520104\n",
       "55. 0.779745979852686\n",
       "56. 0.375555555555556\n",
       "57. 0.575470191632187\n",
       "58. 0.444164448582921\n",
       "59. 0.123793530543531\n",
       "60. 0.394624221962658\n",
       "61. 0.787017562218569\n",
       "62. 0.0126242424242424\n",
       "63. 0.0957952380952381\n",
       "64. 0.143419180819181\n",
       "65. 0.0484333333333333\n",
       "66. 0.0992571428571428\n",
       "67. 0.00917142857142857\n",
       "68. 0.0188188110026619\n",
       "69. 0.544796632996633\n",
       "70. 0.125974548440066\n",
       "71. 0.481474910143135\n",
       "72. 0.0384257703081232\n",
       "73. 0.0466444444444444\n",
       "74. 0.503750648882545\n",
       "75. 0.0580952380952381\n",
       "76. 0.203208080808081\n",
       "77. 0.530479579137357\n",
       "78. 0.0368943977591036\n",
       "79. 0.25924207414433\n",
       "80. 0.0973109557109557\n",
       "81. 0.794891299403227\n",
       "82. 0.0474490028490029\n",
       "83. 0.0257497326203209\n",
       "84. 0.54952819565919\n",
       "85. 0.233513556115693\n",
       "86. 0.220663402613324\n",
       "87. 0.00313333333333333\n",
       "88. 0.0868405797101449\n",
       "89. 0.710100044877115\n",
       "90. 0.2369\n",
       "91. 0.0212975723622782\n",
       "92. 0.0691290043290043\n",
       "93. 0.0969240468804379\n",
       "94. 0.663681517942318\n",
       "95. 0.142473367487064\n",
       "96. 0.292054991818389\n",
       "97. 0.0947949096880131\n",
       "98. 0.064574358974359\n",
       "99. 0.332497741192073\n",
       "100. 0.333328000196019\n",
       "101. 0.525190712488606\n",
       "102. 0.0287575757575758\n",
       "103. 0.489178923361334\n",
       "104. 0.242720091605708\n",
       "105. 0.386952646541243\n",
       "106. 0.342856513695812\n",
       "107. 0.032830303030303\n",
       "108. 0.189535353535354\n",
       "109. 0.210482655945651\n",
       "110. 0.0498095238095238\n",
       "111. 0.0331333333333333\n",
       "112. 0.644927177881025\n",
       "113. 0.141847619047619\n",
       "114. 0.583307089324737\n",
       "115. 0.213456439993848\n",
       "116. 0.0228242424242424\n",
       "117. 0.0101480519480519\n",
       "118. 0.30572274359553\n",
       "119. 0.040411167945439\n",
       "120. 0.0219428571428571\n",
       "121. 0.0237333333333333\n",
       "122. 0.0600539542194715\n",
       "123. 0.0337307189542484\n",
       "124. 0.171074208304011\n",
       "125. 0.181488568614938\n",
       "126. 0.0301389978213508\n",
       "127. 0.0276758620689655\n",
       "128. 0.294828571428571\n",
       "129. 0.629823419909178\n",
       "130. 0.0543151515151515\n",
       "131. 0.59868781383623\n",
       "132. 0.256419523585888\n",
       "133. 0.306866698214982\n",
       "134. 0.282727478564128\n",
       "135. 0.340124195763443\n",
       "136. 0.50100325846369\n",
       "137. 0.0508666666666667\n",
       "138. 0.469653497438885\n",
       "139. 0.118086150015562\n",
       "140. 0.572841387756388\n",
       "141. 0.314598026815766\n",
       "142. 0.172211644749381\n",
       "143. 0.312275030741055\n",
       "144. 0.428704688112038\n",
       "145. 0.316485521866962\n",
       "146. 0.218884753213629\n",
       "147. 0.538241060137602\n",
       "148. 0.212618794480247\n",
       "149. 0.271734902537344\n",
       "150. 0.0656476190476191\n",
       "151. 0.0681892849447471\n",
       "152. 0.270012535356496\n",
       "153. 0.219599945265462\n",
       "154. 0.127282352941176\n",
       "155. 0.0117241830065359\n",
       "156. 0.129743353682011\n",
       "157. 0.0368705882352941\n",
       "158. 0.425313918564177\n",
       "159. 0.250565328386548\n",
       "160. 0.811754268262775\n",
       "161. 0.10218750106103\n",
       "162. 0.118322222222222\n",
       "163. 0.0282564102564103\n",
       "164. 0.0904044334975369\n",
       "165. 0.0226837789661319\n",
       "166. 0.441826362146999\n",
       "167. 0.0640064935064935\n",
       "168. 0.371058461660636\n",
       "169. 0.0340666666666667\n",
       "170. 0.137093351424695\n",
       "171. 0.621160071352579\n",
       "172. 0.240783019252167\n",
       "173. 0.358916487664258\n",
       "174. 0.0433751322751323\n",
       "175. 0.299365270831587\n",
       "176. 0.0096\n",
       "177. 0.245954102884316\n",
       "178. 0.253102089913986\n",
       "179. 0.578974293628385\n",
       "180. 0.775791193067877\n",
       "181. 0.733348996767748\n",
       "182. 0.64141104881558\n",
       "183. 0.437606539316601\n",
       "184. 0.00446666666666667\n",
       "185. 0.693518958187248\n",
       "186. 0.0239629465354663\n",
       "187. 0.001\n",
       "188. 0.134897917177162\n",
       "189. 0.226644142647396\n",
       "190. 0.159959516856069\n",
       "191. 0.000244897959183673\n",
       "192. 0.359045390437543\n",
       "193. 0.477865833621991\n",
       "194. 0.214059271331079\n",
       "195. 0.00755555555555555\n",
       "196. 0.000333333333333333\n",
       "197. 0.0472571428571429\n",
       "198. 0.0151666666666667\n",
       "199. 0.0358242424242424\n",
       "200. 0.637158012728281\n",
       "201. 0.482731520614185\n",
       "202. 0.111332323232323\n",
       "203. 0.152250127437984\n",
       "204. 0.497167088916823\n",
       "205. 0.271479273781657\n",
       "206. 0.455203818836828\n",
       "207. 0.0448757575757576\n",
       "208. 0.763935244230151\n",
       "209. 0.105061904761905\n",
       "210. 0.606830139245331\n",
       "211. 0.0266\n",
       "212. 8e-04\n",
       "213. 0.0942739541160594\n",
       "214. 0.438700079703044\n",
       "215. 0.243512348779888\n",
       "216. 0.503079037870049\n",
       "217. 0.133569126842656\n",
       "218. 0.0412037037037037\n",
       "219. 0.0102036923860453\n",
       "220. 0.0580067873303167\n",
       "221. 0.0811474842767296\n",
       "222. 0.00489090909090909\n",
       "223. 0.330810339039293\n",
       "224. 0.464420454461405\n",
       "225. 0.0700666666666667\n",
       "226. 0.0973904761904762\n",
       "227. 0.163766416955655\n",
       "228. 0.523274217931655\n",
       "229. 0.199467024929094\n",
       "230. 0.00769090909090909\n",
       "231. 0.677031551758182\n",
       "232. 0.0696396825396825\n",
       "233. 0.413304805937685\n",
       "234. 0.0611194805194805\n",
       "235. 0.0169884057971014\n",
       "236. 0.0607285714285714\n",
       "237. 0.43492685720077\n",
       "238. 0.216448023096463\n",
       "239. 0.355853453920927\n",
       "240. 0.527556616736744\n",
       "241. 0.0530505494505495\n",
       "242. 0.0183686274509804\n",
       "243. 0.0113011879804333\n",
       "244. 0.0813967679379444\n",
       "245. 0.0760971448228414\n",
       "246. 0.0506560846560846\n",
       "247. 0.400735353535354\n",
       "248. 0.515018519229181\n",
       "249. 0.360409195402299\n",
       "250. 0.00824489795918367\n",
       "251. 0.0383333333333333\n",
       "252. 0.603874657746505\n",
       "253. 0.648163117389417\n",
       "254. 0.366674876626466\n",
       "255. 0.0546925925925926\n",
       "256. 0.490030525302997\n",
       "257. 0.741960171940443\n",
       "258. 0.200962947992688\n",
       "259. 0.144319152112256\n",
       "260. 0.141622799128338\n",
       "261. 0.465051818921838\n",
       "262. 0.190328571428571\n",
       "263. 0.0465833333333333\n",
       "264. 0.317557253604386\n",
       "265. 0.478611274427798\n",
       "266. 0.268190451176945\n",
       "267. 0.0878809523809524\n",
       "268. 0.0964445887445887\n",
       "269. 0.060128140486964\n",
       "270. 0.0906093514328808\n",
       "271. 0.267174812202963\n",
       "272. 0.188604597701149\n",
       "273. 0.239579927702157\n",
       "274. 0.199477994063257\n",
       "275. 0.0481555555555555\n",
       "276. 0.280759291894501\n",
       "277. 0.307182655465532\n",
       "278. 0.0674291316526611\n",
       "279. 0.350158234524836\n",
       "280. 0.165481001203937\n",
       "281. 0.222794253780082\n",
       "282. 0.0572920060331825\n",
       "283. 0.190103982016468\n",
       "284. 0.102093410818928\n",
       "285. 0.52706734155081\n",
       "286. 0.216547063522906\n",
       "287. 0.0828364145658263\n",
       "288. 0.428922504024337\n",
       "289. 0.57723485821564\n",
       "290. 0.291559191394476\n",
       "291. 0.405754794645023\n",
       "292. 0.0627333333333333\n",
       "293. 0.295443444275023\n",
       "294. 0.341839561683823\n",
       "295. 0.348153534318752\n",
       "296. 0.803763277603732\n",
       "297. 0.538657546415613\n",
       "298. 0.155703567421288\n",
       "299. 0.0284\n",
       "300. 0.0653951825951826\n",
       "301. 0.681006936820214\n",
       "302. 0.0250909090909091\n",
       "303. 0.0407265010351967\n",
       "304. 0.000662337662337662\n",
       "305. 0.0378315298736351\n",
       "306. 0.0378666666666667\n",
       "307. 0.119664280225878\n",
       "308. 0.253418844823313\n",
       "309. 0.0625019607843137\n",
       "310. 0.00442424242424242\n",
       "311. 0.11971220043573\n",
       "312. 0.348656986068479\n",
       "313. 0.415787282375247\n",
       "314. 0.0486095238095238\n",
       "315. 0.039\n",
       "316. 0.596140883735525\n",
       "317. 0.265724630671763\n",
       "318. 0.172893900808187\n",
       "319. 0.457941012983484\n",
       "320. 0.392812214827628\n",
       "321. 0.0157106674420107\n",
       "322. 0.135243434343434\n",
       "323. 0.0194\n",
       "324. 0.220547340632649\n",
       "325. 0.187424999411323\n",
       "326. 0.447072904356384\n",
       "327. 0.387715328246304\n",
       "328. 0.454045868945869\n",
       "329. 0.0378849539815926\n",
       "330. 0.398228772866494\n",
       "331. 0.390477508204148\n",
       "332. 0.152133478471214\n",
       "333. 0.58755787726083\n",
       "334. 0.428309944239527\n",
       "335. 0.744634570602112\n",
       "336. 0.190821217682475\n",
       "337. 0.455303171132771\n",
       "338. 0.640504928814659\n",
       "339. 0.0156773341086774\n",
       "340. 0.144972089314195\n",
       "341. 0.0595555555555556\n",
       "342. 0.150691562003639\n",
       "343. 0.0750747474747475\n",
       "344. 0.310302594492603\n",
       "345. 0.304238847453225\n",
       "346. 0.032149855877015\n",
       "347. 0.0784596684017737\n",
       "348. 0.301930155269706\n",
       "349. 0.0478380952380952\n",
       "350. 0.0727528138528139\n",
       "351. 0.0867385281385281\n",
       "352. 0.0228714285714286\n",
       "353. 0.150608940453403\n",
       "354. 0.62865894012919\n",
       "355. 0.100176750378682\n",
       "356. 0.277604366968881\n",
       "357. 0.676867492779548\n",
       "358. 0.168737254901961\n",
       "359. 0.0787162297397592\n",
       "360. 0.148563647801376\n",
       "361. 0.0766138645315855\n",
       "362. 0.0373333333333333\n",
       "363. 0.110430241941356\n",
       "364. 0.0932695970695971\n",
       "365. 0.389445683546134\n",
       "366. 0.273769264069264\n",
       "367. 0.552587839483542\n",
       "368. 0.0196666666666667\n",
       "369. 0.0512603174603175\n",
       "370. 0.0923436637436637\n",
       "371. 0.612239563359861\n",
       "372. 0.0641617464283838\n",
       "373. 0.0096734693877551\n",
       "374. 0.0711967218771567\n",
       "375. 0.105190565756083\n",
       "376. 0.629535813059332\n",
       "377. 0.1719504254532\n",
       "378. 0.0893333333333333\n",
       "379. 0.00613333333333333\n",
       "380. 0.282449427904003\n",
       "381. 0.190488111815043\n",
       "382. 0.372815155640314\n",
       "383. 0.0424769841269841\n",
       "384. 0.536972732218383\n",
       "385. 0.157903174603175\n",
       "386. 0.0797116138763197\n",
       "387. 0.00526666666666667\n",
       "388. 0.360509642846561\n",
       "389. 0.0704133889099406\n",
       "390. 0.27020156324479\n",
       "391. 0.011979797979798\n",
       "392. 0.111183679466106\n",
       "393. 0.615802007704841\n",
       "394. 0.199248881974001\n",
       "395. 0.0918197064989517\n",
       "396. 0.385197457521753\n",
       "397. 0.0917285159285159\n",
       "398. 0.0815746031746032\n",
       "399. 0.0373609883854351\n",
       "400. 0.0331238095238095\n",
       "401. 0.105266666666667\n",
       "402. 0.717849442812553\n",
       "403. 0.150977777777778\n",
       "404. 0.709632212560175\n",
       "405. 0.0595047619047619\n",
       "406. 0.325248575518024\n",
       "407. 0.100281481481481\n",
       "408. 0.328013947814903\n",
       "409. 0.6880333793035\n",
       "410. 0.0663131169709263\n",
       "411. 0.227163834795735\n",
       "412. 0.0328\n",
       "413. 0.393872574704721\n",
       "414. 0.0248666666666667\n",
       "415. 0.405740652973506\n",
       "416. 0.438756417032222\n",
       "417. 0.254374236623581\n",
       "418. 0.698981838075479\n",
       "419. 0.39408582741861\n",
       "420. 0.495920752085532\n",
       "421. 0.324486990786991\n",
       "422. 0.510677126966246\n",
       "423. 0.0100571428571429\n",
       "424. 0.0342899930986887\n",
       "425. 0.154558033299447\n",
       "426. 0.411068151456582\n",
       "427. 0.122001678329519\n",
       "428. 0.308214343854729\n",
       "429. 0.164942251950948\n",
       "430. 0.360687897186617\n",
       "431. 0.483508579166423\n",
       "432. 0.0729142857142857\n",
       "433. 0.395253367660061\n",
       "434. 0.263278816817658\n",
       "435. 0.422276190476191\n",
       "436. 0.229750385846038\n",
       "437. 0.0108047619047619\n",
       "438. 0.176508485316802\n",
       "439. 0.474306200535843\n",
       "440. 0.276660728826316\n",
       "441. 0.0947634920634921\n",
       "442. 0.105395238095238\n",
       "443. 0.112504761904762\n",
       "444. 0.806234385012738\n",
       "445. 0.136431402244281\n",
       "446. 0.435833333333333\n",
       "447. 0.009\n",
       "448. 0.0114545454545455\n",
       "449. 0.482571392629974\n",
       "450. 0.0602242424242424\n",
       "451. 0.431337554020286\n",
       "452. 0.27681027662206\n",
       "453. 0.0367380952380952\n",
       "454. 0.62318882370584\n",
       "455. 0.209622273151685\n",
       "456. 0.0078\n",
       "457. 0.289953713232959\n",
       "458. 0.315235664322403\n",
       "459. 0.65275649266471\n",
       "460. 0.575027900354182\n",
       "461. 0.107434295029988\n",
       "462. 0.0997706959706959\n",
       "463. 0.213135012951385\n",
       "464. 0.743771433091391\n",
       "465. 0.0764550724637681\n",
       "466. 0.318349534459089\n",
       "467. 0.140746664609543\n",
       "468. 0.0353240413406742\n",
       "469. 0.0570884057971014\n",
       "470. 0.538681238884521\n",
       "471. 0.363733099135034\n",
       "472. 0.619358197014435\n",
       "473. 0.111166666666667\n",
       "474. 0.0520356840080327\n",
       "475. 0.516450076827099\n",
       "476. 0.0251210884353741\n",
       "477. 0.294442302470784\n",
       "478. 0.672557520676255\n",
       "479. 0.240112825653878\n",
       "480. 0.0230758620689655\n",
       "481. 0.0054\n",
       "482. 0.139863492063492\n",
       "483. 0.66436611124831\n",
       "484. 0.0980222222222222\n",
       "485. 0.0244666666666667\n",
       "486. 0.104156228956229\n",
       "487. 0.550629126047257\n",
       "488. 0.0130447438918766\n",
       "489. 0.338244206903106\n",
       "490. 0.0842\n",
       "491. 0.541711652052044\n",
       "492. 0.349881345415894\n",
       "493. 0.531041458306449\n",
       "494. 0.625788822549875\n",
       "495. 0.047374358974359\n",
       "496. 0.0038\n",
       "497. 0.676460203701692\n",
       "498. 0.00993333333333333\n",
       "499. 0.677239550859892\n",
       "500. 0.106911111111111\n",
       "501. 0.145854197654198\n",
       "502. 0.144922518680328\n",
       "503. 0.00817391304347826\n",
       "504. 0.0178575757575758\n",
       "505. 0.12160293040293\n",
       "506. 0.017\n",
       "507. 0.357125539177879\n",
       "508. 0.338516133398352\n",
       "509. 0.113944444444444\n",
       "510. 0.0374086834733894\n",
       "511. 0.3655002859727\n",
       "512. 0.294799301435585\n",
       "513. 0.0244242424242424\n",
       "514. 0.0357666666666667\n",
       "515. 0.0600285714285714\n",
       "516. 0.233565641098458\n",
       "517. 0.0560986717216531\n",
       "518. 0.102466666666667\n",
       "519. 0.53278700277892\n",
       "520. 0.429590465914747\n",
       "521. 0.00677435897435897\n",
       "522. 0.0343099415204678\n",
       "523. 0.452730246566324\n",
       "524. 0.0253333333333333\n",
       "525. 0.202371600076164\n",
       "526. 0.289645265148295\n",
       "527. 0.0232778566970787\n",
       "528. 0.626740403927503\n",
       "529. 0.198095894271918\n",
       "530. 0.00922424242424242\n",
       "531. 0.245906109155619\n",
       "532. 0.0288666666666667\n",
       "533. 0.361919314443584\n",
       "534. 0.534399308386028\n",
       "535. 0.31039062049062\n",
       "536. 0.00497309479231676\n",
       "537. 0.838908268033228\n",
       "538. 0.640486206224888\n",
       "539. 0.311127300973258\n",
       "540. 0.204467377398721\n",
       "541. 0.149169358178054\n",
       "542. 0.778886613952768\n",
       "543. 0.0356888888888889\n",
       "544. 0.151863942822034\n",
       "545. 0.109799652199652\n",
       "546. 0.0533871794871795\n",
       "547. 0.519950469788172\n",
       "548. 0.639161009661332\n",
       "549. 0.0181238095238095\n",
       "550. 0.535326076236431\n",
       "551. 0.0178448979591837\n",
       "552. 0.0374705882352941\n",
       "553. 0.0935802197802198\n",
       "554. 0.706244476520718\n",
       "555. 0.421852941176471\n",
       "556. 0.289691481982747\n",
       "557. 0.103676039646628\n",
       "558. 0.0519054726368159\n",
       "559. 0.0708282717282717\n",
       "560. 0.322005043614739\n",
       "561. 0.0589529411764706\n",
       "562. 0.0485163265306122\n",
       "563. 0.0538380952380952\n",
       "564. 0.214603949392259\n",
       "565. 0.177011039617733\n",
       "566. 0.0306333333333333\n",
       "567. 0.376446677990467\n",
       "568. 0.0774028886655573\n",
       "569. 0.575840860387835\n",
       "570. 0.257164879195585\n",
       "571. 0.0392948306595365\n",
       "572. 0.208189830090342\n",
       "573. 0.226569102144005\n",
       "574. 0.340363301614579\n",
       "575. 0.302163923844582\n",
       "576. 0.419854541732668\n",
       "577. 0.468705948432882\n",
       "578. 0.239743221162657\n",
       "579. 0.0556585266585267\n",
       "580. 0.0598242424242424\n",
       "581. 0.0223986013986014\n",
       "582. 0.0138555555555556\n",
       "583. 0.331105283357457\n",
       "584. 0.466434140198813\n",
       "585. 0.260565651018484\n",
       "586. 0.459321698842957\n",
       "587. 0.0558666666666667\n",
       "588. 0.141293395195313\n",
       "589. 0.242394352190637\n",
       "590. 0.039630303030303\n",
       "591. 0.403255845886581\n",
       "592. 0.189922408963585\n",
       "593. 0.0342166666666667\n",
       "594. 0.0112\n",
       "595. 0.024988332302618\n",
       "596. 0.481976695508585\n",
       "597. 0.0444\n",
       "598. 0.0191777777777778\n",
       "599. 0.0296380952380952\n",
       "600. 0.367306685423997\n",
       "601. 0.0778380952380952\n",
       "602. 0.289312944745478\n",
       "603. 0.559175706432625\n",
       "604. 0.0308967032967033\n",
       "605. 0.101347619047619\n",
       "606. 0.293640599268311\n",
       "607. 0.798060181418768\n",
       "608. 0.134602661064426\n",
       "609. 0.099288198757764\n",
       "610. 0.0940862745098039\n",
       "611. 0.218115708812261\n",
       "612. 0.466214721183192\n",
       "613. 0.565577330470973\n",
       "614. 0.15545166495444\n",
       "615. 0.0932862745098039\n",
       "616. 0.514839955371036\n",
       "617. 0.0177645962732919\n",
       "618. 0.289443145743146\n",
       "619. 0.197659414659415\n",
       "620. 0.813799249039808\n",
       "621. 0.228247875815897\n",
       "622. 0.536628586862503\n",
       "623. 0.693993902066963\n",
       "624. 0.467215482556659\n",
       "625. 0.262524816893888\n",
       "626. 0.193785871798834\n",
       "627. 0.153193650793651\n",
       "628. 0.0451210884353742\n",
       "629. 0.197006647324272\n",
       "630. 0.0701230769230769\n",
       "631. 0.0704074534161491\n",
       "632. 0.116060029294737\n",
       "633. 0.509875081675082\n",
       "634. 0.240945731135154\n",
       "635. 0.214629835415337\n",
       "636. 0.00522816399286987\n",
       "637. 0.312126499687464\n",
       "638. 0.243039882810303\n",
       "639. 0.0422291316526611\n",
       "640. 0.120124239317968\n",
       "641. 0.127603242816995\n",
       "642. 0.0698856065503124\n",
       "643. 0.0162931677018634\n",
       "644. 0.176088888888889\n",
       "645. 0.114416246498599\n",
       "646. 0.0174888888888889\n",
       "647. 0.123037458837459\n",
       "648. 0.111888888888889\n",
       "649. 0.366326102952682\n",
       "650. 0.243043587613955\n",
       "651. 0.137607246376812\n",
       "652. 0.0491333333333333\n",
       "653. 0.466450994433474\n",
       "654. 0.366124988474531\n",
       "655. 0.262872307543707\n",
       "656. 0.169280866503436\n",
       "657. 0.630904319669879\n",
       "658. 0.505980371336913\n",
       "659. 0.564482970921137\n",
       "660. 0.252544876044818\n",
       "661. 0.308024242424242\n",
       "662. 0.0314147186147186\n",
       "663. 0.0708261437908497\n",
       "664. 0.564847425424129\n",
       "665. 0.314746867051548\n",
       "666. 0.148449940752894\n",
       "667. 0.443782443770596\n",
       "668. 0.0527897435897436\n",
       "669. 0.00142424242424242\n",
       "670. 0.572227885200541\n",
       "671. 0.474441772961746\n",
       "672. 0.0553878787878788\n",
       "673. 9.09090909090909e-05\n",
       "674. 0.502523426944876\n",
       "675. 0.415692500564034\n",
       "676. 0.513565347639667\n",
       "677. 0.0237333333333333\n",
       "678. 0.394099602454177\n",
       "679. 0.342877724748259\n",
       "680. 0.212610875831639\n",
       "681. 0.0403761904761905\n",
       "682. 0.763012831118629\n",
       "683. 0.175763636363636\n",
       "684. 0.186389016978732\n",
       "685. 0.148859383431903\n",
       "686. 0.525763076000812\n",
       "687. 0.298697784291618\n",
       "688. 0.2990466729727\n",
       "689. 0.137109195402299\n",
       "690. 0.225788481176847\n",
       "691. 0.0500863731656185\n",
       "692. 0.099187031408308\n",
       "693. 0.390662444166576\n",
       "694. 0.351110446598191\n",
       "695. 0.0279019607843137\n",
       "696. 0.309185570434518\n",
       "697. 0.0349238095238095\n",
       "698. 0.0476739130434783\n",
       "699. 0.0385333333333333\n",
       "700. 0.245717893335458\n",
       "701. 0.0527222222222222\n",
       "702. 0.284787878787879\n",
       "703. 0.627641641703345\n",
       "704. 0.21879732640771\n",
       "705. 0.469045016904163\n",
       "706. 0.594151958049773\n",
       "707. 0.373316017316017\n",
       "708. 0.0264714285714286\n",
       "709. 0.254669573718702\n",
       "710. 0.041559589830252\n",
       "711. 0.311598036268764\n",
       "712. 0.841786907647478\n",
       "713. 0.0159636363636364\n",
       "714. 0.0312666666666667\n",
       "715. 0.279655611653983\n",
       "716. 0.412263653821177\n",
       "717. 0.379312290957525\n",
       "718. 0.439653944513374\n",
       "719. 0.784785885812251\n",
       "720. 0.0545271708683473\n",
       "721. 0.188017638927227\n",
       "722. 0.344273470384587\n",
       "723. 0.562868361213509\n",
       "724. 0.0802\n",
       "725. 0.062573544973545\n",
       "726. 0.0359350649350649\n",
       "727. 0.391175023359331\n",
       "728. 0.408658303032619\n",
       "729. 0.398098078280914\n",
       "730. 0.0789806239737274\n",
       "731. 0.271022290241679\n",
       "732. 0.551962166207989\n",
       "733. 0.212661741946708\n",
       "734. 0.0751591431556949\n",
       "735. 0.00578840579710145\n",
       "736. 0.0592\n",
       "737. 0.0317575757575758\n",
       "738. 0.400923240463152\n",
       "739. 0.0474380952380952\n",
       "740. 0.0545703703703704\n",
       "741. 0.00231632653061224\n",
       "742. 0.105234798534799\n",
       "743. 0.587191562464047\n",
       "744. 0.0900055555555555\n",
       "745. 0.683906811495236\n",
       "746. 0.132463157894737\n",
       "747. 0.481602978553558\n",
       "748. 0.495182859851671\n",
       "749. 0.0279650793650794\n",
       "750. 0.252402575687401\n",
       "751. 0.018\n",
       "752. 0.0356\n",
       "753. 0.174758494177985\n",
       "754. 0.153344139434442\n",
       "755. 0.0726857142857143\n",
       "756. 0.683499165226029\n",
       "757. 0.153202628398228\n",
       "758. 0.0596564102564103\n",
       "759. 0.0799514105585433\n",
       "760. 0.0164047619047619\n",
       "761. 0.263091465594213\n",
       "762. 0.474782800125594\n",
       "763. 0.0670761904761905\n",
       "764. 0.280788345367725\n",
       "765. 0.704240171759427\n",
       "766. 0.102052052545156\n",
       "767. 0.00213333333333333\n",
       "768. 0.0387119975262832\n",
       "769. 0.425337059203743\n",
       "770. 0.331278391850968\n",
       "771. 0.0281194805194805\n",
       "772. 0.0349555555555556\n",
       "773. 0.475729114435763\n",
       "774. 0.676810280270968\n",
       "775. 0.523089841479637\n",
       "776. 0.139011388611389\n",
       "777. 0.779318549866474\n",
       "778. 0.367001849785385\n",
       "779. 0.0572705882352941\n",
       "780. 0.314250992428384\n",
       "781. 0.161834335058676\n",
       "782. 0.00213333333333333\n",
       "783. 0.412955823826079\n",
       "784. 0.0199333333333333\n",
       "785. 0.0458928329888822\n",
       "786. 0.319390905296081\n",
       "787. 0.420037351306949\n",
       "788. 0.364102566004655\n",
       "789. 0.146947619047619\n",
       "790. 0.301476381253682\n",
       "791. 0.00517391304347826\n",
       "792. 0.205660606060606\n",
       "793. 0.61367634871572\n",
       "794. 0.0319714285714286\n",
       "795. 0.150427973512282\n",
       "796. 0.0271909090909091\n",
       "797. 0.0048\n",
       "798. 0.0260666666666667\n",
       "799. 0.0725269841269841\n",
       "800. 0.374685717160095\n",
       "801. 0.150709401709402\n",
       "802. 0.0235884057971015\n",
       "803. 0.308170014728704\n",
       "804. 0.211549318295058\n",
       "805. 0.32041062603476\n",
       "806. 0.082\n",
       "807. 0.418124033184996\n",
       "808. 0.0607015873015873\n",
       "809. 0.0659246443211961\n",
       "810. 0.535017418519619\n",
       "811. 0.354019973464449\n",
       "812. 0.102384283076097\n",
       "813. 0.012\n",
       "814. 0.041809977324263\n",
       "815. 0.279769062472443\n",
       "816. 0.353489633400662\n",
       "817. 0.207294912736624\n",
       "818. 0.376647132086538\n",
       "819. 0.415349870433962\n",
       "820. 0.0468809523809524\n",
       "821. 0.157501944025473\n",
       "822. 0.0733975130133025\n",
       "823. 0.595211111111111\n",
       "824. 0.445251352382422\n",
       "825. 0.0661666666666667\n",
       "826. 0.0519172161172161\n",
       "827. 0.480804215675408\n",
       "828. 0.0038\n",
       "829. 0.195854665844033\n",
       "830. 0.439200500428439\n",
       "831. 0.0630340440653873\n",
       "832. 0.0118\n",
       "833. 0.516250192853268\n",
       "834. 0.498544916984795\n",
       "835. 0.760591175551091\n",
       "836. 0.365394422263113\n",
       "837. 0.119327103413597\n",
       "838. 0.412778176927255\n",
       "839. 0.0782047619047619\n",
       "840. 0.591030502491688\n",
       "841. 0.27405786440186\n",
       "842. 0.63467417689523\n",
       "843. 0.0106\n",
       "844. 0.632594316947072\n",
       "845. 0.57018670495502\n",
       "846. 0.542914132310845\n",
       "847. 0.0448222222222222\n",
       "848. 0.100646464646465\n",
       "849. 0.0688333333333333\n",
       "850. 0.391323809523809\n",
       "851. 0.134269378091727\n",
       "852. 0.358933333333333\n",
       "853. 0.569480904485631\n",
       "854. 0.0074\n",
       "855. 0.584502109134811\n",
       "856. 0.210745425641771\n",
       "857. 0.0312941798941799\n",
       "858. 0.0048\n",
       "859. 0.0028\n",
       "860. 0.501271491786265\n",
       "861. 0.0104\n",
       "862. 0.0524666666666667\n",
       "863. 0.0858979296066253\n",
       "864. 0.289262426194988\n",
       "865. 0.424623370845067\n",
       "866. 0.072231746031746\n",
       "867. 0.135167303685025\n",
       "868. 0.369611016635817\n",
       "869. 0.275685940420367\n",
       "870. 0.0731760683760684\n",
       "871. 0.0586952380952381\n",
       "872. 0.0121575757575758\n",
       "873. 0.00907142857142857\n",
       "874. 0.230981594733769\n",
       "875. 0.253574848576014\n",
       "876. 0.485489586741761\n",
       "877. 0.126477845234978\n",
       "878. 0.252266854242723\n",
       "879. 0.461561185201059\n",
       "880. 0.714396163342505\n",
       "881. 0.888072725437863\n",
       "882. 0.38059559683715\n",
       "883. 0.300584668691521\n",
       "884. 0.186609146156279\n",
       "885. 0.0910128978864273\n",
       "886. 0.0950509379509379\n",
       "887. 0.0445714285714286\n",
       "888. 0.733074423960969\n",
       "889. 0.313762255278213\n",
       "890. 0.391056897334929\n",
       "891. 0.537918660469894\n",
       "892. 0.545035145836005\n",
       "893. 0.183944370730748\n",
       "894. 0.191006626546944\n",
       "895. 0.229416466490255\n",
       "896. 0.14783951833607\n",
       "897. 0.175466380362932\n",
       "898. 0.300386921867483\n",
       "899. 0.119599076512164\n",
       "900. 0.189289243351157\n",
       "901. 0.139527909784603\n",
       "902. 0.509938308558842\n",
       "903. 0.00901062801932367\n",
       "904. 0.316608146554032\n",
       "905. 0.573690208720772\n",
       "906. 0.0828583241320083\n",
       "907. 0.115489599620943\n",
       "908. 0.0204444444444444\n",
       "909. 0.344281594427353\n",
       "910. 0.502059748478351\n",
       "911. 0.0368265010351967\n",
       "912. 0.639156377249408\n",
       "913. 0.605557813580397\n",
       "914. 0.355263089775196\n",
       "915. 0.060209653092006\n",
       "916. 0.0283333333333333\n",
       "917. 0.465417845947928\n",
       "918. 0.383771867073044\n",
       "919. 0.609687775060824\n",
       "920. 0.104822222222222\n",
       "921. 0.0186\n",
       "922. 0.660421304876933\n",
       "923. 0.481409203757834\n",
       "924. 0.809747766725594\n",
       "925. 0.0787714795008913\n",
       "926. 0.017379797979798\n",
       "927. 0.314007293648733\n",
       "928. 0.209001180072758\n",
       "929. 0.305927732644714\n",
       "930. 0.525786629452419\n",
       "931. 0.862981592950686\n",
       "932. 0.0898273769450979\n",
       "933. 0.107311111111111\n",
       "934. 0.688490254374722\n",
       "935. 0.178830976430976\n",
       "936. 0.110597198879552\n",
       "937. 0.441491346096358\n",
       "938. 0.0665282106782107\n",
       "939. 0.0869333333333333\n",
       "940. 0.281417149517143\n",
       "941. 0.319920834356719\n",
       "942. 0.441589346278222\n",
       "943. 0.312142531467055\n",
       "944. 0.00666666666666667\n",
       "945. 0.182681203746909\n",
       "946. 0.0879587301587302\n",
       "947. 0.0939294190086643\n",
       "948. 0.848838044240041\n",
       "949. 0.679419157351322\n",
       "950. 0.695131142211447\n",
       "951. 0.277875562262769\n",
       "952. 0.290326504867681\n",
       "953. 0.130597198879552\n",
       "954. 0.7560411117244\n",
       "955. 0.154198364850985\n",
       "956. 0.357935797873526\n",
       "957. 0.0274550724637681\n",
       "958. 0.0098\n",
       "959. 0.515103801446009\n",
       "960. 0.0518568637711495\n",
       "961. 0.0602037037037037\n",
       "962. 0.212746951374304\n",
       "963. 0.0082\n",
       "964. 0.104533333333333\n",
       "965. 0.0296\n",
       "966. 0.360223821220606\n",
       "967. 0.216810084033613\n",
       "968. 0.078130303030303\n",
       "969. 0.418873876001768\n",
       "970. 0.552865475261585\n",
       "971. 0.0629212666467276\n",
       "972. 0.318102412502822\n",
       "973. 0.55304407831155\n",
       "974. 0.422692927377851\n",
       "975. 0.163141247394718\n",
       "976. 0.0651428571428571\n",
       "977. 0.807349226711289\n",
       "978. 0.308089905490167\n",
       "979. 0.27977538174277\n",
       "980. 0.594811542869982\n",
       "981. 0.782831426708755\n",
       "982. 0.592180283430288\n",
       "983. 0.304757152840505\n",
       "984. 0.513852942636185\n",
       "985. 0.598395093538502\n",
       "986. 0.367660878040489\n",
       "987. 0.208479783050437\n",
       "988. 0.387837330440576\n",
       "989. 0.337120928477463\n",
       "990. 0.00942424242424242\n",
       "991. 0.044831746031746\n",
       "992. 0.0331037037037037\n",
       "993. 0.083404662004662\n",
       "994. 0.265355856123919\n",
       "995. 0.0668297996121526\n",
       "996. 0.739165205095001\n",
       "997. 0.379921036381695\n",
       "998. 0.0158047619047619\n",
       "999. 0.00509090909090909\n",
       "1000. 0.540346084545982\n",
       "1001. 0.051837037037037\n",
       "1002. 0.3089820785677\n",
       "1003. 0.367711224104156\n",
       "1004. 0.307965492429222\n",
       "1005. 0.238451413027758\n",
       "1006. 0.77765227334506\n",
       "1007. 0.575820802620802\n",
       "1008. 0.0733235294117647\n",
       "1009. 0.0016\n",
       "1010. 0.0384962370962371\n",
       "1011. 0.299365504309043\n",
       "1012. 0.155018217260323\n",
       "1013. 0.0672619047619048\n",
       "1014. 0.46806110634685\n",
       "1015. 0.114472421695951\n",
       "1016. 0.620615416911482\n",
       "1017. 0.200309037855656\n",
       "1018. 0.270726019802373\n",
       "1019. 0.174452762703832\n",
       "1020. 0.587592527390301\n",
       "1021. 0.199300398780202\n",
       "1022. 0.0102448979591837\n",
       "1023. 0.0397285714285714\n",
       "1024. 0.0162666666666667\n",
       "1025. 0.0114666666666667\n",
       "1026. 0.195693463730689\n",
       "1027. 0.201188496217982\n",
       "1028. 0.393090790408653\n",
       "1029. 0.310266666666667\n",
       "1030. 0.0288448979591837\n",
       "1031. 0.109350793650794\n",
       "1032. 0.526247670438228\n",
       "1033. 0.711136294459992\n",
       "1034. 0.377678952147725\n",
       "1035. 0.425689399257037\n",
       "1036. 0.0698122028914482\n",
       "1037. 0.0600952380952381\n",
       "1038. 0.473020623873165\n",
       "1039. 0.0322267806267806\n",
       "1040. 0.0808721393034826\n",
       "1041. 0.784359690569302\n",
       "1042. 0.437332651057119\n",
       "1043. 0.652655224508978\n",
       "1044. 0.0578666666666667\n",
       "1045. 0.03442030651341\n",
       "1046. 0.0703958041958042\n",
       "1047. 0.21453961981213\n",
       "1048. 0.100506293706294\n",
       "1049. 0.679990166363317\n",
       "1050. 0.251027323722086\n",
       "1051. 0.409398757763975\n",
       "1052. 0.0698380952380952\n",
       "1053. 0.250403572359249\n",
       "1054. 0.0121380952380952\n",
       "1055. 0.0570124015418133\n",
       "1056. 0.368781249022565\n",
       "1057. 0.00433333333333333\n",
       "1058. 0.00822222222222222\n",
       "1059. 0.294180077417447\n",
       "1060. 0.0293575757575758\n",
       "1061. 0.00322424242424242\n",
       "1062. 0.0991636363636363\n",
       "1063. 0.351744250426449\n",
       "1064. 0.257699518506827\n",
       "1065. 0.255360956057483\n",
       "1066. 0.21061965666277\n",
       "1067. 0.342812896325479\n",
       "1068. 0.0142909090909091\n",
       "1069. 0.42633098475994\n",
       "1070. 0.73804566827312\n",
       "1071. 0.229479614256193\n",
       "1072. 0.593167357105254\n",
       "1073. 0.572870918258768\n",
       "1074. 0.245210611004808\n",
       "1075. 0.114496825396825\n",
       "1076. 0.0745897435897436\n",
       "1077. 0.241977230232595\n",
       "1078. 0.0665761904761905\n",
       "1079. 0.356498184074626\n",
       "1080. 0.541662314184965\n",
       "1081. 0.537599046032529\n",
       "1082. 0.054593837535014\n",
       "1083. 0.0462564102564103\n",
       "1084. 0.415336751592259\n",
       "1085. 0.524927851005866\n",
       "1086. 0.00467777777777778\n",
       "1087. 0.475826385918094\n",
       "1088. 0.298654006808713\n",
       "1089. 0.855227625357486\n",
       "1090. 0.156878479375031\n",
       "1091. 0.526695958421396\n",
       "1092. 0.633782167395233\n",
       "1093. 0.658225612556666\n",
       "1094. 0.0858749979905152\n",
       "1095. 0.278734372594227\n",
       "1096. 0.194383150874964\n",
       "1097. 0.244549500497125\n",
       "1098. 0.448434606708141\n",
       "1099. 0.0427064935064935\n",
       "1100. 0.809052842494791\n",
       "1101. 0.232185414585415\n",
       "1102. 0.036\n",
       "1103. 0.194143083738472\n",
       "1104. 0.477569492304233\n",
       "1105. 0.147252756393933\n",
       "1106. 0.21487685482351\n",
       "1107. 0.372432718280477\n",
       "1108. 0.184978190739358\n",
       "1109. 0.1006\n",
       "1110. 0.513453282164648\n",
       "1111. 0.004\n",
       "1112. 0.162217543859649\n",
       "1113. 0.208779351650147\n",
       "1114. 0.237044627023114\n",
       "1115. 0.474138448593901\n",
       "1116. 0.29975804069693\n",
       "1117. 0.604209224874742\n",
       "1118. 0.122096914700544\n",
       "1119. 0.466259836896186\n",
       "1120. 0.0594948306595365\n",
       "1121. 0.317124363814219\n",
       "1122. 0.220421340446081\n",
       "1123. 0.0895954699121028\n",
       "1124. 0.692376534300207\n",
       "1125. 0.437216273156773\n",
       "1126. 0.239452729415019\n",
       "1127. 0.00766666666666667\n",
       "1128. 0.0688725274725275\n",
       "1129. 0.0409777777777778\n",
       "1130. 0.051658608058608\n",
       "1131. 0.827669993834232\n",
       "1132. 0.256817988778487\n",
       "1133. 0.135108371040724\n",
       "1134. 0.147487914230019\n",
       "1135. 0.0393908496732026\n",
       "1136. 0.191258657995136\n",
       "1137. 0.170636590222294\n",
       "1138. 0.0640787620924583\n",
       "1139. 0.304324378077303\n",
       "1140. 0.0631428571428572\n",
       "1141. 0.422405163006667\n",
       "1142. 0.0111333333333333\n",
       "1143. 0.482947030955207\n",
       "1144. 0.108928515928516\n",
       "1145. 0.012\n",
       "1146. 0.582924486417423\n",
       "1147. 0.669187156890966\n",
       "1148. 0.195216707870222\n",
       "1149. 0.119292722371968\n",
       "1150. 0.0868337068160597\n",
       "1151. 0.0808148459383754\n",
       "1152. 0.373955251864044\n",
       "1153. 0.0430459096459096\n",
       "1154. 0.367033420004021\n",
       "1155. 0.115671100164204\n",
       "1156. 0.0130147186147186\n",
       "1157. 0.161323099164703\n",
       "1158. 0.431120396557889\n",
       "1159. 0.107273400673401\n",
       "1160. 0.56055297739209\n",
       "1161. 0.625714444208014\n",
       "1162. 0.251417791468039\n",
       "1163. 0.551041303289911\n",
       "1164. 0.248545818387376\n",
       "1165. 0.3295071523647\n",
       "1166. 0.572191952414778\n",
       "1167. 0.0928666666666667\n",
       "1168. 0.443179929067408\n",
       "1169. 0.290382491582492\n",
       "1170. 0.197602196406712\n",
       "1171. 0.145585847485847\n",
       "1172. 0.495879747696854\n",
       "1173. 0.568835176867885\n",
       "1174. 0.178825097125097\n",
       "1175. 0.263006722689076\n",
       "1176. 0.0122666666666667\n",
       "1177. 0.204530764277533\n",
       "1178. 0.276815621836675\n",
       "1179. 0.0919473193473193\n",
       "1180. 0.285776930746126\n",
       "1181. 0.0861026915113871\n",
       "1182. 0.365720549442655\n",
       "1183. 0.0246666666666667\n",
       "1184. 0.341597836374044\n",
       "1185. 0.0703703703703704\n",
       "1186. 0.226417106965196\n",
       "1187. 0.145939853626176\n",
       "1188. 0.0578666666666667\n",
       "1189. 0.0203358070500928\n",
       "1190. 0.102344897959184\n",
       "1191. 0.257191002355077\n",
       "1192. 0.380270430813124\n",
       "1193. 0.22801255729577\n",
       "1194. 0.0496\n",
       "1195. 0.0243550524687487\n",
       "1196. 0.0796714285714286\n",
       "1197. 0.30290950428222\n",
       "1198. 0.089566905363544\n",
       "1199. 0.157804761904762\n",
       "1200. 0.193304568404978\n",
       "1201. 0.327884993163203\n",
       "1202. 0.188250444061209\n",
       "1203. 0.285145124113595\n",
       "1204. 0.100728571428571\n",
       "1205. 0.160782740375844\n",
       "1206. 0.0167884057971014\n",
       "1207. 0.231235686066859\n",
       "1208. 0.0508666666666667\n",
       "1209. 0.0850083550661644\n",
       "1210. 0.0765111111111111\n",
       "1211. 0.0360731601731602\n",
       "1212. 0.419970039631746\n",
       "1213. 0.0469543988739774\n",
       "1214. 0.0241040293040293\n",
       "1215. 0.67258345763243\n",
       "1216. 0.526772102040275\n",
       "1217. 0.633922369109906\n",
       "1218. 0.122292661306947\n",
       "1219. 0.170855978803628\n",
       "1220. 0.130204650852884\n",
       "1221. 0.264960078116749\n",
       "1222. 0.00929090909090909\n",
       "1223. 0.0296571428571429\n",
       "1224. 0.27537588885422\n",
       "1225. 0.0365619047619048\n",
       "1226. 0.0838814814814815\n",
       "1227. 0.4780178032596\n",
       "1228. 0.40745326915483\n",
       "1229. 0.315968944272752\n",
       "1230. 0.0460666666666667\n",
       "1231. 0.0749979171771625\n",
       "1232. 0.643386147970091\n",
       "1233. 0.503910814401044\n",
       "1234. 0.0305651662063427\n",
       "1235. 0.128120613051956\n",
       "1236. 0.239764250808907\n",
       "1237. 0.2464965057763\n",
       "1238. 0.808017646322295\n",
       "1239. 0.712779408041\n",
       "1240. 0.335160432306445\n",
       "1241. 0.218738630654148\n",
       "1242. 0.00213333333333333\n",
       "1243. 0.0358\n",
       "1244. 0.502336046085691\n",
       "1245. 0.283245683728037\n",
       "1246. 0.0552761686075119\n",
       "1247. 0.422017850833885\n",
       "1248. 0.02720853432282\n",
       "1249. 0.109115909918344\n",
       "1250. 0.330525630675507\n",
       "1251. 0.43256635663063\n",
       "1252. 0.79601703297908\n",
       "1253. 0.358075324599865\n",
       "1254. 0.35064557697692\n",
       "1255. 0.139632051473871\n",
       "1256. 0.819280051850075\n",
       "1257. 0.0324\n",
       "1258. 0.0297039215686275\n",
       "1259. 0.351741122153351\n",
       "1260. 0.0433333333333333\n",
       "1261. 0.225181928350018\n",
       "1262. 0.308127300339668\n",
       "1263. 0.0231741076533529\n",
       "1264. 0.0617448801742919\n",
       "1265. 0.390556047016344\n",
       "1266. 0.0494229691876751\n",
       "1267. 0.102529310344828\n",
       "1268. 0.385699812602209\n",
       "1269. 0.0902761904761905\n",
       "1270. 0.422158145200042\n",
       "1271. 0.0518428571428571\n",
       "1272. 0.0448\n",
       "1273. 0.427318604533966\n",
       "1274. 0.0626666666666667\n",
       "1275. 0.300187172971319\n",
       "1276. 0.590423419714938\n",
       "1277. 0.337406643222685\n",
       "1278. 0.251815816530413\n",
       "1279. 0.0184242424242424\n",
       "1280. 0.040888198757764\n",
       "1281. 0.0682119174942704\n",
       "1282. 0.446553384055558\n",
       "1283. 0.0181931677018634\n",
       "1284. 0.0421333333333333\n",
       "1285. 0.0714842572497745\n",
       "1286. 0.00566666666666667\n",
       "1287. 0.372760666433911\n",
       "1288. 0.0704874458874459\n",
       "1289. 0.546724902835585\n",
       "1290. 0.154972089314195\n",
       "1291. 0.21515633364454\n",
       "1292. 0.731486138052025\n",
       "1293. 0.303174705009781\n",
       "1294. 0.556912440704805\n",
       "1295. 0.02880853432282\n",
       "1296. 0.68762645898416\n",
       "1297. 0.631088747276892\n",
       "1298. 0.291282484948002\n",
       "1299. 0.0409713879681124\n",
       "1300. 0.226215568600665\n",
       "1301. 0.38698600377361\n",
       "1302. 0.00946666666666667\n",
       "1303. 0.270738610408377\n",
       "1304. 0.48055685617084\n",
       "1305. 0.264662890731995\n",
       "1306. 0.544753775571682\n",
       "1307. 0.0512095238095238\n",
       "1308. 0.0368659768384966\n",
       "1309. 0.589843703943426\n",
       "1310. 0.00633333333333333\n",
       "1311. 0.149412822175022\n",
       "1312. 0.0962490028490029\n",
       "1313. 0.175590482506\n",
       "1314. 0.0204595365418895\n",
       "1315. 0.330935438341927\n",
       "1316. 0.36837963138542\n",
       "1317. 0.00209090909090909\n",
       "1318. 0.196189743589744\n",
       "1319. 0.37747921222951\n",
       "1320. 0.674150171699797\n",
       "1321. 0.588513467924093\n",
       "1322. 0.17639066039779\n",
       "1323. 0.269172343714798\n",
       "1324. 0.213655305949585\n",
       "1325. 0.0186095238095238\n",
       "1326. 0.545525636802509\n",
       "1327. 0.158670588235294\n",
       "1328. 0.00466666666666667\n",
       "1329. 0.237250675472075\n",
       "1330. 0.0629230769230769\n",
       "1331. 0.0241676767676768\n",
       "1332. 0.230720018354539\n",
       "1333. 0.486711145925037\n",
       "1334. 0.223129877036028\n",
       "1335. 0.151466005022699\n",
       "1336. 0.669874251341531\n",
       "1337. 0.418571734587252\n",
       "1338. 0.0190091954022989\n",
       "1339. 0.00943330375628512\n",
       "1340. 0.1595184278895\n",
       "1341. 0.112942857142857\n",
       "1342. 0.236210827746021\n",
       "1343. 0.010578231292517\n",
       "1344. 0.193046918172156\n",
       "1345. 0.363330145081496\n",
       "1346. 0.233797144352314\n",
       "1347. 8e-04\n",
       "1348. 0.38461456012582\n",
       "1349. 0.0407333333333333\n",
       "1350. 0.167713804713805\n",
       "1351. 0.335765623012756\n",
       "1352. 0.0341247794811705\n",
       "1353. 0.058774358974359\n",
       "1354. 0.29656016885274\n",
       "1355. 0.0757377084167\n",
       "1356. 0.661128647775432\n",
       "1357. 0.486335239752585\n",
       "1358. 0.274125876299378\n",
       "1359. 0.398757004765239\n",
       "1360. 0.312901092470678\n",
       "1361. 0.0167878787878788\n",
       "1362. 0.49212230802782\n",
       "1363. 0.417712580646714\n",
       "1364. 0.368876819281209\n",
       "1365. 0.399285034115656\n",
       "1366. 0.358669524090498\n",
       "1367. 0.0128\n",
       "1368. 0.317530757368029\n",
       "1369. 0.198690318213617\n",
       "1370. 0.0969715728715729\n",
       "1371. 0.314874138665033\n",
       "1372. 0.210215561198177\n",
       "1373. 0.547314875872405\n",
       "1374. 0.16773030602238\n",
       "1375. 0.043155587228107\n",
       "1376. 0.0465238095238095\n",
       "1377. 0.302429760258227\n",
       "1378. 0.0102242424242424\n",
       "1379. 0.783340565198175\n",
       "1380. 0.187846866202753\n",
       "1381. 0.345716502198186\n",
       "1382. 0.259585062890326\n",
       "1383. 0.213997280585225\n",
       "1384. 0.507869077340633\n",
       "1385. 0.175453119868637\n",
       "1386. 0.414077009493443\n",
       "1387. 0.316329633371636\n",
       "1388. 0.384129505114509\n",
       "1389. 0.33053247964253\n",
       "1390. 0.256483765463015\n",
       "1391. 0.116038211146528\n",
       "1392. 0.035851355661882\n",
       "1393. 0.0248974232117089\n",
       "1394. 0.0848666666666666\n",
       "1395. 0.207570096225025\n",
       "1396. 0.268501287085481\n",
       "1397. 0.0632095238095238\n",
       "1398. 0.25781656063235\n",
       "1399. 0.0502459096459096\n",
       "1400. 0.0831904761904762\n",
       "1401. 0.0328032924190819\n",
       "1402. 0.242366292328361\n",
       "1403. 0.454527494236908\n",
       "1404. 0.0289380952380952\n",
       "1405. 0.427383136581075\n",
       "1406. 0.785156169090612\n",
       "1407. 0.0652\n",
       "1408. 0.447523396573703\n",
       "1409. 0.189641133579641\n",
       "1410. 0.0413380952380952\n",
       "1411. 0.146168975468975\n",
       "1412. 0.511608303418371\n",
       "1413. 0.0105575757575758\n",
       "1414. 0.684172588569353\n",
       "1415. 0.142118803418803\n",
       "1416. 0.0595370515841843\n",
       "1417. 0.656540648053628\n",
       "1418. 0.668990329836979\n",
       "1419. 0.108187426694816\n",
       "1420. 0.0119358070500928\n",
       "1421. 0.392743430515844\n",
       "1422. 0.223110197439574\n",
       "1423. 0.543278103035985\n",
       "1424. 0.277935719686639\n",
       "1425. 0.389467782217782\n",
       "1426. 0.39109015811024\n",
       "1427. 0.368649629375681\n",
       "1428. 0.231261163081585\n",
       "1429. 0.031578231292517\n",
       "1430. 0.661937946552801\n",
       "1431. 0.0328666666666667\n",
       "1432. 0.291528407340625\n",
       "1433. 0.0371926406926407\n",
       "1434. 0.632235050781604\n",
       "1435. 0.0629591836734694\n",
       "1436. 0.209477182680085\n",
       "1437. 0.20159488046791\n",
       "1438. 0.0161333333333333\n",
       "1439. 0.318255149005995\n",
       "1440. 0.517569014595609\n",
       "1441. 0.0742421245421245\n",
       "1442. 0.020978231292517\n",
       "1443. 0.621984153615674\n",
       "1444. 0.134348891459418\n",
       "1445. 0.211825940473073\n",
       "1446. 0.514845440695551\n",
       "1447. 0.0434428571428571\n",
       "1448. 0.22194794062147\n",
       "1449. 0.372580601195712\n",
       "1450. 0.0650410256410256\n",
       "1451. 0.0517\n",
       "1452. 0.23162295662591\n",
       "1453. 0.324765915803077\n",
       "1454. 0.150927674624226\n",
       "1455. 0.596247355676311\n",
       "1456. 0.368936985923394\n",
       "1457. 0.305086965500248\n",
       "1458. 0.00426233766233766\n",
       "1459. 0.264110301120331\n",
       "1460. 0.424242617232572\n",
       "1461. 0.00784489795918367\n",
       "1462. 0.475891914981982\n",
       "1463. 0.382656273186503\n",
       "1464. 0.662802837499407\n",
       "1465. 0.150482051282051\n",
       "1466. 0.682377327830452\n",
       "1467. 0.0924564102564103\n",
       "1468. 0.045995670995671\n",
       "1469. 0.0108\n",
       "1470. 0.702087492624913\n",
       "1471. 0.379392145539237\n",
       "1472. 0.001\n",
       "1473. 0.0559300366300366\n",
       "1474. 0.0210909090909091\n",
       "1475. 0.0730904761904762\n",
       "1476. 0.120644289044289\n",
       "1477. 0.382560243451559\n",
       "1478. 0.428814084119177\n",
       "1479. 0.146867040149393\n",
       "1480. 0.827661620155868\n",
       "1481. 0.0658507936507936\n",
       "1482. 0.186260212093377\n",
       "1483. 0.748145069080872\n",
       "1484. 0.00766666666666667\n",
       "1485. 0.144147362499376\n",
       "1486. 0.367816674617236\n",
       "1487. 0.249503010399562\n",
       "1488. 0.270551945145766\n",
       "1489. 0.232542773974768\n",
       "1490. 0.494969044686016\n",
       "1491. 0.730202050551766\n",
       "1492. 0.588403663316362\n",
       "1493. 0.371865109186483\n",
       "1494. 0.208005259239406\n",
       "1495. 0.195637169043863\n",
       "1496. 0.160361701261701\n",
       "1497. 0.0193714285714286\n",
       "1498. 0.0158666666666667\n",
       "1499. 0.0280538461538462\n",
       "1500. 0.729376768112002\n",
       "1501. 0.244041215973569\n",
       "1502. 0.118790849673203\n",
       "1503. 0.106571717171717\n",
       "1504. 0.433991872100007\n",
       "1505. 0.0608857142857143\n",
       "1506. 0.0691333333333333\n",
       "1507. 0.65000191022047\n",
       "1508. 0.651639088112206\n",
       "1509. 0.439207165959861\n",
       "1510. 0.346891056032109\n",
       "1511. 0.29402385335275\n",
       "1512. 0.00659815546772069\n",
       "1513. 0.0982682431995865\n",
       "1514. 0.0739904761904762\n",
       "1515. 0.761544699197131\n",
       "1516. 0.00649090909090909\n",
       "1517. 0.107051307847005\n",
       "1518. 0.349951635559961\n",
       "1519. 0.433959200461904\n",
       "1520. 0.0274949096880131\n",
       "1521. 0.0141333333333333\n",
       "1522. 0.235675316496369\n",
       "1523. 0.474336717738204\n",
       "1524. 0.709188648804476\n",
       "1525. 0.327763742506913\n",
       "1526. 0.0388714285714286\n",
       "1527. 0.2240654584846\n",
       "1528. 0.346826801367435\n",
       "1529. 0.757722394429666\n",
       "1530. 0.0300666666666667\n",
       "1531. 0.199370531767083\n",
       "1532. 0.600581905761181\n",
       "1533. 0.368189963898077\n",
       "1534. 0.201154013491749\n",
       "1535. 0.668778506396282\n",
       "1536. 0.0204\n",
       "1537. 0.154136507936508\n",
       "1538. 0.672268360737523\n",
       "1539. 0.157450273855882\n",
       "1540. 0.000235294117647059\n",
       "1541. 0.302083590675208\n",
       "1542. 0.140525925925926\n",
       "1543. 0.0714481792717087\n",
       "1544. 0.608711319649256\n",
       "1545. 0.112314285714286\n",
       "1546. 0.333080615800697\n",
       "1547. 0.00973333333333333\n",
       "1548. 0.263663806950509\n",
       "1549. 0.006\n",
       "1550. 0.0402598219390672\n",
       "1551. 0.0670656990068755\n",
       "1552. 0.106415586712138\n",
       "1553. 0.0468166666666667\n",
       "1554. 0.0395619047619048\n",
       "1555. 0.0919690476190476\n",
       "1556. 0.0633897435897436\n",
       "1557. 0.0445566923497958\n",
       "1558. 0.0189047619047619\n",
       "1559. 0.115769140383426\n",
       "1560. 0.0618047619047619\n",
       "1561. 0.0203686274509804\n",
       "1562. 0.00142424242424242\n",
       "1563. 0.38342849197586\n",
       "1564. 0.852856601307143\n",
       "1565. 0.260003782953814\n",
       "1566. 0.0663207600281492\n",
       "1567. 0.0197575757575758\n",
       "1568. 0.499076422777154\n",
       "1569. 0.227236408036408\n",
       "1570. 0.128069024840808\n",
       "1571. 0.604572949178595\n",
       "1572. 0.6104474259193\n",
       "1573. 0.0547285714285714\n",
       "1574. 0.538792271155307\n",
       "1575. 0.0779490842490843\n",
       "1576. 0.0194666666666667\n",
       "1577. 0.687524851748526\n",
       "1578. 0.248955861279166\n",
       "1579. 0.425913216230292\n",
       "1580. 0.261883747789335\n",
       "1581. 0.301614519580037\n",
       "1582. 0.227101960784314\n",
       "1583. 0.0228\n",
       "1584. 0.458696448784663\n",
       "1585. 0.0623428571428571\n",
       "1586. 0.0670854700854701\n",
       "1587. 0.329848336813399\n",
       "1588. 0.374627178391022\n",
       "1589. 0.42265869101895\n",
       "1590. 0.117145064086241\n",
       "1591. 0.670749969824215\n",
       "1592. 0.379973106276359\n",
       "1593. 0.220000872163182\n",
       "1594. 0.148253374076904\n",
       "1595. 0.0697224089635854\n",
       "1596. 0.391243062271746\n",
       "1597. 0.0974380952380952\n",
       "1598. 0.00289090909090909\n",
       "1599. 0.447340962760438\n",
       "1600. 0.352059064356158\n",
       "1601. 0.328763576174391\n",
       "1602. 0.00995757575757576\n",
       "1603. 0.0564047619047619\n",
       "1604. 0.499517306054529\n",
       "1605. 0.264267144492313\n",
       "1606. 0.648435212339\n",
       "1607. 0.0607229524288348\n",
       "1608. 0.459917647058823\n",
       "1609. 0.0934151882975412\n",
       "1610. 0.814013599147317\n",
       "1611. 0.00746666666666667\n",
       "1612. 0.027237037037037\n",
       "1613. 0.278787963272301\n",
       "1614. 0.644552081252081\n",
       "1615. 0.706449246404825\n",
       "1616. 0.0048\n",
       "1617. 0.0580745098039216\n",
       "1618. 0.0184047619047619\n",
       "1619. 0.219033211233211\n",
       "1620. 0.362470971727596\n",
       "1621. 0.00954285714285714\n",
       "1622. 0.865554624940664\n",
       "1623. 0.604889026290867\n",
       "1624. 0.45710416171468\n",
       "1625. 0.321347386013069\n",
       "1626. 0.556274691057301\n",
       "1627. 0.144271763175016\n",
       "1628. 0.536879351917059\n",
       "1629. 0.233402319335823\n",
       "1630. 0.10163894993895\n",
       "1631. 0.212447939519923\n",
       "1632. 0.230654565473356\n",
       "1633. 0.413968897690306\n",
       "1634. 0.19102449294001\n",
       "1635. 0.319777827286465\n",
       "1636. 0.291493981579849\n",
       "1637. 0.00809090909090909\n",
       "1638. 0.148263362156457\n",
       "1639. 0.002\n",
       "1640. 0.474988858590471\n",
       "1641. 0.293499554269232\n",
       "1642. 0.133104761904762\n",
       "1643. 0.4678877201779\n",
       "1644. 0.0895974842767296\n",
       "1645. 0.174179310344828\n",
       "1646. 0.301087263839736\n",
       "1647. 0.0460444444444444\n",
       "1648. 0.267754674263379\n",
       "1649. 0.12642614379085\n",
       "1650. 0.0196242424242424\n",
       "1651. 0.537195793473164\n",
       "1652. 0.0935333333333333\n",
       "1653. 0.0628047619047619\n",
       "1654. 0.0784666666666667\n",
       "1655. 0.347248441551687\n",
       "1656. 0.243234312177997\n",
       "1657. 0.0110024737167594\n",
       "1658. 0.389838466243758\n",
       "1659. 0.492113499292125\n",
       "1660. 0.027778231292517\n",
       "1661. 0.514638372923188\n",
       "1662. 0.126111111111111\n",
       "1663. 0.0444206349206349\n",
       "1664. 0.252349997713857\n",
       "1665. 0.280270562056559\n",
       "1666. 0.319795079305615\n",
       "1667. 0.0180242424242424\n",
       "1668. 0.183724018838305\n",
       "1669. 0.0775756187783935\n",
       "1670. 0.383244588744589\n",
       "1671. 0.175342857142857\n",
       "1672. 0.0135931677018634\n",
       "1673. 0.0409020408163265\n",
       "1674. 0.335437364471853\n",
       "1675. 0.454953809948352\n",
       "1676. 0.810441547633601\n",
       "1677. 0.0352747474747475\n",
       "1678. 0.211952941176471\n",
       "1679. 0.0235285714285714\n",
       "1680. 0.457761027950668\n",
       "1681. 0.212335246282094\n",
       "1682. 0.160738095238095\n",
       "1683. 0.0646730158730159\n",
       "1684. 0.399322159631053\n",
       "1685. 0.171839071603778\n",
       "1686. 0.052356862745098\n",
       "1687. 0.0319560090702948\n",
       "1688. 0.335196471069994\n",
       "1689. 0.190525165001189\n",
       "1690. 0.00909090909090909\n",
       "1691. 0.0393619047619048\n",
       "1692. 0.432405458834251\n",
       "1693. 0.580810988130229\n",
       "1694. 0.143869741369741\n",
       "1695. 0.0404943504256937\n",
       "1696. 9.09090909090909e-05\n",
       "1697. 0.41961652229829\n",
       "1698. 0.0870783103254431\n",
       "1699. 0.208521699193482\n",
       "1700. 0.0914508899709711\n",
       "1701. 0.507317141920146\n",
       "1702. 0.0702642857142857\n",
       "1703. 0.230625986019151\n",
       "1704. 0.123316172463305\n",
       "1705. 0.486371238702818\n",
       "1706. 0.127573405211141\n",
       "1707. 0.0808883933676387\n",
       "1708. 0.655226094955746\n",
       "1709. 0.640896748726266\n",
       "1710. 0.120290708007341\n",
       "1711. 0.418326940814628\n",
       "1712. 0.174287601034734\n",
       "1713. 0.0178337662337662\n",
       "1714. 0.0485200241671569\n",
       "1715. 0.166570586584339\n",
       "1716. 0.223664595976089\n",
       "1717. 0.360729834841786\n",
       "1718. 0.22123163725692\n",
       "1719. 0.483856692740181\n",
       "1720. 0.0284039215686275\n",
       "1721. 0.217811970217412\n",
       "1722. 0.637398736824343\n",
       "1723. 0.775120870356815\n",
       "1724. 0.121545982906221\n",
       "1725. 0.0895825396825397\n",
       "1726. 0.00440952380952381\n",
       "1727. 0.378986217358631\n",
       "1728. 0.0328405797101449\n",
       "1729. 0.0235205128205128\n",
       "1730. 0.663065234842642\n",
       "1731. 0.1168\n",
       "1732. 0.185046405228758\n",
       "1733. 0.33281228668113\n",
       "1734. 0.729494346308559\n",
       "1735. 0.0794250377073906\n",
       "1736. 0.0691716603882932\n",
       "1737. 0.175877432869996\n",
       "1738. 0.0268047619047619\n",
       "1739. 0.421053451095202\n",
       "1740. 0.487261947186317\n",
       "1741. 0.640005404166208\n",
       "1742. 0.248536716178094\n",
       "1743. 0.122024242424242\n",
       "1744. 0.0637686274509804\n",
       "1745. 0.0780758907758907\n",
       "1746. 0.0116666666666667\n",
       "1747. 0.547958892884658\n",
       "1748. 0.421848057427041\n",
       "1749. 0.0413247225205872\n",
       "1750. 0.36810381275491\n",
       "1751. 0.0498666666666667\n",
       "1752. 0.677631688827879\n",
       "1753. 0.249111493724492\n",
       "1754. 0.735682519824392\n",
       "1755. 0.0276269841269841\n",
       "1756. 0.591329460068238\n",
       "1757. 0.440550995239788\n",
       "1758. 0.154649595687332\n",
       "1759. 0.0480047619047619\n",
       "1760. 0.235115653936707\n",
       "1761. 0.0869333333333333\n",
       "1762. 0.132469841269841\n",
       "1763. 0.507676108528792\n",
       "1764. 0.237856732801472\n",
       "1765. 0.00933333333333333\n",
       "1766. 0.250326537118351\n",
       "1767. 0.237531614062577\n",
       "1768. 0.0612571428571429\n",
       "1769. 0.0281230769230769\n",
       "1770. 0.543370069951374\n",
       "1771. 0.295349594632302\n",
       "1772. 0.195602748441943\n",
       "1773. 0.0141333333333333\n",
       "1774. 0.282436814327739\n",
       "1775. 0.214038128809445\n",
       "1776. 0.355551123924004\n",
       "1777. 0.199634717337053\n",
       "1778. 0.0886120082815735\n",
       "1779. 0.476181227928559\n",
       "1780. 0.313311821803927\n",
       "1781. 0.655482090457493\n",
       "1782. 0.0180909090909091\n",
       "1783. 0.188301565432909\n",
       "1784. 0.0731425287356322\n",
       "1785. 0.0162879869189055\n",
       "1786. 0.0262969696969697\n",
       "1787. 0.114137254580363\n",
       "1788. 0.195256132756133\n",
       "1789. 0.645082171530284\n",
       "1790. 0.0866\n",
       "1791. 0.173414285714286\n",
       "1792. 0.04620853432282\n",
       "1793. 0.729948878366983\n",
       "1794. 0.0715447293447293\n",
       "1795. 0.595765895398517\n",
       "1796. 0.0353555555555556\n",
       "1797. 0.0638862745098039\n",
       "1798. 0.155683641307982\n",
       "1799. 0.60683780633572\n",
       "1800. 0.524395606518497\n",
       "1801. 0.418284042284305\n",
       "1802. 0.646343786709744\n",
       "1803. 0.0231333333333333\n",
       "1804. 0.351036757660706\n",
       "1805. 0.584150542615313\n",
       "1806. 0.115877922077922\n",
       "1807. 0.0738506879478207\n",
       "1808. 0.638429688940546\n",
       "1809. 0.0088\n",
       "1810. 0.218502071516578\n",
       "1811. 0.236234709396882\n",
       "1812. 0.0561294264607698\n",
       "1813. 0.386173278994332\n",
       "1814. 0.122357142857143\n",
       "1815. 0.270787372063464\n",
       "1816. 0.512690389846322\n",
       "1817. 0.0201197583511016\n",
       "1818. 0.538740727771748\n",
       "1819. 0.0373230769230769\n",
       "1820. 0.557972159663091\n",
       "1821. 0.218904509525562\n",
       "1822. 0.430485959095843\n",
       "1823. 0.6906847732655\n",
       "1824. 0.0185422463893792\n",
       "1825. 0.0116913626056483\n",
       "1826. 0.0365115646258503\n",
       "1827. 0.697792762657136\n",
       "1828. 0.785361399116238\n",
       "1829. 0.0461333333333333\n",
       "1830. 0.006\n",
       "1831. 0.0286761904761905\n",
       "1832. 0.10196790986791\n",
       "1833. 0.0511050583628677\n",
       "1834. 0.0375675213675214\n",
       "1835. 0.0632798316111749\n",
       "1836. 0.643925769277783\n",
       "1837. 0.143593310657596\n",
       "1838. 0.0197217391304348\n",
       "1839. 0.509039841027984\n",
       "1840. 0.631491229789319\n",
       "1841. 0.15473932016072\n",
       "1842. 0.0619948868741321\n",
       "1843. 0.631510040245023\n",
       "1844. 0.0200285714285714\n",
       "1845. 0.40157776900883\n",
       "1846. 0.0524243705985835\n",
       "1847. 0.175101887372963\n",
       "1848. 0.00566666666666667\n",
       "1849. 0.243075635964627\n",
       "1850. 0.420006484119292\n",
       "1851. 0.109966666666667\n",
       "1852. 0.220755648745138\n",
       "1853. 0.0024\n",
       "1854. 0.513641429945596\n",
       "1855. 0.17123538716673\n",
       "1856. 0.00209090909090909\n",
       "1857. 0.557749673749674\n",
       "1858. 0.0597333333333333\n",
       "1859. 0.439517909276698\n",
       "1860. 0.454787856259751\n",
       "1861. 0.248552413254401\n",
       "1862. 0.134250549450549\n",
       "1863. 0.657388560901124\n",
       "1864. 0.111611111111111\n",
       "1865. 0.106639956122309\n",
       "1866. 0.345859708561882\n",
       "1867. 0.0540047619047619\n",
       "1868. 0.367525596681185\n",
       "1869. 0.593266379342398\n",
       "1870. 0.176785411067838\n",
       "1871. 0.0188\n",
       "1872. 0.290231616643269\n",
       "1873. 0.0480683982683983\n",
       "1874. 0.176752496860813\n",
       "1875. 0.00733333333333333\n",
       "1876. 0.50031639077822\n",
       "1877. 0.412294262644807\n",
       "1878. 0.518579294848777\n",
       "1879. 0.0261326007326007\n",
       "1880. 0.01905983436853\n",
       "1881. 0.0651991231275119\n",
       "1882. 0.72489130706926\n",
       "1883. 0.0268290043290043\n",
       "1884. 0.404482163427366\n",
       "1885. 0.0928116447493806\n",
       "1886. 0.0546692677070828\n",
       "1887. 0.140716596247331\n",
       "1888. 0.00679763177998472\n",
       "1889. 0.358955824179798\n",
       "1890. 0.0767428571428571\n",
       "1891. 0.532318555576708\n",
       "1892. 0.504254436513197\n",
       "1893. 0.518370962383905\n",
       "1894. 0.0338566473655403\n",
       "1895. 0.0302464646464646\n",
       "1896. 0.251135388545539\n",
       "1897. 0.00133333333333333\n",
       "1898. 0.323852168737935\n",
       "1899. 0.0388\n",
       "1900. 0.730835524248414\n",
       "1901. 0.620188874874476\n",
       "1902. 0.630244244419824\n",
       "1903. 0.0721142638456071\n",
       "1904. 0.12166031812939\n",
       "1905. 0.305540265889635\n",
       "1906. 0.245012615605485\n",
       "1907. 0.490300430941607\n",
       "1908. 0.0034\n",
       "1909. 0.0107575757575758\n",
       "1910. 0.882408533589661\n",
       "1911. 0.126232492997199\n",
       "1912. 0.636695134235557\n",
       "1913. 0.615489800748144\n",
       "1914. 0.0198761904761905\n",
       "1915. 0.468966866621097\n",
       "1916. 0.0903596273291925\n",
       "1917. 0.3519291679354\n",
       "1918. 0.388504130427536\n",
       "1919. 0.251495953222723\n",
       "1920. 0.170214946457052\n",
       "1921. 0.341623667918618\n",
       "1922. 0.299819894697688\n",
       "1923. 0.0504290043290043\n",
       "1924. 0.0949139573070608\n",
       "1925. 0.549730149364976\n",
       "1926. 0.416010308146513\n",
       "1927. 0.331928693275643\n",
       "1928. 0.150220757352101\n",
       "1929. 0.156840427251817\n",
       "1930. 0.537939348869069\n",
       "1931. 0.314179757321879\n",
       "1932. 0.560541568132123\n",
       "1933. 0.25118514292517\n",
       "1934. 0.288552866902346\n",
       "1935. 0.407984128813522\n",
       "1936. 0.176759706959707\n",
       "1937. 0.0212\n",
       "1938. 0.641934489245435\n",
       "1939. 0.0203396825396825\n",
       "1940. 0.0052\n",
       "1941. 0.401647385027601\n",
       "1942. 0.511356057862999\n",
       "1943. 0.00623809523809524\n",
       "1944. 0.0592004329004329\n",
       "1945. 0.237141352014374\n",
       "1946. 0.085396392243525\n",
       "1947. 0.788862512826833\n",
       "1948. 0.0808649572649573\n",
       "1949. 0.473144649730227\n",
       "1950. 0.0228\n",
       "1951. 0.136904761904762\n",
       "1952. 0.140655555555556\n",
       "1953. 0.765634914227787\n",
       "1954. 0.6158\n",
       "1955. 0.539687413737704\n",
       "1956. 0.0686062027229007\n",
       "1957. 0.483012053932509\n",
       "1958. 0.0733139573070608\n",
       "1959. 0.215238805970149\n",
       "1960. 0.264931591323405\n",
       "1961. 0.318030064563293\n",
       "1962. 0.631477491312862\n",
       "1963. 0.407353968045321\n",
       "1964. 0.00818468899521531\n",
       "1965. 0.186946111375077\n",
       "1966. 0.086162981352753\n",
       "1967. 0.215811473127263\n",
       "1968. 0.0303543436955202\n",
       "1969. 0.604514698161444\n",
       "1970. 0.723084875091197\n",
       "1971. 0.289415592624221\n",
       "1972. 0.372545743145743\n",
       "1973. 0.336713213980879\n",
       "1974. 0.039318315018315\n",
       "1975. 0.309967764625572\n",
       "1976. 0.0734321043249836\n",
       "1977. 0.201138644688645\n",
       "1978. 0.256224259866968\n",
       "1979. 0.314791419962736\n",
       "1980. 0.140282184809471\n",
       "1981. 0.099782683982684\n",
       "1982. 0.704344831895001\n",
       "1983. 0.201224498525836\n",
       "1984. 0.0415333333333333\n",
       "1985. 0.0519019607843137\n",
       "1986. 0.694303316604856\n",
       "1987. 0.282422205186471\n",
       "1988. 0.120420660603014\n",
       "1989. 0.172652940854872\n",
       "1990. 0.304401309154514\n",
       "1991. 0.05003186537773\n",
       "1992. 0.508016796690953\n",
       "1993. 0.70500700439963\n",
       "1994. 0.504013885833157\n",
       "1995. 0.343224242424242\n",
       "1996. 0.0454884057971014\n",
       "1997. 0.570207676366393\n",
       "1998. 0.125894983688087\n",
       "1999. 0.133616987610091\n",
       "2000. 0.545872231676956\n",
       "2001. 0.209753091684435\n",
       "2002. 0.255642772443036\n",
       "2003. 0.00151428571428571\n",
       "2004. 0.0741897435897436\n",
       "2005. 0.465913523165361\n",
       "2006. 0.166758862165556\n",
       "2007. 0.16904126984127\n",
       "2008. 0.333526921480466\n",
       "2009. 0.200735817805383\n",
       "2010. 0.0346285714285714\n",
       "2011. 0.299428793348988\n",
       "2012. 0.284971170474937\n",
       "2013. 0.244132627546813\n",
       "2014. 0.311459815778714\n",
       "2015. 0.314835655038608\n",
       "2016. 0.288097508742712\n",
       "2017. 0.00133333333333333\n",
       "2018. 0.686166787728018\n",
       "2019. 0.282616631571023\n",
       "2020. 0.3848515984014\n",
       "2021. 0.129150549450549\n",
       "2022. 0.415681447726581\n",
       "2023. 0.223621388443156\n",
       "2024. 0.469968886032506\n",
       "2025. 0.154807008086253\n",
       "2026. 0.299682621888885\n",
       "2027. 0.299367211511165\n",
       "2028. 0.601434676318298\n",
       "2029. 0.610963313102379\n",
       "2030. 0.272414718614719\n",
       "2031. 0.0222\n",
       "2032. 0.118647619047619\n",
       "2033. 0.659677105017832\n",
       "2034. 0.207591088833194\n",
       "2035. 0.006\n",
       "2036. 0.0170242424242424\n",
       "2037. 0.247568264688533\n",
       "2038. 0.0280666666666667\n",
       "2039. 0.290326480640085\n",
       "2040. 0.32214621413716\n",
       "2041. 0.0250448979591837\n",
       "2042. 0.0167897435897436\n",
       "2043. 0.0112666666666667\n",
       "2044. 0.075847619047619\n",
       "2045. 0.102975200989487\n",
       "2046. 0.0895166666666667\n",
       "2047. 0.21335544210567\n",
       "2048. 0.0940847880914818\n",
       "2049. 0.294917309481363\n",
       "2050. 0.031970695970696\n",
       "2051. 0.0320560846560847\n",
       "2052. 0.316107836913662\n",
       "2053. 0.0400818070818071\n",
       "2054. 0.183807070707071\n",
       "2055. 0.648082372111897\n",
       "2056. 0.266412670232578\n",
       "2057. 0.60368545869941\n",
       "2058. 0.178455544164321\n",
       "2059. 0.0357380952380952\n",
       "2060. 0.0612242424242424\n",
       "2061. 0.22099898989899\n",
       "2062. 0.0977333333333333\n",
       "2063. 0.0270196078431373\n",
       "2064. 0.263916995610636\n",
       "2065. 0.034178231292517\n",
       "2066. 0.650701455301455\n",
       "2067. 0.00899567099567099\n",
       "2068. 0.179704777489037\n",
       "2069. 0.247198619248563\n",
       "2070. 0.0494666666666667\n",
       "2071. 0.242614081012061\n",
       "2072. 0.0611890756302521\n",
       "2073. 0.161042148813548\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   [1] 3.866667e-02 2.843078e-01 3.579878e-01 4.943333e-02 5.893788e-01\n",
       "   [6] 1.114552e-01 3.231461e-01 3.234192e-01 7.320361e-02 9.090909e-05\n",
       "  [11] 3.718062e-01 2.535243e-01 3.271156e-02 7.685779e-01 1.183765e-01\n",
       "  [16] 3.899673e-02 3.056427e-02 1.530952e-01 2.024832e-01 1.140247e-02\n",
       "  [21] 1.060464e-01 2.934638e-01 7.995700e-01 5.733333e-03 5.085798e-01\n",
       "  [26] 6.829295e-02 2.879339e-01 4.187266e-01 4.265804e-01 3.144693e-01\n",
       "  [31] 2.928203e-01 6.570497e-01 2.560764e-02 3.519963e-01 5.664684e-01\n",
       "  [36] 2.686390e-01 6.732718e-01 6.392887e-01 7.877833e-01 2.333333e-02\n",
       "  [41] 1.715005e-01 1.192854e-01 6.533627e-02 6.276116e-01 2.593067e-01\n",
       "  [46] 6.574646e-01 2.233958e-01 2.016364e-02 2.163198e-01 1.311094e-01\n",
       "  [51] 1.984444e-02 2.436033e-02 2.046137e-01 2.320451e-01 7.797460e-01\n",
       "  [56] 3.755556e-01 5.754702e-01 4.441644e-01 1.237935e-01 3.946242e-01\n",
       "  [61] 7.870176e-01 1.262424e-02 9.579524e-02 1.434192e-01 4.843333e-02\n",
       "  [66] 9.925714e-02 9.171429e-03 1.881881e-02 5.447966e-01 1.259745e-01\n",
       "  [71] 4.814749e-01 3.842577e-02 4.664444e-02 5.037506e-01 5.809524e-02\n",
       "  [76] 2.032081e-01 5.304796e-01 3.689440e-02 2.592421e-01 9.731096e-02\n",
       "  [81] 7.948913e-01 4.744900e-02 2.574973e-02 5.495282e-01 2.335136e-01\n",
       "  [86] 2.206634e-01 3.133333e-03 8.684058e-02 7.101000e-01 2.369000e-01\n",
       "  [91] 2.129757e-02 6.912900e-02 9.692405e-02 6.636815e-01 1.424734e-01\n",
       "  [96] 2.920550e-01 9.479491e-02 6.457436e-02 3.324977e-01 3.333280e-01\n",
       " [101] 5.251907e-01 2.875758e-02 4.891789e-01 2.427201e-01 3.869526e-01\n",
       " [106] 3.428565e-01 3.283030e-02 1.895354e-01 2.104827e-01 4.980952e-02\n",
       " [111] 3.313333e-02 6.449272e-01 1.418476e-01 5.833071e-01 2.134564e-01\n",
       " [116] 2.282424e-02 1.014805e-02 3.057227e-01 4.041117e-02 2.194286e-02\n",
       " [121] 2.373333e-02 6.005395e-02 3.373072e-02 1.710742e-01 1.814886e-01\n",
       " [126] 3.013900e-02 2.767586e-02 2.948286e-01 6.298234e-01 5.431515e-02\n",
       " [131] 5.986878e-01 2.564195e-01 3.068667e-01 2.827275e-01 3.401242e-01\n",
       " [136] 5.010033e-01 5.086667e-02 4.696535e-01 1.180862e-01 5.728414e-01\n",
       " [141] 3.145980e-01 1.722116e-01 3.122750e-01 4.287047e-01 3.164855e-01\n",
       " [146] 2.188848e-01 5.382411e-01 2.126188e-01 2.717349e-01 6.564762e-02\n",
       " [151] 6.818928e-02 2.700125e-01 2.195999e-01 1.272824e-01 1.172418e-02\n",
       " [156] 1.297434e-01 3.687059e-02 4.253139e-01 2.505653e-01 8.117543e-01\n",
       " [161] 1.021875e-01 1.183222e-01 2.825641e-02 9.040443e-02 2.268378e-02\n",
       " [166] 4.418264e-01 6.400649e-02 3.710585e-01 3.406667e-02 1.370934e-01\n",
       " [171] 6.211601e-01 2.407830e-01 3.589165e-01 4.337513e-02 2.993653e-01\n",
       " [176] 9.600000e-03 2.459541e-01 2.531021e-01 5.789743e-01 7.757912e-01\n",
       " [181] 7.333490e-01 6.414110e-01 4.376065e-01 4.466667e-03 6.935190e-01\n",
       " [186] 2.396295e-02 1.000000e-03 1.348979e-01 2.266441e-01 1.599595e-01\n",
       " [191] 2.448980e-04 3.590454e-01 4.778658e-01 2.140593e-01 7.555556e-03\n",
       " [196] 3.333333e-04 4.725714e-02 1.516667e-02 3.582424e-02 6.371580e-01\n",
       " [201] 4.827315e-01 1.113323e-01 1.522501e-01 4.971671e-01 2.714793e-01\n",
       " [206] 4.552038e-01 4.487576e-02 7.639352e-01 1.050619e-01 6.068301e-01\n",
       " [211] 2.660000e-02 8.000000e-04 9.427395e-02 4.387001e-01 2.435123e-01\n",
       " [216] 5.030790e-01 1.335691e-01 4.120370e-02 1.020369e-02 5.800679e-02\n",
       " [221] 8.114748e-02 4.890909e-03 3.308103e-01 4.644205e-01 7.006667e-02\n",
       " [226] 9.739048e-02 1.637664e-01 5.232742e-01 1.994670e-01 7.690909e-03\n",
       " [231] 6.770316e-01 6.963968e-02 4.133048e-01 6.111948e-02 1.698841e-02\n",
       " [236] 6.072857e-02 4.349269e-01 2.164480e-01 3.558535e-01 5.275566e-01\n",
       " [241] 5.305055e-02 1.836863e-02 1.130119e-02 8.139677e-02 7.609714e-02\n",
       " [246] 5.065608e-02 4.007354e-01 5.150185e-01 3.604092e-01 8.244898e-03\n",
       " [251] 3.833333e-02 6.038747e-01 6.481631e-01 3.666749e-01 5.469259e-02\n",
       " [256] 4.900305e-01 7.419602e-01 2.009629e-01 1.443192e-01 1.416228e-01\n",
       " [261] 4.650518e-01 1.903286e-01 4.658333e-02 3.175573e-01 4.786113e-01\n",
       " [266] 2.681905e-01 8.788095e-02 9.644459e-02 6.012814e-02 9.060935e-02\n",
       " [271] 2.671748e-01 1.886046e-01 2.395799e-01 1.994780e-01 4.815556e-02\n",
       " [276] 2.807593e-01 3.071827e-01 6.742913e-02 3.501582e-01 1.654810e-01\n",
       " [281] 2.227943e-01 5.729201e-02 1.901040e-01 1.020934e-01 5.270673e-01\n",
       " [286] 2.165471e-01 8.283641e-02 4.289225e-01 5.772349e-01 2.915592e-01\n",
       " [291] 4.057548e-01 6.273333e-02 2.954434e-01 3.418396e-01 3.481535e-01\n",
       " [296] 8.037633e-01 5.386575e-01 1.557036e-01 2.840000e-02 6.539518e-02\n",
       " [301] 6.810069e-01 2.509091e-02 4.072650e-02 6.623377e-04 3.783153e-02\n",
       " [306] 3.786667e-02 1.196643e-01 2.534188e-01 6.250196e-02 4.424242e-03\n",
       " [311] 1.197122e-01 3.486570e-01 4.157873e-01 4.860952e-02 3.900000e-02\n",
       " [316] 5.961409e-01 2.657246e-01 1.728939e-01 4.579410e-01 3.928122e-01\n",
       " [321] 1.571067e-02 1.352434e-01 1.940000e-02 2.205473e-01 1.874250e-01\n",
       " [326] 4.470729e-01 3.877153e-01 4.540459e-01 3.788495e-02 3.982288e-01\n",
       " [331] 3.904775e-01 1.521335e-01 5.875579e-01 4.283099e-01 7.446346e-01\n",
       " [336] 1.908212e-01 4.553032e-01 6.405049e-01 1.567733e-02 1.449721e-01\n",
       " [341] 5.955556e-02 1.506916e-01 7.507475e-02 3.103026e-01 3.042388e-01\n",
       " [346] 3.214986e-02 7.845967e-02 3.019302e-01 4.783810e-02 7.275281e-02\n",
       " [351] 8.673853e-02 2.287143e-02 1.506089e-01 6.286589e-01 1.001768e-01\n",
       " [356] 2.776044e-01 6.768675e-01 1.687373e-01 7.871623e-02 1.485636e-01\n",
       " [361] 7.661386e-02 3.733333e-02 1.104302e-01 9.326960e-02 3.894457e-01\n",
       " [366] 2.737693e-01 5.525878e-01 1.966667e-02 5.126032e-02 9.234366e-02\n",
       " [371] 6.122396e-01 6.416175e-02 9.673469e-03 7.119672e-02 1.051906e-01\n",
       " [376] 6.295358e-01 1.719504e-01 8.933333e-02 6.133333e-03 2.824494e-01\n",
       " [381] 1.904881e-01 3.728152e-01 4.247698e-02 5.369727e-01 1.579032e-01\n",
       " [386] 7.971161e-02 5.266667e-03 3.605096e-01 7.041339e-02 2.702016e-01\n",
       " [391] 1.197980e-02 1.111837e-01 6.158020e-01 1.992489e-01 9.181971e-02\n",
       " [396] 3.851975e-01 9.172852e-02 8.157460e-02 3.736099e-02 3.312381e-02\n",
       " [401] 1.052667e-01 7.178494e-01 1.509778e-01 7.096322e-01 5.950476e-02\n",
       " [406] 3.252486e-01 1.002815e-01 3.280139e-01 6.880334e-01 6.631312e-02\n",
       " [411] 2.271638e-01 3.280000e-02 3.938726e-01 2.486667e-02 4.057407e-01\n",
       " [416] 4.387564e-01 2.543742e-01 6.989818e-01 3.940858e-01 4.959208e-01\n",
       " [421] 3.244870e-01 5.106771e-01 1.005714e-02 3.428999e-02 1.545580e-01\n",
       " [426] 4.110682e-01 1.220017e-01 3.082143e-01 1.649423e-01 3.606879e-01\n",
       " [431] 4.835086e-01 7.291429e-02 3.952534e-01 2.632788e-01 4.222762e-01\n",
       " [436] 2.297504e-01 1.080476e-02 1.765085e-01 4.743062e-01 2.766607e-01\n",
       " [441] 9.476349e-02 1.053952e-01 1.125048e-01 8.062344e-01 1.364314e-01\n",
       " [446] 4.358333e-01 9.000000e-03 1.145455e-02 4.825714e-01 6.022424e-02\n",
       " [451] 4.313376e-01 2.768103e-01 3.673810e-02 6.231888e-01 2.096223e-01\n",
       " [456] 7.800000e-03 2.899537e-01 3.152357e-01 6.527565e-01 5.750279e-01\n",
       " [461] 1.074343e-01 9.977070e-02 2.131350e-01 7.437714e-01 7.645507e-02\n",
       " [466] 3.183495e-01 1.407467e-01 3.532404e-02 5.708841e-02 5.386812e-01\n",
       " [471] 3.637331e-01 6.193582e-01 1.111667e-01 5.203568e-02 5.164501e-01\n",
       " [476] 2.512109e-02 2.944423e-01 6.725575e-01 2.401128e-01 2.307586e-02\n",
       " [481] 5.400000e-03 1.398635e-01 6.643661e-01 9.802222e-02 2.446667e-02\n",
       " [486] 1.041562e-01 5.506291e-01 1.304474e-02 3.382442e-01 8.420000e-02\n",
       " [491] 5.417117e-01 3.498813e-01 5.310415e-01 6.257888e-01 4.737436e-02\n",
       " [496] 3.800000e-03 6.764602e-01 9.933333e-03 6.772396e-01 1.069111e-01\n",
       " [501] 1.458542e-01 1.449225e-01 8.173913e-03 1.785758e-02 1.216029e-01\n",
       " [506] 1.700000e-02 3.571255e-01 3.385161e-01 1.139444e-01 3.740868e-02\n",
       " [511] 3.655003e-01 2.947993e-01 2.442424e-02 3.576667e-02 6.002857e-02\n",
       " [516] 2.335656e-01 5.609867e-02 1.024667e-01 5.327870e-01 4.295905e-01\n",
       " [521] 6.774359e-03 3.430994e-02 4.527302e-01 2.533333e-02 2.023716e-01\n",
       " [526] 2.896453e-01 2.327786e-02 6.267404e-01 1.980959e-01 9.224242e-03\n",
       " [531] 2.459061e-01 2.886667e-02 3.619193e-01 5.343993e-01 3.103906e-01\n",
       " [536] 4.973095e-03 8.389083e-01 6.404862e-01 3.111273e-01 2.044674e-01\n",
       " [541] 1.491694e-01 7.788866e-01 3.568889e-02 1.518639e-01 1.097997e-01\n",
       " [546] 5.338718e-02 5.199505e-01 6.391610e-01 1.812381e-02 5.353261e-01\n",
       " [551] 1.784490e-02 3.747059e-02 9.358022e-02 7.062445e-01 4.218529e-01\n",
       " [556] 2.896915e-01 1.036760e-01 5.190547e-02 7.082827e-02 3.220050e-01\n",
       " [561] 5.895294e-02 4.851633e-02 5.383810e-02 2.146039e-01 1.770110e-01\n",
       " [566] 3.063333e-02 3.764467e-01 7.740289e-02 5.758409e-01 2.571649e-01\n",
       " [571] 3.929483e-02 2.081898e-01 2.265691e-01 3.403633e-01 3.021639e-01\n",
       " [576] 4.198545e-01 4.687059e-01 2.397432e-01 5.565853e-02 5.982424e-02\n",
       " [581] 2.239860e-02 1.385556e-02 3.311053e-01 4.664341e-01 2.605657e-01\n",
       " [586] 4.593217e-01 5.586667e-02 1.412934e-01 2.423944e-01 3.963030e-02\n",
       " [591] 4.032558e-01 1.899224e-01 3.421667e-02 1.120000e-02 2.498833e-02\n",
       " [596] 4.819767e-01 4.440000e-02 1.917778e-02 2.963810e-02 3.673067e-01\n",
       " [601] 7.783810e-02 2.893129e-01 5.591757e-01 3.089670e-02 1.013476e-01\n",
       " [606] 2.936406e-01 7.980602e-01 1.346027e-01 9.928820e-02 9.408627e-02\n",
       " [611] 2.181157e-01 4.662147e-01 5.655773e-01 1.554517e-01 9.328627e-02\n",
       " [616] 5.148400e-01 1.776460e-02 2.894431e-01 1.976594e-01 8.137992e-01\n",
       " [621] 2.282479e-01 5.366286e-01 6.939939e-01 4.672155e-01 2.625248e-01\n",
       " [626] 1.937859e-01 1.531937e-01 4.512109e-02 1.970066e-01 7.012308e-02\n",
       " [631] 7.040745e-02 1.160600e-01 5.098751e-01 2.409457e-01 2.146298e-01\n",
       " [636] 5.228164e-03 3.121265e-01 2.430399e-01 4.222913e-02 1.201242e-01\n",
       " [641] 1.276032e-01 6.988561e-02 1.629317e-02 1.760889e-01 1.144162e-01\n",
       " [646] 1.748889e-02 1.230375e-01 1.118889e-01 3.663261e-01 2.430436e-01\n",
       " [651] 1.376072e-01 4.913333e-02 4.664510e-01 3.661250e-01 2.628723e-01\n",
       " [656] 1.692809e-01 6.309043e-01 5.059804e-01 5.644830e-01 2.525449e-01\n",
       " [661] 3.080242e-01 3.141472e-02 7.082614e-02 5.648474e-01 3.147469e-01\n",
       " [666] 1.484499e-01 4.437824e-01 5.278974e-02 1.424242e-03 5.722279e-01\n",
       " [671] 4.744418e-01 5.538788e-02 9.090909e-05 5.025234e-01 4.156925e-01\n",
       " [676] 5.135653e-01 2.373333e-02 3.940996e-01 3.428777e-01 2.126109e-01\n",
       " [681] 4.037619e-02 7.630128e-01 1.757636e-01 1.863890e-01 1.488594e-01\n",
       " [686] 5.257631e-01 2.986978e-01 2.990467e-01 1.371092e-01 2.257885e-01\n",
       " [691] 5.008637e-02 9.918703e-02 3.906624e-01 3.511104e-01 2.790196e-02\n",
       " [696] 3.091856e-01 3.492381e-02 4.767391e-02 3.853333e-02 2.457179e-01\n",
       " [701] 5.272222e-02 2.847879e-01 6.276416e-01 2.187973e-01 4.690450e-01\n",
       " [706] 5.941520e-01 3.733160e-01 2.647143e-02 2.546696e-01 4.155959e-02\n",
       " [711] 3.115980e-01 8.417869e-01 1.596364e-02 3.126667e-02 2.796556e-01\n",
       " [716] 4.122637e-01 3.793123e-01 4.396539e-01 7.847859e-01 5.452717e-02\n",
       " [721] 1.880176e-01 3.442735e-01 5.628684e-01 8.020000e-02 6.257354e-02\n",
       " [726] 3.593506e-02 3.911750e-01 4.086583e-01 3.980981e-01 7.898062e-02\n",
       " [731] 2.710223e-01 5.519622e-01 2.126617e-01 7.515914e-02 5.788406e-03\n",
       " [736] 5.920000e-02 3.175758e-02 4.009232e-01 4.743810e-02 5.457037e-02\n",
       " [741] 2.316327e-03 1.052348e-01 5.871916e-01 9.000556e-02 6.839068e-01\n",
       " [746] 1.324632e-01 4.816030e-01 4.951829e-01 2.796508e-02 2.524026e-01\n",
       " [751] 1.800000e-02 3.560000e-02 1.747585e-01 1.533441e-01 7.268571e-02\n",
       " [756] 6.834992e-01 1.532026e-01 5.965641e-02 7.995141e-02 1.640476e-02\n",
       " [761] 2.630915e-01 4.747828e-01 6.707619e-02 2.807883e-01 7.042402e-01\n",
       " [766] 1.020521e-01 2.133333e-03 3.871200e-02 4.253371e-01 3.312784e-01\n",
       " [771] 2.811948e-02 3.495556e-02 4.757291e-01 6.768103e-01 5.230898e-01\n",
       " [776] 1.390114e-01 7.793185e-01 3.670018e-01 5.727059e-02 3.142510e-01\n",
       " [781] 1.618343e-01 2.133333e-03 4.129558e-01 1.993333e-02 4.589283e-02\n",
       " [786] 3.193909e-01 4.200374e-01 3.641026e-01 1.469476e-01 3.014764e-01\n",
       " [791] 5.173913e-03 2.056606e-01 6.136763e-01 3.197143e-02 1.504280e-01\n",
       " [796] 2.719091e-02 4.800000e-03 2.606667e-02 7.252698e-02 3.746857e-01\n",
       " [801] 1.507094e-01 2.358841e-02 3.081700e-01 2.115493e-01 3.204106e-01\n",
       " [806] 8.200000e-02 4.181240e-01 6.070159e-02 6.592464e-02 5.350174e-01\n",
       " [811] 3.540200e-01 1.023843e-01 1.200000e-02 4.180998e-02 2.797691e-01\n",
       " [816] 3.534896e-01 2.072949e-01 3.766471e-01 4.153499e-01 4.688095e-02\n",
       " [821] 1.575019e-01 7.339751e-02 5.952111e-01 4.452514e-01 6.616667e-02\n",
       " [826] 5.191722e-02 4.808042e-01 3.800000e-03 1.958547e-01 4.392005e-01\n",
       " [831] 6.303404e-02 1.180000e-02 5.162502e-01 4.985449e-01 7.605912e-01\n",
       " [836] 3.653944e-01 1.193271e-01 4.127782e-01 7.820476e-02 5.910305e-01\n",
       " [841] 2.740579e-01 6.346742e-01 1.060000e-02 6.325943e-01 5.701867e-01\n",
       " [846] 5.429141e-01 4.482222e-02 1.006465e-01 6.883333e-02 3.913238e-01\n",
       " [851] 1.342694e-01 3.589333e-01 5.694809e-01 7.400000e-03 5.845021e-01\n",
       " [856] 2.107454e-01 3.129418e-02 4.800000e-03 2.800000e-03 5.012715e-01\n",
       " [861] 1.040000e-02 5.246667e-02 8.589793e-02 2.892624e-01 4.246234e-01\n",
       " [866] 7.223175e-02 1.351673e-01 3.696110e-01 2.756859e-01 7.317607e-02\n",
       " [871] 5.869524e-02 1.215758e-02 9.071429e-03 2.309816e-01 2.535748e-01\n",
       " [876] 4.854896e-01 1.264778e-01 2.522669e-01 4.615612e-01 7.143962e-01\n",
       " [881] 8.880727e-01 3.805956e-01 3.005847e-01 1.866091e-01 9.101290e-02\n",
       " [886] 9.505094e-02 4.457143e-02 7.330744e-01 3.137623e-01 3.910569e-01\n",
       " [891] 5.379187e-01 5.450351e-01 1.839444e-01 1.910066e-01 2.294165e-01\n",
       " [896] 1.478395e-01 1.754664e-01 3.003869e-01 1.195991e-01 1.892892e-01\n",
       " [901] 1.395279e-01 5.099383e-01 9.010628e-03 3.166081e-01 5.736902e-01\n",
       " [906] 8.285832e-02 1.154896e-01 2.044444e-02 3.442816e-01 5.020597e-01\n",
       " [911] 3.682650e-02 6.391564e-01 6.055578e-01 3.552631e-01 6.020965e-02\n",
       " [916] 2.833333e-02 4.654178e-01 3.837719e-01 6.096878e-01 1.048222e-01\n",
       " [921] 1.860000e-02 6.604213e-01 4.814092e-01 8.097478e-01 7.877148e-02\n",
       " [926] 1.737980e-02 3.140073e-01 2.090012e-01 3.059277e-01 5.257866e-01\n",
       " [931] 8.629816e-01 8.982738e-02 1.073111e-01 6.884903e-01 1.788310e-01\n",
       " [936] 1.105972e-01 4.414913e-01 6.652821e-02 8.693333e-02 2.814171e-01\n",
       " [941] 3.199208e-01 4.415893e-01 3.121425e-01 6.666667e-03 1.826812e-01\n",
       " [946] 8.795873e-02 9.392942e-02 8.488380e-01 6.794192e-01 6.951311e-01\n",
       " [951] 2.778756e-01 2.903265e-01 1.305972e-01 7.560411e-01 1.541984e-01\n",
       " [956] 3.579358e-01 2.745507e-02 9.800000e-03 5.151038e-01 5.185686e-02\n",
       " [961] 6.020370e-02 2.127470e-01 8.200000e-03 1.045333e-01 2.960000e-02\n",
       " [966] 3.602238e-01 2.168101e-01 7.813030e-02 4.188739e-01 5.528655e-01\n",
       " [971] 6.292127e-02 3.181024e-01 5.530441e-01 4.226929e-01 1.631412e-01\n",
       " [976] 6.514286e-02 8.073492e-01 3.080899e-01 2.797754e-01 5.948115e-01\n",
       " [981] 7.828314e-01 5.921803e-01 3.047572e-01 5.138529e-01 5.983951e-01\n",
       " [986] 3.676609e-01 2.084798e-01 3.878373e-01 3.371209e-01 9.424242e-03\n",
       " [991] 4.483175e-02 3.310370e-02 8.340466e-02 2.653559e-01 6.682980e-02\n",
       " [996] 7.391652e-01 3.799210e-01 1.580476e-02 5.090909e-03 5.403461e-01\n",
       "[1001] 5.183704e-02 3.089821e-01 3.677112e-01 3.079655e-01 2.384514e-01\n",
       "[1006] 7.776523e-01 5.758208e-01 7.332353e-02 1.600000e-03 3.849624e-02\n",
       "[1011] 2.993655e-01 1.550182e-01 6.726190e-02 4.680611e-01 1.144724e-01\n",
       "[1016] 6.206154e-01 2.003090e-01 2.707260e-01 1.744528e-01 5.875925e-01\n",
       "[1021] 1.993004e-01 1.024490e-02 3.972857e-02 1.626667e-02 1.146667e-02\n",
       "[1026] 1.956935e-01 2.011885e-01 3.930908e-01 3.102667e-01 2.884490e-02\n",
       "[1031] 1.093508e-01 5.262477e-01 7.111363e-01 3.776790e-01 4.256894e-01\n",
       "[1036] 6.981220e-02 6.009524e-02 4.730206e-01 3.222678e-02 8.087214e-02\n",
       "[1041] 7.843597e-01 4.373327e-01 6.526552e-01 5.786667e-02 3.442031e-02\n",
       "[1046] 7.039580e-02 2.145396e-01 1.005063e-01 6.799902e-01 2.510273e-01\n",
       "[1051] 4.093988e-01 6.983810e-02 2.504036e-01 1.213810e-02 5.701240e-02\n",
       "[1056] 3.687812e-01 4.333333e-03 8.222222e-03 2.941801e-01 2.935758e-02\n",
       "[1061] 3.224242e-03 9.916364e-02 3.517443e-01 2.576995e-01 2.553610e-01\n",
       "[1066] 2.106197e-01 3.428129e-01 1.429091e-02 4.263310e-01 7.380457e-01\n",
       "[1071] 2.294796e-01 5.931674e-01 5.728709e-01 2.452106e-01 1.144968e-01\n",
       "[1076] 7.458974e-02 2.419772e-01 6.657619e-02 3.564982e-01 5.416623e-01\n",
       "[1081] 5.375990e-01 5.459384e-02 4.625641e-02 4.153368e-01 5.249279e-01\n",
       "[1086] 4.677778e-03 4.758264e-01 2.986540e-01 8.552276e-01 1.568785e-01\n",
       "[1091] 5.266960e-01 6.337822e-01 6.582256e-01 8.587500e-02 2.787344e-01\n",
       "[1096] 1.943832e-01 2.445495e-01 4.484346e-01 4.270649e-02 8.090528e-01\n",
       "[1101] 2.321854e-01 3.600000e-02 1.941431e-01 4.775695e-01 1.472528e-01\n",
       "[1106] 2.148769e-01 3.724327e-01 1.849782e-01 1.006000e-01 5.134533e-01\n",
       "[1111] 4.000000e-03 1.622175e-01 2.087794e-01 2.370446e-01 4.741384e-01\n",
       "[1116] 2.997580e-01 6.042092e-01 1.220969e-01 4.662598e-01 5.949483e-02\n",
       "[1121] 3.171244e-01 2.204213e-01 8.959547e-02 6.923765e-01 4.372163e-01\n",
       "[1126] 2.394527e-01 7.666667e-03 6.887253e-02 4.097778e-02 5.165861e-02\n",
       "[1131] 8.276700e-01 2.568180e-01 1.351084e-01 1.474879e-01 3.939085e-02\n",
       "[1136] 1.912587e-01 1.706366e-01 6.407876e-02 3.043244e-01 6.314286e-02\n",
       "[1141] 4.224052e-01 1.113333e-02 4.829470e-01 1.089285e-01 1.200000e-02\n",
       "[1146] 5.829245e-01 6.691872e-01 1.952167e-01 1.192927e-01 8.683371e-02\n",
       "[1151] 8.081485e-02 3.739553e-01 4.304591e-02 3.670334e-01 1.156711e-01\n",
       "[1156] 1.301472e-02 1.613231e-01 4.311204e-01 1.072734e-01 5.605530e-01\n",
       "[1161] 6.257144e-01 2.514178e-01 5.510413e-01 2.485458e-01 3.295072e-01\n",
       "[1166] 5.721920e-01 9.286667e-02 4.431799e-01 2.903825e-01 1.976022e-01\n",
       "[1171] 1.455858e-01 4.958797e-01 5.688352e-01 1.788251e-01 2.630067e-01\n",
       "[1176] 1.226667e-02 2.045308e-01 2.768156e-01 9.194732e-02 2.857769e-01\n",
       "[1181] 8.610269e-02 3.657205e-01 2.466667e-02 3.415978e-01 7.037037e-02\n",
       "[1186] 2.264171e-01 1.459399e-01 5.786667e-02 2.033581e-02 1.023449e-01\n",
       "[1191] 2.571910e-01 3.802704e-01 2.280126e-01 4.960000e-02 2.435505e-02\n",
       "[1196] 7.967143e-02 3.029095e-01 8.956691e-02 1.578048e-01 1.933046e-01\n",
       "[1201] 3.278850e-01 1.882504e-01 2.851451e-01 1.007286e-01 1.607827e-01\n",
       "[1206] 1.678841e-02 2.312357e-01 5.086667e-02 8.500836e-02 7.651111e-02\n",
       "[1211] 3.607316e-02 4.199700e-01 4.695440e-02 2.410403e-02 6.725835e-01\n",
       "[1216] 5.267721e-01 6.339224e-01 1.222927e-01 1.708560e-01 1.302047e-01\n",
       "[1221] 2.649601e-01 9.290909e-03 2.965714e-02 2.753759e-01 3.656190e-02\n",
       "[1226] 8.388148e-02 4.780178e-01 4.074533e-01 3.159689e-01 4.606667e-02\n",
       "[1231] 7.499792e-02 6.433861e-01 5.039108e-01 3.056517e-02 1.281206e-01\n",
       "[1236] 2.397643e-01 2.464965e-01 8.080176e-01 7.127794e-01 3.351604e-01\n",
       "[1241] 2.187386e-01 2.133333e-03 3.580000e-02 5.023360e-01 2.832457e-01\n",
       "[1246] 5.527617e-02 4.220179e-01 2.720853e-02 1.091159e-01 3.305256e-01\n",
       "[1251] 4.325664e-01 7.960170e-01 3.580753e-01 3.506456e-01 1.396321e-01\n",
       "[1256] 8.192801e-01 3.240000e-02 2.970392e-02 3.517411e-01 4.333333e-02\n",
       "[1261] 2.251819e-01 3.081273e-01 2.317411e-02 6.174488e-02 3.905560e-01\n",
       "[1266] 4.942297e-02 1.025293e-01 3.856998e-01 9.027619e-02 4.221581e-01\n",
       "[1271] 5.184286e-02 4.480000e-02 4.273186e-01 6.266667e-02 3.001872e-01\n",
       "[1276] 5.904234e-01 3.374066e-01 2.518158e-01 1.842424e-02 4.088820e-02\n",
       "[1281] 6.821192e-02 4.465534e-01 1.819317e-02 4.213333e-02 7.148426e-02\n",
       "[1286] 5.666667e-03 3.727607e-01 7.048745e-02 5.467249e-01 1.549721e-01\n",
       "[1291] 2.151563e-01 7.314861e-01 3.031747e-01 5.569124e-01 2.880853e-02\n",
       "[1296] 6.876265e-01 6.310887e-01 2.912825e-01 4.097139e-02 2.262156e-01\n",
       "[1301] 3.869860e-01 9.466667e-03 2.707386e-01 4.805569e-01 2.646629e-01\n",
       "[1306] 5.447538e-01 5.120952e-02 3.686598e-02 5.898437e-01 6.333333e-03\n",
       "[1311] 1.494128e-01 9.624900e-02 1.755905e-01 2.045954e-02 3.309354e-01\n",
       "[1316] 3.683796e-01 2.090909e-03 1.961897e-01 3.774792e-01 6.741502e-01\n",
       "[1321] 5.885135e-01 1.763907e-01 2.691723e-01 2.136553e-01 1.860952e-02\n",
       "[1326] 5.455256e-01 1.586706e-01 4.666667e-03 2.372507e-01 6.292308e-02\n",
       "[1331] 2.416768e-02 2.307200e-01 4.867111e-01 2.231299e-01 1.514660e-01\n",
       "[1336] 6.698743e-01 4.185717e-01 1.900920e-02 9.433304e-03 1.595184e-01\n",
       "[1341] 1.129429e-01 2.362108e-01 1.057823e-02 1.930469e-01 3.633301e-01\n",
       "[1346] 2.337971e-01 8.000000e-04 3.846146e-01 4.073333e-02 1.677138e-01\n",
       "[1351] 3.357656e-01 3.412478e-02 5.877436e-02 2.965602e-01 7.573771e-02\n",
       "[1356] 6.611286e-01 4.863352e-01 2.741259e-01 3.987570e-01 3.129011e-01\n",
       "[1361] 1.678788e-02 4.921223e-01 4.177126e-01 3.688768e-01 3.992850e-01\n",
       "[1366] 3.586695e-01 1.280000e-02 3.175308e-01 1.986903e-01 9.697157e-02\n",
       "[1371] 3.148741e-01 2.102156e-01 5.473149e-01 1.677303e-01 4.315559e-02\n",
       "[1376] 4.652381e-02 3.024298e-01 1.022424e-02 7.833406e-01 1.878469e-01\n",
       "[1381] 3.457165e-01 2.595851e-01 2.139973e-01 5.078691e-01 1.754531e-01\n",
       "[1386] 4.140770e-01 3.163296e-01 3.841295e-01 3.305325e-01 2.564838e-01\n",
       "[1391] 1.160382e-01 3.585136e-02 2.489742e-02 8.486667e-02 2.075701e-01\n",
       "[1396] 2.685013e-01 6.320952e-02 2.578166e-01 5.024591e-02 8.319048e-02\n",
       "[1401] 3.280329e-02 2.423663e-01 4.545275e-01 2.893810e-02 4.273831e-01\n",
       "[1406] 7.851562e-01 6.520000e-02 4.475234e-01 1.896411e-01 4.133810e-02\n",
       "[1411] 1.461690e-01 5.116083e-01 1.055758e-02 6.841726e-01 1.421188e-01\n",
       "[1416] 5.953705e-02 6.565406e-01 6.689903e-01 1.081874e-01 1.193581e-02\n",
       "[1421] 3.927434e-01 2.231102e-01 5.432781e-01 2.779357e-01 3.894678e-01\n",
       "[1426] 3.910902e-01 3.686496e-01 2.312612e-01 3.157823e-02 6.619379e-01\n",
       "[1431] 3.286667e-02 2.915284e-01 3.719264e-02 6.322351e-01 6.295918e-02\n",
       "[1436] 2.094772e-01 2.015949e-01 1.613333e-02 3.182551e-01 5.175690e-01\n",
       "[1441] 7.424212e-02 2.097823e-02 6.219842e-01 1.343489e-01 2.118259e-01\n",
       "[1446] 5.148454e-01 4.344286e-02 2.219479e-01 3.725806e-01 6.504103e-02\n",
       "[1451] 5.170000e-02 2.316230e-01 3.247659e-01 1.509277e-01 5.962474e-01\n",
       "[1456] 3.689370e-01 3.050870e-01 4.262338e-03 2.641103e-01 4.242426e-01\n",
       "[1461] 7.844898e-03 4.758919e-01 3.826563e-01 6.628028e-01 1.504821e-01\n",
       "[1466] 6.823773e-01 9.245641e-02 4.599567e-02 1.080000e-02 7.020875e-01\n",
       "[1471] 3.793921e-01 1.000000e-03 5.593004e-02 2.109091e-02 7.309048e-02\n",
       "[1476] 1.206443e-01 3.825602e-01 4.288141e-01 1.468670e-01 8.276616e-01\n",
       "[1481] 6.585079e-02 1.862602e-01 7.481451e-01 7.666667e-03 1.441474e-01\n",
       "[1486] 3.678167e-01 2.495030e-01 2.705519e-01 2.325428e-01 4.949690e-01\n",
       "[1491] 7.302021e-01 5.884037e-01 3.718651e-01 2.080053e-01 1.956372e-01\n",
       "[1496] 1.603617e-01 1.937143e-02 1.586667e-02 2.805385e-02 7.293768e-01\n",
       "[1501] 2.440412e-01 1.187908e-01 1.065717e-01 4.339919e-01 6.088571e-02\n",
       "[1506] 6.913333e-02 6.500019e-01 6.516391e-01 4.392072e-01 3.468911e-01\n",
       "[1511] 2.940239e-01 6.598155e-03 9.826824e-02 7.399048e-02 7.615447e-01\n",
       "[1516] 6.490909e-03 1.070513e-01 3.499516e-01 4.339592e-01 2.749491e-02\n",
       "[1521] 1.413333e-02 2.356753e-01 4.743367e-01 7.091886e-01 3.277637e-01\n",
       "[1526] 3.887143e-02 2.240655e-01 3.468268e-01 7.577224e-01 3.006667e-02\n",
       "[1531] 1.993705e-01 6.005819e-01 3.681900e-01 2.011540e-01 6.687785e-01\n",
       "[1536] 2.040000e-02 1.541365e-01 6.722684e-01 1.574503e-01 2.352941e-04\n",
       "[1541] 3.020836e-01 1.405259e-01 7.144818e-02 6.087113e-01 1.123143e-01\n",
       "[1546] 3.330806e-01 9.733333e-03 2.636638e-01 6.000000e-03 4.025982e-02\n",
       "[1551] 6.706570e-02 1.064156e-01 4.681667e-02 3.956190e-02 9.196905e-02\n",
       "[1556] 6.338974e-02 4.455669e-02 1.890476e-02 1.157691e-01 6.180476e-02\n",
       "[1561] 2.036863e-02 1.424242e-03 3.834285e-01 8.528566e-01 2.600038e-01\n",
       "[1566] 6.632076e-02 1.975758e-02 4.990764e-01 2.272364e-01 1.280690e-01\n",
       "[1571] 6.045729e-01 6.104474e-01 5.472857e-02 5.387923e-01 7.794908e-02\n",
       "[1576] 1.946667e-02 6.875249e-01 2.489559e-01 4.259132e-01 2.618837e-01\n",
       "[1581] 3.016145e-01 2.271020e-01 2.280000e-02 4.586964e-01 6.234286e-02\n",
       "[1586] 6.708547e-02 3.298483e-01 3.746272e-01 4.226587e-01 1.171451e-01\n",
       "[1591] 6.707500e-01 3.799731e-01 2.200009e-01 1.482534e-01 6.972241e-02\n",
       "[1596] 3.912431e-01 9.743810e-02 2.890909e-03 4.473410e-01 3.520591e-01\n",
       "[1601] 3.287636e-01 9.957576e-03 5.640476e-02 4.995173e-01 2.642671e-01\n",
       "[1606] 6.484352e-01 6.072295e-02 4.599176e-01 9.341519e-02 8.140136e-01\n",
       "[1611] 7.466667e-03 2.723704e-02 2.787880e-01 6.445521e-01 7.064492e-01\n",
       "[1616] 4.800000e-03 5.807451e-02 1.840476e-02 2.190332e-01 3.624710e-01\n",
       "[1621] 9.542857e-03 8.655546e-01 6.048890e-01 4.571042e-01 3.213474e-01\n",
       "[1626] 5.562747e-01 1.442718e-01 5.368794e-01 2.334023e-01 1.016389e-01\n",
       "[1631] 2.124479e-01 2.306546e-01 4.139689e-01 1.910245e-01 3.197778e-01\n",
       "[1636] 2.914940e-01 8.090909e-03 1.482634e-01 2.000000e-03 4.749889e-01\n",
       "[1641] 2.934996e-01 1.331048e-01 4.678877e-01 8.959748e-02 1.741793e-01\n",
       "[1646] 3.010873e-01 4.604444e-02 2.677547e-01 1.264261e-01 1.962424e-02\n",
       "[1651] 5.371958e-01 9.353333e-02 6.280476e-02 7.846667e-02 3.472484e-01\n",
       "[1656] 2.432343e-01 1.100247e-02 3.898385e-01 4.921135e-01 2.777823e-02\n",
       "[1661] 5.146384e-01 1.261111e-01 4.442063e-02 2.523500e-01 2.802706e-01\n",
       "[1666] 3.197951e-01 1.802424e-02 1.837240e-01 7.757562e-02 3.832446e-01\n",
       "[1671] 1.753429e-01 1.359317e-02 4.090204e-02 3.354374e-01 4.549538e-01\n",
       "[1676] 8.104415e-01 3.527475e-02 2.119529e-01 2.352857e-02 4.577610e-01\n",
       "[1681] 2.123352e-01 1.607381e-01 6.467302e-02 3.993222e-01 1.718391e-01\n",
       "[1686] 5.235686e-02 3.195601e-02 3.351965e-01 1.905252e-01 9.090909e-03\n",
       "[1691] 3.936190e-02 4.324055e-01 5.808110e-01 1.438697e-01 4.049435e-02\n",
       "[1696] 9.090909e-05 4.196165e-01 8.707831e-02 2.085217e-01 9.145089e-02\n",
       "[1701] 5.073171e-01 7.026429e-02 2.306260e-01 1.233162e-01 4.863712e-01\n",
       "[1706] 1.275734e-01 8.088839e-02 6.552261e-01 6.408967e-01 1.202907e-01\n",
       "[1711] 4.183269e-01 1.742876e-01 1.783377e-02 4.852002e-02 1.665706e-01\n",
       "[1716] 2.236646e-01 3.607298e-01 2.212316e-01 4.838567e-01 2.840392e-02\n",
       "[1721] 2.178120e-01 6.373987e-01 7.751209e-01 1.215460e-01 8.958254e-02\n",
       "[1726] 4.409524e-03 3.789862e-01 3.284058e-02 2.352051e-02 6.630652e-01\n",
       "[1731] 1.168000e-01 1.850464e-01 3.328123e-01 7.294943e-01 7.942504e-02\n",
       "[1736] 6.917166e-02 1.758774e-01 2.680476e-02 4.210535e-01 4.872619e-01\n",
       "[1741] 6.400054e-01 2.485367e-01 1.220242e-01 6.376863e-02 7.807589e-02\n",
       "[1746] 1.166667e-02 5.479589e-01 4.218481e-01 4.132472e-02 3.681038e-01\n",
       "[1751] 4.986667e-02 6.776317e-01 2.491115e-01 7.356825e-01 2.762698e-02\n",
       "[1756] 5.913295e-01 4.405510e-01 1.546496e-01 4.800476e-02 2.351157e-01\n",
       "[1761] 8.693333e-02 1.324698e-01 5.076761e-01 2.378567e-01 9.333333e-03\n",
       "[1766] 2.503265e-01 2.375316e-01 6.125714e-02 2.812308e-02 5.433701e-01\n",
       "[1771] 2.953496e-01 1.956027e-01 1.413333e-02 2.824368e-01 2.140381e-01\n",
       "[1776] 3.555511e-01 1.996347e-01 8.861201e-02 4.761812e-01 3.133118e-01\n",
       "[1781] 6.554821e-01 1.809091e-02 1.883016e-01 7.314253e-02 1.628799e-02\n",
       "[1786] 2.629697e-02 1.141373e-01 1.952561e-01 6.450822e-01 8.660000e-02\n",
       "[1791] 1.734143e-01 4.620853e-02 7.299489e-01 7.154473e-02 5.957659e-01\n",
       "[1796] 3.535556e-02 6.388627e-02 1.556836e-01 6.068378e-01 5.243956e-01\n",
       "[1801] 4.182840e-01 6.463438e-01 2.313333e-02 3.510368e-01 5.841505e-01\n",
       "[1806] 1.158779e-01 7.385069e-02 6.384297e-01 8.800000e-03 2.185021e-01\n",
       "[1811] 2.362347e-01 5.612943e-02 3.861733e-01 1.223571e-01 2.707874e-01\n",
       "[1816] 5.126904e-01 2.011976e-02 5.387407e-01 3.732308e-02 5.579722e-01\n",
       "[1821] 2.189045e-01 4.304860e-01 6.906848e-01 1.854225e-02 1.169136e-02\n",
       "[1826] 3.651156e-02 6.977928e-01 7.853614e-01 4.613333e-02 6.000000e-03\n",
       "[1831] 2.867619e-02 1.019679e-01 5.110506e-02 3.756752e-02 6.327983e-02\n",
       "[1836] 6.439258e-01 1.435933e-01 1.972174e-02 5.090398e-01 6.314912e-01\n",
       "[1841] 1.547393e-01 6.199489e-02 6.315100e-01 2.002857e-02 4.015778e-01\n",
       "[1846] 5.242437e-02 1.751019e-01 5.666667e-03 2.430756e-01 4.200065e-01\n",
       "[1851] 1.099667e-01 2.207556e-01 2.400000e-03 5.136414e-01 1.712354e-01\n",
       "[1856] 2.090909e-03 5.577497e-01 5.973333e-02 4.395179e-01 4.547879e-01\n",
       "[1861] 2.485524e-01 1.342505e-01 6.573886e-01 1.116111e-01 1.066400e-01\n",
       "[1866] 3.458597e-01 5.400476e-02 3.675256e-01 5.932664e-01 1.767854e-01\n",
       "[1871] 1.880000e-02 2.902316e-01 4.806840e-02 1.767525e-01 7.333333e-03\n",
       "[1876] 5.003164e-01 4.122943e-01 5.185793e-01 2.613260e-02 1.905983e-02\n",
       "[1881] 6.519912e-02 7.248913e-01 2.682900e-02 4.044822e-01 9.281164e-02\n",
       "[1886] 5.466927e-02 1.407166e-01 6.797632e-03 3.589558e-01 7.674286e-02\n",
       "[1891] 5.323186e-01 5.042544e-01 5.183710e-01 3.385665e-02 3.024646e-02\n",
       "[1896] 2.511354e-01 1.333333e-03 3.238522e-01 3.880000e-02 7.308355e-01\n",
       "[1901] 6.201889e-01 6.302442e-01 7.211426e-02 1.216603e-01 3.055403e-01\n",
       "[1906] 2.450126e-01 4.903004e-01 3.400000e-03 1.075758e-02 8.824085e-01\n",
       "[1911] 1.262325e-01 6.366951e-01 6.154898e-01 1.987619e-02 4.689669e-01\n",
       "[1916] 9.035963e-02 3.519292e-01 3.885041e-01 2.514960e-01 1.702149e-01\n",
       "[1921] 3.416237e-01 2.998199e-01 5.042900e-02 9.491396e-02 5.497301e-01\n",
       "[1926] 4.160103e-01 3.319287e-01 1.502208e-01 1.568404e-01 5.379393e-01\n",
       "[1931] 3.141798e-01 5.605416e-01 2.511851e-01 2.885529e-01 4.079841e-01\n",
       "[1936] 1.767597e-01 2.120000e-02 6.419345e-01 2.033968e-02 5.200000e-03\n",
       "[1941] 4.016474e-01 5.113561e-01 6.238095e-03 5.920043e-02 2.371414e-01\n",
       "[1946] 8.539639e-02 7.888625e-01 8.086496e-02 4.731446e-01 2.280000e-02\n",
       "[1951] 1.369048e-01 1.406556e-01 7.656349e-01 6.158000e-01 5.396874e-01\n",
       "[1956] 6.860620e-02 4.830121e-01 7.331396e-02 2.152388e-01 2.649316e-01\n",
       "[1961] 3.180301e-01 6.314775e-01 4.073540e-01 8.184689e-03 1.869461e-01\n",
       "[1966] 8.616298e-02 2.158115e-01 3.035434e-02 6.045147e-01 7.230849e-01\n",
       "[1971] 2.894156e-01 3.725457e-01 3.367132e-01 3.931832e-02 3.099678e-01\n",
       "[1976] 7.343210e-02 2.011386e-01 2.562243e-01 3.147914e-01 1.402822e-01\n",
       "[1981] 9.978268e-02 7.043448e-01 2.012245e-01 4.153333e-02 5.190196e-02\n",
       "[1986] 6.943033e-01 2.824222e-01 1.204207e-01 1.726529e-01 3.044013e-01\n",
       "[1991] 5.003187e-02 5.080168e-01 7.050070e-01 5.040139e-01 3.432242e-01\n",
       "[1996] 4.548841e-02 5.702077e-01 1.258950e-01 1.336170e-01 5.458722e-01\n",
       "[2001] 2.097531e-01 2.556428e-01 1.514286e-03 7.418974e-02 4.659135e-01\n",
       "[2006] 1.667589e-01 1.690413e-01 3.335269e-01 2.007358e-01 3.462857e-02\n",
       "[2011] 2.994288e-01 2.849712e-01 2.441326e-01 3.114598e-01 3.148357e-01\n",
       "[2016] 2.880975e-01 1.333333e-03 6.861668e-01 2.826166e-01 3.848516e-01\n",
       "[2021] 1.291505e-01 4.156814e-01 2.236214e-01 4.699689e-01 1.548070e-01\n",
       "[2026] 2.996826e-01 2.993672e-01 6.014347e-01 6.109633e-01 2.724147e-01\n",
       "[2031] 2.220000e-02 1.186476e-01 6.596771e-01 2.075911e-01 6.000000e-03\n",
       "[2036] 1.702424e-02 2.475683e-01 2.806667e-02 2.903265e-01 3.221462e-01\n",
       "[2041] 2.504490e-02 1.678974e-02 1.126667e-02 7.584762e-02 1.029752e-01\n",
       "[2046] 8.951667e-02 2.133554e-01 9.408479e-02 2.949173e-01 3.197070e-02\n",
       "[2051] 3.205608e-02 3.161078e-01 4.008181e-02 1.838071e-01 6.480824e-01\n",
       "[2056] 2.664127e-01 6.036855e-01 1.784555e-01 3.573810e-02 6.122424e-02\n",
       "[2061] 2.209990e-01 9.773333e-02 2.701961e-02 2.639170e-01 3.417823e-02\n",
       "[2066] 6.507015e-01 8.995671e-03 1.797048e-01 2.471986e-01 4.946667e-02\n",
       "[2071] 2.426141e-01 6.118908e-02 1.610421e-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(750)\n",
    "predtest4rf <- predict(RF4,submitdata,type = 'prob')  #testdata \n",
    "predtest4rf$b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Format OK\"\n",
      "$submission\n",
      "[0.0387,0.2843,0.358,0.0494,0.5894,0.1115,0.3231,0.3234,0.0732,0.0001,0.3718,0.2535,0.0327,0.7686,0.1184,0.039,0.0306,0.1531,0.2025,0.0114,0.106,0.2935,0.7996,0.0057,0.5086,0.0683,0.2879,0.4187,0.4266,0.3145,0.2928,0.657,0.0256,0.352,0.5665,0.2686,0.6733,0.6393,0.7878,0.0233,0.1715,0.1193,0.0653,0.6276,0.2593,0.6575,0.2234,0.0202,0.2163,0.1311,0.0198,0.0244,0.2046,0.232,0.7797,0.3756,0.5755,0.4442,0.1238,0.3946,0.787,0.0126,0.0958,0.1434,0.0484,0.0993,0.0092,0.0188,0.5448,0.126,0.4815,0.0384,0.0466,0.5038,0.0581,0.2032,0.5305,0.0369,0.2592,0.0973,0.7949,0.0474,0.0257,0.5495,0.2335,0.2207,0.0031,0.0868,0.7101,0.2369,0.0213,0.0691,0.0969,0.6637,0.1425,0.2921,0.0948,0.0646,0.3325,0.3333,0.5252,0.0288,0.4892,0.2427,0.387,0.3429,0.0328,0.1895,0.2105,0.0498,0.0331,0.6449,0.1418,0.5833,0.2135,0.0228,0.0101,0.3057,0.0404,0.0219,0.0237,0.0601,0.0337,0.1711,0.1815,0.0301,0.0277,0.2948,0.6298,0.0543,0.5987,0.2564,0.3069,0.2827,0.3401,0.501,0.0509,0.4697,0.1181,0.5728,0.3146,0.1722,0.3123,0.4287,0.3165,0.2189,0.5382,0.2126,0.2717,0.0656,0.0682,0.27,0.2196,0.1273,0.0117,0.1297,0.0369,0.4253,0.2506,0.8118,0.1022,0.1183,0.0283,0.0904,0.0227,0.4418,0.064,0.3711,0.0341,0.1371,0.6212,0.2408,0.3589,0.0434,0.2994,0.0096,0.246,0.2531,0.579,0.7758,0.7333,0.6414,0.4376,0.0045,0.6935,0.024,0.001,0.1349,0.2266,0.16,0.0002,0.359,0.4779,0.2141,0.0076,0.0003,0.0473,0.0152,0.0358,0.6372,0.4827,0.1113,0.1523,0.4972,0.2715,0.4552,0.0449,0.7639,0.1051,0.6068,0.0266,0.0008,0.0943,0.4387,0.2435,0.5031,0.1336,0.0412,0.0102,0.058,0.0811,0.0049,0.3308,0.4644,0.0701,0.0974,0.1638,0.5233,0.1995,0.0077,0.677,0.0696,0.4133,0.0611,0.017,0.0607,0.4349,0.2164,0.3559,0.5276,0.0531,0.0184,0.0113,0.0814,0.0761,0.0507,0.4007,0.515,0.3604,0.0082,0.0383,0.6039,0.6482,0.3667,0.0547,0.49,0.742,0.201,0.1443,0.1416,0.4651,0.1903,0.0466,0.3176,0.4786,0.2682,0.0879,0.0964,0.0601,0.0906,0.2672,0.1886,0.2396,0.1995,0.0482,0.2808,0.3072,0.0674,0.3502,0.1655,0.2228,0.0573,0.1901,0.1021,0.5271,0.2165,0.0828,0.4289,0.5772,0.2916,0.4058,0.0627,0.2954,0.3418,0.3482,0.8038,0.5387,0.1557,0.0284,0.0654,0.681,0.0251,0.0407,0.0007,0.0378,0.0379,0.1197,0.2534,0.0625,0.0044,0.1197,0.3487,0.4158,0.0486,0.039,0.5961,0.2657,0.1729,0.4579,0.3928,0.0157,0.1352,0.0194,0.2205,0.1874,0.4471,0.3877,0.454,0.0379,0.3982,0.3905,0.1521,0.5876,0.4283,0.7446,0.1908,0.4553,0.6405,0.0157,0.145,0.0596,0.1507,0.0751,0.3103,0.3042,0.0321,0.0785,0.3019,0.0478,0.0728,0.0867,0.0229,0.1506,0.6287,0.1002,0.2776,0.6769,0.1687,0.0787,0.1486,0.0766,0.0373,0.1104,0.0933,0.3894,0.2738,0.5526,0.0197,0.0513,0.0923,0.6122,0.0642,0.0097,0.0712,0.1052,0.6295,0.172,0.0893,0.0061,0.2824,0.1905,0.3728,0.0425,0.537,0.1579,0.0797,0.0053,0.3605,0.0704,0.2702,0.012,0.1112,0.6158,0.1992,0.0918,0.3852,0.0917,0.0816,0.0374,0.0331,0.1053,0.7178,0.151,0.7096,0.0595,0.3252,0.1003,0.328,0.688,0.0663,0.2272,0.0328,0.3939,0.0249,0.4057,0.4388,0.2544,0.699,0.3941,0.4959,0.3245,0.5107,0.0101,0.0343,0.1546,0.4111,0.122,0.3082,0.1649,0.3607,0.4835,0.0729,0.3953,0.2633,0.4223,0.2298,0.0108,0.1765,0.4743,0.2767,0.0948,0.1054,0.1125,0.8062,0.1364,0.4358,0.009,0.0115,0.4826,0.0602,0.4313,0.2768,0.0367,0.6232,0.2096,0.0078,0.29,0.3152,0.6528,0.575,0.1074,0.0998,0.2131,0.7438,0.0765,0.3183,0.1407,0.0353,0.0571,0.5387,0.3637,0.6194,0.1112,0.052,0.5165,0.0251,0.2944,0.6726,0.2401,0.0231,0.0054,0.1399,0.6644,0.098,0.0245,0.1042,0.5506,0.013,0.3382,0.0842,0.5417,0.3499,0.531,0.6258,0.0474,0.0038,0.6765,0.0099,0.6772,0.1069,0.1459,0.1449,0.0082,0.0179,0.1216,0.017,0.3571,0.3385,0.1139,0.0374,0.3655,0.2948,0.0244,0.0358,0.06,0.2336,0.0561,0.1025,0.5328,0.4296,0.0068,0.0343,0.4527,0.0253,0.2024,0.2896,0.0233,0.6267,0.1981,0.0092,0.2459,0.0289,0.3619,0.5344,0.3104,0.005,0.8389,0.6405,0.3111,0.2045,0.1492,0.7789,0.0357,0.1519,0.1098,0.0534,0.52,0.6392,0.0181,0.5353,0.0178,0.0375,0.0936,0.7062,0.4219,0.2897,0.1037,0.0519,0.0708,0.322,0.059,0.0485,0.0538,0.2146,0.177,0.0306,0.3764,0.0774,0.5758,0.2572,0.0393,0.2082,0.2266,0.3404,0.3022,0.4199,0.4687,0.2397,0.0557,0.0598,0.0224,0.0139,0.3311,0.4664,0.2606,0.4593,0.0559,0.1413,0.2424,0.0396,0.4033,0.1899,0.0342,0.0112,0.025,0.482,0.0444,0.0192,0.0296,0.3673,0.0778,0.2893,0.5592,0.0309,0.1013,0.2936,0.7981,0.1346,0.0993,0.0941,0.2181,0.4662,0.5656,0.1555,0.0933,0.5148,0.0178,0.2894,0.1977,0.8138,0.2282,0.5366,0.694,0.4672,0.2625,0.1938,0.1532,0.0451,0.197,0.0701,0.0704,0.1161,0.5099,0.2409,0.2146,0.0052,0.3121,0.243,0.0422,0.1201,0.1276,0.0699,0.0163,0.1761,0.1144,0.0175,0.123,0.1119,0.3663,0.243,0.1376,0.0491,0.4665,0.3661,0.2629,0.1693,0.6309,0.506,0.5645,0.2525,0.308,0.0314,0.0708,0.5648,0.3147,0.1484,0.4438,0.0528,0.0014,0.5722,0.4744,0.0554,0.0001,0.5025,0.4157,0.5136,0.0237,0.3941,0.3429,0.2126,0.0404,0.763,0.1758,0.1864,0.1489,0.5258,0.2987,0.299,0.1371,0.2258,0.0501,0.0992,0.3907,0.3511,0.0279,0.3092,0.0349,0.0477,0.0385,0.2457,0.0527,0.2848,0.6276,0.2188,0.469,0.5942,0.3733,0.0265,0.2547,0.0416,0.3116,0.8418,0.016,0.0313,0.2797,0.4123,0.3793,0.4397,0.7848,0.0545,0.188,0.3443,0.5629,0.0802,0.0626,0.0359,0.3912,0.4087,0.3981,0.079,0.271,0.552,0.2127,0.0752,0.0058,0.0592,0.0318,0.4009,0.0474,0.0546,0.0023,0.1052,0.5872,0.09,0.6839,0.1325,0.4816,0.4952,0.028,0.2524,0.018,0.0356,0.1748,0.1533,0.0727,0.6835,0.1532,0.0597,0.08,0.0164,0.2631,0.4748,0.0671,0.2808,0.7042,0.1021,0.0021,0.0387,0.4253,0.3313,0.0281,0.035,0.4757,0.6768,0.5231,0.139,0.7793,0.367,0.0573,0.3143,0.1618,0.0021,0.413,0.0199,0.0459,0.3194,0.42,0.3641,0.1469,0.3015,0.0052,0.2057,0.6137,0.032,0.1504,0.0272,0.0048,0.0261,0.0725,0.3747,0.1507,0.0236,0.3082,0.2115,0.3204,0.082,0.4181,0.0607,0.0659,0.535,0.354,0.1024,0.012,0.0418,0.2798,0.3535,0.2073,0.3766,0.4153,0.0469,0.1575,0.0734,0.5952,0.4453,0.0662,0.0519,0.4808,0.0038,0.1959,0.4392,0.063,0.0118,0.5163,0.4985,0.7606,0.3654,0.1193,0.4128,0.0782,0.591,0.2741,0.6347,0.0106,0.6326,0.5702,0.5429,0.0448,0.1006,0.0688,0.3913,0.1343,0.3589,0.5695,0.0074,0.5845,0.2107,0.0313,0.0048,0.0028,0.5013,0.0104,0.0525,0.0859,0.2893,0.4246,0.0722,0.1352,0.3696,0.2757,0.0732,0.0587,0.0122,0.0091,0.231,0.2536,0.4855,0.1265,0.2523,0.4616,0.7144,0.8881,0.3806,0.3006,0.1866,0.091,0.0951,0.0446,0.7331,0.3138,0.3911,0.5379,0.545,0.1839,0.191,0.2294,0.1478,0.1755,0.3004,0.1196,0.1893,0.1395,0.5099,0.009,0.3166,0.5737,0.0829,0.1155,0.0204,0.3443,0.5021,0.0368,0.6392,0.6056,0.3553,0.0602,0.0283,0.4654,0.3838,0.6097,0.1048,0.0186,0.6604,0.4814,0.8097,0.0788,0.0174,0.314,0.209,0.3059,0.5258,0.863,0.0898,0.1073,0.6885,0.1788,0.1106,0.4415,0.0665,0.0869,0.2814,0.3199,0.4416,0.3121,0.0067,0.1827,0.088,0.0939,0.8488,0.6794,0.6951,0.2779,0.2903,0.1306,0.756,0.1542,0.3579,0.0275,0.0098,0.5151,0.0519,0.0602,0.2127,0.0082,0.1045,0.0296,0.3602,0.2168,0.0781,0.4189,0.5529,0.0629,0.3181,0.553,0.4227,0.1631,0.0651,0.8073,0.3081,0.2798,0.5948,0.7828,0.5922,0.3048,0.5139,0.5984,0.3677,0.2085,0.3878,0.3371,0.0094,0.0448,0.0331,0.0834,0.2654,0.0668,0.7392,0.3799,0.0158,0.0051,0.5403,0.0518,0.309,0.3677,0.308,0.2385,0.7777,0.5758,0.0733,0.0016,0.0385,0.2994,0.155,0.0673,0.4681,0.1145,0.6206,0.2003,0.2707,0.1745,0.5876,0.1993,0.0102,0.0397,0.0163,0.0115,0.1957,0.2012,0.3931,0.3103,0.0288,0.1094,0.5262,0.7111,0.3777,0.4257,0.0698,0.0601,0.473,0.0322,0.0809,0.7844,0.4373,0.6527,0.0579,0.0344,0.0704,0.2145,0.1005,0.68,0.251,0.4094,0.0698,0.2504,0.0121,0.057,0.3688,0.0043,0.0082,0.2942,0.0294,0.0032,0.0992,0.3517,0.2577,0.2554,0.2106,0.3428,0.0143,0.4263,0.738,0.2295,0.5932,0.5729,0.2452,0.1145,0.0746,0.242,0.0666,0.3565,0.5417,0.5376,0.0546,0.0463,0.4153,0.5249,0.0047,0.4758,0.2987,0.8552,0.1569,0.5267,0.6338,0.6582,0.0859,0.2787,0.1944,0.2445,0.4484,0.0427,0.8091,0.2322,0.036,0.1941,0.4776,0.1473,0.2149,0.3724,0.185,0.1006,0.5135,0.004,0.1622,0.2088,0.237,0.4741,0.2998,0.6042,0.1221,0.4663,0.0595,0.3171,0.2204,0.0896,0.6924,0.4372,0.2395,0.0077,0.0689,0.041,0.0517,0.8277,0.2568,0.1351,0.1475,0.0394,0.1913,0.1706,0.0641,0.3043,0.0631,0.4224,0.0111,0.4829,0.1089,0.012,0.5829,0.6692,0.1952,0.1193,0.0868,0.0808,0.374,0.043,0.367,0.1157,0.013,0.1613,0.4311,0.1073,0.5606,0.6257,0.2514,0.551,0.2485,0.3295,0.5722,0.0929,0.4432,0.2904,0.1976,0.1456,0.4959,0.5688,0.1788,0.263,0.0123,0.2045,0.2768,0.0919,0.2858,0.0861,0.3657,0.0247,0.3416,0.0704,0.2264,0.1459,0.0579,0.0203,0.1023,0.2572,0.3803,0.228,0.0496,0.0244,0.0797,0.3029,0.0896,0.1578,0.1933,0.3279,0.1883,0.2851,0.1007,0.1608,0.0168,0.2312,0.0509,0.085,0.0765,0.0361,0.42,0.047,0.0241,0.6726,0.5268,0.6339,0.1223,0.1709,0.1302,0.265,0.0093,0.0297,0.2754,0.0366,0.0839,0.478,0.4075,0.316,0.0461,0.075,0.6434,0.5039,0.0306,0.1281,0.2398,0.2465,0.808,0.7128,0.3352,0.2187,0.0021,0.0358,0.5023,0.2832,0.0553,0.422,0.0272,0.1091,0.3305,0.4326,0.796,0.3581,0.3506,0.1396,0.8193,0.0324,0.0297,0.3517,0.0433,0.2252,0.3081,0.0232,0.0617,0.3906,0.0494,0.1025,0.3857,0.0903,0.4222,0.0518,0.0448,0.4273,0.0627,0.3002,0.5904,0.3374,0.2518,0.0184,0.0409,0.0682,0.4466,0.0182,0.0421,0.0715,0.0057,0.3728,0.0705,0.5467,0.155,0.2152,0.7315,0.3032,0.5569,0.0288,0.6876,0.6311,0.2913,0.041,0.2262,0.387,0.0095,0.2707,0.4806,0.2647,0.5448,0.0512,0.0369,0.5898,0.0063,0.1494,0.0962,0.1756,0.0205,0.3309,0.3684,0.0021,0.1962,0.3775,0.6742,0.5885,0.1764,0.2692,0.2137,0.0186,0.5455,0.1587,0.0047,0.2373,0.0629,0.0242,0.2307,0.4867,0.2231,0.1515,0.6699,0.4186,0.019,0.0094,0.1595,0.1129,0.2362,0.0106,0.193,0.3633,0.2338,0.0008,0.3846,0.0407,0.1677,0.3358,0.0341,0.0588,0.2966,0.0757,0.6611,0.4863,0.2741,0.3988,0.3129,0.0168,0.4921,0.4177,0.3689,0.3993,0.3587,0.0128,0.3175,0.1987,0.097,0.3149,0.2102,0.5473,0.1677,0.0432,0.0465,0.3024,0.0102,0.7833,0.1878,0.3457,0.2596,0.214,0.5079,0.1755,0.4141,0.3163,0.3841,0.3305,0.2565,0.116,0.0359,0.0249,0.0849,0.2076,0.2685,0.0632,0.2578,0.0502,0.0832,0.0328,0.2424,0.4545,0.0289,0.4274,0.7852,0.0652,0.4475,0.1896,0.0413,0.1462,0.5116,0.0106,0.6842,0.1421,0.0595,0.6565,0.669,0.1082,0.0119,0.3927,0.2231,0.5433,0.2779,0.3895,0.3911,0.3686,0.2313,0.0316,0.6619,0.0329,0.2915,0.0372,0.6322,0.063,0.2095,0.2016,0.0161,0.3183,0.5176,0.0742,0.021,0.622,0.1343,0.2118,0.5148,0.0434,0.2219,0.3726,0.065,0.0517,0.2316,0.3248,0.1509,0.5962,0.3689,0.3051,0.0043,0.2641,0.4242,0.0078,0.4759,0.3827,0.6628,0.1505,0.6824,0.0925,0.046,0.0108,0.7021,0.3794,0.001,0.0559,0.0211,0.0731,0.1206,0.3826,0.4288,0.1469,0.8277,0.0659,0.1863,0.7481,0.0077,0.1441,0.3678,0.2495,0.2706,0.2325,0.495,0.7302,0.5884,0.3719,0.208,0.1956,0.1604,0.0194,0.0159,0.0281,0.7294,0.244,0.1188,0.1066,0.434,0.0609,0.0691,0.65,0.6516,0.4392,0.3469,0.294,0.0066,0.0983,0.074,0.7615,0.0065,0.1071,0.35,0.434,0.0275,0.0141,0.2357,0.4743,0.7092,0.3278,0.0389,0.2241,0.3468,0.7577,0.0301,0.1994,0.6006,0.3682,0.2012,0.6688,0.0204,0.1541,0.6723,0.1575,0.0002,0.3021,0.1405,0.0714,0.6087,0.1123,0.3331,0.0097,0.2637,0.006,0.0403,0.0671,0.1064,0.0468,0.0396,0.092,0.0634,0.0446,0.0189,0.1158,0.0618,0.0204,0.0014,0.3834,0.8529,0.26,0.0663,0.0198,0.4991,0.2272,0.1281,0.6046,0.6104,0.0547,0.5388,0.0779,0.0195,0.6875,0.249,0.4259,0.2619,0.3016,0.2271,0.0228,0.4587,0.0623,0.0671,0.3298,0.3746,0.4227,0.1171,0.6707,0.38,0.22,0.1483,0.0697,0.3912,0.0974,0.0029,0.4473,0.3521,0.3288,0.01,0.0564,0.4995,0.2643,0.6484,0.0607,0.4599,0.0934,0.814,0.0075,0.0272,0.2788,0.6446,0.7064,0.0048,0.0581,0.0184,0.219,0.3625,0.0095,0.8656,0.6049,0.4571,0.3213,0.5563,0.1443,0.5369,0.2334,0.1016,0.2124,0.2307,0.414,0.191,0.3198,0.2915,0.0081,0.1483,0.002,0.475,0.2935,0.1331,0.4679,0.0896,0.1742,0.3011,0.046,0.2678,0.1264,0.0196,0.5372,0.0935,0.0628,0.0785,0.3472,0.2432,0.011,0.3898,0.4921,0.0278,0.5146,0.1261,0.0444,0.2523,0.2803,0.3198,0.018,0.1837,0.0776,0.3832,0.1753,0.0136,0.0409,0.3354,0.455,0.8104,0.0353,0.212,0.0235,0.4578,0.2123,0.1607,0.0647,0.3993,0.1718,0.0524,0.032,0.3352,0.1905,0.0091,0.0394,0.4324,0.5808,0.1439,0.0405,0.0001,0.4196,0.0871,0.2085,0.0915,0.5073,0.0703,0.2306,0.1233,0.4864,0.1276,0.0809,0.6552,0.6409,0.1203,0.4183,0.1743,0.0178,0.0485,0.1666,0.2237,0.3607,0.2212,0.4839,0.0284,0.2178,0.6374,0.7751,0.1215,0.0896,0.0044,0.379,0.0328,0.0235,0.6631,0.1168,0.185,0.3328,0.7295,0.0794,0.0692,0.1759,0.0268,0.4211,0.4873,0.64,0.2485,0.122,0.0638,0.0781,0.0117,0.548,0.4218,0.0413,0.3681,0.0499,0.6776,0.2491,0.7357,0.0276,0.5913,0.4406,0.1546,0.048,0.2351,0.0869,0.1325,0.5077,0.2379,0.0093,0.2503,0.2375,0.0613,0.0281,0.5434,0.2953,0.1956,0.0141,0.2824,0.214,0.3556,0.1996,0.0886,0.4762,0.3133,0.6555,0.0181,0.1883,0.0731,0.0163,0.0263,0.1141,0.1953,0.6451,0.0866,0.1734,0.0462,0.7299,0.0715,0.5958,0.0354,0.0639,0.1557,0.6068,0.5244,0.4183,0.6463,0.0231,0.351,0.5842,0.1159,0.0739,0.6384,0.0088,0.2185,0.2362,0.0561,0.3862,0.1224,0.2708,0.5127,0.0201,0.5387,0.0373,0.558,0.2189,0.4305,0.6907,0.0185,0.0117,0.0365,0.6978,0.7854,0.0461,0.006,0.0287,0.102,0.0511,0.0376,0.0633,0.6439,0.1436,0.0197,0.509,0.6315,0.1547,0.062,0.6315,0.02,0.4016,0.0524,0.1751,0.0057,0.2431,0.42,0.11,0.2208,0.0024,0.5136,0.1712,0.0021,0.5577,0.0597,0.4395,0.4548,0.2486,0.1343,0.6574,0.1116,0.1066,0.3459,0.054,0.3675,0.5933,0.1768,0.0188,0.2902,0.0481,0.1768,0.0073,0.5003,0.4123,0.5186,0.0261,0.0191,0.0652,0.7249,0.0268,0.4045,0.0928,0.0547,0.1407,0.0068,0.359,0.0767,0.5323,0.5043,0.5184,0.0339,0.0302,0.2511,0.0013,0.3239,0.0388,0.7308,0.6202,0.6302,0.0721,0.1217,0.3055,0.245,0.4903,0.0034,0.0108,0.8824,0.1262,0.6367,0.6155,0.0199,0.469,0.0904,0.3519,0.3885,0.2515,0.1702,0.3416,0.2998,0.0504,0.0949,0.5497,0.416,0.3319,0.1502,0.1568,0.5379,0.3142,0.5605,0.2512,0.2886,0.408,0.1768,0.0212,0.6419,0.0203,0.0052,0.4016,0.5114,0.0062,0.0592,0.2371,0.0854,0.7889,0.0809,0.4731,0.0228,0.1369,0.1407,0.7656,0.6158,0.5397,0.0686,0.483,0.0733,0.2152,0.2649,0.318,0.6315,0.4074,0.0082,0.1869,0.0862,0.2158,0.0304,0.6045,0.7231,0.2894,0.3725,0.3367,0.0393,0.31,0.0734,0.2011,0.2562,0.3148,0.1403,0.0998,0.7043,0.2012,0.0415,0.0519,0.6943,0.2824,0.1204,0.1727,0.3044,0.05,0.508,0.705,0.504,0.3432,0.0455,0.5702,0.1259,0.1336,0.5459,0.2098,0.2556,0.0015,0.0742,0.4659,0.1668,0.169,0.3335,0.2007,0.0346,0.2994,0.285,0.2441,0.3115,0.3148,0.2881,0.0013,0.6862,0.2826,0.3849,0.1292,0.4157,0.2236,0.47,0.1548,0.2997,0.2994,0.6014,0.611,0.2724,0.0222,0.1186,0.6597,0.2076,0.006,0.017,0.2476,0.0281,0.2903,0.3221,0.025,0.0168,0.0113,0.0758,0.103,0.0895,0.2134,0.0941,0.2949,0.032,0.0321,0.3161,0.0401,0.1838,0.6481,0.2664,0.6037,0.1785,0.0357,0.0612,0.221,0.0977,0.027,0.2639,0.0342,0.6507,0.009,0.1797,0.2472,0.0495,0.2426,0.0612,0.161] \n",
      "\n",
      "[1] \"You did not submit.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "FALSE"
      ],
      "text/latex": [
       "FALSE"
      ],
      "text/markdown": [
       "FALSE"
      ],
      "text/plain": [
       "[1] FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions <- predtest4rf$b\n",
    "send_submission(predictions, token, url=subm_url, submit_now= submit_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. Stochastic Gradient Boosting I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1111             nan     0.0100    0.0018\n",
      "     2        1.1076             nan     0.0100    0.0018\n",
      "     3        1.1042             nan     0.0100    0.0018\n",
      "     4        1.1005             nan     0.0100    0.0018\n",
      "     5        1.0972             nan     0.0100    0.0017\n",
      "     6        1.0935             nan     0.0100    0.0017\n",
      "     7        1.0903             nan     0.0100    0.0017\n",
      "     8        1.0868             nan     0.0100    0.0016\n",
      "     9        1.0834             nan     0.0100    0.0016\n",
      "    10        1.0804             nan     0.0100    0.0016\n",
      "    20        1.0531             nan     0.0100    0.0013\n",
      "    40        1.0090             nan     0.0100    0.0010\n",
      "    60        0.9732             nan     0.0100    0.0006\n",
      "    80        0.9436             nan     0.0100    0.0004\n",
      "   100        0.9202             nan     0.0100    0.0005\n",
      "   120        0.8992             nan     0.0100    0.0005\n",
      "   140        0.8811             nan     0.0100    0.0005\n",
      "   160        0.8652             nan     0.0100    0.0004\n",
      "   180        0.8507             nan     0.0100    0.0004\n",
      "   200        0.8381             nan     0.0100    0.0003\n",
      "   220        0.8268             nan     0.0100    0.0002\n",
      "   240        0.8156             nan     0.0100    0.0003\n",
      "   250        0.8105             nan     0.0100    0.0001\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1078             nan     0.0100    0.0031\n",
      "     2        1.1013             nan     0.0100    0.0031\n",
      "     3        1.0948             nan     0.0100    0.0031\n",
      "     4        1.0886             nan     0.0100    0.0029\n",
      "     5        1.0823             nan     0.0100    0.0029\n",
      "     6        1.0763             nan     0.0100    0.0030\n",
      "     7        1.0707             nan     0.0100    0.0024\n",
      "     8        1.0648             nan     0.0100    0.0026\n",
      "     9        1.0592             nan     0.0100    0.0027\n",
      "    10        1.0537             nan     0.0100    0.0024\n",
      "    20        1.0058             nan     0.0100    0.0020\n",
      "    40        0.9329             nan     0.0100    0.0013\n",
      "    60        0.8799             nan     0.0100    0.0010\n",
      "    80        0.8383             nan     0.0100    0.0008\n",
      "   100        0.8047             nan     0.0100    0.0005\n",
      "   120        0.7780             nan     0.0100    0.0003\n",
      "   140        0.7550             nan     0.0100    0.0003\n",
      "   160        0.7354             nan     0.0100    0.0003\n",
      "   180        0.7185             nan     0.0100    0.0002\n",
      "   200        0.7039             nan     0.0100    0.0002\n",
      "   220        0.6905             nan     0.0100    0.0002\n",
      "   240        0.6796             nan     0.0100    0.0001\n",
      "   250        0.6742             nan     0.0100    0.0001\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0032\n",
      "     2        1.1003             nan     0.0100    0.0032\n",
      "     3        1.0938             nan     0.0100    0.0027\n",
      "     4        1.0877             nan     0.0100    0.0027\n",
      "     5        1.0809             nan     0.0100    0.0030\n",
      "     6        1.0738             nan     0.0100    0.0032\n",
      "     7        1.0675             nan     0.0100    0.0028\n",
      "     8        1.0612             nan     0.0100    0.0030\n",
      "     9        1.0555             nan     0.0100    0.0026\n",
      "    10        1.0495             nan     0.0100    0.0027\n",
      "    20        0.9954             nan     0.0100    0.0021\n",
      "    40        0.9120             nan     0.0100    0.0015\n",
      "    60        0.8526             nan     0.0100    0.0011\n",
      "    80        0.8045             nan     0.0100    0.0007\n",
      "   100        0.7656             nan     0.0100    0.0008\n",
      "   120        0.7350             nan     0.0100    0.0005\n",
      "   140        0.7080             nan     0.0100    0.0005\n",
      "   160        0.6863             nan     0.0100    0.0003\n",
      "   180        0.6665             nan     0.0100    0.0002\n",
      "   200        0.6487             nan     0.0100    0.0003\n",
      "   220        0.6326             nan     0.0100    0.0002\n",
      "   240        0.6191             nan     0.0100    0.0001\n",
      "   250        0.6126             nan     0.0100    0.0001\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0977             nan     0.0500    0.0091\n",
      "     2        1.0814             nan     0.0500    0.0083\n",
      "     3        1.0657             nan     0.0500    0.0075\n",
      "     4        1.0511             nan     0.0500    0.0067\n",
      "     5        1.0400             nan     0.0500    0.0057\n",
      "     6        1.0278             nan     0.0500    0.0060\n",
      "     7        1.0161             nan     0.0500    0.0055\n",
      "     8        1.0058             nan     0.0500    0.0049\n",
      "     9        0.9973             nan     0.0500    0.0036\n",
      "    10        0.9873             nan     0.0500    0.0045\n",
      "    20        0.9159             nan     0.0500    0.0022\n",
      "    40        0.8337             nan     0.0500    0.0012\n",
      "    60        0.7863             nan     0.0500    0.0005\n",
      "    80        0.7551             nan     0.0500    0.0007\n",
      "   100        0.7332             nan     0.0500    0.0001\n",
      "   120        0.7170             nan     0.0500    0.0004\n",
      "   140        0.7032             nan     0.0500    0.0002\n",
      "   160        0.6938             nan     0.0500    0.0000\n",
      "   180        0.6845             nan     0.0500   -0.0000\n",
      "   200        0.6761             nan     0.0500    0.0000\n",
      "   220        0.6693             nan     0.0500   -0.0000\n",
      "   240        0.6628             nan     0.0500   -0.0001\n",
      "   250        0.6602             nan     0.0500    0.0001\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0815             nan     0.0500    0.0159\n",
      "     2        1.0543             nan     0.0500    0.0143\n",
      "     3        1.0295             nan     0.0500    0.0126\n",
      "     4        1.0057             nan     0.0500    0.0105\n",
      "     5        0.9850             nan     0.0500    0.0097\n",
      "     6        0.9662             nan     0.0500    0.0082\n",
      "     7        0.9469             nan     0.0500    0.0084\n",
      "     8        0.9311             nan     0.0500    0.0066\n",
      "     9        0.9158             nan     0.0500    0.0064\n",
      "    10        0.9008             nan     0.0500    0.0059\n",
      "    20        0.8013             nan     0.0500    0.0029\n",
      "    40        0.7017             nan     0.0500    0.0015\n",
      "    60        0.6484             nan     0.0500    0.0005\n",
      "    80        0.6106             nan     0.0500   -0.0000\n",
      "   100        0.5819             nan     0.0500    0.0002\n",
      "   120        0.5599             nan     0.0500   -0.0004\n",
      "   140        0.5385             nan     0.0500   -0.0007\n",
      "   160        0.5197             nan     0.0500   -0.0002\n",
      "   180        0.5014             nan     0.0500   -0.0002\n",
      "   200        0.4857             nan     0.0500   -0.0000\n",
      "   220        0.4711             nan     0.0500   -0.0004\n",
      "   240        0.4541             nan     0.0500   -0.0006\n",
      "   250        0.4474             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0790             nan     0.0500    0.0170\n",
      "     2        1.0467             nan     0.0500    0.0146\n",
      "     3        1.0193             nan     0.0500    0.0122\n",
      "     4        0.9949             nan     0.0500    0.0115\n",
      "     5        0.9705             nan     0.0500    0.0119\n",
      "     6        0.9474             nan     0.0500    0.0106\n",
      "     7        0.9264             nan     0.0500    0.0081\n",
      "     8        0.9100             nan     0.0500    0.0063\n",
      "     9        0.8938             nan     0.0500    0.0067\n",
      "    10        0.8795             nan     0.0500    0.0053\n",
      "    20        0.7679             nan     0.0500    0.0025\n",
      "    40        0.6502             nan     0.0500    0.0008\n",
      "    60        0.5813             nan     0.0500   -0.0001\n",
      "    80        0.5353             nan     0.0500   -0.0003\n",
      "   100        0.4971             nan     0.0500   -0.0004\n",
      "   120        0.4639             nan     0.0500   -0.0004\n",
      "   140        0.4364             nan     0.0500   -0.0002\n",
      "   160        0.4091             nan     0.0500   -0.0005\n",
      "   180        0.3847             nan     0.0500   -0.0002\n",
      "   200        0.3650             nan     0.0500   -0.0004\n",
      "   220        0.3450             nan     0.0500   -0.0002\n",
      "   240        0.3268             nan     0.0500   -0.0002\n",
      "   250        0.3175             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0775             nan     0.1000    0.0179\n",
      "     2        1.0476             nan     0.1000    0.0140\n",
      "     3        1.0279             nan     0.1000    0.0094\n",
      "     4        1.0061             nan     0.1000    0.0114\n",
      "     5        0.9859             nan     0.1000    0.0082\n",
      "     6        0.9678             nan     0.1000    0.0092\n",
      "     7        0.9523             nan     0.1000    0.0075\n",
      "     8        0.9410             nan     0.1000    0.0055\n",
      "     9        0.9284             nan     0.1000    0.0061\n",
      "    10        0.9178             nan     0.1000    0.0039\n",
      "    20        0.8369             nan     0.1000    0.0011\n",
      "    40        0.7552             nan     0.1000    0.0008\n",
      "    60        0.7167             nan     0.1000    0.0005\n",
      "    80        0.6924             nan     0.1000   -0.0002\n",
      "   100        0.6762             nan     0.1000   -0.0004\n",
      "   120        0.6623             nan     0.1000   -0.0003\n",
      "   140        0.6536             nan     0.1000   -0.0008\n",
      "   160        0.6446             nan     0.1000   -0.0001\n",
      "   180        0.6363             nan     0.1000   -0.0003\n",
      "   200        0.6304             nan     0.1000   -0.0003\n",
      "   220        0.6244             nan     0.1000   -0.0004\n",
      "   240        0.6198             nan     0.1000   -0.0003\n",
      "   250        0.6165             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0531             nan     0.1000    0.0288\n",
      "     2        1.0067             nan     0.1000    0.0215\n",
      "     3        0.9676             nan     0.1000    0.0169\n",
      "     4        0.9355             nan     0.1000    0.0156\n",
      "     5        0.9028             nan     0.1000    0.0155\n",
      "     6        0.8762             nan     0.1000    0.0114\n",
      "     7        0.8550             nan     0.1000    0.0088\n",
      "     8        0.8311             nan     0.1000    0.0097\n",
      "     9        0.8134             nan     0.1000    0.0063\n",
      "    10        0.7984             nan     0.1000    0.0057\n",
      "    20        0.6982             nan     0.1000    0.0022\n",
      "    40        0.6126             nan     0.1000    0.0005\n",
      "    60        0.5641             nan     0.1000   -0.0003\n",
      "    80        0.5221             nan     0.1000   -0.0006\n",
      "   100        0.4894             nan     0.1000   -0.0002\n",
      "   120        0.4610             nan     0.1000   -0.0008\n",
      "   140        0.4333             nan     0.1000   -0.0005\n",
      "   160        0.4072             nan     0.1000   -0.0004\n",
      "   180        0.3857             nan     0.1000   -0.0007\n",
      "   200        0.3662             nan     0.1000   -0.0004\n",
      "   220        0.3454             nan     0.1000   -0.0001\n",
      "   240        0.3274             nan     0.1000   -0.0004\n",
      "   250        0.3188             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0435             nan     0.1000    0.0339\n",
      "     2        0.9895             nan     0.1000    0.0250\n",
      "     3        0.9481             nan     0.1000    0.0189\n",
      "     4        0.9166             nan     0.1000    0.0134\n",
      "     5        0.8850             nan     0.1000    0.0137\n",
      "     6        0.8531             nan     0.1000    0.0132\n",
      "     7        0.8267             nan     0.1000    0.0098\n",
      "     8        0.8038             nan     0.1000    0.0063\n",
      "     9        0.7828             nan     0.1000    0.0073\n",
      "    10        0.7655             nan     0.1000    0.0054\n",
      "    20        0.6494             nan     0.1000    0.0004\n",
      "    40        0.5380             nan     0.1000   -0.0003\n",
      "    60        0.4676             nan     0.1000   -0.0004\n",
      "    80        0.4113             nan     0.1000   -0.0003\n",
      "   100        0.3681             nan     0.1000   -0.0008\n",
      "   120        0.3273             nan     0.1000   -0.0003\n",
      "   140        0.2923             nan     0.1000   -0.0006\n",
      "   160        0.2643             nan     0.1000   -0.0002\n",
      "   180        0.2365             nan     0.1000   -0.0005\n",
      "   200        0.2115             nan     0.1000   -0.0003\n",
      "   220        0.1908             nan     0.1000   -0.0004\n",
      "   240        0.1737             nan     0.1000   -0.0005\n",
      "   250        0.1657             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold01.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0018\n",
      "     2        1.1073             nan     0.0100    0.0018\n",
      "     3        1.1035             nan     0.0100    0.0018\n",
      "     4        1.1002             nan     0.0100    0.0017\n",
      "     5        1.0967             nan     0.0100    0.0017\n",
      "     6        1.0935             nan     0.0100    0.0017\n",
      "     7        1.0903             nan     0.0100    0.0016\n",
      "     8        1.0871             nan     0.0100    0.0016\n",
      "     9        1.0841             nan     0.0100    0.0016\n",
      "    10        1.0809             nan     0.0100    0.0015\n",
      "    20        1.0539             nan     0.0100    0.0013\n",
      "    40        1.0096             nan     0.0100    0.0010\n",
      "    60        0.9766             nan     0.0100    0.0007\n",
      "    80        0.9481             nan     0.0100    0.0006\n",
      "   100        0.9234             nan     0.0100    0.0003\n",
      "   120        0.9027             nan     0.0100    0.0005\n",
      "   140        0.8838             nan     0.0100    0.0004\n",
      "   160        0.8676             nan     0.0100    0.0004\n",
      "   180        0.8533             nan     0.0100    0.0004\n",
      "   200        0.8408             nan     0.0100    0.0003\n",
      "   220        0.8292             nan     0.0100    0.0002\n",
      "   240        0.8185             nan     0.0100    0.0002\n",
      "   250        0.8138             nan     0.0100    0.0001\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1076             nan     0.0100    0.0032\n",
      "     2        1.1014             nan     0.0100    0.0027\n",
      "     3        1.0951             nan     0.0100    0.0028\n",
      "     4        1.0888             nan     0.0100    0.0030\n",
      "     5        1.0824             nan     0.0100    0.0030\n",
      "     6        1.0765             nan     0.0100    0.0028\n",
      "     7        1.0711             nan     0.0100    0.0025\n",
      "     8        1.0652             nan     0.0100    0.0028\n",
      "     9        1.0602             nan     0.0100    0.0023\n",
      "    10        1.0551             nan     0.0100    0.0025\n",
      "    20        1.0068             nan     0.0100    0.0022\n",
      "    40        0.9340             nan     0.0100    0.0015\n",
      "    60        0.8793             nan     0.0100    0.0011\n",
      "    80        0.8380             nan     0.0100    0.0008\n",
      "   100        0.8045             nan     0.0100    0.0007\n",
      "   120        0.7770             nan     0.0100    0.0006\n",
      "   140        0.7542             nan     0.0100    0.0003\n",
      "   160        0.7346             nan     0.0100    0.0003\n",
      "   180        0.7178             nan     0.0100    0.0002\n",
      "   200        0.7028             nan     0.0100    0.0000\n",
      "   220        0.6900             nan     0.0100    0.0002\n",
      "   240        0.6787             nan     0.0100    0.0002\n",
      "   250        0.6731             nan     0.0100    0.0001\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1069             nan     0.0100    0.0033\n",
      "     2        1.0998             nan     0.0100    0.0030\n",
      "     3        1.0929             nan     0.0100    0.0032\n",
      "     4        1.0858             nan     0.0100    0.0034\n",
      "     5        1.0791             nan     0.0100    0.0031\n",
      "     6        1.0726             nan     0.0100    0.0030\n",
      "     7        1.0658             nan     0.0100    0.0029\n",
      "     8        1.0592             nan     0.0100    0.0027\n",
      "     9        1.0527             nan     0.0100    0.0028\n",
      "    10        1.0466             nan     0.0100    0.0028\n",
      "    20        0.9937             nan     0.0100    0.0022\n",
      "    40        0.9123             nan     0.0100    0.0013\n",
      "    60        0.8504             nan     0.0100    0.0010\n",
      "    80        0.8023             nan     0.0100    0.0009\n",
      "   100        0.7656             nan     0.0100    0.0005\n",
      "   120        0.7338             nan     0.0100    0.0004\n",
      "   140        0.7085             nan     0.0100    0.0003\n",
      "   160        0.6872             nan     0.0100    0.0002\n",
      "   180        0.6673             nan     0.0100    0.0001\n",
      "   200        0.6495             nan     0.0100    0.0001\n",
      "   220        0.6332             nan     0.0100    0.0001\n",
      "   240        0.6201             nan     0.0100    0.0000\n",
      "   250        0.6137             nan     0.0100    0.0000\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0962             nan     0.0500    0.0090\n",
      "     2        1.0790             nan     0.0500    0.0081\n",
      "     3        1.0644             nan     0.0500    0.0073\n",
      "     4        1.0513             nan     0.0500    0.0066\n",
      "     5        1.0400             nan     0.0500    0.0059\n",
      "     6        1.0296             nan     0.0500    0.0053\n",
      "     7        1.0184             nan     0.0500    0.0054\n",
      "     8        1.0095             nan     0.0500    0.0042\n",
      "     9        0.9989             nan     0.0500    0.0048\n",
      "    10        0.9891             nan     0.0500    0.0043\n",
      "    20        0.9238             nan     0.0500    0.0018\n",
      "    40        0.8405             nan     0.0500    0.0017\n",
      "    60        0.7945             nan     0.0500    0.0007\n",
      "    80        0.7590             nan     0.0500    0.0004\n",
      "   100        0.7378             nan     0.0500    0.0004\n",
      "   120        0.7212             nan     0.0500   -0.0000\n",
      "   140        0.7087             nan     0.0500    0.0000\n",
      "   160        0.6974             nan     0.0500    0.0001\n",
      "   180        0.6880             nan     0.0500   -0.0002\n",
      "   200        0.6792             nan     0.0500   -0.0002\n",
      "   220        0.6722             nan     0.0500   -0.0001\n",
      "   240        0.6665             nan     0.0500    0.0000\n",
      "   250        0.6634             nan     0.0500    0.0000\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0830             nan     0.0500    0.0150\n",
      "     2        1.0529             nan     0.0500    0.0131\n",
      "     3        1.0274             nan     0.0500    0.0119\n",
      "     4        1.0050             nan     0.0500    0.0105\n",
      "     5        0.9822             nan     0.0500    0.0104\n",
      "     6        0.9646             nan     0.0500    0.0079\n",
      "     7        0.9466             nan     0.0500    0.0087\n",
      "     8        0.9308             nan     0.0500    0.0068\n",
      "     9        0.9142             nan     0.0500    0.0069\n",
      "    10        0.9007             nan     0.0500    0.0057\n",
      "    20        0.8026             nan     0.0500    0.0037\n",
      "    40        0.7062             nan     0.0500    0.0001\n",
      "    60        0.6525             nan     0.0500    0.0005\n",
      "    80        0.6174             nan     0.0500    0.0002\n",
      "   100        0.5885             nan     0.0500   -0.0002\n",
      "   120        0.5651             nan     0.0500   -0.0003\n",
      "   140        0.5450             nan     0.0500   -0.0002\n",
      "   160        0.5245             nan     0.0500   -0.0004\n",
      "   180        0.5043             nan     0.0500   -0.0001\n",
      "   200        0.4885             nan     0.0500   -0.0002\n",
      "   220        0.4748             nan     0.0500   -0.0003\n",
      "   240        0.4595             nan     0.0500   -0.0005\n",
      "   250        0.4521             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0769             nan     0.0500    0.0158\n",
      "     2        1.0462             nan     0.0500    0.0144\n",
      "     3        1.0182             nan     0.0500    0.0134\n",
      "     4        0.9940             nan     0.0500    0.0104\n",
      "     5        0.9714             nan     0.0500    0.0099\n",
      "     6        0.9511             nan     0.0500    0.0092\n",
      "     7        0.9311             nan     0.0500    0.0089\n",
      "     8        0.9122             nan     0.0500    0.0083\n",
      "     9        0.8948             nan     0.0500    0.0069\n",
      "    10        0.8806             nan     0.0500    0.0061\n",
      "    20        0.7710             nan     0.0500    0.0031\n",
      "    40        0.6522             nan     0.0500    0.0014\n",
      "    60        0.5870             nan     0.0500    0.0008\n",
      "    80        0.5396             nan     0.0500   -0.0002\n",
      "   100        0.5046             nan     0.0500   -0.0002\n",
      "   120        0.4716             nan     0.0500   -0.0002\n",
      "   140        0.4424             nan     0.0500   -0.0010\n",
      "   160        0.4160             nan     0.0500   -0.0001\n",
      "   180        0.3915             nan     0.0500   -0.0004\n",
      "   200        0.3698             nan     0.0500   -0.0004\n",
      "   220        0.3502             nan     0.0500   -0.0005\n",
      "   240        0.3332             nan     0.0500   -0.0004\n",
      "   250        0.3259             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0760             nan     0.1000    0.0167\n",
      "     2        1.0483             nan     0.1000    0.0138\n",
      "     3        1.0233             nan     0.1000    0.0110\n",
      "     4        1.0045             nan     0.1000    0.0076\n",
      "     5        0.9910             nan     0.1000    0.0067\n",
      "     6        0.9720             nan     0.1000    0.0076\n",
      "     7        0.9539             nan     0.1000    0.0083\n",
      "     8        0.9438             nan     0.1000    0.0039\n",
      "     9        0.9301             nan     0.1000    0.0068\n",
      "    10        0.9181             nan     0.1000    0.0048\n",
      "    20        0.8375             nan     0.1000    0.0039\n",
      "    40        0.7580             nan     0.1000    0.0010\n",
      "    60        0.7205             nan     0.1000    0.0001\n",
      "    80        0.6990             nan     0.1000    0.0003\n",
      "   100        0.6800             nan     0.1000    0.0002\n",
      "   120        0.6686             nan     0.1000    0.0000\n",
      "   140        0.6577             nan     0.1000    0.0000\n",
      "   160        0.6494             nan     0.1000   -0.0007\n",
      "   180        0.6410             nan     0.1000   -0.0006\n",
      "   200        0.6350             nan     0.1000   -0.0001\n",
      "   220        0.6281             nan     0.1000   -0.0002\n",
      "   240        0.6231             nan     0.1000   -0.0002\n",
      "   250        0.6209             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0579             nan     0.1000    0.0269\n",
      "     2        1.0014             nan     0.1000    0.0252\n",
      "     3        0.9580             nan     0.1000    0.0203\n",
      "     4        0.9245             nan     0.1000    0.0167\n",
      "     5        0.8944             nan     0.1000    0.0121\n",
      "     6        0.8723             nan     0.1000    0.0090\n",
      "     7        0.8512             nan     0.1000    0.0090\n",
      "     8        0.8295             nan     0.1000    0.0091\n",
      "     9        0.8128             nan     0.1000    0.0062\n",
      "    10        0.7981             nan     0.1000    0.0061\n",
      "    20        0.7047             nan     0.1000    0.0014\n",
      "    40        0.6218             nan     0.1000   -0.0004\n",
      "    60        0.5734             nan     0.1000   -0.0003\n",
      "    80        0.5328             nan     0.1000   -0.0001\n",
      "   100        0.4940             nan     0.1000    0.0001\n",
      "   120        0.4624             nan     0.1000   -0.0003\n",
      "   140        0.4312             nan     0.1000   -0.0008\n",
      "   160        0.4040             nan     0.1000   -0.0005\n",
      "   180        0.3772             nan     0.1000   -0.0004\n",
      "   200        0.3548             nan     0.1000   -0.0004\n",
      "   220        0.3360             nan     0.1000   -0.0005\n",
      "   240        0.3163             nan     0.1000   -0.0007\n",
      "   250        0.3081             nan     0.1000   -0.0009\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0425             nan     0.1000    0.0337\n",
      "     2        0.9855             nan     0.1000    0.0273\n",
      "     3        0.9418             nan     0.1000    0.0200\n",
      "     4        0.9043             nan     0.1000    0.0158\n",
      "     5        0.8725             nan     0.1000    0.0124\n",
      "     6        0.8464             nan     0.1000    0.0110\n",
      "     7        0.8249             nan     0.1000    0.0076\n",
      "     8        0.8039             nan     0.1000    0.0094\n",
      "     9        0.7854             nan     0.1000    0.0058\n",
      "    10        0.7668             nan     0.1000    0.0061\n",
      "    20        0.6534             nan     0.1000    0.0020\n",
      "    40        0.5405             nan     0.1000    0.0002\n",
      "    60        0.4709             nan     0.1000   -0.0008\n",
      "    80        0.4149             nan     0.1000   -0.0007\n",
      "   100        0.3681             nan     0.1000   -0.0007\n",
      "   120        0.3284             nan     0.1000   -0.0003\n",
      "   140        0.2955             nan     0.1000   -0.0002\n",
      "   160        0.2661             nan     0.1000   -0.0005\n",
      "   180        0.2412             nan     0.1000   -0.0005\n",
      "   200        0.2189             nan     0.1000   -0.0003\n",
      "   220        0.1984             nan     0.1000   -0.0004\n",
      "   240        0.1809             nan     0.1000   -0.0006\n",
      "   250        0.1725             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold02.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0018\n",
      "     2        1.1075             nan     0.0100    0.0017\n",
      "     3        1.1038             nan     0.0100    0.0017\n",
      "     4        1.1013             nan     0.0100    0.0009\n",
      "     5        1.0977             nan     0.0100    0.0017\n",
      "     6        1.0945             nan     0.0100    0.0016\n",
      "     7        1.0911             nan     0.0100    0.0016\n",
      "     8        1.0879             nan     0.0100    0.0016\n",
      "     9        1.0848             nan     0.0100    0.0015\n",
      "    10        1.0819             nan     0.0100    0.0015\n",
      "    20        1.0544             nan     0.0100    0.0012\n",
      "    40        1.0131             nan     0.0100    0.0007\n",
      "    60        0.9788             nan     0.0100    0.0008\n",
      "    80        0.9507             nan     0.0100    0.0006\n",
      "   100        0.9279             nan     0.0100    0.0004\n",
      "   120        0.9080             nan     0.0100    0.0004\n",
      "   140        0.8903             nan     0.0100    0.0004\n",
      "   160        0.8746             nan     0.0100    0.0004\n",
      "   180        0.8609             nan     0.0100    0.0002\n",
      "   200        0.8475             nan     0.0100    0.0004\n",
      "   220        0.8354             nan     0.0100    0.0003\n",
      "   240        0.8248             nan     0.0100    0.0002\n",
      "   250        0.8198             nan     0.0100    0.0002\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1082             nan     0.0100    0.0030\n",
      "     2        1.1016             nan     0.0100    0.0029\n",
      "     3        1.0955             nan     0.0100    0.0029\n",
      "     4        1.0894             nan     0.0100    0.0031\n",
      "     5        1.0830             nan     0.0100    0.0029\n",
      "     6        1.0774             nan     0.0100    0.0027\n",
      "     7        1.0718             nan     0.0100    0.0025\n",
      "     8        1.0663             nan     0.0100    0.0027\n",
      "     9        1.0609             nan     0.0100    0.0024\n",
      "    10        1.0551             nan     0.0100    0.0028\n",
      "    20        1.0072             nan     0.0100    0.0020\n",
      "    40        0.9350             nan     0.0100    0.0017\n",
      "    60        0.8799             nan     0.0100    0.0010\n",
      "    80        0.8386             nan     0.0100    0.0006\n",
      "   100        0.8056             nan     0.0100    0.0006\n",
      "   120        0.7792             nan     0.0100    0.0005\n",
      "   140        0.7564             nan     0.0100    0.0004\n",
      "   160        0.7375             nan     0.0100    0.0002\n",
      "   180        0.7205             nan     0.0100    0.0002\n",
      "   200        0.7059             nan     0.0100    0.0002\n",
      "   220        0.6937             nan     0.0100   -0.0000\n",
      "   240        0.6824             nan     0.0100    0.0000\n",
      "   250        0.6770             nan     0.0100    0.0001\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1070             nan     0.0100    0.0034\n",
      "     2        1.0999             nan     0.0100    0.0031\n",
      "     3        1.0928             nan     0.0100    0.0034\n",
      "     4        1.0857             nan     0.0100    0.0030\n",
      "     5        1.0787             nan     0.0100    0.0030\n",
      "     6        1.0718             nan     0.0100    0.0035\n",
      "     7        1.0656             nan     0.0100    0.0028\n",
      "     8        1.0589             nan     0.0100    0.0029\n",
      "     9        1.0526             nan     0.0100    0.0029\n",
      "    10        1.0465             nan     0.0100    0.0025\n",
      "    20        0.9944             nan     0.0100    0.0021\n",
      "    40        0.9115             nan     0.0100    0.0015\n",
      "    60        0.8516             nan     0.0100    0.0010\n",
      "    80        0.8052             nan     0.0100    0.0007\n",
      "   100        0.7666             nan     0.0100    0.0007\n",
      "   120        0.7372             nan     0.0100    0.0004\n",
      "   140        0.7107             nan     0.0100    0.0005\n",
      "   160        0.6887             nan     0.0100    0.0002\n",
      "   180        0.6687             nan     0.0100    0.0001\n",
      "   200        0.6519             nan     0.0100   -0.0000\n",
      "   220        0.6366             nan     0.0100    0.0001\n",
      "   240        0.6232             nan     0.0100    0.0001\n",
      "   250        0.6168             nan     0.0100    0.0001\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0980             nan     0.0500    0.0087\n",
      "     2        1.0826             nan     0.0500    0.0080\n",
      "     3        1.0691             nan     0.0500    0.0072\n",
      "     4        1.0579             nan     0.0500    0.0048\n",
      "     5        1.0444             nan     0.0500    0.0065\n",
      "     6        1.0353             nan     0.0500    0.0046\n",
      "     7        1.0240             nan     0.0500    0.0058\n",
      "     8        1.0151             nan     0.0500    0.0040\n",
      "     9        1.0048             nan     0.0500    0.0052\n",
      "    10        0.9947             nan     0.0500    0.0048\n",
      "    20        0.9304             nan     0.0500    0.0023\n",
      "    40        0.8491             nan     0.0500    0.0012\n",
      "    60        0.7959             nan     0.0500    0.0007\n",
      "    80        0.7637             nan     0.0500    0.0005\n",
      "   100        0.7410             nan     0.0500    0.0005\n",
      "   120        0.7243             nan     0.0500    0.0001\n",
      "   140        0.7131             nan     0.0500    0.0003\n",
      "   160        0.7015             nan     0.0500   -0.0001\n",
      "   180        0.6925             nan     0.0500    0.0000\n",
      "   200        0.6841             nan     0.0500   -0.0003\n",
      "   220        0.6774             nan     0.0500   -0.0001\n",
      "   240        0.6714             nan     0.0500   -0.0001\n",
      "   250        0.6682             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0809             nan     0.0500    0.0159\n",
      "     2        1.0532             nan     0.0500    0.0124\n",
      "     3        1.0272             nan     0.0500    0.0127\n",
      "     4        1.0054             nan     0.0500    0.0102\n",
      "     5        0.9856             nan     0.0500    0.0083\n",
      "     6        0.9666             nan     0.0500    0.0076\n",
      "     7        0.9476             nan     0.0500    0.0084\n",
      "     8        0.9322             nan     0.0500    0.0074\n",
      "     9        0.9167             nan     0.0500    0.0077\n",
      "    10        0.9037             nan     0.0500    0.0059\n",
      "    20        0.8015             nan     0.0500    0.0038\n",
      "    40        0.7067             nan     0.0500    0.0009\n",
      "    60        0.6573             nan     0.0500    0.0001\n",
      "    80        0.6212             nan     0.0500   -0.0000\n",
      "   100        0.5934             nan     0.0500   -0.0006\n",
      "   120        0.5692             nan     0.0500   -0.0002\n",
      "   140        0.5460             nan     0.0500   -0.0005\n",
      "   160        0.5266             nan     0.0500    0.0000\n",
      "   180        0.5067             nan     0.0500   -0.0004\n",
      "   200        0.4911             nan     0.0500   -0.0006\n",
      "   220        0.4744             nan     0.0500   -0.0003\n",
      "   240        0.4608             nan     0.0500   -0.0001\n",
      "   250        0.4543             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0791             nan     0.0500    0.0172\n",
      "     2        1.0489             nan     0.0500    0.0138\n",
      "     3        1.0227             nan     0.0500    0.0123\n",
      "     4        0.9966             nan     0.0500    0.0125\n",
      "     5        0.9709             nan     0.0500    0.0125\n",
      "     6        0.9515             nan     0.0500    0.0079\n",
      "     7        0.9327             nan     0.0500    0.0076\n",
      "     8        0.9122             nan     0.0500    0.0094\n",
      "     9        0.8953             nan     0.0500    0.0078\n",
      "    10        0.8796             nan     0.0500    0.0066\n",
      "    20        0.7661             nan     0.0500    0.0024\n",
      "    40        0.6509             nan     0.0500    0.0008\n",
      "    60        0.5887             nan     0.0500    0.0003\n",
      "    80        0.5422             nan     0.0500   -0.0002\n",
      "   100        0.5059             nan     0.0500   -0.0003\n",
      "   120        0.4735             nan     0.0500   -0.0004\n",
      "   140        0.4461             nan     0.0500   -0.0005\n",
      "   160        0.4160             nan     0.0500   -0.0005\n",
      "   180        0.3935             nan     0.0500   -0.0003\n",
      "   200        0.3689             nan     0.0500   -0.0004\n",
      "   220        0.3482             nan     0.0500   -0.0003\n",
      "   240        0.3305             nan     0.0500   -0.0003\n",
      "   250        0.3210             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0790             nan     0.1000    0.0170\n",
      "     2        1.0507             nan     0.1000    0.0136\n",
      "     3        1.0288             nan     0.1000    0.0110\n",
      "     4        1.0136             nan     0.1000    0.0078\n",
      "     5        0.9930             nan     0.1000    0.0108\n",
      "     6        0.9779             nan     0.1000    0.0077\n",
      "     7        0.9614             nan     0.1000    0.0088\n",
      "     8        0.9491             nan     0.1000    0.0059\n",
      "     9        0.9333             nan     0.1000    0.0071\n",
      "    10        0.9240             nan     0.1000    0.0032\n",
      "    20        0.8421             nan     0.1000    0.0020\n",
      "    40        0.7586             nan     0.1000    0.0017\n",
      "    60        0.7223             nan     0.1000    0.0001\n",
      "    80        0.6991             nan     0.1000   -0.0000\n",
      "   100        0.6828             nan     0.1000   -0.0001\n",
      "   120        0.6706             nan     0.1000    0.0001\n",
      "   140        0.6591             nan     0.1000   -0.0003\n",
      "   160        0.6495             nan     0.1000   -0.0001\n",
      "   180        0.6426             nan     0.1000   -0.0002\n",
      "   200        0.6364             nan     0.1000   -0.0004\n",
      "   220        0.6304             nan     0.1000   -0.0001\n",
      "   240        0.6248             nan     0.1000   -0.0002\n",
      "   250        0.6226             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0509             nan     0.1000    0.0285\n",
      "     2        0.9999             nan     0.1000    0.0252\n",
      "     3        0.9617             nan     0.1000    0.0167\n",
      "     4        0.9260             nan     0.1000    0.0180\n",
      "     5        0.8960             nan     0.1000    0.0143\n",
      "     6        0.8733             nan     0.1000    0.0099\n",
      "     7        0.8543             nan     0.1000    0.0074\n",
      "     8        0.8359             nan     0.1000    0.0079\n",
      "     9        0.8195             nan     0.1000    0.0074\n",
      "    10        0.8032             nan     0.1000    0.0076\n",
      "    20        0.7071             nan     0.1000    0.0017\n",
      "    40        0.6247             nan     0.1000   -0.0005\n",
      "    60        0.5734             nan     0.1000    0.0001\n",
      "    80        0.5329             nan     0.1000   -0.0004\n",
      "   100        0.4976             nan     0.1000   -0.0001\n",
      "   120        0.4667             nan     0.1000   -0.0004\n",
      "   140        0.4391             nan     0.1000   -0.0007\n",
      "   160        0.4137             nan     0.1000   -0.0002\n",
      "   180        0.3915             nan     0.1000   -0.0007\n",
      "   200        0.3731             nan     0.1000   -0.0010\n",
      "   220        0.3542             nan     0.1000   -0.0005\n",
      "   240        0.3353             nan     0.1000   -0.0000\n",
      "   250        0.3263             nan     0.1000   -0.0010\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0403             nan     0.1000    0.0373\n",
      "     2        0.9834             nan     0.1000    0.0244\n",
      "     3        0.9430             nan     0.1000    0.0178\n",
      "     4        0.9088             nan     0.1000    0.0125\n",
      "     5        0.8739             nan     0.1000    0.0138\n",
      "     6        0.8497             nan     0.1000    0.0089\n",
      "     7        0.8252             nan     0.1000    0.0096\n",
      "     8        0.8058             nan     0.1000    0.0064\n",
      "     9        0.7853             nan     0.1000    0.0073\n",
      "    10        0.7674             nan     0.1000    0.0074\n",
      "    20        0.6604             nan     0.1000    0.0006\n",
      "    40        0.5486             nan     0.1000   -0.0007\n",
      "    60        0.4760             nan     0.1000   -0.0009\n",
      "    80        0.4230             nan     0.1000   -0.0006\n",
      "   100        0.3759             nan     0.1000   -0.0012\n",
      "   120        0.3332             nan     0.1000   -0.0005\n",
      "   140        0.3000             nan     0.1000   -0.0007\n",
      "   160        0.2695             nan     0.1000   -0.0003\n",
      "   180        0.2439             nan     0.1000   -0.0008\n",
      "   200        0.2214             nan     0.1000   -0.0007\n",
      "   220        0.1994             nan     0.1000   -0.0007\n",
      "   240        0.1823             nan     0.1000   -0.0001\n",
      "   250        0.1728             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold03.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1105             nan     0.0100    0.0019\n",
      "     2        1.1067             nan     0.0100    0.0018\n",
      "     3        1.1030             nan     0.0100    0.0018\n",
      "     4        1.0997             nan     0.0100    0.0017\n",
      "     5        1.0962             nan     0.0100    0.0017\n",
      "     6        1.0929             nan     0.0100    0.0017\n",
      "     7        1.0895             nan     0.0100    0.0016\n",
      "     8        1.0860             nan     0.0100    0.0015\n",
      "     9        1.0827             nan     0.0100    0.0016\n",
      "    10        1.0800             nan     0.0100    0.0015\n",
      "    20        1.0521             nan     0.0100    0.0013\n",
      "    40        1.0074             nan     0.0100    0.0011\n",
      "    60        0.9723             nan     0.0100    0.0006\n",
      "    80        0.9430             nan     0.0100    0.0005\n",
      "   100        0.9185             nan     0.0100    0.0004\n",
      "   120        0.8985             nan     0.0100    0.0003\n",
      "   140        0.8789             nan     0.0100    0.0003\n",
      "   160        0.8625             nan     0.0100    0.0003\n",
      "   180        0.8484             nan     0.0100    0.0003\n",
      "   200        0.8354             nan     0.0100    0.0002\n",
      "   220        0.8236             nan     0.0100    0.0002\n",
      "   240        0.8130             nan     0.0100    0.0003\n",
      "   250        0.8083             nan     0.0100    0.0002\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0033\n",
      "     2        1.1009             nan     0.0100    0.0032\n",
      "     3        1.0945             nan     0.0100    0.0029\n",
      "     4        1.0880             nan     0.0100    0.0030\n",
      "     5        1.0820             nan     0.0100    0.0029\n",
      "     6        1.0761             nan     0.0100    0.0028\n",
      "     7        1.0705             nan     0.0100    0.0025\n",
      "     8        1.0651             nan     0.0100    0.0025\n",
      "     9        1.0597             nan     0.0100    0.0024\n",
      "    10        1.0542             nan     0.0100    0.0022\n",
      "    20        1.0060             nan     0.0100    0.0022\n",
      "    40        0.9301             nan     0.0100    0.0014\n",
      "    60        0.8737             nan     0.0100    0.0012\n",
      "    80        0.8314             nan     0.0100    0.0009\n",
      "   100        0.7977             nan     0.0100    0.0006\n",
      "   120        0.7695             nan     0.0100    0.0005\n",
      "   140        0.7471             nan     0.0100    0.0003\n",
      "   160        0.7270             nan     0.0100    0.0004\n",
      "   180        0.7112             nan     0.0100    0.0002\n",
      "   200        0.6958             nan     0.0100    0.0001\n",
      "   220        0.6830             nan     0.0100   -0.0000\n",
      "   240        0.6714             nan     0.0100    0.0002\n",
      "   250        0.6663             nan     0.0100    0.0001\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1069             nan     0.0100    0.0034\n",
      "     2        1.0992             nan     0.0100    0.0037\n",
      "     3        1.0918             nan     0.0100    0.0035\n",
      "     4        1.0846             nan     0.0100    0.0031\n",
      "     5        1.0783             nan     0.0100    0.0028\n",
      "     6        1.0719             nan     0.0100    0.0032\n",
      "     7        1.0651             nan     0.0100    0.0029\n",
      "     8        1.0591             nan     0.0100    0.0027\n",
      "     9        1.0526             nan     0.0100    0.0027\n",
      "    10        1.0464             nan     0.0100    0.0026\n",
      "    20        0.9918             nan     0.0100    0.0024\n",
      "    40        0.9079             nan     0.0100    0.0013\n",
      "    60        0.8469             nan     0.0100    0.0011\n",
      "    80        0.7985             nan     0.0100    0.0008\n",
      "   100        0.7602             nan     0.0100    0.0005\n",
      "   120        0.7303             nan     0.0100    0.0003\n",
      "   140        0.7052             nan     0.0100    0.0003\n",
      "   160        0.6826             nan     0.0100    0.0003\n",
      "   180        0.6632             nan     0.0100    0.0002\n",
      "   200        0.6460             nan     0.0100    0.0003\n",
      "   220        0.6313             nan     0.0100    0.0002\n",
      "   240        0.6174             nan     0.0100    0.0001\n",
      "   250        0.6112             nan     0.0100    0.0001\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0965             nan     0.0500    0.0090\n",
      "     2        1.0797             nan     0.0500    0.0080\n",
      "     3        1.0657             nan     0.0500    0.0074\n",
      "     4        1.0543             nan     0.0500    0.0054\n",
      "     5        1.0400             nan     0.0500    0.0066\n",
      "     6        1.0279             nan     0.0500    0.0060\n",
      "     7        1.0177             nan     0.0500    0.0047\n",
      "     8        1.0058             nan     0.0500    0.0053\n",
      "     9        0.9945             nan     0.0500    0.0046\n",
      "    10        0.9852             nan     0.0500    0.0041\n",
      "    20        0.9132             nan     0.0500    0.0026\n",
      "    40        0.8338             nan     0.0500    0.0012\n",
      "    60        0.7841             nan     0.0500    0.0011\n",
      "    80        0.7520             nan     0.0500    0.0008\n",
      "   100        0.7264             nan     0.0500    0.0004\n",
      "   120        0.7102             nan     0.0500    0.0001\n",
      "   140        0.6973             nan     0.0500    0.0001\n",
      "   160        0.6883             nan     0.0500   -0.0002\n",
      "   180        0.6787             nan     0.0500   -0.0002\n",
      "   200        0.6704             nan     0.0500   -0.0001\n",
      "   220        0.6640             nan     0.0500   -0.0002\n",
      "   240        0.6577             nan     0.0500   -0.0000\n",
      "   250        0.6551             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0832             nan     0.0500    0.0166\n",
      "     2        1.0531             nan     0.0500    0.0141\n",
      "     3        1.0258             nan     0.0500    0.0112\n",
      "     4        1.0026             nan     0.0500    0.0106\n",
      "     5        0.9814             nan     0.0500    0.0103\n",
      "     6        0.9613             nan     0.0500    0.0091\n",
      "     7        0.9429             nan     0.0500    0.0082\n",
      "     8        0.9251             nan     0.0500    0.0072\n",
      "     9        0.9092             nan     0.0500    0.0070\n",
      "    10        0.8946             nan     0.0500    0.0065\n",
      "    20        0.7986             nan     0.0500    0.0030\n",
      "    40        0.6990             nan     0.0500    0.0007\n",
      "    60        0.6485             nan     0.0500    0.0003\n",
      "    80        0.6102             nan     0.0500   -0.0001\n",
      "   100        0.5812             nan     0.0500   -0.0001\n",
      "   120        0.5587             nan     0.0500    0.0001\n",
      "   140        0.5384             nan     0.0500   -0.0002\n",
      "   160        0.5171             nan     0.0500   -0.0005\n",
      "   180        0.5011             nan     0.0500   -0.0003\n",
      "   200        0.4848             nan     0.0500   -0.0003\n",
      "   220        0.4684             nan     0.0500   -0.0003\n",
      "   240        0.4521             nan     0.0500   -0.0003\n",
      "   250        0.4450             nan     0.0500   -0.0008\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0785             nan     0.0500    0.0163\n",
      "     2        1.0436             nan     0.0500    0.0163\n",
      "     3        1.0131             nan     0.0500    0.0122\n",
      "     4        0.9864             nan     0.0500    0.0121\n",
      "     5        0.9623             nan     0.0500    0.0110\n",
      "     6        0.9423             nan     0.0500    0.0086\n",
      "     7        0.9228             nan     0.0500    0.0082\n",
      "     8        0.9042             nan     0.0500    0.0077\n",
      "     9        0.8878             nan     0.0500    0.0072\n",
      "    10        0.8715             nan     0.0500    0.0068\n",
      "    20        0.7577             nan     0.0500    0.0029\n",
      "    40        0.6445             nan     0.0500    0.0002\n",
      "    60        0.5815             nan     0.0500    0.0002\n",
      "    80        0.5341             nan     0.0500   -0.0001\n",
      "   100        0.4974             nan     0.0500    0.0001\n",
      "   120        0.4661             nan     0.0500   -0.0004\n",
      "   140        0.4387             nan     0.0500   -0.0002\n",
      "   160        0.4129             nan     0.0500   -0.0003\n",
      "   180        0.3887             nan     0.0500   -0.0005\n",
      "   200        0.3675             nan     0.0500   -0.0002\n",
      "   220        0.3458             nan     0.0500   -0.0004\n",
      "   240        0.3260             nan     0.0500   -0.0001\n",
      "   250        0.3178             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0784             nan     0.1000    0.0176\n",
      "     2        1.0504             nan     0.1000    0.0142\n",
      "     3        1.0294             nan     0.1000    0.0099\n",
      "     4        1.0061             nan     0.1000    0.0115\n",
      "     5        0.9872             nan     0.1000    0.0093\n",
      "     6        0.9674             nan     0.1000    0.0091\n",
      "     7        0.9527             nan     0.1000    0.0068\n",
      "     8        0.9412             nan     0.1000    0.0050\n",
      "     9        0.9266             nan     0.1000    0.0071\n",
      "    10        0.9166             nan     0.1000    0.0044\n",
      "    20        0.8322             nan     0.1000    0.0035\n",
      "    40        0.7511             nan     0.1000    0.0010\n",
      "    60        0.7126             nan     0.1000   -0.0002\n",
      "    80        0.6894             nan     0.1000    0.0000\n",
      "   100        0.6727             nan     0.1000   -0.0001\n",
      "   120        0.6585             nan     0.1000   -0.0005\n",
      "   140        0.6490             nan     0.1000   -0.0005\n",
      "   160        0.6405             nan     0.1000   -0.0002\n",
      "   180        0.6341             nan     0.1000    0.0000\n",
      "   200        0.6278             nan     0.1000   -0.0003\n",
      "   220        0.6232             nan     0.1000   -0.0002\n",
      "   240        0.6177             nan     0.1000   -0.0003\n",
      "   250        0.6155             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0481             nan     0.1000    0.0284\n",
      "     2        1.0001             nan     0.1000    0.0224\n",
      "     3        0.9564             nan     0.1000    0.0181\n",
      "     4        0.9187             nan     0.1000    0.0166\n",
      "     5        0.8889             nan     0.1000    0.0122\n",
      "     6        0.8638             nan     0.1000    0.0095\n",
      "     7        0.8394             nan     0.1000    0.0112\n",
      "     8        0.8222             nan     0.1000    0.0065\n",
      "     9        0.8066             nan     0.1000    0.0061\n",
      "    10        0.7944             nan     0.1000    0.0046\n",
      "    20        0.6959             nan     0.1000    0.0017\n",
      "    40        0.6125             nan     0.1000    0.0006\n",
      "    60        0.5609             nan     0.1000   -0.0003\n",
      "    80        0.5218             nan     0.1000   -0.0008\n",
      "   100        0.4861             nan     0.1000   -0.0010\n",
      "   120        0.4567             nan     0.1000   -0.0005\n",
      "   140        0.4264             nan     0.1000   -0.0008\n",
      "   160        0.3994             nan     0.1000   -0.0014\n",
      "   180        0.3773             nan     0.1000   -0.0003\n",
      "   200        0.3548             nan     0.1000   -0.0006\n",
      "   220        0.3355             nan     0.1000   -0.0004\n",
      "   240        0.3199             nan     0.1000   -0.0009\n",
      "   250        0.3125             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0443             nan     0.1000    0.0321\n",
      "     2        0.9895             nan     0.1000    0.0259\n",
      "     3        0.9447             nan     0.1000    0.0218\n",
      "     4        0.9066             nan     0.1000    0.0167\n",
      "     5        0.8714             nan     0.1000    0.0132\n",
      "     6        0.8385             nan     0.1000    0.0140\n",
      "     7        0.8142             nan     0.1000    0.0098\n",
      "     8        0.7931             nan     0.1000    0.0081\n",
      "     9        0.7725             nan     0.1000    0.0082\n",
      "    10        0.7577             nan     0.1000    0.0035\n",
      "    20        0.6463             nan     0.1000    0.0015\n",
      "    40        0.5393             nan     0.1000   -0.0006\n",
      "    60        0.4681             nan     0.1000   -0.0020\n",
      "    80        0.4157             nan     0.1000   -0.0017\n",
      "   100        0.3681             nan     0.1000   -0.0009\n",
      "   120        0.3281             nan     0.1000   -0.0010\n",
      "   140        0.2954             nan     0.1000   -0.0009\n",
      "   160        0.2655             nan     0.1000   -0.0004\n",
      "   180        0.2376             nan     0.1000    0.0001\n",
      "   200        0.2161             nan     0.1000   -0.0005\n",
      "   220        0.1966             nan     0.1000   -0.0003\n",
      "   240        0.1799             nan     0.1000   -0.0007\n",
      "   250        0.1705             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold04.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1110             nan     0.0100    0.0018\n",
      "     2        1.1073             nan     0.0100    0.0017\n",
      "     3        1.1038             nan     0.0100    0.0017\n",
      "     4        1.1006             nan     0.0100    0.0016\n",
      "     5        1.0975             nan     0.0100    0.0016\n",
      "     6        1.0948             nan     0.0100    0.0013\n",
      "     7        1.0917             nan     0.0100    0.0016\n",
      "     8        1.0886             nan     0.0100    0.0015\n",
      "     9        1.0861             nan     0.0100    0.0010\n",
      "    10        1.0832             nan     0.0100    0.0015\n",
      "    20        1.0561             nan     0.0100    0.0013\n",
      "    40        1.0137             nan     0.0100    0.0008\n",
      "    60        0.9810             nan     0.0100    0.0004\n",
      "    80        0.9526             nan     0.0100    0.0006\n",
      "   100        0.9282             nan     0.0100    0.0006\n",
      "   120        0.9069             nan     0.0100    0.0004\n",
      "   140        0.8885             nan     0.0100    0.0004\n",
      "   160        0.8731             nan     0.0100    0.0004\n",
      "   180        0.8601             nan     0.0100    0.0003\n",
      "   200        0.8477             nan     0.0100    0.0003\n",
      "   220        0.8363             nan     0.0100    0.0002\n",
      "   240        0.8262             nan     0.0100    0.0002\n",
      "   250        0.8211             nan     0.0100    0.0002\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1080             nan     0.0100    0.0032\n",
      "     2        1.1019             nan     0.0100    0.0032\n",
      "     3        1.0955             nan     0.0100    0.0031\n",
      "     4        1.0893             nan     0.0100    0.0031\n",
      "     5        1.0834             nan     0.0100    0.0028\n",
      "     6        1.0777             nan     0.0100    0.0025\n",
      "     7        1.0722             nan     0.0100    0.0025\n",
      "     8        1.0666             nan     0.0100    0.0027\n",
      "     9        1.0617             nan     0.0100    0.0023\n",
      "    10        1.0567             nan     0.0100    0.0023\n",
      "    20        1.0101             nan     0.0100    0.0022\n",
      "    40        0.9361             nan     0.0100    0.0012\n",
      "    60        0.8825             nan     0.0100    0.0011\n",
      "    80        0.8408             nan     0.0100    0.0009\n",
      "   100        0.8076             nan     0.0100    0.0006\n",
      "   120        0.7824             nan     0.0100    0.0003\n",
      "   140        0.7609             nan     0.0100    0.0004\n",
      "   160        0.7415             nan     0.0100    0.0003\n",
      "   180        0.7257             nan     0.0100    0.0001\n",
      "   200        0.7118             nan     0.0100    0.0001\n",
      "   220        0.6987             nan     0.0100    0.0002\n",
      "   240        0.6880             nan     0.0100    0.0001\n",
      "   250        0.6832             nan     0.0100    0.0000\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1072             nan     0.0100    0.0035\n",
      "     2        1.1004             nan     0.0100    0.0033\n",
      "     3        1.0935             nan     0.0100    0.0030\n",
      "     4        1.0867             nan     0.0100    0.0030\n",
      "     5        1.0803             nan     0.0100    0.0028\n",
      "     6        1.0734             nan     0.0100    0.0031\n",
      "     7        1.0670             nan     0.0100    0.0027\n",
      "     8        1.0608             nan     0.0100    0.0027\n",
      "     9        1.0550             nan     0.0100    0.0026\n",
      "    10        1.0493             nan     0.0100    0.0025\n",
      "    20        0.9972             nan     0.0100    0.0019\n",
      "    40        0.9159             nan     0.0100    0.0013\n",
      "    60        0.8554             nan     0.0100    0.0008\n",
      "    80        0.8077             nan     0.0100    0.0007\n",
      "   100        0.7697             nan     0.0100    0.0007\n",
      "   120        0.7402             nan     0.0100    0.0005\n",
      "   140        0.7145             nan     0.0100    0.0004\n",
      "   160        0.6934             nan     0.0100    0.0003\n",
      "   180        0.6750             nan     0.0100    0.0001\n",
      "   200        0.6580             nan     0.0100    0.0001\n",
      "   220        0.6425             nan     0.0100    0.0001\n",
      "   240        0.6281             nan     0.0100    0.0001\n",
      "   250        0.6220             nan     0.0100    0.0000\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0965             nan     0.0500    0.0086\n",
      "     2        1.0801             nan     0.0500    0.0076\n",
      "     3        1.0658             nan     0.0500    0.0069\n",
      "     4        1.0536             nan     0.0500    0.0071\n",
      "     5        1.0413             nan     0.0500    0.0060\n",
      "     6        1.0319             nan     0.0500    0.0037\n",
      "     7        1.0206             nan     0.0500    0.0055\n",
      "     8        1.0104             nan     0.0500    0.0049\n",
      "     9        1.0004             nan     0.0500    0.0044\n",
      "    10        0.9909             nan     0.0500    0.0039\n",
      "    20        0.9245             nan     0.0500    0.0029\n",
      "    40        0.8462             nan     0.0500    0.0009\n",
      "    60        0.7982             nan     0.0500    0.0010\n",
      "    80        0.7666             nan     0.0500    0.0006\n",
      "   100        0.7445             nan     0.0500    0.0004\n",
      "   120        0.7283             nan     0.0500    0.0001\n",
      "   140        0.7174             nan     0.0500   -0.0000\n",
      "   160        0.7057             nan     0.0500    0.0001\n",
      "   180        0.6964             nan     0.0500   -0.0001\n",
      "   200        0.6880             nan     0.0500   -0.0003\n",
      "   220        0.6821             nan     0.0500   -0.0000\n",
      "   240        0.6747             nan     0.0500    0.0000\n",
      "   250        0.6723             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0812             nan     0.0500    0.0166\n",
      "     2        1.0550             nan     0.0500    0.0123\n",
      "     3        1.0296             nan     0.0500    0.0116\n",
      "     4        1.0073             nan     0.0500    0.0107\n",
      "     5        0.9860             nan     0.0500    0.0087\n",
      "     6        0.9678             nan     0.0500    0.0084\n",
      "     7        0.9491             nan     0.0500    0.0085\n",
      "     8        0.9342             nan     0.0500    0.0063\n",
      "     9        0.9194             nan     0.0500    0.0072\n",
      "    10        0.9068             nan     0.0500    0.0053\n",
      "    20        0.8073             nan     0.0500    0.0020\n",
      "    40        0.7116             nan     0.0500    0.0009\n",
      "    60        0.6591             nan     0.0500   -0.0002\n",
      "    80        0.6250             nan     0.0500    0.0000\n",
      "   100        0.5971             nan     0.0500    0.0001\n",
      "   120        0.5731             nan     0.0500   -0.0001\n",
      "   140        0.5536             nan     0.0500   -0.0000\n",
      "   160        0.5348             nan     0.0500   -0.0003\n",
      "   180        0.5163             nan     0.0500   -0.0003\n",
      "   200        0.5015             nan     0.0500   -0.0003\n",
      "   220        0.4873             nan     0.0500   -0.0003\n",
      "   240        0.4729             nan     0.0500   -0.0003\n",
      "   250        0.4667             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0821             nan     0.0500    0.0142\n",
      "     2        1.0523             nan     0.0500    0.0141\n",
      "     3        1.0228             nan     0.0500    0.0138\n",
      "     4        0.9985             nan     0.0500    0.0102\n",
      "     5        0.9735             nan     0.0500    0.0112\n",
      "     6        0.9530             nan     0.0500    0.0091\n",
      "     7        0.9320             nan     0.0500    0.0085\n",
      "     8        0.9132             nan     0.0500    0.0079\n",
      "     9        0.8968             nan     0.0500    0.0071\n",
      "    10        0.8819             nan     0.0500    0.0059\n",
      "    20        0.7694             nan     0.0500    0.0025\n",
      "    40        0.6569             nan     0.0500    0.0005\n",
      "    60        0.5912             nan     0.0500    0.0005\n",
      "    80        0.5466             nan     0.0500   -0.0000\n",
      "   100        0.5079             nan     0.0500   -0.0004\n",
      "   120        0.4751             nan     0.0500   -0.0003\n",
      "   140        0.4467             nan     0.0500   -0.0003\n",
      "   160        0.4209             nan     0.0500   -0.0006\n",
      "   180        0.3963             nan     0.0500   -0.0004\n",
      "   200        0.3763             nan     0.0500   -0.0004\n",
      "   220        0.3550             nan     0.0500   -0.0005\n",
      "   240        0.3374             nan     0.0500   -0.0003\n",
      "   250        0.3287             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0810             nan     0.1000    0.0167\n",
      "     2        1.0552             nan     0.1000    0.0135\n",
      "     3        1.0323             nan     0.1000    0.0111\n",
      "     4        1.0113             nan     0.1000    0.0100\n",
      "     5        0.9957             nan     0.1000    0.0065\n",
      "     6        0.9790             nan     0.1000    0.0078\n",
      "     7        0.9615             nan     0.1000    0.0085\n",
      "     8        0.9460             nan     0.1000    0.0069\n",
      "     9        0.9338             nan     0.1000    0.0058\n",
      "    10        0.9237             nan     0.1000    0.0049\n",
      "    20        0.8453             nan     0.1000    0.0016\n",
      "    40        0.7651             nan     0.1000    0.0007\n",
      "    60        0.7284             nan     0.1000    0.0003\n",
      "    80        0.7053             nan     0.1000   -0.0000\n",
      "   100        0.6904             nan     0.1000   -0.0001\n",
      "   120        0.6754             nan     0.1000   -0.0002\n",
      "   140        0.6613             nan     0.1000   -0.0006\n",
      "   160        0.6542             nan     0.1000   -0.0001\n",
      "   180        0.6477             nan     0.1000   -0.0004\n",
      "   200        0.6410             nan     0.1000   -0.0003\n",
      "   220        0.6364             nan     0.1000   -0.0001\n",
      "   240        0.6319             nan     0.1000   -0.0003\n",
      "   250        0.6281             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0574             nan     0.1000    0.0269\n",
      "     2        1.0121             nan     0.1000    0.0223\n",
      "     3        0.9684             nan     0.1000    0.0196\n",
      "     4        0.9359             nan     0.1000    0.0153\n",
      "     5        0.9038             nan     0.1000    0.0147\n",
      "     6        0.8773             nan     0.1000    0.0113\n",
      "     7        0.8535             nan     0.1000    0.0102\n",
      "     8        0.8331             nan     0.1000    0.0074\n",
      "     9        0.8166             nan     0.1000    0.0063\n",
      "    10        0.7992             nan     0.1000    0.0062\n",
      "    20        0.7104             nan     0.1000    0.0021\n",
      "    40        0.6257             nan     0.1000   -0.0009\n",
      "    60        0.5717             nan     0.1000   -0.0001\n",
      "    80        0.5337             nan     0.1000   -0.0001\n",
      "   100        0.5002             nan     0.1000   -0.0009\n",
      "   120        0.4702             nan     0.1000   -0.0013\n",
      "   140        0.4400             nan     0.1000   -0.0005\n",
      "   160        0.4131             nan     0.1000   -0.0009\n",
      "   180        0.3914             nan     0.1000   -0.0010\n",
      "   200        0.3715             nan     0.1000   -0.0006\n",
      "   220        0.3507             nan     0.1000   -0.0004\n",
      "   240        0.3325             nan     0.1000   -0.0003\n",
      "   250        0.3244             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0455             nan     0.1000    0.0305\n",
      "     2        0.9897             nan     0.1000    0.0239\n",
      "     3        0.9430             nan     0.1000    0.0186\n",
      "     4        0.9091             nan     0.1000    0.0148\n",
      "     5        0.8781             nan     0.1000    0.0118\n",
      "     6        0.8492             nan     0.1000    0.0128\n",
      "     7        0.8219             nan     0.1000    0.0123\n",
      "     8        0.7985             nan     0.1000    0.0089\n",
      "     9        0.7768             nan     0.1000    0.0071\n",
      "    10        0.7632             nan     0.1000    0.0029\n",
      "    20        0.6521             nan     0.1000    0.0012\n",
      "    40        0.5507             nan     0.1000   -0.0009\n",
      "    60        0.4814             nan     0.1000   -0.0015\n",
      "    80        0.4236             nan     0.1000   -0.0004\n",
      "   100        0.3798             nan     0.1000   -0.0009\n",
      "   120        0.3393             nan     0.1000   -0.0009\n",
      "   140        0.3040             nan     0.1000   -0.0008\n",
      "   160        0.2738             nan     0.1000   -0.0005\n",
      "   180        0.2485             nan     0.1000   -0.0007\n",
      "   200        0.2267             nan     0.1000   -0.0003\n",
      "   220        0.2056             nan     0.1000   -0.0002\n",
      "   240        0.1866             nan     0.1000   -0.0003\n",
      "   250        0.1788             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold05.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1104             nan     0.0100    0.0019\n",
      "     2        1.1067             nan     0.0100    0.0018\n",
      "     3        1.1029             nan     0.0100    0.0018\n",
      "     4        1.0992             nan     0.0100    0.0018\n",
      "     5        1.0958             nan     0.0100    0.0017\n",
      "     6        1.0922             nan     0.0100    0.0017\n",
      "     7        1.0891             nan     0.0100    0.0016\n",
      "     8        1.0861             nan     0.0100    0.0016\n",
      "     9        1.0831             nan     0.0100    0.0016\n",
      "    10        1.0798             nan     0.0100    0.0016\n",
      "    20        1.0525             nan     0.0100    0.0013\n",
      "    40        1.0097             nan     0.0100    0.0008\n",
      "    60        0.9757             nan     0.0100    0.0009\n",
      "    80        0.9456             nan     0.0100    0.0007\n",
      "   100        0.9218             nan     0.0100    0.0004\n",
      "   120        0.9004             nan     0.0100    0.0003\n",
      "   140        0.8837             nan     0.0100    0.0002\n",
      "   160        0.8674             nan     0.0100    0.0003\n",
      "   180        0.8537             nan     0.0100    0.0002\n",
      "   200        0.8411             nan     0.0100    0.0003\n",
      "   220        0.8297             nan     0.0100    0.0003\n",
      "   240        0.8189             nan     0.0100    0.0003\n",
      "   250        0.8141             nan     0.0100    0.0002\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1074             nan     0.0100    0.0032\n",
      "     2        1.1007             nan     0.0100    0.0029\n",
      "     3        1.0942             nan     0.0100    0.0030\n",
      "     4        1.0882             nan     0.0100    0.0027\n",
      "     5        1.0821             nan     0.0100    0.0028\n",
      "     6        1.0759             nan     0.0100    0.0028\n",
      "     7        1.0702             nan     0.0100    0.0030\n",
      "     8        1.0643             nan     0.0100    0.0025\n",
      "     9        1.0590             nan     0.0100    0.0024\n",
      "    10        1.0536             nan     0.0100    0.0024\n",
      "    20        1.0055             nan     0.0100    0.0022\n",
      "    40        0.9323             nan     0.0100    0.0015\n",
      "    60        0.8782             nan     0.0100    0.0013\n",
      "    80        0.8366             nan     0.0100    0.0008\n",
      "   100        0.8038             nan     0.0100    0.0004\n",
      "   120        0.7762             nan     0.0100    0.0005\n",
      "   140        0.7537             nan     0.0100    0.0005\n",
      "   160        0.7346             nan     0.0100    0.0002\n",
      "   180        0.7181             nan     0.0100    0.0002\n",
      "   200        0.7043             nan     0.0100    0.0002\n",
      "   220        0.6918             nan     0.0100    0.0001\n",
      "   240        0.6798             nan     0.0100    0.0001\n",
      "   250        0.6746             nan     0.0100    0.0001\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1069             nan     0.0100    0.0034\n",
      "     2        1.0996             nan     0.0100    0.0033\n",
      "     3        1.0930             nan     0.0100    0.0032\n",
      "     4        1.0863             nan     0.0100    0.0031\n",
      "     5        1.0795             nan     0.0100    0.0031\n",
      "     6        1.0730             nan     0.0100    0.0029\n",
      "     7        1.0666             nan     0.0100    0.0029\n",
      "     8        1.0603             nan     0.0100    0.0030\n",
      "     9        1.0539             nan     0.0100    0.0027\n",
      "    10        1.0480             nan     0.0100    0.0025\n",
      "    20        0.9947             nan     0.0100    0.0023\n",
      "    40        0.9110             nan     0.0100    0.0014\n",
      "    60        0.8495             nan     0.0100    0.0011\n",
      "    80        0.8025             nan     0.0100    0.0006\n",
      "   100        0.7650             nan     0.0100    0.0007\n",
      "   120        0.7348             nan     0.0100    0.0005\n",
      "   140        0.7100             nan     0.0100    0.0002\n",
      "   160        0.6879             nan     0.0100    0.0002\n",
      "   180        0.6678             nan     0.0100    0.0002\n",
      "   200        0.6519             nan     0.0100    0.0002\n",
      "   220        0.6368             nan     0.0100    0.0001\n",
      "   240        0.6232             nan     0.0100    0.0001\n",
      "   250        0.6169             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0932             nan     0.0500    0.0087\n",
      "     2        1.0768             nan     0.0500    0.0081\n",
      "     3        1.0610             nan     0.0500    0.0073\n",
      "     4        1.0483             nan     0.0500    0.0065\n",
      "     5        1.0387             nan     0.0500    0.0034\n",
      "     6        1.0264             nan     0.0500    0.0060\n",
      "     7        1.0153             nan     0.0500    0.0054\n",
      "     8        1.0045             nan     0.0500    0.0048\n",
      "     9        0.9954             nan     0.0500    0.0038\n",
      "    10        0.9861             nan     0.0500    0.0043\n",
      "    20        0.9170             nan     0.0500    0.0031\n",
      "    40        0.8388             nan     0.0500    0.0017\n",
      "    60        0.7939             nan     0.0500    0.0006\n",
      "    80        0.7611             nan     0.0500    0.0004\n",
      "   100        0.7391             nan     0.0500    0.0001\n",
      "   120        0.7215             nan     0.0500    0.0002\n",
      "   140        0.7080             nan     0.0500    0.0001\n",
      "   160        0.6977             nan     0.0500    0.0002\n",
      "   180        0.6879             nan     0.0500   -0.0001\n",
      "   200        0.6804             nan     0.0500   -0.0001\n",
      "   220        0.6726             nan     0.0500    0.0001\n",
      "   240        0.6663             nan     0.0500   -0.0003\n",
      "   250        0.6628             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0834             nan     0.0500    0.0134\n",
      "     2        1.0556             nan     0.0500    0.0141\n",
      "     3        1.0279             nan     0.0500    0.0123\n",
      "     4        1.0067             nan     0.0500    0.0103\n",
      "     5        0.9837             nan     0.0500    0.0098\n",
      "     6        0.9653             nan     0.0500    0.0091\n",
      "     7        0.9486             nan     0.0500    0.0072\n",
      "     8        0.9330             nan     0.0500    0.0070\n",
      "     9        0.9169             nan     0.0500    0.0077\n",
      "    10        0.9016             nan     0.0500    0.0066\n",
      "    20        0.8023             nan     0.0500    0.0023\n",
      "    40        0.7046             nan     0.0500    0.0006\n",
      "    60        0.6573             nan     0.0500   -0.0001\n",
      "    80        0.6233             nan     0.0500   -0.0000\n",
      "   100        0.5958             nan     0.0500   -0.0002\n",
      "   120        0.5745             nan     0.0500   -0.0003\n",
      "   140        0.5527             nan     0.0500   -0.0004\n",
      "   160        0.5329             nan     0.0500   -0.0003\n",
      "   180        0.5154             nan     0.0500   -0.0005\n",
      "   200        0.4990             nan     0.0500   -0.0001\n",
      "   220        0.4816             nan     0.0500   -0.0001\n",
      "   240        0.4678             nan     0.0500   -0.0003\n",
      "   250        0.4599             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0792             nan     0.0500    0.0153\n",
      "     2        1.0471             nan     0.0500    0.0134\n",
      "     3        1.0198             nan     0.0500    0.0133\n",
      "     4        0.9948             nan     0.0500    0.0108\n",
      "     5        0.9706             nan     0.0500    0.0112\n",
      "     6        0.9481             nan     0.0500    0.0097\n",
      "     7        0.9264             nan     0.0500    0.0094\n",
      "     8        0.9078             nan     0.0500    0.0076\n",
      "     9        0.8905             nan     0.0500    0.0075\n",
      "    10        0.8759             nan     0.0500    0.0061\n",
      "    20        0.7681             nan     0.0500    0.0024\n",
      "    40        0.6582             nan     0.0500    0.0005\n",
      "    60        0.5931             nan     0.0500   -0.0001\n",
      "    80        0.5458             nan     0.0500    0.0002\n",
      "   100        0.5089             nan     0.0500   -0.0002\n",
      "   120        0.4746             nan     0.0500   -0.0001\n",
      "   140        0.4465             nan     0.0500   -0.0003\n",
      "   160        0.4208             nan     0.0500   -0.0004\n",
      "   180        0.3957             nan     0.0500   -0.0003\n",
      "   200        0.3742             nan     0.0500   -0.0003\n",
      "   220        0.3539             nan     0.0500   -0.0001\n",
      "   240        0.3352             nan     0.0500   -0.0004\n",
      "   250        0.3270             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0773             nan     0.1000    0.0178\n",
      "     2        1.0481             nan     0.1000    0.0140\n",
      "     3        1.0229             nan     0.1000    0.0114\n",
      "     4        1.0042             nan     0.1000    0.0080\n",
      "     5        0.9849             nan     0.1000    0.0092\n",
      "     6        0.9679             nan     0.1000    0.0089\n",
      "     7        0.9556             nan     0.1000    0.0054\n",
      "     8        0.9452             nan     0.1000    0.0039\n",
      "     9        0.9307             nan     0.1000    0.0072\n",
      "    10        0.9214             nan     0.1000    0.0042\n",
      "    20        0.8362             nan     0.1000    0.0016\n",
      "    40        0.7579             nan     0.1000    0.0005\n",
      "    60        0.7202             nan     0.1000    0.0006\n",
      "    80        0.6960             nan     0.1000    0.0003\n",
      "   100        0.6812             nan     0.1000    0.0001\n",
      "   120        0.6668             nan     0.1000   -0.0001\n",
      "   140        0.6567             nan     0.1000   -0.0002\n",
      "   160        0.6478             nan     0.1000   -0.0003\n",
      "   180        0.6404             nan     0.1000   -0.0001\n",
      "   200        0.6340             nan     0.1000   -0.0004\n",
      "   220        0.6278             nan     0.1000    0.0001\n",
      "   240        0.6218             nan     0.1000   -0.0003\n",
      "   250        0.6197             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0510             nan     0.1000    0.0281\n",
      "     2        1.0029             nan     0.1000    0.0226\n",
      "     3        0.9651             nan     0.1000    0.0176\n",
      "     4        0.9338             nan     0.1000    0.0151\n",
      "     5        0.9037             nan     0.1000    0.0126\n",
      "     6        0.8790             nan     0.1000    0.0098\n",
      "     7        0.8549             nan     0.1000    0.0106\n",
      "     8        0.8350             nan     0.1000    0.0082\n",
      "     9        0.8163             nan     0.1000    0.0085\n",
      "    10        0.8012             nan     0.1000    0.0058\n",
      "    20        0.7039             nan     0.1000    0.0018\n",
      "    40        0.6206             nan     0.1000    0.0005\n",
      "    60        0.5710             nan     0.1000   -0.0011\n",
      "    80        0.5281             nan     0.1000   -0.0006\n",
      "   100        0.4918             nan     0.1000   -0.0004\n",
      "   120        0.4613             nan     0.1000   -0.0010\n",
      "   140        0.4309             nan     0.1000    0.0001\n",
      "   160        0.4066             nan     0.1000   -0.0007\n",
      "   180        0.3837             nan     0.1000   -0.0007\n",
      "   200        0.3640             nan     0.1000   -0.0005\n",
      "   220        0.3431             nan     0.1000   -0.0001\n",
      "   240        0.3220             nan     0.1000    0.0001\n",
      "   250        0.3154             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0447             nan     0.1000    0.0282\n",
      "     2        0.9868             nan     0.1000    0.0282\n",
      "     3        0.9400             nan     0.1000    0.0184\n",
      "     4        0.9006             nan     0.1000    0.0161\n",
      "     5        0.8662             nan     0.1000    0.0145\n",
      "     6        0.8385             nan     0.1000    0.0101\n",
      "     7        0.8152             nan     0.1000    0.0088\n",
      "     8        0.7950             nan     0.1000    0.0080\n",
      "     9        0.7743             nan     0.1000    0.0072\n",
      "    10        0.7603             nan     0.1000    0.0047\n",
      "    20        0.6508             nan     0.1000    0.0024\n",
      "    40        0.5471             nan     0.1000   -0.0012\n",
      "    60        0.4711             nan     0.1000   -0.0010\n",
      "    80        0.4180             nan     0.1000   -0.0021\n",
      "   100        0.3728             nan     0.1000   -0.0016\n",
      "   120        0.3349             nan     0.1000   -0.0003\n",
      "   140        0.3034             nan     0.1000   -0.0014\n",
      "   160        0.2741             nan     0.1000   -0.0002\n",
      "   180        0.2459             nan     0.1000   -0.0004\n",
      "   200        0.2228             nan     0.1000   -0.0007\n",
      "   220        0.2036             nan     0.1000   -0.0005\n",
      "   240        0.1873             nan     0.1000   -0.0002\n",
      "   250        0.1783             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold06.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1105             nan     0.0100    0.0018\n",
      "     2        1.1068             nan     0.0100    0.0018\n",
      "     3        1.1032             nan     0.0100    0.0017\n",
      "     4        1.0998             nan     0.0100    0.0017\n",
      "     5        1.0965             nan     0.0100    0.0017\n",
      "     6        1.0934             nan     0.0100    0.0016\n",
      "     7        1.0903             nan     0.0100    0.0016\n",
      "     8        1.0873             nan     0.0100    0.0016\n",
      "     9        1.0843             nan     0.0100    0.0015\n",
      "    10        1.0813             nan     0.0100    0.0015\n",
      "    20        1.0545             nan     0.0100    0.0013\n",
      "    40        1.0103             nan     0.0100    0.0008\n",
      "    60        0.9746             nan     0.0100    0.0008\n",
      "    80        0.9460             nan     0.0100    0.0006\n",
      "   100        0.9217             nan     0.0100    0.0006\n",
      "   120        0.9008             nan     0.0100    0.0005\n",
      "   140        0.8829             nan     0.0100    0.0004\n",
      "   160        0.8678             nan     0.0100    0.0004\n",
      "   180        0.8545             nan     0.0100    0.0002\n",
      "   200        0.8415             nan     0.0100    0.0002\n",
      "   220        0.8301             nan     0.0100    0.0002\n",
      "   240        0.8196             nan     0.0100    0.0002\n",
      "   250        0.8149             nan     0.0100    0.0002\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1078             nan     0.0100    0.0031\n",
      "     2        1.1012             nan     0.0100    0.0032\n",
      "     3        1.0947             nan     0.0100    0.0032\n",
      "     4        1.0882             nan     0.0100    0.0031\n",
      "     5        1.0823             nan     0.0100    0.0027\n",
      "     6        1.0769             nan     0.0100    0.0027\n",
      "     7        1.0712             nan     0.0100    0.0027\n",
      "     8        1.0656             nan     0.0100    0.0025\n",
      "     9        1.0601             nan     0.0100    0.0027\n",
      "    10        1.0548             nan     0.0100    0.0026\n",
      "    20        1.0076             nan     0.0100    0.0021\n",
      "    40        0.9341             nan     0.0100    0.0015\n",
      "    60        0.8786             nan     0.0100    0.0010\n",
      "    80        0.8372             nan     0.0100    0.0007\n",
      "   100        0.8037             nan     0.0100    0.0008\n",
      "   120        0.7774             nan     0.0100    0.0005\n",
      "   140        0.7553             nan     0.0100    0.0001\n",
      "   160        0.7367             nan     0.0100    0.0004\n",
      "   180        0.7192             nan     0.0100    0.0002\n",
      "   200        0.7047             nan     0.0100    0.0001\n",
      "   220        0.6925             nan     0.0100    0.0001\n",
      "   240        0.6817             nan     0.0100    0.0000\n",
      "   250        0.6766             nan     0.0100    0.0001\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1073             nan     0.0100    0.0030\n",
      "     2        1.1008             nan     0.0100    0.0030\n",
      "     3        1.0943             nan     0.0100    0.0030\n",
      "     4        1.0870             nan     0.0100    0.0032\n",
      "     5        1.0809             nan     0.0100    0.0028\n",
      "     6        1.0744             nan     0.0100    0.0029\n",
      "     7        1.0681             nan     0.0100    0.0031\n",
      "     8        1.0620             nan     0.0100    0.0028\n",
      "     9        1.0558             nan     0.0100    0.0027\n",
      "    10        1.0497             nan     0.0100    0.0029\n",
      "    20        0.9946             nan     0.0100    0.0021\n",
      "    40        0.9129             nan     0.0100    0.0013\n",
      "    60        0.8521             nan     0.0100    0.0012\n",
      "    80        0.8045             nan     0.0100    0.0008\n",
      "   100        0.7670             nan     0.0100    0.0005\n",
      "   120        0.7365             nan     0.0100    0.0004\n",
      "   140        0.7110             nan     0.0100    0.0004\n",
      "   160        0.6895             nan     0.0100    0.0004\n",
      "   180        0.6708             nan     0.0100    0.0002\n",
      "   200        0.6537             nan     0.0100    0.0001\n",
      "   220        0.6381             nan     0.0100    0.0001\n",
      "   240        0.6246             nan     0.0100    0.0002\n",
      "   250        0.6187             nan     0.0100    0.0000\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0967             nan     0.0500    0.0088\n",
      "     2        1.0793             nan     0.0500    0.0078\n",
      "     3        1.0644             nan     0.0500    0.0071\n",
      "     4        1.0513             nan     0.0500    0.0065\n",
      "     5        1.0398             nan     0.0500    0.0040\n",
      "     6        1.0284             nan     0.0500    0.0058\n",
      "     7        1.0174             nan     0.0500    0.0052\n",
      "     8        1.0084             nan     0.0500    0.0039\n",
      "     9        0.9994             nan     0.0500    0.0044\n",
      "    10        0.9894             nan     0.0500    0.0046\n",
      "    20        0.9203             nan     0.0500    0.0020\n",
      "    40        0.8403             nan     0.0500    0.0006\n",
      "    60        0.7909             nan     0.0500    0.0003\n",
      "    80        0.7615             nan     0.0500    0.0006\n",
      "   100        0.7400             nan     0.0500    0.0004\n",
      "   120        0.7253             nan     0.0500    0.0000\n",
      "   140        0.7121             nan     0.0500   -0.0000\n",
      "   160        0.7008             nan     0.0500   -0.0000\n",
      "   180        0.6927             nan     0.0500   -0.0000\n",
      "   200        0.6834             nan     0.0500   -0.0002\n",
      "   220        0.6759             nan     0.0500   -0.0000\n",
      "   240        0.6695             nan     0.0500   -0.0002\n",
      "   250        0.6661             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0846             nan     0.0500    0.0135\n",
      "     2        1.0552             nan     0.0500    0.0143\n",
      "     3        1.0282             nan     0.0500    0.0132\n",
      "     4        1.0056             nan     0.0500    0.0107\n",
      "     5        0.9837             nan     0.0500    0.0101\n",
      "     6        0.9662             nan     0.0500    0.0079\n",
      "     7        0.9489             nan     0.0500    0.0073\n",
      "     8        0.9336             nan     0.0500    0.0073\n",
      "     9        0.9206             nan     0.0500    0.0061\n",
      "    10        0.9077             nan     0.0500    0.0063\n",
      "    20        0.8065             nan     0.0500    0.0033\n",
      "    40        0.7068             nan     0.0500    0.0008\n",
      "    60        0.6581             nan     0.0500    0.0006\n",
      "    80        0.6234             nan     0.0500   -0.0001\n",
      "   100        0.5959             nan     0.0500   -0.0001\n",
      "   120        0.5740             nan     0.0500   -0.0003\n",
      "   140        0.5535             nan     0.0500   -0.0002\n",
      "   160        0.5329             nan     0.0500   -0.0001\n",
      "   180        0.5141             nan     0.0500   -0.0001\n",
      "   200        0.4953             nan     0.0500   -0.0004\n",
      "   220        0.4792             nan     0.0500   -0.0003\n",
      "   240        0.4633             nan     0.0500   -0.0004\n",
      "   250        0.4575             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0784             nan     0.0500    0.0160\n",
      "     2        1.0474             nan     0.0500    0.0141\n",
      "     3        1.0183             nan     0.0500    0.0125\n",
      "     4        0.9915             nan     0.0500    0.0117\n",
      "     5        0.9689             nan     0.0500    0.0098\n",
      "     6        0.9492             nan     0.0500    0.0090\n",
      "     7        0.9293             nan     0.0500    0.0089\n",
      "     8        0.9105             nan     0.0500    0.0070\n",
      "     9        0.8947             nan     0.0500    0.0071\n",
      "    10        0.8795             nan     0.0500    0.0061\n",
      "    20        0.7679             nan     0.0500    0.0029\n",
      "    40        0.6536             nan     0.0500    0.0004\n",
      "    60        0.5902             nan     0.0500   -0.0003\n",
      "    80        0.5421             nan     0.0500   -0.0003\n",
      "   100        0.5074             nan     0.0500   -0.0007\n",
      "   120        0.4756             nan     0.0500   -0.0004\n",
      "   140        0.4474             nan     0.0500   -0.0003\n",
      "   160        0.4203             nan     0.0500   -0.0001\n",
      "   180        0.3966             nan     0.0500   -0.0003\n",
      "   200        0.3721             nan     0.0500   -0.0004\n",
      "   220        0.3508             nan     0.0500   -0.0001\n",
      "   240        0.3316             nan     0.0500   -0.0003\n",
      "   250        0.3219             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0788             nan     0.1000    0.0172\n",
      "     2        1.0515             nan     0.1000    0.0139\n",
      "     3        1.0271             nan     0.1000    0.0113\n",
      "     4        1.0070             nan     0.1000    0.0087\n",
      "     5        0.9887             nan     0.1000    0.0085\n",
      "     6        0.9740             nan     0.1000    0.0073\n",
      "     7        0.9597             nan     0.1000    0.0061\n",
      "     8        0.9434             nan     0.1000    0.0085\n",
      "     9        0.9290             nan     0.1000    0.0071\n",
      "    10        0.9161             nan     0.1000    0.0058\n",
      "    20        0.8384             nan     0.1000    0.0022\n",
      "    40        0.7589             nan     0.1000    0.0012\n",
      "    60        0.7250             nan     0.1000    0.0000\n",
      "    80        0.7027             nan     0.1000   -0.0006\n",
      "   100        0.6836             nan     0.1000   -0.0002\n",
      "   120        0.6702             nan     0.1000   -0.0002\n",
      "   140        0.6620             nan     0.1000   -0.0002\n",
      "   160        0.6529             nan     0.1000   -0.0003\n",
      "   180        0.6438             nan     0.1000   -0.0003\n",
      "   200        0.6380             nan     0.1000   -0.0005\n",
      "   220        0.6330             nan     0.1000   -0.0003\n",
      "   240        0.6278             nan     0.1000   -0.0003\n",
      "   250        0.6257             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0587             nan     0.1000    0.0273\n",
      "     2        1.0031             nan     0.1000    0.0291\n",
      "     3        0.9645             nan     0.1000    0.0187\n",
      "     4        0.9322             nan     0.1000    0.0147\n",
      "     5        0.9004             nan     0.1000    0.0142\n",
      "     6        0.8751             nan     0.1000    0.0130\n",
      "     7        0.8517             nan     0.1000    0.0103\n",
      "     8        0.8317             nan     0.1000    0.0088\n",
      "     9        0.8151             nan     0.1000    0.0067\n",
      "    10        0.8001             nan     0.1000    0.0062\n",
      "    20        0.7111             nan     0.1000    0.0005\n",
      "    40        0.6205             nan     0.1000    0.0002\n",
      "    60        0.5730             nan     0.1000    0.0002\n",
      "    80        0.5313             nan     0.1000    0.0000\n",
      "   100        0.4968             nan     0.1000   -0.0008\n",
      "   120        0.4684             nan     0.1000   -0.0001\n",
      "   140        0.4396             nan     0.1000    0.0000\n",
      "   160        0.4135             nan     0.1000   -0.0006\n",
      "   180        0.3894             nan     0.1000   -0.0005\n",
      "   200        0.3705             nan     0.1000   -0.0003\n",
      "   220        0.3504             nan     0.1000   -0.0007\n",
      "   240        0.3324             nan     0.1000   -0.0000\n",
      "   250        0.3248             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0393             nan     0.1000    0.0324\n",
      "     2        0.9892             nan     0.1000    0.0204\n",
      "     3        0.9477             nan     0.1000    0.0174\n",
      "     4        0.9092             nan     0.1000    0.0143\n",
      "     5        0.8788             nan     0.1000    0.0119\n",
      "     6        0.8518             nan     0.1000    0.0091\n",
      "     7        0.8289             nan     0.1000    0.0079\n",
      "     8        0.8054             nan     0.1000    0.0073\n",
      "     9        0.7851             nan     0.1000    0.0080\n",
      "    10        0.7681             nan     0.1000    0.0061\n",
      "    20        0.6555             nan     0.1000    0.0027\n",
      "    40        0.5458             nan     0.1000   -0.0008\n",
      "    60        0.4747             nan     0.1000   -0.0000\n",
      "    80        0.4228             nan     0.1000   -0.0002\n",
      "   100        0.3764             nan     0.1000   -0.0008\n",
      "   120        0.3359             nan     0.1000   -0.0006\n",
      "   140        0.3017             nan     0.1000   -0.0008\n",
      "   160        0.2733             nan     0.1000   -0.0007\n",
      "   180        0.2464             nan     0.1000   -0.0005\n",
      "   200        0.2237             nan     0.1000   -0.0003\n",
      "   220        0.2009             nan     0.1000   -0.0001\n",
      "   240        0.1831             nan     0.1000   -0.0002\n",
      "   250        0.1748             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold07.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1104             nan     0.0100    0.0018\n",
      "     2        1.1069             nan     0.0100    0.0017\n",
      "     3        1.1037             nan     0.0100    0.0017\n",
      "     4        1.1004             nan     0.0100    0.0017\n",
      "     5        1.0970             nan     0.0100    0.0016\n",
      "     6        1.0937             nan     0.0100    0.0016\n",
      "     7        1.0913             nan     0.0100    0.0010\n",
      "     8        1.0880             nan     0.0100    0.0016\n",
      "     9        1.0850             nan     0.0100    0.0015\n",
      "    10        1.0820             nan     0.0100    0.0015\n",
      "    20        1.0553             nan     0.0100    0.0013\n",
      "    40        1.0127             nan     0.0100    0.0010\n",
      "    60        0.9791             nan     0.0100    0.0006\n",
      "    80        0.9511             nan     0.0100    0.0006\n",
      "   100        0.9280             nan     0.0100    0.0004\n",
      "   120        0.9075             nan     0.0100    0.0005\n",
      "   140        0.8893             nan     0.0100    0.0004\n",
      "   160        0.8734             nan     0.0100    0.0003\n",
      "   180        0.8595             nan     0.0100    0.0003\n",
      "   200        0.8474             nan     0.0100    0.0002\n",
      "   220        0.8355             nan     0.0100    0.0002\n",
      "   240        0.8254             nan     0.0100    0.0002\n",
      "   250        0.8205             nan     0.0100    0.0001\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1081             nan     0.0100    0.0029\n",
      "     2        1.1020             nan     0.0100    0.0029\n",
      "     3        1.0963             nan     0.0100    0.0030\n",
      "     4        1.0910             nan     0.0100    0.0028\n",
      "     5        1.0849             nan     0.0100    0.0028\n",
      "     6        1.0794             nan     0.0100    0.0027\n",
      "     7        1.0737             nan     0.0100    0.0026\n",
      "     8        1.0685             nan     0.0100    0.0024\n",
      "     9        1.0630             nan     0.0100    0.0023\n",
      "    10        1.0575             nan     0.0100    0.0024\n",
      "    20        1.0106             nan     0.0100    0.0020\n",
      "    40        0.9384             nan     0.0100    0.0015\n",
      "    60        0.8851             nan     0.0100    0.0010\n",
      "    80        0.8441             nan     0.0100    0.0008\n",
      "   100        0.8122             nan     0.0100    0.0006\n",
      "   120        0.7857             nan     0.0100    0.0004\n",
      "   140        0.7637             nan     0.0100    0.0001\n",
      "   160        0.7455             nan     0.0100    0.0003\n",
      "   180        0.7297             nan     0.0100    0.0002\n",
      "   200        0.7157             nan     0.0100    0.0001\n",
      "   220        0.7037             nan     0.0100    0.0001\n",
      "   240        0.6921             nan     0.0100    0.0001\n",
      "   250        0.6865             nan     0.0100    0.0001\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1072             nan     0.0100    0.0032\n",
      "     2        1.1001             nan     0.0100    0.0033\n",
      "     3        1.0934             nan     0.0100    0.0032\n",
      "     4        1.0871             nan     0.0100    0.0031\n",
      "     5        1.0808             nan     0.0100    0.0030\n",
      "     6        1.0745             nan     0.0100    0.0029\n",
      "     7        1.0680             nan     0.0100    0.0025\n",
      "     8        1.0616             nan     0.0100    0.0029\n",
      "     9        1.0560             nan     0.0100    0.0027\n",
      "    10        1.0500             nan     0.0100    0.0028\n",
      "    20        0.9985             nan     0.0100    0.0022\n",
      "    40        0.9190             nan     0.0100    0.0017\n",
      "    60        0.8581             nan     0.0100    0.0010\n",
      "    80        0.8113             nan     0.0100    0.0008\n",
      "   100        0.7738             nan     0.0100    0.0005\n",
      "   120        0.7431             nan     0.0100    0.0004\n",
      "   140        0.7179             nan     0.0100    0.0001\n",
      "   160        0.6964             nan     0.0100    0.0002\n",
      "   180        0.6772             nan     0.0100    0.0002\n",
      "   200        0.6604             nan     0.0100    0.0001\n",
      "   220        0.6452             nan     0.0100    0.0002\n",
      "   240        0.6320             nan     0.0100    0.0001\n",
      "   250        0.6258             nan     0.0100   -0.0001\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0978             nan     0.0500    0.0087\n",
      "     2        1.0819             nan     0.0500    0.0078\n",
      "     3        1.0677             nan     0.0500    0.0072\n",
      "     4        1.0545             nan     0.0500    0.0065\n",
      "     5        1.0425             nan     0.0500    0.0058\n",
      "     6        1.0327             nan     0.0500    0.0043\n",
      "     7        1.0235             nan     0.0500    0.0039\n",
      "     8        1.0124             nan     0.0500    0.0052\n",
      "     9        1.0037             nan     0.0500    0.0039\n",
      "    10        0.9957             nan     0.0500    0.0038\n",
      "    20        0.9258             nan     0.0500    0.0029\n",
      "    40        0.8463             nan     0.0500    0.0014\n",
      "    60        0.7970             nan     0.0500    0.0008\n",
      "    80        0.7671             nan     0.0500    0.0004\n",
      "   100        0.7460             nan     0.0500    0.0002\n",
      "   120        0.7298             nan     0.0500    0.0001\n",
      "   140        0.7176             nan     0.0500    0.0001\n",
      "   160        0.7066             nan     0.0500   -0.0001\n",
      "   180        0.6979             nan     0.0500    0.0000\n",
      "   200        0.6903             nan     0.0500   -0.0003\n",
      "   220        0.6838             nan     0.0500   -0.0000\n",
      "   240        0.6770             nan     0.0500   -0.0000\n",
      "   250        0.6746             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0838             nan     0.0500    0.0144\n",
      "     2        1.0555             nan     0.0500    0.0131\n",
      "     3        1.0301             nan     0.0500    0.0120\n",
      "     4        1.0069             nan     0.0500    0.0104\n",
      "     5        0.9869             nan     0.0500    0.0090\n",
      "     6        0.9680             nan     0.0500    0.0093\n",
      "     7        0.9529             nan     0.0500    0.0073\n",
      "     8        0.9367             nan     0.0500    0.0071\n",
      "     9        0.9245             nan     0.0500    0.0057\n",
      "    10        0.9111             nan     0.0500    0.0060\n",
      "    20        0.8128             nan     0.0500    0.0023\n",
      "    40        0.7128             nan     0.0500    0.0005\n",
      "    60        0.6650             nan     0.0500   -0.0002\n",
      "    80        0.6311             nan     0.0500    0.0002\n",
      "   100        0.6026             nan     0.0500   -0.0002\n",
      "   120        0.5800             nan     0.0500   -0.0002\n",
      "   140        0.5579             nan     0.0500   -0.0005\n",
      "   160        0.5398             nan     0.0500   -0.0006\n",
      "   180        0.5235             nan     0.0500   -0.0005\n",
      "   200        0.5043             nan     0.0500   -0.0001\n",
      "   220        0.4906             nan     0.0500   -0.0003\n",
      "   240        0.4765             nan     0.0500   -0.0005\n",
      "   250        0.4689             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0788             nan     0.0500    0.0149\n",
      "     2        1.0485             nan     0.0500    0.0137\n",
      "     3        1.0231             nan     0.0500    0.0118\n",
      "     4        0.9971             nan     0.0500    0.0108\n",
      "     5        0.9767             nan     0.0500    0.0087\n",
      "     6        0.9543             nan     0.0500    0.0101\n",
      "     7        0.9355             nan     0.0500    0.0077\n",
      "     8        0.9182             nan     0.0500    0.0080\n",
      "     9        0.9010             nan     0.0500    0.0067\n",
      "    10        0.8867             nan     0.0500    0.0053\n",
      "    20        0.7720             nan     0.0500    0.0039\n",
      "    40        0.6608             nan     0.0500    0.0003\n",
      "    60        0.6000             nan     0.0500   -0.0004\n",
      "    80        0.5530             nan     0.0500   -0.0003\n",
      "   100        0.5153             nan     0.0500   -0.0007\n",
      "   120        0.4827             nan     0.0500   -0.0002\n",
      "   140        0.4547             nan     0.0500   -0.0006\n",
      "   160        0.4281             nan     0.0500   -0.0003\n",
      "   180        0.4044             nan     0.0500   -0.0002\n",
      "   200        0.3820             nan     0.0500   -0.0006\n",
      "   220        0.3603             nan     0.0500   -0.0004\n",
      "   240        0.3420             nan     0.0500   -0.0003\n",
      "   250        0.3329             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0798             nan     0.1000    0.0170\n",
      "     2        1.0529             nan     0.1000    0.0137\n",
      "     3        1.0306             nan     0.1000    0.0113\n",
      "     4        1.0119             nan     0.1000    0.0097\n",
      "     5        0.9926             nan     0.1000    0.0090\n",
      "     6        0.9785             nan     0.1000    0.0067\n",
      "     7        0.9613             nan     0.1000    0.0080\n",
      "     8        0.9517             nan     0.1000    0.0044\n",
      "     9        0.9385             nan     0.1000    0.0060\n",
      "    10        0.9243             nan     0.1000    0.0069\n",
      "    20        0.8454             nan     0.1000    0.0039\n",
      "    40        0.7668             nan     0.1000    0.0001\n",
      "    60        0.7303             nan     0.1000    0.0002\n",
      "    80        0.7085             nan     0.1000   -0.0008\n",
      "   100        0.6921             nan     0.1000   -0.0001\n",
      "   120        0.6793             nan     0.1000    0.0001\n",
      "   140        0.6685             nan     0.1000   -0.0003\n",
      "   160        0.6602             nan     0.1000   -0.0003\n",
      "   180        0.6521             nan     0.1000   -0.0001\n",
      "   200        0.6460             nan     0.1000   -0.0002\n",
      "   220        0.6383             nan     0.1000   -0.0001\n",
      "   240        0.6338             nan     0.1000   -0.0005\n",
      "   250        0.6311             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0572             nan     0.1000    0.0285\n",
      "     2        1.0085             nan     0.1000    0.0231\n",
      "     3        0.9702             nan     0.1000    0.0159\n",
      "     4        0.9343             nan     0.1000    0.0178\n",
      "     5        0.9051             nan     0.1000    0.0132\n",
      "     6        0.8807             nan     0.1000    0.0118\n",
      "     7        0.8563             nan     0.1000    0.0102\n",
      "     8        0.8368             nan     0.1000    0.0085\n",
      "     9        0.8204             nan     0.1000    0.0063\n",
      "    10        0.8062             nan     0.1000    0.0051\n",
      "    20        0.7122             nan     0.1000    0.0016\n",
      "    40        0.6296             nan     0.1000   -0.0002\n",
      "    60        0.5828             nan     0.1000   -0.0003\n",
      "    80        0.5392             nan     0.1000   -0.0002\n",
      "   100        0.5070             nan     0.1000   -0.0009\n",
      "   120        0.4785             nan     0.1000   -0.0008\n",
      "   140        0.4495             nan     0.1000   -0.0010\n",
      "   160        0.4246             nan     0.1000   -0.0005\n",
      "   180        0.4018             nan     0.1000    0.0001\n",
      "   200        0.3801             nan     0.1000   -0.0004\n",
      "   220        0.3598             nan     0.1000   -0.0005\n",
      "   240        0.3422             nan     0.1000   -0.0006\n",
      "   250        0.3339             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0428             nan     0.1000    0.0313\n",
      "     2        0.9907             nan     0.1000    0.0222\n",
      "     3        0.9480             nan     0.1000    0.0182\n",
      "     4        0.9117             nan     0.1000    0.0154\n",
      "     5        0.8814             nan     0.1000    0.0134\n",
      "     6        0.8548             nan     0.1000    0.0104\n",
      "     7        0.8358             nan     0.1000    0.0056\n",
      "     8        0.8153             nan     0.1000    0.0066\n",
      "     9        0.7946             nan     0.1000    0.0088\n",
      "    10        0.7746             nan     0.1000    0.0069\n",
      "    20        0.6665             nan     0.1000    0.0010\n",
      "    40        0.5541             nan     0.1000   -0.0001\n",
      "    60        0.4827             nan     0.1000   -0.0004\n",
      "    80        0.4323             nan     0.1000   -0.0013\n",
      "   100        0.3871             nan     0.1000   -0.0006\n",
      "   120        0.3449             nan     0.1000   -0.0001\n",
      "   140        0.3103             nan     0.1000   -0.0005\n",
      "   160        0.2799             nan     0.1000   -0.0006\n",
      "   180        0.2539             nan     0.1000   -0.0005\n",
      "   200        0.2313             nan     0.1000   -0.0003\n",
      "   220        0.2100             nan     0.1000   -0.0003\n",
      "   240        0.1930             nan     0.1000   -0.0004\n",
      "   250        0.1848             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold08.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1117             nan     0.0100    0.0018\n",
      "     2        1.1080             nan     0.0100    0.0018\n",
      "     3        1.1044             nan     0.0100    0.0017\n",
      "     4        1.1012             nan     0.0100    0.0017\n",
      "     5        1.0978             nan     0.0100    0.0017\n",
      "     6        1.0947             nan     0.0100    0.0016\n",
      "     7        1.0914             nan     0.0100    0.0016\n",
      "     8        1.0881             nan     0.0100    0.0016\n",
      "     9        1.0849             nan     0.0100    0.0015\n",
      "    10        1.0823             nan     0.0100    0.0014\n",
      "    20        1.0536             nan     0.0100    0.0013\n",
      "    40        1.0112             nan     0.0100    0.0007\n",
      "    60        0.9760             nan     0.0100    0.0005\n",
      "    80        0.9460             nan     0.0100    0.0007\n",
      "   100        0.9211             nan     0.0100    0.0006\n",
      "   120        0.9007             nan     0.0100    0.0004\n",
      "   140        0.8815             nan     0.0100    0.0004\n",
      "   160        0.8662             nan     0.0100    0.0003\n",
      "   180        0.8523             nan     0.0100    0.0003\n",
      "   200        0.8387             nan     0.0100    0.0003\n",
      "   220        0.8271             nan     0.0100    0.0003\n",
      "   240        0.8168             nan     0.0100    0.0002\n",
      "   250        0.8119             nan     0.0100    0.0001\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1087             nan     0.0100    0.0032\n",
      "     2        1.1020             nan     0.0100    0.0032\n",
      "     3        1.0956             nan     0.0100    0.0031\n",
      "     4        1.0897             nan     0.0100    0.0027\n",
      "     5        1.0835             nan     0.0100    0.0031\n",
      "     6        1.0778             nan     0.0100    0.0026\n",
      "     7        1.0724             nan     0.0100    0.0026\n",
      "     8        1.0671             nan     0.0100    0.0026\n",
      "     9        1.0616             nan     0.0100    0.0027\n",
      "    10        1.0562             nan     0.0100    0.0025\n",
      "    20        1.0070             nan     0.0100    0.0021\n",
      "    40        0.9332             nan     0.0100    0.0016\n",
      "    60        0.8781             nan     0.0100    0.0011\n",
      "    80        0.8353             nan     0.0100    0.0007\n",
      "   100        0.8030             nan     0.0100    0.0004\n",
      "   120        0.7763             nan     0.0100    0.0003\n",
      "   140        0.7537             nan     0.0100    0.0004\n",
      "   160        0.7347             nan     0.0100    0.0003\n",
      "   180        0.7182             nan     0.0100    0.0001\n",
      "   200        0.7043             nan     0.0100    0.0001\n",
      "   220        0.6918             nan     0.0100    0.0002\n",
      "   240        0.6807             nan     0.0100    0.0000\n",
      "   250        0.6754             nan     0.0100    0.0000\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1079             nan     0.0100    0.0032\n",
      "     2        1.1007             nan     0.0100    0.0033\n",
      "     3        1.0939             nan     0.0100    0.0031\n",
      "     4        1.0873             nan     0.0100    0.0033\n",
      "     5        1.0802             nan     0.0100    0.0033\n",
      "     6        1.0738             nan     0.0100    0.0029\n",
      "     7        1.0675             nan     0.0100    0.0031\n",
      "     8        1.0615             nan     0.0100    0.0025\n",
      "     9        1.0552             nan     0.0100    0.0027\n",
      "    10        1.0490             nan     0.0100    0.0028\n",
      "    20        0.9945             nan     0.0100    0.0021\n",
      "    40        0.9121             nan     0.0100    0.0015\n",
      "    60        0.8508             nan     0.0100    0.0010\n",
      "    80        0.8044             nan     0.0100    0.0004\n",
      "   100        0.7671             nan     0.0100    0.0006\n",
      "   120        0.7377             nan     0.0100    0.0005\n",
      "   140        0.7126             nan     0.0100    0.0002\n",
      "   160        0.6893             nan     0.0100    0.0002\n",
      "   180        0.6708             nan     0.0100    0.0002\n",
      "   200        0.6538             nan     0.0100    0.0002\n",
      "   220        0.6384             nan     0.0100    0.0001\n",
      "   240        0.6236             nan     0.0100    0.0001\n",
      "   250        0.6170             nan     0.0100    0.0001\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0981             nan     0.0500    0.0088\n",
      "     2        1.0824             nan     0.0500    0.0080\n",
      "     3        1.0698             nan     0.0500    0.0048\n",
      "     4        1.0546             nan     0.0500    0.0072\n",
      "     5        1.0411             nan     0.0500    0.0065\n",
      "     6        1.0287             nan     0.0500    0.0058\n",
      "     7        1.0183             nan     0.0500    0.0050\n",
      "     8        1.0093             nan     0.0500    0.0043\n",
      "     9        0.9989             nan     0.0500    0.0052\n",
      "    10        0.9914             nan     0.0500    0.0036\n",
      "    20        0.9177             nan     0.0500    0.0022\n",
      "    40        0.8360             nan     0.0500    0.0010\n",
      "    60        0.7883             nan     0.0500    0.0008\n",
      "    80        0.7552             nan     0.0500    0.0004\n",
      "   100        0.7333             nan     0.0500    0.0002\n",
      "   120        0.7170             nan     0.0500    0.0002\n",
      "   140        0.7039             nan     0.0500    0.0001\n",
      "   160        0.6936             nan     0.0500   -0.0000\n",
      "   180        0.6852             nan     0.0500    0.0000\n",
      "   200        0.6777             nan     0.0500   -0.0001\n",
      "   220        0.6713             nan     0.0500   -0.0001\n",
      "   240        0.6658             nan     0.0500   -0.0001\n",
      "   250        0.6633             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0836             nan     0.0500    0.0140\n",
      "     2        1.0565             nan     0.0500    0.0122\n",
      "     3        1.0289             nan     0.0500    0.0128\n",
      "     4        1.0058             nan     0.0500    0.0109\n",
      "     5        0.9860             nan     0.0500    0.0092\n",
      "     6        0.9656             nan     0.0500    0.0079\n",
      "     7        0.9474             nan     0.0500    0.0080\n",
      "     8        0.9308             nan     0.0500    0.0077\n",
      "     9        0.9144             nan     0.0500    0.0073\n",
      "    10        0.9004             nan     0.0500    0.0059\n",
      "    20        0.8006             nan     0.0500    0.0034\n",
      "    40        0.7020             nan     0.0500    0.0006\n",
      "    60        0.6560             nan     0.0500   -0.0007\n",
      "    80        0.6208             nan     0.0500   -0.0000\n",
      "   100        0.5940             nan     0.0500   -0.0000\n",
      "   120        0.5680             nan     0.0500   -0.0002\n",
      "   140        0.5478             nan     0.0500   -0.0002\n",
      "   160        0.5279             nan     0.0500   -0.0005\n",
      "   180        0.5101             nan     0.0500   -0.0003\n",
      "   200        0.4932             nan     0.0500   -0.0004\n",
      "   220        0.4739             nan     0.0500   -0.0004\n",
      "   240        0.4575             nan     0.0500   -0.0002\n",
      "   250        0.4512             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0798             nan     0.0500    0.0177\n",
      "     2        1.0475             nan     0.0500    0.0143\n",
      "     3        1.0195             nan     0.0500    0.0123\n",
      "     4        0.9947             nan     0.0500    0.0105\n",
      "     5        0.9704             nan     0.0500    0.0104\n",
      "     6        0.9501             nan     0.0500    0.0095\n",
      "     7        0.9313             nan     0.0500    0.0085\n",
      "     8        0.9155             nan     0.0500    0.0067\n",
      "     9        0.8968             nan     0.0500    0.0082\n",
      "    10        0.8816             nan     0.0500    0.0060\n",
      "    20        0.7679             nan     0.0500    0.0025\n",
      "    40        0.6544             nan     0.0500    0.0008\n",
      "    60        0.5931             nan     0.0500   -0.0001\n",
      "    80        0.5474             nan     0.0500   -0.0004\n",
      "   100        0.5097             nan     0.0500   -0.0006\n",
      "   120        0.4750             nan     0.0500   -0.0002\n",
      "   140        0.4449             nan     0.0500   -0.0003\n",
      "   160        0.4200             nan     0.0500   -0.0002\n",
      "   180        0.3966             nan     0.0500   -0.0002\n",
      "   200        0.3755             nan     0.0500   -0.0004\n",
      "   220        0.3538             nan     0.0500   -0.0005\n",
      "   240        0.3348             nan     0.0500   -0.0003\n",
      "   250        0.3252             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0822             nan     0.1000    0.0173\n",
      "     2        1.0580             nan     0.1000    0.0102\n",
      "     3        1.0302             nan     0.1000    0.0138\n",
      "     4        1.0142             nan     0.1000    0.0072\n",
      "     5        0.9941             nan     0.1000    0.0086\n",
      "     6        0.9728             nan     0.1000    0.0109\n",
      "     7        0.9627             nan     0.1000    0.0049\n",
      "     8        0.9442             nan     0.1000    0.0088\n",
      "     9        0.9354             nan     0.1000    0.0040\n",
      "    10        0.9195             nan     0.1000    0.0071\n",
      "    20        0.8309             nan     0.1000    0.0034\n",
      "    40        0.7543             nan     0.1000    0.0006\n",
      "    60        0.7160             nan     0.1000    0.0003\n",
      "    80        0.6950             nan     0.1000    0.0001\n",
      "   100        0.6779             nan     0.1000    0.0003\n",
      "   120        0.6668             nan     0.1000   -0.0004\n",
      "   140        0.6556             nan     0.1000   -0.0004\n",
      "   160        0.6445             nan     0.1000   -0.0001\n",
      "   180        0.6360             nan     0.1000   -0.0003\n",
      "   200        0.6289             nan     0.1000   -0.0005\n",
      "   220        0.6216             nan     0.1000   -0.0000\n",
      "   240        0.6173             nan     0.1000   -0.0003\n",
      "   250        0.6148             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0490             nan     0.1000    0.0319\n",
      "     2        1.0038             nan     0.1000    0.0209\n",
      "     3        0.9648             nan     0.1000    0.0184\n",
      "     4        0.9322             nan     0.1000    0.0157\n",
      "     5        0.9048             nan     0.1000    0.0113\n",
      "     6        0.8801             nan     0.1000    0.0108\n",
      "     7        0.8580             nan     0.1000    0.0106\n",
      "     8        0.8383             nan     0.1000    0.0090\n",
      "     9        0.8229             nan     0.1000    0.0057\n",
      "    10        0.8079             nan     0.1000    0.0060\n",
      "    20        0.7065             nan     0.1000    0.0023\n",
      "    40        0.6186             nan     0.1000   -0.0003\n",
      "    60        0.5686             nan     0.1000   -0.0012\n",
      "    80        0.5303             nan     0.1000   -0.0005\n",
      "   100        0.4981             nan     0.1000    0.0002\n",
      "   120        0.4666             nan     0.1000   -0.0007\n",
      "   140        0.4384             nan     0.1000   -0.0008\n",
      "   160        0.4089             nan     0.1000   -0.0006\n",
      "   180        0.3868             nan     0.1000   -0.0005\n",
      "   200        0.3641             nan     0.1000   -0.0006\n",
      "   220        0.3451             nan     0.1000   -0.0009\n",
      "   240        0.3282             nan     0.1000   -0.0005\n",
      "   250        0.3199             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0451             nan     0.1000    0.0329\n",
      "     2        0.9905             nan     0.1000    0.0248\n",
      "     3        0.9434             nan     0.1000    0.0192\n",
      "     4        0.9061             nan     0.1000    0.0159\n",
      "     5        0.8748             nan     0.1000    0.0105\n",
      "     6        0.8510             nan     0.1000    0.0093\n",
      "     7        0.8257             nan     0.1000    0.0106\n",
      "     8        0.8049             nan     0.1000    0.0079\n",
      "     9        0.7875             nan     0.1000    0.0057\n",
      "    10        0.7708             nan     0.1000    0.0053\n",
      "    20        0.6579             nan     0.1000    0.0019\n",
      "    40        0.5457             nan     0.1000    0.0005\n",
      "    60        0.4700             nan     0.1000   -0.0001\n",
      "    80        0.4201             nan     0.1000   -0.0006\n",
      "   100        0.3760             nan     0.1000   -0.0007\n",
      "   120        0.3363             nan     0.1000   -0.0005\n",
      "   140        0.3026             nan     0.1000   -0.0005\n",
      "   160        0.2745             nan     0.1000   -0.0010\n",
      "   180        0.2435             nan     0.1000   -0.0003\n",
      "   200        0.2199             nan     0.1000   -0.0004\n",
      "   220        0.1983             nan     0.1000   -0.0004\n",
      "   240        0.1805             nan     0.1000   -0.0002\n",
      "   250        0.1718             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold09.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1105             nan     0.0100    0.0018\n",
      "     2        1.1069             nan     0.0100    0.0018\n",
      "     3        1.1034             nan     0.0100    0.0017\n",
      "     4        1.0999             nan     0.0100    0.0017\n",
      "     5        1.0965             nan     0.0100    0.0017\n",
      "     6        1.0929             nan     0.0100    0.0016\n",
      "     7        1.0905             nan     0.0100    0.0013\n",
      "     8        1.0870             nan     0.0100    0.0016\n",
      "     9        1.0839             nan     0.0100    0.0016\n",
      "    10        1.0809             nan     0.0100    0.0015\n",
      "    20        1.0532             nan     0.0100    0.0013\n",
      "    40        1.0104             nan     0.0100    0.0010\n",
      "    60        0.9767             nan     0.0100    0.0009\n",
      "    80        0.9474             nan     0.0100    0.0007\n",
      "   100        0.9232             nan     0.0100    0.0005\n",
      "   120        0.9025             nan     0.0100    0.0005\n",
      "   140        0.8844             nan     0.0100    0.0003\n",
      "   160        0.8689             nan     0.0100    0.0002\n",
      "   180        0.8547             nan     0.0100    0.0004\n",
      "   200        0.8420             nan     0.0100    0.0003\n",
      "   220        0.8305             nan     0.0100    0.0003\n",
      "   240        0.8203             nan     0.0100    0.0001\n",
      "   250        0.8154             nan     0.0100    0.0002\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1074             nan     0.0100    0.0033\n",
      "     2        1.1015             nan     0.0100    0.0028\n",
      "     3        1.0953             nan     0.0100    0.0031\n",
      "     4        1.0891             nan     0.0100    0.0028\n",
      "     5        1.0829             nan     0.0100    0.0031\n",
      "     6        1.0769             nan     0.0100    0.0028\n",
      "     7        1.0713             nan     0.0100    0.0027\n",
      "     8        1.0655             nan     0.0100    0.0026\n",
      "     9        1.0604             nan     0.0100    0.0026\n",
      "    10        1.0552             nan     0.0100    0.0024\n",
      "    20        1.0068             nan     0.0100    0.0022\n",
      "    40        0.9355             nan     0.0100    0.0015\n",
      "    60        0.8813             nan     0.0100    0.0010\n",
      "    80        0.8398             nan     0.0100    0.0008\n",
      "   100        0.8067             nan     0.0100    0.0006\n",
      "   120        0.7798             nan     0.0100    0.0006\n",
      "   140        0.7582             nan     0.0100    0.0003\n",
      "   160        0.7394             nan     0.0100    0.0002\n",
      "   180        0.7227             nan     0.0100    0.0003\n",
      "   200        0.7082             nan     0.0100    0.0000\n",
      "   220        0.6960             nan     0.0100    0.0001\n",
      "   240        0.6851             nan     0.0100    0.0001\n",
      "   250        0.6795             nan     0.0100    0.0001\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0032\n",
      "     2        1.0999             nan     0.0100    0.0034\n",
      "     3        1.0931             nan     0.0100    0.0031\n",
      "     4        1.0864             nan     0.0100    0.0032\n",
      "     5        1.0794             nan     0.0100    0.0034\n",
      "     6        1.0731             nan     0.0100    0.0028\n",
      "     7        1.0667             nan     0.0100    0.0027\n",
      "     8        1.0602             nan     0.0100    0.0028\n",
      "     9        1.0542             nan     0.0100    0.0026\n",
      "    10        1.0480             nan     0.0100    0.0026\n",
      "    20        0.9957             nan     0.0100    0.0021\n",
      "    40        0.9138             nan     0.0100    0.0014\n",
      "    60        0.8545             nan     0.0100    0.0010\n",
      "    80        0.8079             nan     0.0100    0.0007\n",
      "   100        0.7707             nan     0.0100    0.0007\n",
      "   120        0.7411             nan     0.0100    0.0006\n",
      "   140        0.7148             nan     0.0100    0.0002\n",
      "   160        0.6926             nan     0.0100    0.0004\n",
      "   180        0.6735             nan     0.0100    0.0003\n",
      "   200        0.6562             nan     0.0100    0.0002\n",
      "   220        0.6410             nan     0.0100    0.0000\n",
      "   240        0.6275             nan     0.0100   -0.0000\n",
      "   250        0.6214             nan     0.0100    0.0001\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0979             nan     0.0500    0.0088\n",
      "     2        1.0810             nan     0.0500    0.0081\n",
      "     3        1.0670             nan     0.0500    0.0073\n",
      "     4        1.0525             nan     0.0500    0.0065\n",
      "     5        1.0399             nan     0.0500    0.0058\n",
      "     6        1.0295             nan     0.0500    0.0051\n",
      "     7        1.0210             nan     0.0500    0.0036\n",
      "     8        1.0136             nan     0.0500    0.0026\n",
      "     9        1.0039             nan     0.0500    0.0043\n",
      "    10        0.9930             nan     0.0500    0.0049\n",
      "    20        0.9231             nan     0.0500    0.0031\n",
      "    40        0.8405             nan     0.0500    0.0015\n",
      "    60        0.7917             nan     0.0500    0.0005\n",
      "    80        0.7603             nan     0.0500    0.0004\n",
      "   100        0.7385             nan     0.0500    0.0003\n",
      "   120        0.7234             nan     0.0500    0.0002\n",
      "   140        0.7099             nan     0.0500    0.0002\n",
      "   160        0.6999             nan     0.0500    0.0001\n",
      "   180        0.6914             nan     0.0500   -0.0002\n",
      "   200        0.6838             nan     0.0500   -0.0000\n",
      "   220        0.6775             nan     0.0500    0.0001\n",
      "   240        0.6707             nan     0.0500    0.0000\n",
      "   250        0.6677             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0827             nan     0.0500    0.0152\n",
      "     2        1.0552             nan     0.0500    0.0117\n",
      "     3        1.0278             nan     0.0500    0.0115\n",
      "     4        1.0051             nan     0.0500    0.0104\n",
      "     5        0.9840             nan     0.0500    0.0097\n",
      "     6        0.9649             nan     0.0500    0.0085\n",
      "     7        0.9472             nan     0.0500    0.0083\n",
      "     8        0.9313             nan     0.0500    0.0078\n",
      "     9        0.9175             nan     0.0500    0.0062\n",
      "    10        0.9039             nan     0.0500    0.0060\n",
      "    20        0.8076             nan     0.0500    0.0034\n",
      "    40        0.7086             nan     0.0500    0.0011\n",
      "    60        0.6587             nan     0.0500   -0.0000\n",
      "    80        0.6231             nan     0.0500   -0.0001\n",
      "   100        0.5969             nan     0.0500   -0.0001\n",
      "   120        0.5731             nan     0.0500   -0.0003\n",
      "   140        0.5525             nan     0.0500   -0.0003\n",
      "   160        0.5321             nan     0.0500   -0.0003\n",
      "   180        0.5164             nan     0.0500   -0.0008\n",
      "   200        0.5008             nan     0.0500   -0.0002\n",
      "   220        0.4834             nan     0.0500   -0.0004\n",
      "   240        0.4680             nan     0.0500   -0.0006\n",
      "   250        0.4603             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0789             nan     0.0500    0.0175\n",
      "     2        1.0473             nan     0.0500    0.0149\n",
      "     3        1.0175             nan     0.0500    0.0139\n",
      "     4        0.9932             nan     0.0500    0.0111\n",
      "     5        0.9730             nan     0.0500    0.0085\n",
      "     6        0.9508             nan     0.0500    0.0090\n",
      "     7        0.9308             nan     0.0500    0.0084\n",
      "     8        0.9133             nan     0.0500    0.0067\n",
      "     9        0.8963             nan     0.0500    0.0076\n",
      "    10        0.8831             nan     0.0500    0.0054\n",
      "    20        0.7712             nan     0.0500    0.0024\n",
      "    40        0.6605             nan     0.0500    0.0009\n",
      "    60        0.5944             nan     0.0500   -0.0003\n",
      "    80        0.5492             nan     0.0500   -0.0003\n",
      "   100        0.5102             nan     0.0500   -0.0004\n",
      "   120        0.4792             nan     0.0500   -0.0002\n",
      "   140        0.4468             nan     0.0500   -0.0006\n",
      "   160        0.4192             nan     0.0500   -0.0001\n",
      "   180        0.3948             nan     0.0500   -0.0001\n",
      "   200        0.3715             nan     0.0500   -0.0007\n",
      "   220        0.3495             nan     0.0500   -0.0004\n",
      "   240        0.3306             nan     0.0500   -0.0004\n",
      "   250        0.3209             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0760             nan     0.1000    0.0167\n",
      "     2        1.0500             nan     0.1000    0.0137\n",
      "     3        1.0293             nan     0.1000    0.0106\n",
      "     4        1.0103             nan     0.1000    0.0088\n",
      "     5        0.9876             nan     0.1000    0.0106\n",
      "     6        0.9716             nan     0.1000    0.0088\n",
      "     7        0.9597             nan     0.1000    0.0063\n",
      "     8        0.9452             nan     0.1000    0.0072\n",
      "     9        0.9317             nan     0.1000    0.0059\n",
      "    10        0.9187             nan     0.1000    0.0040\n",
      "    20        0.8414             nan     0.1000    0.0025\n",
      "    40        0.7604             nan     0.1000    0.0007\n",
      "    60        0.7254             nan     0.1000    0.0009\n",
      "    80        0.7010             nan     0.1000    0.0001\n",
      "   100        0.6833             nan     0.1000   -0.0002\n",
      "   120        0.6691             nan     0.1000   -0.0002\n",
      "   140        0.6585             nan     0.1000    0.0000\n",
      "   160        0.6493             nan     0.1000   -0.0003\n",
      "   180        0.6413             nan     0.1000   -0.0006\n",
      "   200        0.6358             nan     0.1000   -0.0001\n",
      "   220        0.6316             nan     0.1000   -0.0002\n",
      "   240        0.6259             nan     0.1000   -0.0005\n",
      "   250        0.6236             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0471             nan     0.1000    0.0302\n",
      "     2        0.9987             nan     0.1000    0.0214\n",
      "     3        0.9585             nan     0.1000    0.0179\n",
      "     4        0.9216             nan     0.1000    0.0164\n",
      "     5        0.8944             nan     0.1000    0.0134\n",
      "     6        0.8704             nan     0.1000    0.0111\n",
      "     7        0.8502             nan     0.1000    0.0084\n",
      "     8        0.8339             nan     0.1000    0.0063\n",
      "     9        0.8159             nan     0.1000    0.0074\n",
      "    10        0.8002             nan     0.1000    0.0065\n",
      "    20        0.7030             nan     0.1000    0.0017\n",
      "    40        0.6178             nan     0.1000    0.0002\n",
      "    60        0.5673             nan     0.1000   -0.0005\n",
      "    80        0.5299             nan     0.1000   -0.0004\n",
      "   100        0.4952             nan     0.1000   -0.0004\n",
      "   120        0.4687             nan     0.1000   -0.0002\n",
      "   140        0.4369             nan     0.1000   -0.0008\n",
      "   160        0.4126             nan     0.1000   -0.0007\n",
      "   180        0.3921             nan     0.1000   -0.0010\n",
      "   200        0.3719             nan     0.1000   -0.0000\n",
      "   220        0.3508             nan     0.1000   -0.0012\n",
      "   240        0.3320             nan     0.1000   -0.0007\n",
      "   250        0.3231             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0490             nan     0.1000    0.0315\n",
      "     2        0.9907             nan     0.1000    0.0248\n",
      "     3        0.9466             nan     0.1000    0.0177\n",
      "     4        0.9099             nan     0.1000    0.0144\n",
      "     5        0.8790             nan     0.1000    0.0130\n",
      "     6        0.8533             nan     0.1000    0.0093\n",
      "     7        0.8289             nan     0.1000    0.0092\n",
      "     8        0.8054             nan     0.1000    0.0097\n",
      "     9        0.7844             nan     0.1000    0.0073\n",
      "    10        0.7667             nan     0.1000    0.0061\n",
      "    20        0.6572             nan     0.1000    0.0001\n",
      "    40        0.5494             nan     0.1000   -0.0002\n",
      "    60        0.4767             nan     0.1000   -0.0005\n",
      "    80        0.4210             nan     0.1000   -0.0009\n",
      "   100        0.3766             nan     0.1000   -0.0016\n",
      "   120        0.3372             nan     0.1000   -0.0006\n",
      "   140        0.3023             nan     0.1000   -0.0006\n",
      "   160        0.2730             nan     0.1000   -0.0009\n",
      "   180        0.2475             nan     0.1000   -0.0004\n",
      "   200        0.2208             nan     0.1000   -0.0004\n",
      "   220        0.2007             nan     0.1000   -0.0008\n",
      "   240        0.1830             nan     0.1000   -0.0000\n",
      "   250        0.1742             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold10.Rep1: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1107             nan     0.0100    0.0017\n",
      "     2        1.1072             nan     0.0100    0.0017\n",
      "     3        1.1040             nan     0.0100    0.0017\n",
      "     4        1.1007             nan     0.0100    0.0016\n",
      "     5        1.0974             nan     0.0100    0.0016\n",
      "     6        1.0943             nan     0.0100    0.0016\n",
      "     7        1.0911             nan     0.0100    0.0015\n",
      "     8        1.0883             nan     0.0100    0.0015\n",
      "     9        1.0854             nan     0.0100    0.0015\n",
      "    10        1.0824             nan     0.0100    0.0014\n",
      "    20        1.0566             nan     0.0100    0.0012\n",
      "    40        1.0149             nan     0.0100    0.0011\n",
      "    60        0.9787             nan     0.0100    0.0008\n",
      "    80        0.9485             nan     0.0100    0.0007\n",
      "   100        0.9239             nan     0.0100    0.0004\n",
      "   120        0.9024             nan     0.0100    0.0004\n",
      "   140        0.8842             nan     0.0100    0.0004\n",
      "   160        0.8670             nan     0.0100    0.0004\n",
      "   180        0.8525             nan     0.0100    0.0003\n",
      "   200        0.8390             nan     0.0100    0.0003\n",
      "   220        0.8276             nan     0.0100    0.0002\n",
      "   240        0.8168             nan     0.0100    0.0002\n",
      "   250        0.8120             nan     0.0100    0.0002\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0033\n",
      "     2        1.1007             nan     0.0100    0.0031\n",
      "     3        1.0945             nan     0.0100    0.0030\n",
      "     4        1.0882             nan     0.0100    0.0027\n",
      "     5        1.0822             nan     0.0100    0.0032\n",
      "     6        1.0762             nan     0.0100    0.0030\n",
      "     7        1.0705             nan     0.0100    0.0028\n",
      "     8        1.0648             nan     0.0100    0.0023\n",
      "     9        1.0589             nan     0.0100    0.0025\n",
      "    10        1.0535             nan     0.0100    0.0023\n",
      "    20        1.0055             nan     0.0100    0.0022\n",
      "    40        0.9319             nan     0.0100    0.0014\n",
      "    60        0.8768             nan     0.0100    0.0012\n",
      "    80        0.8348             nan     0.0100    0.0008\n",
      "   100        0.8023             nan     0.0100    0.0007\n",
      "   120        0.7756             nan     0.0100    0.0004\n",
      "   140        0.7517             nan     0.0100    0.0005\n",
      "   160        0.7333             nan     0.0100    0.0003\n",
      "   180        0.7168             nan     0.0100    0.0002\n",
      "   200        0.7025             nan     0.0100    0.0004\n",
      "   220        0.6895             nan     0.0100    0.0001\n",
      "   240        0.6780             nan     0.0100    0.0000\n",
      "   250        0.6727             nan     0.0100    0.0000\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1066             nan     0.0100    0.0036\n",
      "     2        1.0995             nan     0.0100    0.0033\n",
      "     3        1.0925             nan     0.0100    0.0031\n",
      "     4        1.0854             nan     0.0100    0.0031\n",
      "     5        1.0787             nan     0.0100    0.0027\n",
      "     6        1.0723             nan     0.0100    0.0029\n",
      "     7        1.0659             nan     0.0100    0.0028\n",
      "     8        1.0596             nan     0.0100    0.0028\n",
      "     9        1.0537             nan     0.0100    0.0026\n",
      "    10        1.0476             nan     0.0100    0.0027\n",
      "    20        0.9945             nan     0.0100    0.0023\n",
      "    40        0.9115             nan     0.0100    0.0017\n",
      "    60        0.8513             nan     0.0100    0.0010\n",
      "    80        0.8046             nan     0.0100    0.0007\n",
      "   100        0.7663             nan     0.0100    0.0006\n",
      "   120        0.7347             nan     0.0100    0.0005\n",
      "   140        0.7087             nan     0.0100    0.0004\n",
      "   160        0.6858             nan     0.0100    0.0003\n",
      "   180        0.6665             nan     0.0100    0.0001\n",
      "   200        0.6490             nan     0.0100    0.0001\n",
      "   220        0.6332             nan     0.0100    0.0001\n",
      "   240        0.6196             nan     0.0100    0.0002\n",
      "   250        0.6129             nan     0.0100    0.0001\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0969             nan     0.0500    0.0085\n",
      "     2        1.0848             nan     0.0500    0.0053\n",
      "     3        1.0740             nan     0.0500    0.0047\n",
      "     4        1.0603             nan     0.0500    0.0075\n",
      "     5        1.0470             nan     0.0500    0.0068\n",
      "     6        1.0361             nan     0.0500    0.0047\n",
      "     7        1.0272             nan     0.0500    0.0036\n",
      "     8        1.0152             nan     0.0500    0.0060\n",
      "     9        1.0033             nan     0.0500    0.0055\n",
      "    10        0.9931             nan     0.0500    0.0050\n",
      "    20        0.9212             nan     0.0500    0.0031\n",
      "    40        0.8384             nan     0.0500    0.0012\n",
      "    60        0.7882             nan     0.0500    0.0010\n",
      "    80        0.7569             nan     0.0500    0.0005\n",
      "   100        0.7350             nan     0.0500   -0.0000\n",
      "   120        0.7179             nan     0.0500   -0.0003\n",
      "   140        0.7041             nan     0.0500   -0.0001\n",
      "   160        0.6931             nan     0.0500   -0.0001\n",
      "   180        0.6849             nan     0.0500   -0.0001\n",
      "   200        0.6773             nan     0.0500   -0.0000\n",
      "   220        0.6701             nan     0.0500   -0.0000\n",
      "   240        0.6631             nan     0.0500   -0.0000\n",
      "   250        0.6594             nan     0.0500    0.0001\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0812             nan     0.0500    0.0156\n",
      "     2        1.0520             nan     0.0500    0.0145\n",
      "     3        1.0261             nan     0.0500    0.0118\n",
      "     4        1.0046             nan     0.0500    0.0106\n",
      "     5        0.9856             nan     0.0500    0.0080\n",
      "     6        0.9666             nan     0.0500    0.0094\n",
      "     7        0.9491             nan     0.0500    0.0081\n",
      "     8        0.9340             nan     0.0500    0.0073\n",
      "     9        0.9177             nan     0.0500    0.0077\n",
      "    10        0.9041             nan     0.0500    0.0054\n",
      "    20        0.8031             nan     0.0500    0.0025\n",
      "    40        0.7035             nan     0.0500    0.0002\n",
      "    60        0.6528             nan     0.0500   -0.0004\n",
      "    80        0.6164             nan     0.0500   -0.0000\n",
      "   100        0.5880             nan     0.0500   -0.0002\n",
      "   120        0.5640             nan     0.0500   -0.0004\n",
      "   140        0.5404             nan     0.0500   -0.0001\n",
      "   160        0.5219             nan     0.0500   -0.0003\n",
      "   180        0.5042             nan     0.0500   -0.0001\n",
      "   200        0.4864             nan     0.0500   -0.0003\n",
      "   220        0.4705             nan     0.0500   -0.0001\n",
      "   240        0.4567             nan     0.0500   -0.0004\n",
      "   250        0.4490             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0788             nan     0.0500    0.0159\n",
      "     2        1.0458             nan     0.0500    0.0149\n",
      "     3        1.0183             nan     0.0500    0.0126\n",
      "     4        0.9912             nan     0.0500    0.0120\n",
      "     5        0.9692             nan     0.0500    0.0083\n",
      "     6        0.9478             nan     0.0500    0.0102\n",
      "     7        0.9283             nan     0.0500    0.0081\n",
      "     8        0.9097             nan     0.0500    0.0084\n",
      "     9        0.8927             nan     0.0500    0.0071\n",
      "    10        0.8767             nan     0.0500    0.0064\n",
      "    20        0.7621             nan     0.0500    0.0035\n",
      "    40        0.6529             nan     0.0500    0.0003\n",
      "    60        0.5867             nan     0.0500    0.0004\n",
      "    80        0.5412             nan     0.0500   -0.0006\n",
      "   100        0.5015             nan     0.0500   -0.0006\n",
      "   120        0.4664             nan     0.0500   -0.0003\n",
      "   140        0.4363             nan     0.0500    0.0001\n",
      "   160        0.4104             nan     0.0500   -0.0001\n",
      "   180        0.3865             nan     0.0500   -0.0002\n",
      "   200        0.3619             nan     0.0500   -0.0006\n",
      "   220        0.3428             nan     0.0500   -0.0002\n",
      "   240        0.3237             nan     0.0500   -0.0004\n",
      "   250        0.3156             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0787             nan     0.1000    0.0161\n",
      "     2        1.0520             nan     0.1000    0.0131\n",
      "     3        1.0294             nan     0.1000    0.0085\n",
      "     4        1.0082             nan     0.1000    0.0094\n",
      "     5        0.9938             nan     0.1000    0.0067\n",
      "     6        0.9743             nan     0.1000    0.0100\n",
      "     7        0.9635             nan     0.1000    0.0048\n",
      "     8        0.9468             nan     0.1000    0.0082\n",
      "     9        0.9318             nan     0.1000    0.0066\n",
      "    10        0.9205             nan     0.1000    0.0049\n",
      "    20        0.8336             nan     0.1000    0.0024\n",
      "    40        0.7587             nan     0.1000    0.0006\n",
      "    60        0.7206             nan     0.1000    0.0009\n",
      "    80        0.6968             nan     0.1000   -0.0002\n",
      "   100        0.6797             nan     0.1000    0.0001\n",
      "   120        0.6669             nan     0.1000   -0.0000\n",
      "   140        0.6556             nan     0.1000   -0.0002\n",
      "   160        0.6488             nan     0.1000   -0.0002\n",
      "   180        0.6421             nan     0.1000   -0.0005\n",
      "   200        0.6360             nan     0.1000   -0.0008\n",
      "   220        0.6293             nan     0.1000   -0.0006\n",
      "   240        0.6226             nan     0.1000   -0.0002\n",
      "   250        0.6204             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0472             nan     0.1000    0.0295\n",
      "     2        0.9968             nan     0.1000    0.0264\n",
      "     3        0.9597             nan     0.1000    0.0163\n",
      "     4        0.9261             nan     0.1000    0.0158\n",
      "     5        0.8959             nan     0.1000    0.0145\n",
      "     6        0.8716             nan     0.1000    0.0096\n",
      "     7        0.8494             nan     0.1000    0.0094\n",
      "     8        0.8290             nan     0.1000    0.0072\n",
      "     9        0.8151             nan     0.1000    0.0049\n",
      "    10        0.8013             nan     0.1000    0.0055\n",
      "    20        0.7061             nan     0.1000    0.0021\n",
      "    40        0.6220             nan     0.1000    0.0003\n",
      "    60        0.5636             nan     0.1000   -0.0005\n",
      "    80        0.5294             nan     0.1000   -0.0004\n",
      "   100        0.4968             nan     0.1000   -0.0008\n",
      "   120        0.4680             nan     0.1000   -0.0009\n",
      "   140        0.4384             nan     0.1000   -0.0008\n",
      "   160        0.4126             nan     0.1000   -0.0004\n",
      "   180        0.3904             nan     0.1000   -0.0002\n",
      "   200        0.3714             nan     0.1000   -0.0010\n",
      "   220        0.3536             nan     0.1000   -0.0002\n",
      "   240        0.3350             nan     0.1000   -0.0007\n",
      "   250        0.3259             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0465             nan     0.1000    0.0315\n",
      "     2        0.9873             nan     0.1000    0.0252\n",
      "     3        0.9425             nan     0.1000    0.0209\n",
      "     4        0.9058             nan     0.1000    0.0156\n",
      "     5        0.8711             nan     0.1000    0.0139\n",
      "     6        0.8409             nan     0.1000    0.0114\n",
      "     7        0.8202             nan     0.1000    0.0065\n",
      "     8        0.8017             nan     0.1000    0.0064\n",
      "     9        0.7802             nan     0.1000    0.0073\n",
      "    10        0.7635             nan     0.1000    0.0055\n",
      "    20        0.6503             nan     0.1000   -0.0003\n",
      "    40        0.5359             nan     0.1000   -0.0004\n",
      "    60        0.4603             nan     0.1000   -0.0010\n",
      "    80        0.4113             nan     0.1000   -0.0003\n",
      "   100        0.3653             nan     0.1000   -0.0006\n",
      "   120        0.3242             nan     0.1000   -0.0010\n",
      "   140        0.2917             nan     0.1000   -0.0009\n",
      "   160        0.2637             nan     0.1000   -0.0006\n",
      "   180        0.2373             nan     0.1000   -0.0006\n",
      "   200        0.2162             nan     0.1000   -0.0007\n",
      "   220        0.1957             nan     0.1000   -0.0004\n",
      "   240        0.1771             nan     0.1000   -0.0002\n",
      "   250        0.1675             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold01.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0019\n",
      "     2        1.1070             nan     0.0100    0.0018\n",
      "     3        1.1034             nan     0.0100    0.0018\n",
      "     4        1.0999             nan     0.0100    0.0018\n",
      "     5        1.0965             nan     0.0100    0.0017\n",
      "     6        1.0932             nan     0.0100    0.0017\n",
      "     7        1.0899             nan     0.0100    0.0017\n",
      "     8        1.0865             nan     0.0100    0.0016\n",
      "     9        1.0832             nan     0.0100    0.0016\n",
      "    10        1.0801             nan     0.0100    0.0016\n",
      "    20        1.0525             nan     0.0100    0.0013\n",
      "    40        1.0078             nan     0.0100    0.0010\n",
      "    60        0.9737             nan     0.0100    0.0007\n",
      "    80        0.9445             nan     0.0100    0.0007\n",
      "   100        0.9198             nan     0.0100    0.0004\n",
      "   120        0.9005             nan     0.0100    0.0005\n",
      "   140        0.8830             nan     0.0100    0.0005\n",
      "   160        0.8666             nan     0.0100    0.0003\n",
      "   180        0.8529             nan     0.0100    0.0002\n",
      "   200        0.8402             nan     0.0100    0.0003\n",
      "   220        0.8290             nan     0.0100    0.0002\n",
      "   240        0.8192             nan     0.0100    0.0002\n",
      "   250        0.8147             nan     0.0100    0.0002\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1074             nan     0.0100    0.0035\n",
      "     2        1.1008             nan     0.0100    0.0029\n",
      "     3        1.0943             nan     0.0100    0.0030\n",
      "     4        1.0877             nan     0.0100    0.0030\n",
      "     5        1.0819             nan     0.0100    0.0028\n",
      "     6        1.0757             nan     0.0100    0.0029\n",
      "     7        1.0699             nan     0.0100    0.0027\n",
      "     8        1.0641             nan     0.0100    0.0027\n",
      "     9        1.0584             nan     0.0100    0.0024\n",
      "    10        1.0528             nan     0.0100    0.0028\n",
      "    20        1.0042             nan     0.0100    0.0020\n",
      "    40        0.9308             nan     0.0100    0.0013\n",
      "    60        0.8771             nan     0.0100    0.0010\n",
      "    80        0.8353             nan     0.0100    0.0006\n",
      "   100        0.8022             nan     0.0100    0.0007\n",
      "   120        0.7754             nan     0.0100    0.0004\n",
      "   140        0.7529             nan     0.0100    0.0003\n",
      "   160        0.7333             nan     0.0100    0.0004\n",
      "   180        0.7159             nan     0.0100    0.0002\n",
      "   200        0.7014             nan     0.0100    0.0002\n",
      "   220        0.6894             nan     0.0100    0.0001\n",
      "   240        0.6781             nan     0.0100    0.0001\n",
      "   250        0.6725             nan     0.0100    0.0001\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0031\n",
      "     2        1.1003             nan     0.0100    0.0032\n",
      "     3        1.0928             nan     0.0100    0.0034\n",
      "     4        1.0860             nan     0.0100    0.0033\n",
      "     5        1.0794             nan     0.0100    0.0030\n",
      "     6        1.0727             nan     0.0100    0.0029\n",
      "     7        1.0666             nan     0.0100    0.0026\n",
      "     8        1.0600             nan     0.0100    0.0029\n",
      "     9        1.0537             nan     0.0100    0.0031\n",
      "    10        1.0474             nan     0.0100    0.0027\n",
      "    20        0.9931             nan     0.0100    0.0023\n",
      "    40        0.9091             nan     0.0100    0.0015\n",
      "    60        0.8472             nan     0.0100    0.0012\n",
      "    80        0.8004             nan     0.0100    0.0007\n",
      "   100        0.7625             nan     0.0100    0.0006\n",
      "   120        0.7313             nan     0.0100    0.0004\n",
      "   140        0.7051             nan     0.0100    0.0004\n",
      "   160        0.6831             nan     0.0100    0.0004\n",
      "   180        0.6628             nan     0.0100    0.0002\n",
      "   200        0.6462             nan     0.0100    0.0002\n",
      "   220        0.6308             nan     0.0100    0.0001\n",
      "   240        0.6177             nan     0.0100    0.0001\n",
      "   250        0.6115             nan     0.0100    0.0000\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0963             nan     0.0500    0.0092\n",
      "     2        1.0794             nan     0.0500    0.0082\n",
      "     3        1.0653             nan     0.0500    0.0074\n",
      "     4        1.0517             nan     0.0500    0.0068\n",
      "     5        1.0401             nan     0.0500    0.0054\n",
      "     6        1.0311             nan     0.0500    0.0035\n",
      "     7        1.0193             nan     0.0500    0.0061\n",
      "     8        1.0082             nan     0.0500    0.0055\n",
      "     9        0.9991             nan     0.0500    0.0040\n",
      "    10        0.9917             nan     0.0500    0.0035\n",
      "    20        0.9191             nan     0.0500    0.0031\n",
      "    40        0.8376             nan     0.0500    0.0016\n",
      "    60        0.7903             nan     0.0500    0.0006\n",
      "    80        0.7584             nan     0.0500    0.0005\n",
      "   100        0.7359             nan     0.0500    0.0002\n",
      "   120        0.7188             nan     0.0500    0.0002\n",
      "   140        0.7063             nan     0.0500    0.0001\n",
      "   160        0.6965             nan     0.0500   -0.0001\n",
      "   180        0.6860             nan     0.0500    0.0001\n",
      "   200        0.6773             nan     0.0500   -0.0000\n",
      "   220        0.6706             nan     0.0500   -0.0002\n",
      "   240        0.6642             nan     0.0500   -0.0002\n",
      "   250        0.6618             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0819             nan     0.0500    0.0136\n",
      "     2        1.0560             nan     0.0500    0.0136\n",
      "     3        1.0277             nan     0.0500    0.0135\n",
      "     4        1.0034             nan     0.0500    0.0112\n",
      "     5        0.9833             nan     0.0500    0.0101\n",
      "     6        0.9667             nan     0.0500    0.0081\n",
      "     7        0.9484             nan     0.0500    0.0090\n",
      "     8        0.9324             nan     0.0500    0.0081\n",
      "     9        0.9171             nan     0.0500    0.0073\n",
      "    10        0.9024             nan     0.0500    0.0067\n",
      "    20        0.8033             nan     0.0500    0.0040\n",
      "    40        0.7048             nan     0.0500    0.0007\n",
      "    60        0.6527             nan     0.0500    0.0002\n",
      "    80        0.6187             nan     0.0500    0.0002\n",
      "   100        0.5896             nan     0.0500   -0.0001\n",
      "   120        0.5679             nan     0.0500   -0.0001\n",
      "   140        0.5471             nan     0.0500   -0.0002\n",
      "   160        0.5267             nan     0.0500    0.0001\n",
      "   180        0.5079             nan     0.0500   -0.0004\n",
      "   200        0.4920             nan     0.0500   -0.0003\n",
      "   220        0.4759             nan     0.0500   -0.0003\n",
      "   240        0.4612             nan     0.0500   -0.0006\n",
      "   250        0.4540             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0804             nan     0.0500    0.0148\n",
      "     2        1.0495             nan     0.0500    0.0144\n",
      "     3        1.0216             nan     0.0500    0.0124\n",
      "     4        0.9950             nan     0.0500    0.0108\n",
      "     5        0.9688             nan     0.0500    0.0117\n",
      "     6        0.9474             nan     0.0500    0.0101\n",
      "     7        0.9276             nan     0.0500    0.0089\n",
      "     8        0.9095             nan     0.0500    0.0068\n",
      "     9        0.8933             nan     0.0500    0.0062\n",
      "    10        0.8769             nan     0.0500    0.0063\n",
      "    20        0.7671             nan     0.0500    0.0035\n",
      "    40        0.6500             nan     0.0500    0.0007\n",
      "    60        0.5845             nan     0.0500    0.0002\n",
      "    80        0.5395             nan     0.0500   -0.0005\n",
      "   100        0.5025             nan     0.0500   -0.0002\n",
      "   120        0.4674             nan     0.0500   -0.0007\n",
      "   140        0.4389             nan     0.0500   -0.0006\n",
      "   160        0.4147             nan     0.0500   -0.0002\n",
      "   180        0.3902             nan     0.0500   -0.0004\n",
      "   200        0.3692             nan     0.0500   -0.0004\n",
      "   220        0.3503             nan     0.0500   -0.0003\n",
      "   240        0.3300             nan     0.0500   -0.0004\n",
      "   250        0.3202             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0770             nan     0.1000    0.0178\n",
      "     2        1.0529             nan     0.1000    0.0097\n",
      "     3        1.0284             nan     0.1000    0.0137\n",
      "     4        1.0035             nan     0.1000    0.0118\n",
      "     5        0.9830             nan     0.1000    0.0096\n",
      "     6        0.9694             nan     0.1000    0.0057\n",
      "     7        0.9546             nan     0.1000    0.0074\n",
      "     8        0.9370             nan     0.1000    0.0071\n",
      "     9        0.9268             nan     0.1000    0.0050\n",
      "    10        0.9191             nan     0.1000    0.0034\n",
      "    20        0.8347             nan     0.1000    0.0022\n",
      "    40        0.7547             nan     0.1000    0.0008\n",
      "    60        0.7158             nan     0.1000    0.0003\n",
      "    80        0.6940             nan     0.1000   -0.0000\n",
      "   100        0.6774             nan     0.1000   -0.0001\n",
      "   120        0.6636             nan     0.1000   -0.0006\n",
      "   140        0.6547             nan     0.1000   -0.0000\n",
      "   160        0.6451             nan     0.1000    0.0000\n",
      "   180        0.6376             nan     0.1000   -0.0004\n",
      "   200        0.6311             nan     0.1000   -0.0007\n",
      "   220        0.6258             nan     0.1000   -0.0001\n",
      "   240        0.6220             nan     0.1000   -0.0005\n",
      "   250        0.6186             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0501             nan     0.1000    0.0312\n",
      "     2        0.9991             nan     0.1000    0.0213\n",
      "     3        0.9567             nan     0.1000    0.0191\n",
      "     4        0.9244             nan     0.1000    0.0156\n",
      "     5        0.8964             nan     0.1000    0.0129\n",
      "     6        0.8725             nan     0.1000    0.0114\n",
      "     7        0.8531             nan     0.1000    0.0083\n",
      "     8        0.8355             nan     0.1000    0.0073\n",
      "     9        0.8183             nan     0.1000    0.0072\n",
      "    10        0.8026             nan     0.1000    0.0061\n",
      "    20        0.7063             nan     0.1000    0.0014\n",
      "    40        0.6202             nan     0.1000   -0.0011\n",
      "    60        0.5640             nan     0.1000   -0.0009\n",
      "    80        0.5234             nan     0.1000   -0.0008\n",
      "   100        0.4902             nan     0.1000   -0.0004\n",
      "   120        0.4617             nan     0.1000   -0.0009\n",
      "   140        0.4344             nan     0.1000   -0.0011\n",
      "   160        0.4100             nan     0.1000   -0.0010\n",
      "   180        0.3876             nan     0.1000   -0.0007\n",
      "   200        0.3674             nan     0.1000   -0.0007\n",
      "   220        0.3480             nan     0.1000   -0.0005\n",
      "   240        0.3285             nan     0.1000   -0.0006\n",
      "   250        0.3190             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0456             nan     0.1000    0.0304\n",
      "     2        0.9889             nan     0.1000    0.0234\n",
      "     3        0.9449             nan     0.1000    0.0200\n",
      "     4        0.9084             nan     0.1000    0.0152\n",
      "     5        0.8718             nan     0.1000    0.0170\n",
      "     6        0.8435             nan     0.1000    0.0113\n",
      "     7        0.8189             nan     0.1000    0.0095\n",
      "     8        0.7976             nan     0.1000    0.0084\n",
      "     9        0.7775             nan     0.1000    0.0076\n",
      "    10        0.7610             nan     0.1000    0.0061\n",
      "    20        0.6492             nan     0.1000    0.0025\n",
      "    40        0.5424             nan     0.1000   -0.0007\n",
      "    60        0.4766             nan     0.1000   -0.0005\n",
      "    80        0.4197             nan     0.1000    0.0001\n",
      "   100        0.3769             nan     0.1000   -0.0008\n",
      "   120        0.3372             nan     0.1000   -0.0007\n",
      "   140        0.2981             nan     0.1000   -0.0002\n",
      "   160        0.2684             nan     0.1000   -0.0007\n",
      "   180        0.2424             nan     0.1000   -0.0006\n",
      "   200        0.2202             nan     0.1000   -0.0005\n",
      "   220        0.1987             nan     0.1000    0.0000\n",
      "   240        0.1818             nan     0.1000   -0.0005\n",
      "   250        0.1739             nan     0.1000    0.0000\n",
      "\n",
      "- Fold02.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1109             nan     0.0100    0.0018\n",
      "     2        1.1074             nan     0.0100    0.0018\n",
      "     3        1.1040             nan     0.0100    0.0018\n",
      "     4        1.1006             nan     0.0100    0.0018\n",
      "     5        1.0971             nan     0.0100    0.0017\n",
      "     6        1.0938             nan     0.0100    0.0017\n",
      "     7        1.0906             nan     0.0100    0.0016\n",
      "     8        1.0875             nan     0.0100    0.0016\n",
      "     9        1.0844             nan     0.0100    0.0016\n",
      "    10        1.0812             nan     0.0100    0.0016\n",
      "    20        1.0530             nan     0.0100    0.0013\n",
      "    40        1.0088             nan     0.0100    0.0010\n",
      "    60        0.9736             nan     0.0100    0.0009\n",
      "    80        0.9437             nan     0.0100    0.0007\n",
      "   100        0.9181             nan     0.0100    0.0006\n",
      "   120        0.8968             nan     0.0100    0.0005\n",
      "   140        0.8783             nan     0.0100    0.0003\n",
      "   160        0.8615             nan     0.0100    0.0003\n",
      "   180        0.8476             nan     0.0100    0.0001\n",
      "   200        0.8353             nan     0.0100    0.0004\n",
      "   220        0.8234             nan     0.0100    0.0001\n",
      "   240        0.8128             nan     0.0100    0.0002\n",
      "   250        0.8077             nan     0.0100    0.0002\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1076             nan     0.0100    0.0030\n",
      "     2        1.1010             nan     0.0100    0.0030\n",
      "     3        1.0943             nan     0.0100    0.0031\n",
      "     4        1.0881             nan     0.0100    0.0030\n",
      "     5        1.0823             nan     0.0100    0.0027\n",
      "     6        1.0765             nan     0.0100    0.0028\n",
      "     7        1.0704             nan     0.0100    0.0030\n",
      "     8        1.0651             nan     0.0100    0.0027\n",
      "     9        1.0594             nan     0.0100    0.0028\n",
      "    10        1.0541             nan     0.0100    0.0026\n",
      "    20        1.0051             nan     0.0100    0.0021\n",
      "    40        0.9278             nan     0.0100    0.0015\n",
      "    60        0.8724             nan     0.0100    0.0011\n",
      "    80        0.8295             nan     0.0100    0.0009\n",
      "   100        0.7952             nan     0.0100    0.0005\n",
      "   120        0.7680             nan     0.0100    0.0004\n",
      "   140        0.7456             nan     0.0100    0.0003\n",
      "   160        0.7268             nan     0.0100    0.0002\n",
      "   180        0.7102             nan     0.0100    0.0003\n",
      "   200        0.6959             nan     0.0100    0.0001\n",
      "   220        0.6830             nan     0.0100    0.0001\n",
      "   240        0.6716             nan     0.0100    0.0001\n",
      "   250        0.6663             nan     0.0100    0.0000\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1065             nan     0.0100    0.0037\n",
      "     2        1.0992             nan     0.0100    0.0034\n",
      "     3        1.0920             nan     0.0100    0.0035\n",
      "     4        1.0851             nan     0.0100    0.0029\n",
      "     5        1.0780             nan     0.0100    0.0030\n",
      "     6        1.0716             nan     0.0100    0.0029\n",
      "     7        1.0648             nan     0.0100    0.0030\n",
      "     8        1.0586             nan     0.0100    0.0028\n",
      "     9        1.0526             nan     0.0100    0.0025\n",
      "    10        1.0463             nan     0.0100    0.0030\n",
      "    20        0.9913             nan     0.0100    0.0024\n",
      "    40        0.9072             nan     0.0100    0.0015\n",
      "    60        0.8445             nan     0.0100    0.0011\n",
      "    80        0.7979             nan     0.0100    0.0010\n",
      "   100        0.7599             nan     0.0100    0.0005\n",
      "   120        0.7295             nan     0.0100    0.0004\n",
      "   140        0.7027             nan     0.0100    0.0003\n",
      "   160        0.6801             nan     0.0100    0.0002\n",
      "   180        0.6601             nan     0.0100    0.0002\n",
      "   200        0.6431             nan     0.0100    0.0001\n",
      "   220        0.6284             nan     0.0100    0.0001\n",
      "   240        0.6144             nan     0.0100    0.0001\n",
      "   250        0.6082             nan     0.0100    0.0001\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0971             nan     0.0500    0.0090\n",
      "     2        1.0812             nan     0.0500    0.0082\n",
      "     3        1.0667             nan     0.0500    0.0074\n",
      "     4        1.0518             nan     0.0500    0.0066\n",
      "     5        1.0389             nan     0.0500    0.0061\n",
      "     6        1.0276             nan     0.0500    0.0052\n",
      "     7        1.0175             nan     0.0500    0.0047\n",
      "     8        1.0085             nan     0.0500    0.0045\n",
      "     9        0.9973             nan     0.0500    0.0053\n",
      "    10        0.9896             nan     0.0500    0.0035\n",
      "    20        0.9189             nan     0.0500    0.0025\n",
      "    40        0.8318             nan     0.0500    0.0008\n",
      "    60        0.7824             nan     0.0500    0.0008\n",
      "    80        0.7499             nan     0.0500    0.0006\n",
      "   100        0.7287             nan     0.0500    0.0005\n",
      "   120        0.7128             nan     0.0500    0.0000\n",
      "   140        0.6992             nan     0.0500    0.0001\n",
      "   160        0.6897             nan     0.0500   -0.0002\n",
      "   180        0.6808             nan     0.0500   -0.0003\n",
      "   200        0.6730             nan     0.0500   -0.0002\n",
      "   220        0.6645             nan     0.0500   -0.0000\n",
      "   240        0.6578             nan     0.0500   -0.0000\n",
      "   250        0.6554             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0807             nan     0.0500    0.0153\n",
      "     2        1.0515             nan     0.0500    0.0143\n",
      "     3        1.0241             nan     0.0500    0.0123\n",
      "     4        1.0003             nan     0.0500    0.0114\n",
      "     5        0.9797             nan     0.0500    0.0098\n",
      "     6        0.9609             nan     0.0500    0.0081\n",
      "     7        0.9438             nan     0.0500    0.0080\n",
      "     8        0.9265             nan     0.0500    0.0088\n",
      "     9        0.9103             nan     0.0500    0.0080\n",
      "    10        0.8966             nan     0.0500    0.0059\n",
      "    20        0.7971             nan     0.0500    0.0037\n",
      "    40        0.6966             nan     0.0500   -0.0000\n",
      "    60        0.6434             nan     0.0500   -0.0002\n",
      "    80        0.6077             nan     0.0500    0.0002\n",
      "   100        0.5813             nan     0.0500   -0.0004\n",
      "   120        0.5583             nan     0.0500    0.0000\n",
      "   140        0.5364             nan     0.0500   -0.0002\n",
      "   160        0.5166             nan     0.0500   -0.0001\n",
      "   180        0.4994             nan     0.0500   -0.0005\n",
      "   200        0.4827             nan     0.0500   -0.0002\n",
      "   220        0.4655             nan     0.0500   -0.0004\n",
      "   240        0.4517             nan     0.0500   -0.0004\n",
      "   250        0.4457             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0772             nan     0.0500    0.0174\n",
      "     2        1.0447             nan     0.0500    0.0150\n",
      "     3        1.0158             nan     0.0500    0.0133\n",
      "     4        0.9899             nan     0.0500    0.0106\n",
      "     5        0.9653             nan     0.0500    0.0116\n",
      "     6        0.9419             nan     0.0500    0.0098\n",
      "     7        0.9223             nan     0.0500    0.0079\n",
      "     8        0.9051             nan     0.0500    0.0078\n",
      "     9        0.8890             nan     0.0500    0.0066\n",
      "    10        0.8725             nan     0.0500    0.0064\n",
      "    20        0.7595             nan     0.0500    0.0019\n",
      "    40        0.6395             nan     0.0500    0.0008\n",
      "    60        0.5773             nan     0.0500   -0.0001\n",
      "    80        0.5330             nan     0.0500   -0.0011\n",
      "   100        0.4971             nan     0.0500   -0.0004\n",
      "   120        0.4657             nan     0.0500   -0.0003\n",
      "   140        0.4381             nan     0.0500    0.0001\n",
      "   160        0.4123             nan     0.0500   -0.0004\n",
      "   180        0.3871             nan     0.0500   -0.0004\n",
      "   200        0.3642             nan     0.0500   -0.0002\n",
      "   220        0.3442             nan     0.0500   -0.0004\n",
      "   240        0.3258             nan     0.0500   -0.0006\n",
      "   250        0.3176             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0789             nan     0.1000    0.0177\n",
      "     2        1.0511             nan     0.1000    0.0144\n",
      "     3        1.0274             nan     0.1000    0.0118\n",
      "     4        1.0067             nan     0.1000    0.0095\n",
      "     5        0.9858             nan     0.1000    0.0095\n",
      "     6        0.9718             nan     0.1000    0.0061\n",
      "     7        0.9532             nan     0.1000    0.0087\n",
      "     8        0.9380             nan     0.1000    0.0075\n",
      "     9        0.9254             nan     0.1000    0.0064\n",
      "    10        0.9154             nan     0.1000    0.0049\n",
      "    20        0.8310             nan     0.1000    0.0037\n",
      "    40        0.7488             nan     0.1000    0.0010\n",
      "    60        0.7118             nan     0.1000   -0.0000\n",
      "    80        0.6902             nan     0.1000   -0.0001\n",
      "   100        0.6715             nan     0.1000    0.0005\n",
      "   120        0.6597             nan     0.1000   -0.0002\n",
      "   140        0.6491             nan     0.1000   -0.0004\n",
      "   160        0.6410             nan     0.1000   -0.0006\n",
      "   180        0.6329             nan     0.1000   -0.0003\n",
      "   200        0.6244             nan     0.1000   -0.0001\n",
      "   220        0.6178             nan     0.1000    0.0000\n",
      "   240        0.6120             nan     0.1000   -0.0003\n",
      "   250        0.6083             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0473             nan     0.1000    0.0317\n",
      "     2        0.9959             nan     0.1000    0.0232\n",
      "     3        0.9566             nan     0.1000    0.0190\n",
      "     4        0.9221             nan     0.1000    0.0166\n",
      "     5        0.8944             nan     0.1000    0.0122\n",
      "     6        0.8676             nan     0.1000    0.0124\n",
      "     7        0.8440             nan     0.1000    0.0115\n",
      "     8        0.8241             nan     0.1000    0.0077\n",
      "     9        0.8073             nan     0.1000    0.0070\n",
      "    10        0.7927             nan     0.1000    0.0059\n",
      "    20        0.6942             nan     0.1000    0.0008\n",
      "    40        0.6075             nan     0.1000   -0.0000\n",
      "    60        0.5591             nan     0.1000   -0.0005\n",
      "    80        0.5228             nan     0.1000   -0.0010\n",
      "   100        0.4884             nan     0.1000   -0.0005\n",
      "   120        0.4566             nan     0.1000   -0.0005\n",
      "   140        0.4304             nan     0.1000   -0.0004\n",
      "   160        0.4057             nan     0.1000   -0.0007\n",
      "   180        0.3816             nan     0.1000   -0.0010\n",
      "   200        0.3604             nan     0.1000   -0.0005\n",
      "   220        0.3398             nan     0.1000   -0.0008\n",
      "   240        0.3218             nan     0.1000    0.0000\n",
      "   250        0.3135             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0428             nan     0.1000    0.0332\n",
      "     2        0.9905             nan     0.1000    0.0239\n",
      "     3        0.9479             nan     0.1000    0.0193\n",
      "     4        0.9073             nan     0.1000    0.0163\n",
      "     5        0.8726             nan     0.1000    0.0133\n",
      "     6        0.8422             nan     0.1000    0.0100\n",
      "     7        0.8180             nan     0.1000    0.0085\n",
      "     8        0.7942             nan     0.1000    0.0089\n",
      "     9        0.7776             nan     0.1000    0.0049\n",
      "    10        0.7631             nan     0.1000    0.0047\n",
      "    20        0.6440             nan     0.1000    0.0020\n",
      "    40        0.5400             nan     0.1000   -0.0006\n",
      "    60        0.4678             nan     0.1000   -0.0007\n",
      "    80        0.4090             nan     0.1000   -0.0003\n",
      "   100        0.3682             nan     0.1000   -0.0006\n",
      "   120        0.3268             nan     0.1000   -0.0009\n",
      "   140        0.2905             nan     0.1000   -0.0004\n",
      "   160        0.2600             nan     0.1000   -0.0005\n",
      "   180        0.2363             nan     0.1000   -0.0007\n",
      "   200        0.2145             nan     0.1000   -0.0007\n",
      "   220        0.1976             nan     0.1000   -0.0006\n",
      "   240        0.1792             nan     0.1000   -0.0005\n",
      "   250        0.1710             nan     0.1000   -0.0008\n",
      "\n",
      "- Fold03.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1110             nan     0.0100    0.0018\n",
      "     2        1.1074             nan     0.0100    0.0018\n",
      "     3        1.1037             nan     0.0100    0.0017\n",
      "     4        1.1003             nan     0.0100    0.0017\n",
      "     5        1.0970             nan     0.0100    0.0017\n",
      "     6        1.0937             nan     0.0100    0.0016\n",
      "     7        1.0905             nan     0.0100    0.0016\n",
      "     8        1.0873             nan     0.0100    0.0016\n",
      "     9        1.0846             nan     0.0100    0.0015\n",
      "    10        1.0814             nan     0.0100    0.0015\n",
      "    20        1.0538             nan     0.0100    0.0013\n",
      "    40        1.0107             nan     0.0100    0.0010\n",
      "    60        0.9773             nan     0.0100    0.0005\n",
      "    80        0.9490             nan     0.0100    0.0007\n",
      "   100        0.9242             nan     0.0100    0.0006\n",
      "   120        0.9034             nan     0.0100    0.0005\n",
      "   140        0.8855             nan     0.0100    0.0003\n",
      "   160        0.8694             nan     0.0100    0.0003\n",
      "   180        0.8561             nan     0.0100    0.0003\n",
      "   200        0.8425             nan     0.0100    0.0003\n",
      "   220        0.8302             nan     0.0100    0.0002\n",
      "   240        0.8202             nan     0.0100    0.0002\n",
      "   250        0.8156             nan     0.0100    0.0001\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1079             nan     0.0100    0.0031\n",
      "     2        1.1015             nan     0.0100    0.0031\n",
      "     3        1.0952             nan     0.0100    0.0028\n",
      "     4        1.0896             nan     0.0100    0.0029\n",
      "     5        1.0833             nan     0.0100    0.0029\n",
      "     6        1.0783             nan     0.0100    0.0025\n",
      "     7        1.0726             nan     0.0100    0.0024\n",
      "     8        1.0674             nan     0.0100    0.0026\n",
      "     9        1.0619             nan     0.0100    0.0026\n",
      "    10        1.0566             nan     0.0100    0.0026\n",
      "    20        1.0086             nan     0.0100    0.0020\n",
      "    40        0.9353             nan     0.0100    0.0011\n",
      "    60        0.8805             nan     0.0100    0.0010\n",
      "    80        0.8394             nan     0.0100    0.0007\n",
      "   100        0.8059             nan     0.0100    0.0006\n",
      "   120        0.7796             nan     0.0100    0.0005\n",
      "   140        0.7573             nan     0.0100    0.0003\n",
      "   160        0.7385             nan     0.0100    0.0001\n",
      "   180        0.7229             nan     0.0100    0.0002\n",
      "   200        0.7091             nan     0.0100    0.0001\n",
      "   220        0.6972             nan     0.0100    0.0001\n",
      "   240        0.6865             nan     0.0100    0.0001\n",
      "   250        0.6809             nan     0.0100    0.0001\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1080             nan     0.0100    0.0033\n",
      "     2        1.1008             nan     0.0100    0.0033\n",
      "     3        1.0933             nan     0.0100    0.0030\n",
      "     4        1.0865             nan     0.0100    0.0030\n",
      "     5        1.0800             nan     0.0100    0.0029\n",
      "     6        1.0737             nan     0.0100    0.0029\n",
      "     7        1.0677             nan     0.0100    0.0028\n",
      "     8        1.0616             nan     0.0100    0.0026\n",
      "     9        1.0556             nan     0.0100    0.0029\n",
      "    10        1.0497             nan     0.0100    0.0026\n",
      "    20        0.9967             nan     0.0100    0.0023\n",
      "    40        0.9146             nan     0.0100    0.0014\n",
      "    60        0.8547             nan     0.0100    0.0010\n",
      "    80        0.8072             nan     0.0100    0.0009\n",
      "   100        0.7703             nan     0.0100    0.0006\n",
      "   120        0.7394             nan     0.0100    0.0005\n",
      "   140        0.7148             nan     0.0100    0.0003\n",
      "   160        0.6926             nan     0.0100    0.0001\n",
      "   180        0.6730             nan     0.0100    0.0002\n",
      "   200        0.6564             nan     0.0100    0.0001\n",
      "   220        0.6406             nan     0.0100    0.0001\n",
      "   240        0.6272             nan     0.0100    0.0001\n",
      "   250        0.6213             nan     0.0100    0.0001\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0955             nan     0.0500    0.0088\n",
      "     2        1.0804             nan     0.0500    0.0079\n",
      "     3        1.0667             nan     0.0500    0.0072\n",
      "     4        1.0538             nan     0.0500    0.0066\n",
      "     5        1.0413             nan     0.0500    0.0060\n",
      "     6        1.0313             nan     0.0500    0.0038\n",
      "     7        1.0223             nan     0.0500    0.0037\n",
      "     8        1.0116             nan     0.0500    0.0053\n",
      "     9        1.0014             nan     0.0500    0.0044\n",
      "    10        0.9919             nan     0.0500    0.0048\n",
      "    20        0.9223             nan     0.0500    0.0030\n",
      "    40        0.8411             nan     0.0500    0.0014\n",
      "    60        0.7946             nan     0.0500    0.0009\n",
      "    80        0.7623             nan     0.0500    0.0003\n",
      "   100        0.7394             nan     0.0500    0.0000\n",
      "   120        0.7224             nan     0.0500    0.0006\n",
      "   140        0.7113             nan     0.0500   -0.0001\n",
      "   160        0.7014             nan     0.0500    0.0000\n",
      "   180        0.6927             nan     0.0500   -0.0000\n",
      "   200        0.6846             nan     0.0500   -0.0000\n",
      "   220        0.6768             nan     0.0500   -0.0000\n",
      "   240        0.6713             nan     0.0500   -0.0000\n",
      "   250        0.6679             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0847             nan     0.0500    0.0140\n",
      "     2        1.0550             nan     0.0500    0.0133\n",
      "     3        1.0295             nan     0.0500    0.0120\n",
      "     4        1.0062             nan     0.0500    0.0112\n",
      "     5        0.9843             nan     0.0500    0.0107\n",
      "     6        0.9660             nan     0.0500    0.0088\n",
      "     7        0.9487             nan     0.0500    0.0078\n",
      "     8        0.9317             nan     0.0500    0.0077\n",
      "     9        0.9166             nan     0.0500    0.0067\n",
      "    10        0.9031             nan     0.0500    0.0063\n",
      "    20        0.8082             nan     0.0500    0.0037\n",
      "    40        0.7071             nan     0.0500    0.0005\n",
      "    60        0.6531             nan     0.0500    0.0002\n",
      "    80        0.6178             nan     0.0500    0.0002\n",
      "   100        0.5912             nan     0.0500   -0.0006\n",
      "   120        0.5692             nan     0.0500   -0.0002\n",
      "   140        0.5484             nan     0.0500   -0.0000\n",
      "   160        0.5288             nan     0.0500   -0.0006\n",
      "   180        0.5116             nan     0.0500   -0.0003\n",
      "   200        0.4970             nan     0.0500   -0.0002\n",
      "   220        0.4814             nan     0.0500   -0.0005\n",
      "   240        0.4672             nan     0.0500   -0.0005\n",
      "   250        0.4612             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0781             nan     0.0500    0.0164\n",
      "     2        1.0473             nan     0.0500    0.0138\n",
      "     3        1.0201             nan     0.0500    0.0126\n",
      "     4        0.9964             nan     0.0500    0.0103\n",
      "     5        0.9723             nan     0.0500    0.0104\n",
      "     6        0.9497             nan     0.0500    0.0107\n",
      "     7        0.9296             nan     0.0500    0.0094\n",
      "     8        0.9118             nan     0.0500    0.0074\n",
      "     9        0.8944             nan     0.0500    0.0077\n",
      "    10        0.8789             nan     0.0500    0.0062\n",
      "    20        0.7714             nan     0.0500    0.0026\n",
      "    40        0.6560             nan     0.0500    0.0005\n",
      "    60        0.5938             nan     0.0500   -0.0000\n",
      "    80        0.5462             nan     0.0500   -0.0001\n",
      "   100        0.5096             nan     0.0500    0.0002\n",
      "   120        0.4769             nan     0.0500   -0.0005\n",
      "   140        0.4440             nan     0.0500   -0.0002\n",
      "   160        0.4179             nan     0.0500   -0.0002\n",
      "   180        0.3938             nan     0.0500    0.0001\n",
      "   200        0.3707             nan     0.0500   -0.0004\n",
      "   220        0.3494             nan     0.0500   -0.0002\n",
      "   240        0.3306             nan     0.0500   -0.0001\n",
      "   250        0.3214             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0820             nan     0.1000    0.0174\n",
      "     2        1.0546             nan     0.1000    0.0143\n",
      "     3        1.0352             nan     0.1000    0.0096\n",
      "     4        1.0130             nan     0.1000    0.0115\n",
      "     5        0.9939             nan     0.1000    0.0095\n",
      "     6        0.9771             nan     0.1000    0.0084\n",
      "     7        0.9609             nan     0.1000    0.0075\n",
      "     8        0.9477             nan     0.1000    0.0061\n",
      "     9        0.9359             nan     0.1000    0.0053\n",
      "    10        0.9271             nan     0.1000    0.0035\n",
      "    20        0.8413             nan     0.1000    0.0016\n",
      "    40        0.7621             nan     0.1000   -0.0001\n",
      "    60        0.7235             nan     0.1000   -0.0003\n",
      "    80        0.7036             nan     0.1000   -0.0000\n",
      "   100        0.6876             nan     0.1000   -0.0002\n",
      "   120        0.6731             nan     0.1000   -0.0007\n",
      "   140        0.6630             nan     0.1000   -0.0000\n",
      "   160        0.6531             nan     0.1000   -0.0006\n",
      "   180        0.6451             nan     0.1000   -0.0003\n",
      "   200        0.6376             nan     0.1000   -0.0004\n",
      "   220        0.6317             nan     0.1000   -0.0001\n",
      "   240        0.6278             nan     0.1000   -0.0005\n",
      "   250        0.6248             nan     0.1000   -0.0000\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0507             nan     0.1000    0.0305\n",
      "     2        1.0012             nan     0.1000    0.0239\n",
      "     3        0.9601             nan     0.1000    0.0179\n",
      "     4        0.9305             nan     0.1000    0.0134\n",
      "     5        0.9021             nan     0.1000    0.0127\n",
      "     6        0.8777             nan     0.1000    0.0109\n",
      "     7        0.8520             nan     0.1000    0.0116\n",
      "     8        0.8335             nan     0.1000    0.0081\n",
      "     9        0.8152             nan     0.1000    0.0078\n",
      "    10        0.7986             nan     0.1000    0.0067\n",
      "    20        0.7085             nan     0.1000    0.0010\n",
      "    40        0.6236             nan     0.1000   -0.0002\n",
      "    60        0.5737             nan     0.1000   -0.0009\n",
      "    80        0.5351             nan     0.1000   -0.0000\n",
      "   100        0.5015             nan     0.1000   -0.0006\n",
      "   120        0.4707             nan     0.1000   -0.0005\n",
      "   140        0.4427             nan     0.1000   -0.0009\n",
      "   160        0.4173             nan     0.1000   -0.0001\n",
      "   180        0.3938             nan     0.1000    0.0002\n",
      "   200        0.3718             nan     0.1000   -0.0008\n",
      "   220        0.3532             nan     0.1000   -0.0007\n",
      "   240        0.3330             nan     0.1000   -0.0003\n",
      "   250        0.3252             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0449             nan     0.1000    0.0314\n",
      "     2        0.9925             nan     0.1000    0.0228\n",
      "     3        0.9457             nan     0.1000    0.0201\n",
      "     4        0.9053             nan     0.1000    0.0151\n",
      "     5        0.8737             nan     0.1000    0.0137\n",
      "     6        0.8473             nan     0.1000    0.0099\n",
      "     7        0.8203             nan     0.1000    0.0116\n",
      "     8        0.8000             nan     0.1000    0.0060\n",
      "     9        0.7817             nan     0.1000    0.0066\n",
      "    10        0.7651             nan     0.1000    0.0050\n",
      "    20        0.6548             nan     0.1000    0.0002\n",
      "    40        0.5459             nan     0.1000   -0.0006\n",
      "    60        0.4748             nan     0.1000    0.0001\n",
      "    80        0.4276             nan     0.1000   -0.0006\n",
      "   100        0.3812             nan     0.1000   -0.0008\n",
      "   120        0.3448             nan     0.1000   -0.0013\n",
      "   140        0.3122             nan     0.1000   -0.0004\n",
      "   160        0.2810             nan     0.1000   -0.0004\n",
      "   180        0.2548             nan     0.1000   -0.0007\n",
      "   200        0.2302             nan     0.1000   -0.0003\n",
      "   220        0.2070             nan     0.1000   -0.0002\n",
      "   240        0.1874             nan     0.1000   -0.0002\n",
      "   250        0.1786             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold04.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1107             nan     0.0100    0.0019\n",
      "     2        1.1071             nan     0.0100    0.0019\n",
      "     3        1.1035             nan     0.0100    0.0019\n",
      "     4        1.0996             nan     0.0100    0.0018\n",
      "     5        1.0960             nan     0.0100    0.0018\n",
      "     6        1.0922             nan     0.0100    0.0017\n",
      "     7        1.0886             nan     0.0100    0.0017\n",
      "     8        1.0853             nan     0.0100    0.0017\n",
      "     9        1.0823             nan     0.0100    0.0016\n",
      "    10        1.0790             nan     0.0100    0.0016\n",
      "    20        1.0505             nan     0.0100    0.0013\n",
      "    40        1.0062             nan     0.0100    0.0011\n",
      "    60        0.9699             nan     0.0100    0.0009\n",
      "    80        0.9407             nan     0.0100    0.0004\n",
      "   100        0.9161             nan     0.0100    0.0007\n",
      "   120        0.8939             nan     0.0100    0.0004\n",
      "   140        0.8739             nan     0.0100    0.0004\n",
      "   160        0.8574             nan     0.0100    0.0004\n",
      "   180        0.8434             nan     0.0100    0.0002\n",
      "   200        0.8300             nan     0.0100    0.0004\n",
      "   220        0.8181             nan     0.0100    0.0002\n",
      "   240        0.8080             nan     0.0100    0.0002\n",
      "   250        0.8026             nan     0.0100    0.0003\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0033\n",
      "     2        1.1009             nan     0.0100    0.0033\n",
      "     3        1.0942             nan     0.0100    0.0033\n",
      "     4        1.0879             nan     0.0100    0.0030\n",
      "     5        1.0817             nan     0.0100    0.0028\n",
      "     6        1.0756             nan     0.0100    0.0029\n",
      "     7        1.0699             nan     0.0100    0.0028\n",
      "     8        1.0642             nan     0.0100    0.0028\n",
      "     9        1.0585             nan     0.0100    0.0028\n",
      "    10        1.0533             nan     0.0100    0.0026\n",
      "    20        1.0040             nan     0.0100    0.0020\n",
      "    40        0.9271             nan     0.0100    0.0014\n",
      "    60        0.8701             nan     0.0100    0.0014\n",
      "    80        0.8277             nan     0.0100    0.0009\n",
      "   100        0.7934             nan     0.0100    0.0006\n",
      "   120        0.7660             nan     0.0100    0.0006\n",
      "   140        0.7436             nan     0.0100    0.0004\n",
      "   160        0.7240             nan     0.0100    0.0003\n",
      "   180        0.7077             nan     0.0100    0.0002\n",
      "   200        0.6927             nan     0.0100    0.0003\n",
      "   220        0.6804             nan     0.0100    0.0002\n",
      "   240        0.6695             nan     0.0100    0.0001\n",
      "   250        0.6643             nan     0.0100    0.0001\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1072             nan     0.0100    0.0032\n",
      "     2        1.1009             nan     0.0100    0.0029\n",
      "     3        1.0935             nan     0.0100    0.0032\n",
      "     4        1.0866             nan     0.0100    0.0034\n",
      "     5        1.0801             nan     0.0100    0.0031\n",
      "     6        1.0735             nan     0.0100    0.0029\n",
      "     7        1.0673             nan     0.0100    0.0029\n",
      "     8        1.0609             nan     0.0100    0.0028\n",
      "     9        1.0546             nan     0.0100    0.0027\n",
      "    10        1.0490             nan     0.0100    0.0025\n",
      "    20        0.9938             nan     0.0100    0.0022\n",
      "    40        0.9096             nan     0.0100    0.0015\n",
      "    60        0.8472             nan     0.0100    0.0011\n",
      "    80        0.7975             nan     0.0100    0.0011\n",
      "   100        0.7600             nan     0.0100    0.0004\n",
      "   120        0.7282             nan     0.0100    0.0005\n",
      "   140        0.7008             nan     0.0100    0.0004\n",
      "   160        0.6790             nan     0.0100    0.0002\n",
      "   180        0.6582             nan     0.0100    0.0002\n",
      "   200        0.6401             nan     0.0100    0.0002\n",
      "   220        0.6237             nan     0.0100    0.0002\n",
      "   240        0.6099             nan     0.0100    0.0000\n",
      "   250        0.6036             nan     0.0100    0.0001\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0957             nan     0.0500    0.0095\n",
      "     2        1.0779             nan     0.0500    0.0085\n",
      "     3        1.0629             nan     0.0500    0.0077\n",
      "     4        1.0497             nan     0.0500    0.0069\n",
      "     5        1.0368             nan     0.0500    0.0064\n",
      "     6        1.0244             nan     0.0500    0.0056\n",
      "     7        1.0144             nan     0.0500    0.0050\n",
      "     8        1.0046             nan     0.0500    0.0048\n",
      "     9        0.9940             nan     0.0500    0.0051\n",
      "    10        0.9860             nan     0.0500    0.0040\n",
      "    20        0.9131             nan     0.0500    0.0032\n",
      "    40        0.8318             nan     0.0500    0.0012\n",
      "    60        0.7791             nan     0.0500    0.0011\n",
      "    80        0.7489             nan     0.0500    0.0005\n",
      "   100        0.7241             nan     0.0500    0.0002\n",
      "   120        0.7084             nan     0.0500    0.0002\n",
      "   140        0.6948             nan     0.0500    0.0002\n",
      "   160        0.6856             nan     0.0500   -0.0002\n",
      "   180        0.6755             nan     0.0500   -0.0001\n",
      "   200        0.6677             nan     0.0500   -0.0001\n",
      "   220        0.6618             nan     0.0500   -0.0001\n",
      "   240        0.6562             nan     0.0500   -0.0001\n",
      "   250        0.6537             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0822             nan     0.0500    0.0159\n",
      "     2        1.0527             nan     0.0500    0.0147\n",
      "     3        1.0249             nan     0.0500    0.0124\n",
      "     4        1.0018             nan     0.0500    0.0111\n",
      "     5        0.9807             nan     0.0500    0.0102\n",
      "     6        0.9612             nan     0.0500    0.0085\n",
      "     7        0.9428             nan     0.0500    0.0079\n",
      "     8        0.9273             nan     0.0500    0.0072\n",
      "     9        0.9121             nan     0.0500    0.0062\n",
      "    10        0.8961             nan     0.0500    0.0075\n",
      "    20        0.7937             nan     0.0500    0.0030\n",
      "    40        0.6961             nan     0.0500    0.0013\n",
      "    60        0.6420             nan     0.0500    0.0003\n",
      "    80        0.6051             nan     0.0500   -0.0001\n",
      "   100        0.5767             nan     0.0500   -0.0001\n",
      "   120        0.5531             nan     0.0500   -0.0002\n",
      "   140        0.5323             nan     0.0500   -0.0003\n",
      "   160        0.5125             nan     0.0500   -0.0004\n",
      "   180        0.4968             nan     0.0500   -0.0004\n",
      "   200        0.4784             nan     0.0500   -0.0004\n",
      "   220        0.4619             nan     0.0500   -0.0003\n",
      "   240        0.4469             nan     0.0500   -0.0003\n",
      "   250        0.4391             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0775             nan     0.0500    0.0178\n",
      "     2        1.0430             nan     0.0500    0.0155\n",
      "     3        1.0147             nan     0.0500    0.0118\n",
      "     4        0.9895             nan     0.0500    0.0116\n",
      "     5        0.9642             nan     0.0500    0.0111\n",
      "     6        0.9431             nan     0.0500    0.0101\n",
      "     7        0.9224             nan     0.0500    0.0096\n",
      "     8        0.9036             nan     0.0500    0.0083\n",
      "     9        0.8868             nan     0.0500    0.0074\n",
      "    10        0.8713             nan     0.0500    0.0054\n",
      "    20        0.7573             nan     0.0500    0.0036\n",
      "    40        0.6403             nan     0.0500    0.0006\n",
      "    60        0.5770             nan     0.0500   -0.0003\n",
      "    80        0.5274             nan     0.0500   -0.0001\n",
      "   100        0.4898             nan     0.0500   -0.0002\n",
      "   120        0.4558             nan     0.0500   -0.0003\n",
      "   140        0.4266             nan     0.0500   -0.0001\n",
      "   160        0.4003             nan     0.0500   -0.0001\n",
      "   180        0.3746             nan     0.0500    0.0001\n",
      "   200        0.3527             nan     0.0500   -0.0001\n",
      "   220        0.3335             nan     0.0500   -0.0004\n",
      "   240        0.3153             nan     0.0500   -0.0005\n",
      "   250        0.3061             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0752             nan     0.1000    0.0181\n",
      "     2        1.0522             nan     0.1000    0.0103\n",
      "     3        1.0249             nan     0.1000    0.0144\n",
      "     4        0.9994             nan     0.1000    0.0117\n",
      "     5        0.9792             nan     0.1000    0.0094\n",
      "     6        0.9606             nan     0.1000    0.0086\n",
      "     7        0.9489             nan     0.1000    0.0050\n",
      "     8        0.9323             nan     0.1000    0.0072\n",
      "     9        0.9181             nan     0.1000    0.0065\n",
      "    10        0.9099             nan     0.1000    0.0035\n",
      "    20        0.8253             nan     0.1000    0.0032\n",
      "    40        0.7472             nan     0.1000    0.0011\n",
      "    60        0.7059             nan     0.1000    0.0005\n",
      "    80        0.6837             nan     0.1000   -0.0000\n",
      "   100        0.6676             nan     0.1000    0.0000\n",
      "   120        0.6539             nan     0.1000   -0.0004\n",
      "   140        0.6442             nan     0.1000   -0.0002\n",
      "   160        0.6353             nan     0.1000   -0.0002\n",
      "   180        0.6288             nan     0.1000   -0.0002\n",
      "   200        0.6208             nan     0.1000   -0.0002\n",
      "   220        0.6145             nan     0.1000   -0.0001\n",
      "   240        0.6097             nan     0.1000   -0.0006\n",
      "   250        0.6075             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0482             nan     0.1000    0.0293\n",
      "     2        0.9989             nan     0.1000    0.0235\n",
      "     3        0.9549             nan     0.1000    0.0203\n",
      "     4        0.9218             nan     0.1000    0.0143\n",
      "     5        0.8907             nan     0.1000    0.0133\n",
      "     6        0.8637             nan     0.1000    0.0121\n",
      "     7        0.8450             nan     0.1000    0.0075\n",
      "     8        0.8239             nan     0.1000    0.0094\n",
      "     9        0.8089             nan     0.1000    0.0054\n",
      "    10        0.7940             nan     0.1000    0.0060\n",
      "    20        0.6894             nan     0.1000    0.0022\n",
      "    40        0.6060             nan     0.1000   -0.0005\n",
      "    60        0.5593             nan     0.1000   -0.0006\n",
      "    80        0.5204             nan     0.1000   -0.0011\n",
      "   100        0.4861             nan     0.1000   -0.0004\n",
      "   120        0.4564             nan     0.1000   -0.0006\n",
      "   140        0.4254             nan     0.1000   -0.0007\n",
      "   160        0.4005             nan     0.1000    0.0003\n",
      "   180        0.3754             nan     0.1000   -0.0007\n",
      "   200        0.3545             nan     0.1000   -0.0007\n",
      "   220        0.3357             nan     0.1000   -0.0003\n",
      "   240        0.3181             nan     0.1000   -0.0010\n",
      "   250        0.3081             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0437             nan     0.1000    0.0328\n",
      "     2        0.9898             nan     0.1000    0.0232\n",
      "     3        0.9471             nan     0.1000    0.0196\n",
      "     4        0.9068             nan     0.1000    0.0140\n",
      "     5        0.8737             nan     0.1000    0.0133\n",
      "     6        0.8425             nan     0.1000    0.0131\n",
      "     7        0.8168             nan     0.1000    0.0103\n",
      "     8        0.7939             nan     0.1000    0.0086\n",
      "     9        0.7752             nan     0.1000    0.0067\n",
      "    10        0.7555             nan     0.1000    0.0059\n",
      "    20        0.6416             nan     0.1000    0.0010\n",
      "    40        0.5359             nan     0.1000    0.0001\n",
      "    60        0.4678             nan     0.1000   -0.0005\n",
      "    80        0.4142             nan     0.1000   -0.0013\n",
      "   100        0.3644             nan     0.1000   -0.0009\n",
      "   120        0.3228             nan     0.1000   -0.0007\n",
      "   140        0.2897             nan     0.1000   -0.0003\n",
      "   160        0.2620             nan     0.1000   -0.0007\n",
      "   180        0.2363             nan     0.1000   -0.0004\n",
      "   200        0.2151             nan     0.1000   -0.0002\n",
      "   220        0.1949             nan     0.1000   -0.0005\n",
      "   240        0.1756             nan     0.1000   -0.0004\n",
      "   250        0.1674             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold05.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1109             nan     0.0100    0.0017\n",
      "     2        1.1075             nan     0.0100    0.0017\n",
      "     3        1.1042             nan     0.0100    0.0017\n",
      "     4        1.1011             nan     0.0100    0.0016\n",
      "     5        1.0977             nan     0.0100    0.0016\n",
      "     6        1.0942             nan     0.0100    0.0015\n",
      "     7        1.0908             nan     0.0100    0.0015\n",
      "     8        1.0878             nan     0.0100    0.0015\n",
      "     9        1.0848             nan     0.0100    0.0015\n",
      "    10        1.0818             nan     0.0100    0.0014\n",
      "    20        1.0562             nan     0.0100    0.0012\n",
      "    40        1.0151             nan     0.0100    0.0006\n",
      "    60        0.9823             nan     0.0100    0.0006\n",
      "    80        0.9538             nan     0.0100    0.0006\n",
      "   100        0.9295             nan     0.0100    0.0006\n",
      "   120        0.9086             nan     0.0100    0.0005\n",
      "   140        0.8915             nan     0.0100    0.0004\n",
      "   160        0.8759             nan     0.0100    0.0004\n",
      "   180        0.8618             nan     0.0100    0.0003\n",
      "   200        0.8496             nan     0.0100    0.0002\n",
      "   220        0.8388             nan     0.0100    0.0003\n",
      "   240        0.8283             nan     0.0100    0.0002\n",
      "   250        0.8232             nan     0.0100    0.0002\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1080             nan     0.0100    0.0031\n",
      "     2        1.1013             nan     0.0100    0.0029\n",
      "     3        1.0952             nan     0.0100    0.0031\n",
      "     4        1.0896             nan     0.0100    0.0028\n",
      "     5        1.0834             nan     0.0100    0.0028\n",
      "     6        1.0775             nan     0.0100    0.0027\n",
      "     7        1.0717             nan     0.0100    0.0026\n",
      "     8        1.0665             nan     0.0100    0.0024\n",
      "     9        1.0615             nan     0.0100    0.0024\n",
      "    10        1.0568             nan     0.0100    0.0022\n",
      "    20        1.0088             nan     0.0100    0.0018\n",
      "    40        0.9380             nan     0.0100    0.0013\n",
      "    60        0.8844             nan     0.0100    0.0011\n",
      "    80        0.8430             nan     0.0100    0.0008\n",
      "   100        0.8107             nan     0.0100    0.0006\n",
      "   120        0.7845             nan     0.0100    0.0005\n",
      "   140        0.7620             nan     0.0100    0.0003\n",
      "   160        0.7431             nan     0.0100    0.0002\n",
      "   180        0.7278             nan     0.0100    0.0002\n",
      "   200        0.7141             nan     0.0100    0.0002\n",
      "   220        0.7019             nan     0.0100    0.0001\n",
      "   240        0.6916             nan     0.0100    0.0000\n",
      "   250        0.6859             nan     0.0100    0.0001\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1074             nan     0.0100    0.0032\n",
      "     2        1.1005             nan     0.0100    0.0035\n",
      "     3        1.0929             nan     0.0100    0.0033\n",
      "     4        1.0860             nan     0.0100    0.0031\n",
      "     5        1.0795             nan     0.0100    0.0030\n",
      "     6        1.0735             nan     0.0100    0.0027\n",
      "     7        1.0679             nan     0.0100    0.0024\n",
      "     8        1.0619             nan     0.0100    0.0027\n",
      "     9        1.0559             nan     0.0100    0.0024\n",
      "    10        1.0501             nan     0.0100    0.0027\n",
      "    20        0.9973             nan     0.0100    0.0021\n",
      "    40        0.9155             nan     0.0100    0.0012\n",
      "    60        0.8558             nan     0.0100    0.0011\n",
      "    80        0.8092             nan     0.0100    0.0009\n",
      "   100        0.7721             nan     0.0100    0.0005\n",
      "   120        0.7426             nan     0.0100    0.0004\n",
      "   140        0.7179             nan     0.0100    0.0003\n",
      "   160        0.6974             nan     0.0100    0.0003\n",
      "   180        0.6788             nan     0.0100    0.0002\n",
      "   200        0.6615             nan     0.0100    0.0003\n",
      "   220        0.6472             nan     0.0100    0.0000\n",
      "   240        0.6337             nan     0.0100    0.0002\n",
      "   250        0.6277             nan     0.0100    0.0001\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0994             nan     0.0500    0.0084\n",
      "     2        1.0871             nan     0.0500    0.0053\n",
      "     3        1.0709             nan     0.0500    0.0076\n",
      "     4        1.0569             nan     0.0500    0.0068\n",
      "     5        1.0424             nan     0.0500    0.0056\n",
      "     6        1.0304             nan     0.0500    0.0053\n",
      "     7        1.0199             nan     0.0500    0.0049\n",
      "     8        1.0111             nan     0.0500    0.0049\n",
      "     9        1.0013             nan     0.0500    0.0046\n",
      "    10        0.9925             nan     0.0500    0.0040\n",
      "    20        0.9247             nan     0.0500    0.0020\n",
      "    40        0.8466             nan     0.0500    0.0011\n",
      "    60        0.7999             nan     0.0500    0.0009\n",
      "    80        0.7705             nan     0.0500    0.0002\n",
      "   100        0.7482             nan     0.0500    0.0001\n",
      "   120        0.7333             nan     0.0500    0.0001\n",
      "   140        0.7199             nan     0.0500   -0.0000\n",
      "   160        0.7098             nan     0.0500    0.0000\n",
      "   180        0.7007             nan     0.0500    0.0000\n",
      "   200        0.6937             nan     0.0500   -0.0004\n",
      "   220        0.6867             nan     0.0500   -0.0000\n",
      "   240        0.6794             nan     0.0500   -0.0000\n",
      "   250        0.6767             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0822             nan     0.0500    0.0141\n",
      "     2        1.0540             nan     0.0500    0.0130\n",
      "     3        1.0298             nan     0.0500    0.0122\n",
      "     4        1.0070             nan     0.0500    0.0105\n",
      "     5        0.9870             nan     0.0500    0.0087\n",
      "     6        0.9686             nan     0.0500    0.0087\n",
      "     7        0.9499             nan     0.0500    0.0081\n",
      "     8        0.9343             nan     0.0500    0.0075\n",
      "     9        0.9185             nan     0.0500    0.0072\n",
      "    10        0.9059             nan     0.0500    0.0053\n",
      "    20        0.8089             nan     0.0500    0.0036\n",
      "    40        0.7145             nan     0.0500    0.0004\n",
      "    60        0.6610             nan     0.0500   -0.0001\n",
      "    80        0.6273             nan     0.0500   -0.0006\n",
      "   100        0.6007             nan     0.0500   -0.0004\n",
      "   120        0.5762             nan     0.0500   -0.0002\n",
      "   140        0.5565             nan     0.0500   -0.0009\n",
      "   160        0.5370             nan     0.0500   -0.0003\n",
      "   180        0.5202             nan     0.0500   -0.0008\n",
      "   200        0.5043             nan     0.0500   -0.0006\n",
      "   220        0.4879             nan     0.0500   -0.0002\n",
      "   240        0.4735             nan     0.0500   -0.0004\n",
      "   250        0.4677             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0782             nan     0.0500    0.0163\n",
      "     2        1.0454             nan     0.0500    0.0133\n",
      "     3        1.0174             nan     0.0500    0.0124\n",
      "     4        0.9942             nan     0.0500    0.0110\n",
      "     5        0.9721             nan     0.0500    0.0096\n",
      "     6        0.9508             nan     0.0500    0.0098\n",
      "     7        0.9319             nan     0.0500    0.0085\n",
      "     8        0.9133             nan     0.0500    0.0082\n",
      "     9        0.8947             nan     0.0500    0.0079\n",
      "    10        0.8813             nan     0.0500    0.0056\n",
      "    20        0.7713             nan     0.0500    0.0030\n",
      "    40        0.6626             nan     0.0500    0.0005\n",
      "    60        0.5966             nan     0.0500    0.0002\n",
      "    80        0.5531             nan     0.0500   -0.0004\n",
      "   100        0.5145             nan     0.0500   -0.0002\n",
      "   120        0.4840             nan     0.0500   -0.0004\n",
      "   140        0.4546             nan     0.0500   -0.0004\n",
      "   160        0.4296             nan     0.0500   -0.0009\n",
      "   180        0.4064             nan     0.0500   -0.0005\n",
      "   200        0.3830             nan     0.0500   -0.0007\n",
      "   220        0.3637             nan     0.0500   -0.0002\n",
      "   240        0.3452             nan     0.0500   -0.0002\n",
      "   250        0.3369             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0833             nan     0.1000    0.0165\n",
      "     2        1.0603             nan     0.1000    0.0088\n",
      "     3        1.0360             nan     0.1000    0.0132\n",
      "     4        1.0128             nan     0.1000    0.0108\n",
      "     5        0.9942             nan     0.1000    0.0086\n",
      "     6        0.9782             nan     0.1000    0.0063\n",
      "     7        0.9656             nan     0.1000    0.0064\n",
      "     8        0.9477             nan     0.1000    0.0083\n",
      "     9        0.9361             nan     0.1000    0.0045\n",
      "    10        0.9220             nan     0.1000    0.0064\n",
      "    20        0.8456             nan     0.1000    0.0030\n",
      "    40        0.7696             nan     0.1000    0.0010\n",
      "    60        0.7311             nan     0.1000    0.0004\n",
      "    80        0.7083             nan     0.1000    0.0003\n",
      "   100        0.6916             nan     0.1000   -0.0001\n",
      "   120        0.6782             nan     0.1000    0.0003\n",
      "   140        0.6668             nan     0.1000    0.0001\n",
      "   160        0.6581             nan     0.1000   -0.0003\n",
      "   180        0.6489             nan     0.1000   -0.0005\n",
      "   200        0.6437             nan     0.1000   -0.0003\n",
      "   220        0.6375             nan     0.1000   -0.0004\n",
      "   240        0.6322             nan     0.1000   -0.0002\n",
      "   250        0.6295             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0563             nan     0.1000    0.0304\n",
      "     2        1.0103             nan     0.1000    0.0191\n",
      "     3        0.9679             nan     0.1000    0.0171\n",
      "     4        0.9321             nan     0.1000    0.0171\n",
      "     5        0.9030             nan     0.1000    0.0131\n",
      "     6        0.8801             nan     0.1000    0.0096\n",
      "     7        0.8567             nan     0.1000    0.0102\n",
      "     8        0.8370             nan     0.1000    0.0077\n",
      "     9        0.8203             nan     0.1000    0.0068\n",
      "    10        0.8076             nan     0.1000    0.0038\n",
      "    20        0.7121             nan     0.1000    0.0024\n",
      "    40        0.6271             nan     0.1000    0.0001\n",
      "    60        0.5773             nan     0.1000   -0.0005\n",
      "    80        0.5398             nan     0.1000   -0.0005\n",
      "   100        0.5060             nan     0.1000   -0.0006\n",
      "   120        0.4750             nan     0.1000   -0.0009\n",
      "   140        0.4459             nan     0.1000   -0.0007\n",
      "   160        0.4216             nan     0.1000   -0.0006\n",
      "   180        0.3992             nan     0.1000   -0.0004\n",
      "   200        0.3791             nan     0.1000   -0.0009\n",
      "   220        0.3571             nan     0.1000   -0.0007\n",
      "   240        0.3378             nan     0.1000   -0.0006\n",
      "   250        0.3297             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0485             nan     0.1000    0.0296\n",
      "     2        0.9930             nan     0.1000    0.0265\n",
      "     3        0.9530             nan     0.1000    0.0181\n",
      "     4        0.9190             nan     0.1000    0.0144\n",
      "     5        0.8831             nan     0.1000    0.0152\n",
      "     6        0.8571             nan     0.1000    0.0102\n",
      "     7        0.8314             nan     0.1000    0.0100\n",
      "     8        0.8087             nan     0.1000    0.0090\n",
      "     9        0.7902             nan     0.1000    0.0058\n",
      "    10        0.7745             nan     0.1000    0.0053\n",
      "    20        0.6634             nan     0.1000    0.0025\n",
      "    40        0.5619             nan     0.1000   -0.0002\n",
      "    60        0.4912             nan     0.1000   -0.0001\n",
      "    80        0.4352             nan     0.1000   -0.0018\n",
      "   100        0.3865             nan     0.1000   -0.0005\n",
      "   120        0.3470             nan     0.1000   -0.0005\n",
      "   140        0.3148             nan     0.1000   -0.0014\n",
      "   160        0.2831             nan     0.1000   -0.0006\n",
      "   180        0.2560             nan     0.1000   -0.0008\n",
      "   200        0.2317             nan     0.1000   -0.0006\n",
      "   220        0.2110             nan     0.1000   -0.0007\n",
      "   240        0.1925             nan     0.1000   -0.0005\n",
      "   250        0.1843             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold06.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1109             nan     0.0100    0.0017\n",
      "     2        1.1075             nan     0.0100    0.0017\n",
      "     3        1.1039             nan     0.0100    0.0017\n",
      "     4        1.1007             nan     0.0100    0.0016\n",
      "     5        1.0974             nan     0.0100    0.0016\n",
      "     6        1.0944             nan     0.0100    0.0016\n",
      "     7        1.0914             nan     0.0100    0.0015\n",
      "     8        1.0883             nan     0.0100    0.0015\n",
      "     9        1.0852             nan     0.0100    0.0015\n",
      "    10        1.0822             nan     0.0100    0.0015\n",
      "    20        1.0566             nan     0.0100    0.0009\n",
      "    40        1.0144             nan     0.0100    0.0010\n",
      "    60        0.9811             nan     0.0100    0.0008\n",
      "    80        0.9536             nan     0.0100    0.0005\n",
      "   100        0.9308             nan     0.0100    0.0005\n",
      "   120        0.9121             nan     0.0100    0.0004\n",
      "   140        0.8945             nan     0.0100    0.0003\n",
      "   160        0.8794             nan     0.0100    0.0002\n",
      "   180        0.8653             nan     0.0100    0.0003\n",
      "   200        0.8534             nan     0.0100    0.0002\n",
      "   220        0.8412             nan     0.0100    0.0001\n",
      "   240        0.8309             nan     0.0100    0.0002\n",
      "   250        0.8263             nan     0.0100    0.0002\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1074             nan     0.0100    0.0030\n",
      "     2        1.1011             nan     0.0100    0.0030\n",
      "     3        1.0953             nan     0.0100    0.0029\n",
      "     4        1.0896             nan     0.0100    0.0028\n",
      "     5        1.0839             nan     0.0100    0.0024\n",
      "     6        1.0784             nan     0.0100    0.0027\n",
      "     7        1.0730             nan     0.0100    0.0025\n",
      "     8        1.0673             nan     0.0100    0.0027\n",
      "     9        1.0619             nan     0.0100    0.0026\n",
      "    10        1.0571             nan     0.0100    0.0023\n",
      "    20        1.0100             nan     0.0100    0.0018\n",
      "    40        0.9374             nan     0.0100    0.0015\n",
      "    60        0.8847             nan     0.0100    0.0010\n",
      "    80        0.8435             nan     0.0100    0.0007\n",
      "   100        0.8120             nan     0.0100    0.0007\n",
      "   120        0.7852             nan     0.0100    0.0004\n",
      "   140        0.7633             nan     0.0100    0.0004\n",
      "   160        0.7450             nan     0.0100    0.0002\n",
      "   180        0.7289             nan     0.0100    0.0002\n",
      "   200        0.7144             nan     0.0100    0.0002\n",
      "   220        0.7028             nan     0.0100    0.0001\n",
      "   240        0.6915             nan     0.0100    0.0002\n",
      "   250        0.6856             nan     0.0100    0.0001\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0032\n",
      "     2        1.1007             nan     0.0100    0.0031\n",
      "     3        1.0939             nan     0.0100    0.0030\n",
      "     4        1.0874             nan     0.0100    0.0033\n",
      "     5        1.0811             nan     0.0100    0.0027\n",
      "     6        1.0752             nan     0.0100    0.0027\n",
      "     7        1.0691             nan     0.0100    0.0024\n",
      "     8        1.0628             nan     0.0100    0.0028\n",
      "     9        1.0569             nan     0.0100    0.0026\n",
      "    10        1.0509             nan     0.0100    0.0028\n",
      "    20        0.9985             nan     0.0100    0.0019\n",
      "    40        0.9169             nan     0.0100    0.0013\n",
      "    60        0.8573             nan     0.0100    0.0009\n",
      "    80        0.8107             nan     0.0100    0.0005\n",
      "   100        0.7739             nan     0.0100    0.0006\n",
      "   120        0.7433             nan     0.0100    0.0003\n",
      "   140        0.7185             nan     0.0100    0.0002\n",
      "   160        0.6970             nan     0.0100    0.0002\n",
      "   180        0.6783             nan     0.0100    0.0002\n",
      "   200        0.6618             nan     0.0100    0.0002\n",
      "   220        0.6470             nan     0.0100   -0.0001\n",
      "   240        0.6336             nan     0.0100    0.0001\n",
      "   250        0.6270             nan     0.0100    0.0001\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0979             nan     0.0500    0.0086\n",
      "     2        1.0832             nan     0.0500    0.0078\n",
      "     3        1.0699             nan     0.0500    0.0070\n",
      "     4        1.0585             nan     0.0500    0.0049\n",
      "     5        1.0452             nan     0.0500    0.0063\n",
      "     6        1.0336             nan     0.0500    0.0057\n",
      "     7        1.0251             nan     0.0500    0.0041\n",
      "     8        1.0140             nan     0.0500    0.0051\n",
      "     9        1.0041             nan     0.0500    0.0045\n",
      "    10        0.9958             nan     0.0500    0.0042\n",
      "    20        0.9290             nan     0.0500    0.0024\n",
      "    40        0.8504             nan     0.0500    0.0011\n",
      "    60        0.8012             nan     0.0500    0.0005\n",
      "    80        0.7698             nan     0.0500    0.0001\n",
      "   100        0.7471             nan     0.0500    0.0006\n",
      "   120        0.7307             nan     0.0500   -0.0000\n",
      "   140        0.7183             nan     0.0500    0.0000\n",
      "   160        0.7087             nan     0.0500   -0.0001\n",
      "   180        0.7001             nan     0.0500   -0.0001\n",
      "   200        0.6921             nan     0.0500   -0.0000\n",
      "   220        0.6848             nan     0.0500   -0.0001\n",
      "   240        0.6769             nan     0.0500    0.0000\n",
      "   250        0.6738             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0849             nan     0.0500    0.0152\n",
      "     2        1.0547             nan     0.0500    0.0132\n",
      "     3        1.0296             nan     0.0500    0.0107\n",
      "     4        1.0083             nan     0.0500    0.0092\n",
      "     5        0.9884             nan     0.0500    0.0085\n",
      "     6        0.9691             nan     0.0500    0.0081\n",
      "     7        0.9517             nan     0.0500    0.0084\n",
      "     8        0.9349             nan     0.0500    0.0088\n",
      "     9        0.9210             nan     0.0500    0.0057\n",
      "    10        0.9069             nan     0.0500    0.0056\n",
      "    20        0.8107             nan     0.0500    0.0019\n",
      "    40        0.7107             nan     0.0500    0.0008\n",
      "    60        0.6616             nan     0.0500    0.0004\n",
      "    80        0.6291             nan     0.0500    0.0001\n",
      "   100        0.6025             nan     0.0500   -0.0002\n",
      "   120        0.5795             nan     0.0500   -0.0004\n",
      "   140        0.5571             nan     0.0500   -0.0002\n",
      "   160        0.5396             nan     0.0500   -0.0002\n",
      "   180        0.5214             nan     0.0500   -0.0001\n",
      "   200        0.5048             nan     0.0500   -0.0001\n",
      "   220        0.4865             nan     0.0500   -0.0007\n",
      "   240        0.4700             nan     0.0500   -0.0006\n",
      "   250        0.4627             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0789             nan     0.0500    0.0148\n",
      "     2        1.0471             nan     0.0500    0.0143\n",
      "     3        1.0196             nan     0.0500    0.0125\n",
      "     4        0.9945             nan     0.0500    0.0115\n",
      "     5        0.9736             nan     0.0500    0.0086\n",
      "     6        0.9530             nan     0.0500    0.0076\n",
      "     7        0.9329             nan     0.0500    0.0080\n",
      "     8        0.9149             nan     0.0500    0.0078\n",
      "     9        0.8981             nan     0.0500    0.0070\n",
      "    10        0.8834             nan     0.0500    0.0058\n",
      "    20        0.7743             nan     0.0500    0.0022\n",
      "    40        0.6621             nan     0.0500    0.0005\n",
      "    60        0.6009             nan     0.0500   -0.0001\n",
      "    80        0.5551             nan     0.0500   -0.0001\n",
      "   100        0.5175             nan     0.0500   -0.0003\n",
      "   120        0.4831             nan     0.0500   -0.0005\n",
      "   140        0.4563             nan     0.0500   -0.0003\n",
      "   160        0.4299             nan     0.0500   -0.0004\n",
      "   180        0.4041             nan     0.0500   -0.0002\n",
      "   200        0.3812             nan     0.0500   -0.0003\n",
      "   220        0.3601             nan     0.0500   -0.0006\n",
      "   240        0.3402             nan     0.0500   -0.0001\n",
      "   250        0.3322             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0821             nan     0.1000    0.0166\n",
      "     2        1.0540             nan     0.1000    0.0136\n",
      "     3        1.0335             nan     0.1000    0.0092\n",
      "     4        1.0121             nan     0.1000    0.0108\n",
      "     5        0.9943             nan     0.1000    0.0089\n",
      "     6        0.9787             nan     0.1000    0.0074\n",
      "     7        0.9635             nan     0.1000    0.0072\n",
      "     8        0.9511             nan     0.1000    0.0066\n",
      "     9        0.9388             nan     0.1000    0.0058\n",
      "    10        0.9290             nan     0.1000    0.0040\n",
      "    20        0.8541             nan     0.1000    0.0028\n",
      "    40        0.7690             nan     0.1000    0.0008\n",
      "    60        0.7324             nan     0.1000   -0.0000\n",
      "    80        0.7077             nan     0.1000    0.0004\n",
      "   100        0.6933             nan     0.1000   -0.0003\n",
      "   120        0.6771             nan     0.1000    0.0001\n",
      "   140        0.6688             nan     0.1000   -0.0003\n",
      "   160        0.6591             nan     0.1000   -0.0001\n",
      "   180        0.6499             nan     0.1000   -0.0001\n",
      "   200        0.6444             nan     0.1000   -0.0003\n",
      "   220        0.6396             nan     0.1000   -0.0005\n",
      "   240        0.6348             nan     0.1000   -0.0004\n",
      "   250        0.6320             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0551             nan     0.1000    0.0271\n",
      "     2        1.0078             nan     0.1000    0.0197\n",
      "     3        0.9676             nan     0.1000    0.0185\n",
      "     4        0.9293             nan     0.1000    0.0183\n",
      "     5        0.9016             nan     0.1000    0.0131\n",
      "     6        0.8780             nan     0.1000    0.0100\n",
      "     7        0.8543             nan     0.1000    0.0097\n",
      "     8        0.8369             nan     0.1000    0.0078\n",
      "     9        0.8218             nan     0.1000    0.0065\n",
      "    10        0.8083             nan     0.1000    0.0057\n",
      "    20        0.7170             nan     0.1000    0.0005\n",
      "    40        0.6352             nan     0.1000    0.0005\n",
      "    60        0.5871             nan     0.1000   -0.0000\n",
      "    80        0.5488             nan     0.1000   -0.0005\n",
      "   100        0.5123             nan     0.1000   -0.0009\n",
      "   120        0.4799             nan     0.1000   -0.0003\n",
      "   140        0.4505             nan     0.1000   -0.0009\n",
      "   160        0.4236             nan     0.1000   -0.0006\n",
      "   180        0.3980             nan     0.1000   -0.0001\n",
      "   200        0.3774             nan     0.1000   -0.0004\n",
      "   220        0.3584             nan     0.1000   -0.0007\n",
      "   240        0.3401             nan     0.1000   -0.0005\n",
      "   250        0.3309             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0455             nan     0.1000    0.0343\n",
      "     2        0.9893             nan     0.1000    0.0231\n",
      "     3        0.9492             nan     0.1000    0.0174\n",
      "     4        0.9096             nan     0.1000    0.0172\n",
      "     5        0.8786             nan     0.1000    0.0118\n",
      "     6        0.8501             nan     0.1000    0.0119\n",
      "     7        0.8251             nan     0.1000    0.0111\n",
      "     8        0.8021             nan     0.1000    0.0084\n",
      "     9        0.7835             nan     0.1000    0.0065\n",
      "    10        0.7644             nan     0.1000    0.0068\n",
      "    20        0.6636             nan     0.1000    0.0006\n",
      "    40        0.5639             nan     0.1000   -0.0006\n",
      "    60        0.4973             nan     0.1000   -0.0011\n",
      "    80        0.4418             nan     0.1000   -0.0018\n",
      "   100        0.3922             nan     0.1000   -0.0007\n",
      "   120        0.3517             nan     0.1000    0.0001\n",
      "   140        0.3163             nan     0.1000   -0.0011\n",
      "   160        0.2856             nan     0.1000   -0.0013\n",
      "   180        0.2593             nan     0.1000   -0.0004\n",
      "   200        0.2358             nan     0.1000   -0.0008\n",
      "   220        0.2144             nan     0.1000   -0.0005\n",
      "   240        0.1945             nan     0.1000   -0.0005\n",
      "   250        0.1859             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold07.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1107             nan     0.0100    0.0018\n",
      "     2        1.1070             nan     0.0100    0.0018\n",
      "     3        1.1036             nan     0.0100    0.0018\n",
      "     4        1.1005             nan     0.0100    0.0017\n",
      "     5        1.0973             nan     0.0100    0.0017\n",
      "     6        1.0939             nan     0.0100    0.0017\n",
      "     7        1.0906             nan     0.0100    0.0016\n",
      "     8        1.0885             nan     0.0100    0.0008\n",
      "     9        1.0850             nan     0.0100    0.0016\n",
      "    10        1.0819             nan     0.0100    0.0016\n",
      "    20        1.0540             nan     0.0100    0.0009\n",
      "    40        1.0099             nan     0.0100    0.0008\n",
      "    60        0.9742             nan     0.0100    0.0008\n",
      "    80        0.9458             nan     0.0100    0.0007\n",
      "   100        0.9222             nan     0.0100    0.0005\n",
      "   120        0.9015             nan     0.0100    0.0003\n",
      "   140        0.8835             nan     0.0100    0.0003\n",
      "   160        0.8687             nan     0.0100    0.0002\n",
      "   180        0.8549             nan     0.0100    0.0003\n",
      "   200        0.8426             nan     0.0100    0.0003\n",
      "   220        0.8321             nan     0.0100    0.0002\n",
      "   240        0.8217             nan     0.0100    0.0003\n",
      "   250        0.8167             nan     0.0100    0.0002\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1082             nan     0.0100    0.0030\n",
      "     2        1.1013             nan     0.0100    0.0031\n",
      "     3        1.0951             nan     0.0100    0.0032\n",
      "     4        1.0888             nan     0.0100    0.0027\n",
      "     5        1.0834             nan     0.0100    0.0026\n",
      "     6        1.0776             nan     0.0100    0.0027\n",
      "     7        1.0722             nan     0.0100    0.0028\n",
      "     8        1.0666             nan     0.0100    0.0023\n",
      "     9        1.0612             nan     0.0100    0.0026\n",
      "    10        1.0560             nan     0.0100    0.0025\n",
      "    20        1.0084             nan     0.0100    0.0019\n",
      "    40        0.9354             nan     0.0100    0.0014\n",
      "    60        0.8806             nan     0.0100    0.0007\n",
      "    80        0.8400             nan     0.0100    0.0007\n",
      "   100        0.8075             nan     0.0100    0.0004\n",
      "   120        0.7813             nan     0.0100    0.0005\n",
      "   140        0.7591             nan     0.0100    0.0004\n",
      "   160        0.7404             nan     0.0100    0.0003\n",
      "   180        0.7243             nan     0.0100    0.0002\n",
      "   200        0.7098             nan     0.0100    0.0001\n",
      "   220        0.6973             nan     0.0100    0.0002\n",
      "   240        0.6864             nan     0.0100    0.0002\n",
      "   250        0.6810             nan     0.0100    0.0001\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0034\n",
      "     2        1.1005             nan     0.0100    0.0032\n",
      "     3        1.0935             nan     0.0100    0.0029\n",
      "     4        1.0869             nan     0.0100    0.0029\n",
      "     5        1.0804             nan     0.0100    0.0030\n",
      "     6        1.0740             nan     0.0100    0.0029\n",
      "     7        1.0677             nan     0.0100    0.0029\n",
      "     8        1.0622             nan     0.0100    0.0026\n",
      "     9        1.0562             nan     0.0100    0.0027\n",
      "    10        1.0506             nan     0.0100    0.0025\n",
      "    20        0.9987             nan     0.0100    0.0021\n",
      "    40        0.9179             nan     0.0100    0.0015\n",
      "    60        0.8579             nan     0.0100    0.0012\n",
      "    80        0.8108             nan     0.0100    0.0006\n",
      "   100        0.7746             nan     0.0100    0.0004\n",
      "   120        0.7427             nan     0.0100    0.0005\n",
      "   140        0.7168             nan     0.0100    0.0005\n",
      "   160        0.6942             nan     0.0100    0.0002\n",
      "   180        0.6746             nan     0.0100    0.0002\n",
      "   200        0.6577             nan     0.0100    0.0000\n",
      "   220        0.6422             nan     0.0100    0.0001\n",
      "   240        0.6287             nan     0.0100    0.0001\n",
      "   250        0.6225             nan     0.0100    0.0000\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0972             nan     0.0500    0.0090\n",
      "     2        1.0825             nan     0.0500    0.0081\n",
      "     3        1.0669             nan     0.0500    0.0075\n",
      "     4        1.0520             nan     0.0500    0.0067\n",
      "     5        1.0389             nan     0.0500    0.0060\n",
      "     6        1.0292             nan     0.0500    0.0043\n",
      "     7        1.0205             nan     0.0500    0.0032\n",
      "     8        1.0081             nan     0.0500    0.0051\n",
      "     9        0.9992             nan     0.0500    0.0044\n",
      "    10        0.9896             nan     0.0500    0.0047\n",
      "    20        0.9201             nan     0.0500    0.0023\n",
      "    40        0.8403             nan     0.0500    0.0014\n",
      "    60        0.7937             nan     0.0500    0.0007\n",
      "    80        0.7607             nan     0.0500    0.0005\n",
      "   100        0.7406             nan     0.0500    0.0005\n",
      "   120        0.7247             nan     0.0500    0.0001\n",
      "   140        0.7117             nan     0.0500    0.0002\n",
      "   160        0.7019             nan     0.0500   -0.0005\n",
      "   180        0.6929             nan     0.0500   -0.0002\n",
      "   200        0.6846             nan     0.0500   -0.0002\n",
      "   220        0.6780             nan     0.0500    0.0000\n",
      "   240        0.6722             nan     0.0500    0.0000\n",
      "   250        0.6690             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0831             nan     0.0500    0.0154\n",
      "     2        1.0535             nan     0.0500    0.0140\n",
      "     3        1.0289             nan     0.0500    0.0117\n",
      "     4        1.0068             nan     0.0500    0.0105\n",
      "     5        0.9858             nan     0.0500    0.0096\n",
      "     6        0.9660             nan     0.0500    0.0092\n",
      "     7        0.9497             nan     0.0500    0.0073\n",
      "     8        0.9344             nan     0.0500    0.0069\n",
      "     9        0.9213             nan     0.0500    0.0062\n",
      "    10        0.9060             nan     0.0500    0.0068\n",
      "    20        0.8087             nan     0.0500    0.0033\n",
      "    40        0.7108             nan     0.0500    0.0009\n",
      "    60        0.6617             nan     0.0500    0.0000\n",
      "    80        0.6264             nan     0.0500   -0.0003\n",
      "   100        0.5988             nan     0.0500   -0.0004\n",
      "   120        0.5767             nan     0.0500   -0.0003\n",
      "   140        0.5549             nan     0.0500   -0.0006\n",
      "   160        0.5365             nan     0.0500   -0.0008\n",
      "   180        0.5183             nan     0.0500   -0.0001\n",
      "   200        0.5000             nan     0.0500   -0.0004\n",
      "   220        0.4847             nan     0.0500   -0.0006\n",
      "   240        0.4702             nan     0.0500   -0.0003\n",
      "   250        0.4631             nan     0.0500   -0.0008\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0801             nan     0.0500    0.0159\n",
      "     2        1.0489             nan     0.0500    0.0147\n",
      "     3        1.0216             nan     0.0500    0.0119\n",
      "     4        0.9978             nan     0.0500    0.0110\n",
      "     5        0.9749             nan     0.0500    0.0093\n",
      "     6        0.9537             nan     0.0500    0.0089\n",
      "     7        0.9342             nan     0.0500    0.0090\n",
      "     8        0.9166             nan     0.0500    0.0068\n",
      "     9        0.8991             nan     0.0500    0.0063\n",
      "    10        0.8835             nan     0.0500    0.0065\n",
      "    20        0.7724             nan     0.0500    0.0025\n",
      "    40        0.6659             nan     0.0500    0.0004\n",
      "    60        0.6001             nan     0.0500    0.0000\n",
      "    80        0.5526             nan     0.0500   -0.0002\n",
      "   100        0.5118             nan     0.0500   -0.0002\n",
      "   120        0.4786             nan     0.0500   -0.0003\n",
      "   140        0.4492             nan     0.0500   -0.0002\n",
      "   160        0.4225             nan     0.0500   -0.0005\n",
      "   180        0.3990             nan     0.0500   -0.0005\n",
      "   200        0.3759             nan     0.0500   -0.0001\n",
      "   220        0.3558             nan     0.0500   -0.0002\n",
      "   240        0.3381             nan     0.0500   -0.0004\n",
      "   250        0.3292             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0801             nan     0.1000    0.0177\n",
      "     2        1.0480             nan     0.1000    0.0139\n",
      "     3        1.0285             nan     0.1000    0.0078\n",
      "     4        1.0044             nan     0.1000    0.0110\n",
      "     5        0.9866             nan     0.1000    0.0086\n",
      "     6        0.9675             nan     0.1000    0.0088\n",
      "     7        0.9536             nan     0.1000    0.0068\n",
      "     8        0.9424             nan     0.1000    0.0055\n",
      "     9        0.9274             nan     0.1000    0.0071\n",
      "    10        0.9153             nan     0.1000    0.0057\n",
      "    20        0.8385             nan     0.1000    0.0023\n",
      "    40        0.7601             nan     0.1000    0.0009\n",
      "    60        0.7240             nan     0.1000    0.0006\n",
      "    80        0.7010             nan     0.1000   -0.0006\n",
      "   100        0.6861             nan     0.1000    0.0003\n",
      "   120        0.6737             nan     0.1000    0.0004\n",
      "   140        0.6625             nan     0.1000    0.0001\n",
      "   160        0.6536             nan     0.1000   -0.0005\n",
      "   180        0.6467             nan     0.1000   -0.0001\n",
      "   200        0.6407             nan     0.1000   -0.0002\n",
      "   220        0.6349             nan     0.1000   -0.0001\n",
      "   240        0.6291             nan     0.1000   -0.0002\n",
      "   250        0.6276             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0544             nan     0.1000    0.0299\n",
      "     2        1.0076             nan     0.1000    0.0239\n",
      "     3        0.9677             nan     0.1000    0.0194\n",
      "     4        0.9357             nan     0.1000    0.0140\n",
      "     5        0.9054             nan     0.1000    0.0149\n",
      "     6        0.8829             nan     0.1000    0.0108\n",
      "     7        0.8634             nan     0.1000    0.0071\n",
      "     8        0.8417             nan     0.1000    0.0096\n",
      "     9        0.8249             nan     0.1000    0.0074\n",
      "    10        0.8095             nan     0.1000    0.0056\n",
      "    20        0.7157             nan     0.1000    0.0015\n",
      "    40        0.6294             nan     0.1000   -0.0007\n",
      "    60        0.5742             nan     0.1000    0.0001\n",
      "    80        0.5373             nan     0.1000   -0.0012\n",
      "   100        0.5067             nan     0.1000   -0.0008\n",
      "   120        0.4776             nan     0.1000   -0.0004\n",
      "   140        0.4529             nan     0.1000   -0.0007\n",
      "   160        0.4284             nan     0.1000   -0.0010\n",
      "   180        0.4017             nan     0.1000   -0.0001\n",
      "   200        0.3782             nan     0.1000   -0.0008\n",
      "   220        0.3575             nan     0.1000   -0.0005\n",
      "   240        0.3366             nan     0.1000   -0.0004\n",
      "   250        0.3274             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0497             nan     0.1000    0.0283\n",
      "     2        0.9941             nan     0.1000    0.0236\n",
      "     3        0.9507             nan     0.1000    0.0189\n",
      "     4        0.9139             nan     0.1000    0.0143\n",
      "     5        0.8824             nan     0.1000    0.0137\n",
      "     6        0.8534             nan     0.1000    0.0124\n",
      "     7        0.8266             nan     0.1000    0.0116\n",
      "     8        0.8058             nan     0.1000    0.0083\n",
      "     9        0.7869             nan     0.1000    0.0057\n",
      "    10        0.7705             nan     0.1000    0.0059\n",
      "    20        0.6613             nan     0.1000    0.0011\n",
      "    40        0.5602             nan     0.1000   -0.0010\n",
      "    60        0.4841             nan     0.1000   -0.0013\n",
      "    80        0.4276             nan     0.1000   -0.0010\n",
      "   100        0.3806             nan     0.1000   -0.0002\n",
      "   120        0.3440             nan     0.1000   -0.0001\n",
      "   140        0.3101             nan     0.1000   -0.0003\n",
      "   160        0.2816             nan     0.1000   -0.0005\n",
      "   180        0.2555             nan     0.1000   -0.0005\n",
      "   200        0.2313             nan     0.1000   -0.0005\n",
      "   220        0.2093             nan     0.1000   -0.0003\n",
      "   240        0.1907             nan     0.1000   -0.0007\n",
      "   250        0.1841             nan     0.1000   -0.0008\n",
      "\n",
      "- Fold08.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0018\n",
      "     2        1.1068             nan     0.0100    0.0017\n",
      "     3        1.1035             nan     0.0100    0.0017\n",
      "     4        1.1001             nan     0.0100    0.0017\n",
      "     5        1.0971             nan     0.0100    0.0017\n",
      "     6        1.0937             nan     0.0100    0.0016\n",
      "     7        1.0907             nan     0.0100    0.0016\n",
      "     8        1.0883             nan     0.0100    0.0009\n",
      "     9        1.0855             nan     0.0100    0.0016\n",
      "    10        1.0823             nan     0.0100    0.0015\n",
      "    20        1.0553             nan     0.0100    0.0009\n",
      "    40        1.0126             nan     0.0100    0.0010\n",
      "    60        0.9773             nan     0.0100    0.0008\n",
      "    80        0.9490             nan     0.0100    0.0007\n",
      "   100        0.9241             nan     0.0100    0.0004\n",
      "   120        0.9043             nan     0.0100    0.0004\n",
      "   140        0.8877             nan     0.0100    0.0003\n",
      "   160        0.8718             nan     0.0100    0.0003\n",
      "   180        0.8580             nan     0.0100    0.0002\n",
      "   200        0.8442             nan     0.0100    0.0003\n",
      "   220        0.8330             nan     0.0100    0.0002\n",
      "   240        0.8226             nan     0.0100    0.0002\n",
      "   250        0.8174             nan     0.0100    0.0002\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1083             nan     0.0100    0.0030\n",
      "     2        1.1016             nan     0.0100    0.0030\n",
      "     3        1.0949             nan     0.0100    0.0029\n",
      "     4        1.0892             nan     0.0100    0.0028\n",
      "     5        1.0827             nan     0.0100    0.0033\n",
      "     6        1.0770             nan     0.0100    0.0029\n",
      "     7        1.0713             nan     0.0100    0.0027\n",
      "     8        1.0656             nan     0.0100    0.0027\n",
      "     9        1.0602             nan     0.0100    0.0027\n",
      "    10        1.0549             nan     0.0100    0.0022\n",
      "    20        1.0071             nan     0.0100    0.0019\n",
      "    40        0.9343             nan     0.0100    0.0014\n",
      "    60        0.8807             nan     0.0100    0.0012\n",
      "    80        0.8390             nan     0.0100    0.0007\n",
      "   100        0.8061             nan     0.0100    0.0006\n",
      "   120        0.7788             nan     0.0100    0.0005\n",
      "   140        0.7572             nan     0.0100    0.0003\n",
      "   160        0.7390             nan     0.0100    0.0003\n",
      "   180        0.7223             nan     0.0100    0.0003\n",
      "   200        0.7078             nan     0.0100    0.0001\n",
      "   220        0.6956             nan     0.0100    0.0001\n",
      "   240        0.6843             nan     0.0100    0.0001\n",
      "   250        0.6787             nan     0.0100    0.0000\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1065             nan     0.0100    0.0034\n",
      "     2        1.0993             nan     0.0100    0.0034\n",
      "     3        1.0922             nan     0.0100    0.0031\n",
      "     4        1.0856             nan     0.0100    0.0032\n",
      "     5        1.0789             nan     0.0100    0.0033\n",
      "     6        1.0726             nan     0.0100    0.0027\n",
      "     7        1.0667             nan     0.0100    0.0027\n",
      "     8        1.0606             nan     0.0100    0.0028\n",
      "     9        1.0543             nan     0.0100    0.0026\n",
      "    10        1.0485             nan     0.0100    0.0025\n",
      "    20        0.9961             nan     0.0100    0.0021\n",
      "    40        0.9156             nan     0.0100    0.0017\n",
      "    60        0.8547             nan     0.0100    0.0011\n",
      "    80        0.8078             nan     0.0100    0.0008\n",
      "   100        0.7710             nan     0.0100    0.0006\n",
      "   120        0.7393             nan     0.0100    0.0005\n",
      "   140        0.7128             nan     0.0100    0.0003\n",
      "   160        0.6908             nan     0.0100    0.0003\n",
      "   180        0.6718             nan     0.0100    0.0002\n",
      "   200        0.6545             nan     0.0100    0.0002\n",
      "   220        0.6389             nan     0.0100    0.0001\n",
      "   240        0.6246             nan     0.0100    0.0001\n",
      "   250        0.6189             nan     0.0100   -0.0001\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0971             nan     0.0500    0.0088\n",
      "     2        1.0818             nan     0.0500    0.0080\n",
      "     3        1.0674             nan     0.0500    0.0072\n",
      "     4        1.0536             nan     0.0500    0.0064\n",
      "     5        1.0407             nan     0.0500    0.0058\n",
      "     6        1.0307             nan     0.0500    0.0043\n",
      "     7        1.0220             nan     0.0500    0.0040\n",
      "     8        1.0105             nan     0.0500    0.0051\n",
      "     9        1.0031             nan     0.0500    0.0029\n",
      "    10        0.9939             nan     0.0500    0.0043\n",
      "    20        0.9234             nan     0.0500    0.0020\n",
      "    40        0.8427             nan     0.0500    0.0010\n",
      "    60        0.7929             nan     0.0500    0.0010\n",
      "    80        0.7631             nan     0.0500    0.0003\n",
      "   100        0.7421             nan     0.0500    0.0002\n",
      "   120        0.7246             nan     0.0500   -0.0001\n",
      "   140        0.7124             nan     0.0500    0.0000\n",
      "   160        0.7036             nan     0.0500    0.0001\n",
      "   180        0.6939             nan     0.0500   -0.0000\n",
      "   200        0.6876             nan     0.0500   -0.0000\n",
      "   220        0.6809             nan     0.0500   -0.0002\n",
      "   240        0.6748             nan     0.0500   -0.0002\n",
      "   250        0.6720             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0829             nan     0.0500    0.0157\n",
      "     2        1.0539             nan     0.0500    0.0139\n",
      "     3        1.0289             nan     0.0500    0.0116\n",
      "     4        1.0055             nan     0.0500    0.0107\n",
      "     5        0.9854             nan     0.0500    0.0083\n",
      "     6        0.9667             nan     0.0500    0.0089\n",
      "     7        0.9495             nan     0.0500    0.0084\n",
      "     8        0.9338             nan     0.0500    0.0072\n",
      "     9        0.9180             nan     0.0500    0.0069\n",
      "    10        0.9055             nan     0.0500    0.0056\n",
      "    20        0.8055             nan     0.0500    0.0030\n",
      "    40        0.7061             nan     0.0500    0.0005\n",
      "    60        0.6542             nan     0.0500    0.0005\n",
      "    80        0.6209             nan     0.0500    0.0002\n",
      "   100        0.5949             nan     0.0500    0.0000\n",
      "   120        0.5716             nan     0.0500   -0.0004\n",
      "   140        0.5513             nan     0.0500   -0.0001\n",
      "   160        0.5321             nan     0.0500   -0.0005\n",
      "   180        0.5134             nan     0.0500   -0.0002\n",
      "   200        0.4968             nan     0.0500   -0.0004\n",
      "   220        0.4787             nan     0.0500   -0.0001\n",
      "   240        0.4640             nan     0.0500   -0.0002\n",
      "   250        0.4572             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0781             nan     0.0500    0.0164\n",
      "     2        1.0465             nan     0.0500    0.0145\n",
      "     3        1.0177             nan     0.0500    0.0138\n",
      "     4        0.9922             nan     0.0500    0.0109\n",
      "     5        0.9698             nan     0.0500    0.0104\n",
      "     6        0.9463             nan     0.0500    0.0107\n",
      "     7        0.9260             nan     0.0500    0.0089\n",
      "     8        0.9090             nan     0.0500    0.0070\n",
      "     9        0.8934             nan     0.0500    0.0063\n",
      "    10        0.8784             nan     0.0500    0.0066\n",
      "    20        0.7677             nan     0.0500    0.0027\n",
      "    40        0.6544             nan     0.0500    0.0005\n",
      "    60        0.5900             nan     0.0500   -0.0003\n",
      "    80        0.5439             nan     0.0500    0.0003\n",
      "   100        0.5083             nan     0.0500   -0.0004\n",
      "   120        0.4725             nan     0.0500   -0.0003\n",
      "   140        0.4416             nan     0.0500   -0.0007\n",
      "   160        0.4139             nan     0.0500   -0.0004\n",
      "   180        0.3892             nan     0.0500   -0.0000\n",
      "   200        0.3670             nan     0.0500   -0.0003\n",
      "   220        0.3471             nan     0.0500   -0.0002\n",
      "   240        0.3287             nan     0.0500   -0.0003\n",
      "   250        0.3209             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0788             nan     0.1000    0.0172\n",
      "     2        1.0499             nan     0.1000    0.0138\n",
      "     3        1.0253             nan     0.1000    0.0109\n",
      "     4        1.0061             nan     0.1000    0.0100\n",
      "     5        0.9913             nan     0.1000    0.0069\n",
      "     6        0.9705             nan     0.1000    0.0073\n",
      "     7        0.9523             nan     0.1000    0.0083\n",
      "     8        0.9427             nan     0.1000    0.0039\n",
      "     9        0.9302             nan     0.1000    0.0058\n",
      "    10        0.9208             nan     0.1000    0.0028\n",
      "    20        0.8437             nan     0.1000    0.0014\n",
      "    40        0.7604             nan     0.1000    0.0009\n",
      "    60        0.7231             nan     0.1000    0.0003\n",
      "    80        0.7017             nan     0.1000    0.0004\n",
      "   100        0.6845             nan     0.1000   -0.0004\n",
      "   120        0.6710             nan     0.1000   -0.0002\n",
      "   140        0.6603             nan     0.1000   -0.0002\n",
      "   160        0.6528             nan     0.1000   -0.0005\n",
      "   180        0.6440             nan     0.1000   -0.0002\n",
      "   200        0.6357             nan     0.1000   -0.0002\n",
      "   220        0.6305             nan     0.1000   -0.0006\n",
      "   240        0.6264             nan     0.1000   -0.0006\n",
      "   250        0.6238             nan     0.1000   -0.0007\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0575             nan     0.1000    0.0264\n",
      "     2        1.0082             nan     0.1000    0.0240\n",
      "     3        0.9662             nan     0.1000    0.0206\n",
      "     4        0.9306             nan     0.1000    0.0154\n",
      "     5        0.9026             nan     0.1000    0.0126\n",
      "     6        0.8783             nan     0.1000    0.0117\n",
      "     7        0.8581             nan     0.1000    0.0092\n",
      "     8        0.8421             nan     0.1000    0.0070\n",
      "     9        0.8255             nan     0.1000    0.0057\n",
      "    10        0.8084             nan     0.1000    0.0080\n",
      "    20        0.7116             nan     0.1000    0.0014\n",
      "    40        0.6247             nan     0.1000    0.0005\n",
      "    60        0.5757             nan     0.1000   -0.0012\n",
      "    80        0.5365             nan     0.1000   -0.0008\n",
      "   100        0.5030             nan     0.1000   -0.0008\n",
      "   120        0.4749             nan     0.1000   -0.0013\n",
      "   140        0.4492             nan     0.1000   -0.0015\n",
      "   160        0.4219             nan     0.1000   -0.0009\n",
      "   180        0.3983             nan     0.1000   -0.0003\n",
      "   200        0.3754             nan     0.1000   -0.0008\n",
      "   220        0.3519             nan     0.1000   -0.0005\n",
      "   240        0.3309             nan     0.1000   -0.0010\n",
      "   250        0.3208             nan     0.1000   -0.0008\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0524             nan     0.1000    0.0284\n",
      "     2        1.0010             nan     0.1000    0.0224\n",
      "     3        0.9597             nan     0.1000    0.0202\n",
      "     4        0.9227             nan     0.1000    0.0146\n",
      "     5        0.8888             nan     0.1000    0.0134\n",
      "     6        0.8583             nan     0.1000    0.0125\n",
      "     7        0.8337             nan     0.1000    0.0077\n",
      "     8        0.8093             nan     0.1000    0.0089\n",
      "     9        0.7914             nan     0.1000    0.0050\n",
      "    10        0.7748             nan     0.1000    0.0050\n",
      "    20        0.6634             nan     0.1000    0.0031\n",
      "    40        0.5529             nan     0.1000   -0.0010\n",
      "    60        0.4876             nan     0.1000   -0.0015\n",
      "    80        0.4280             nan     0.1000   -0.0007\n",
      "   100        0.3790             nan     0.1000   -0.0008\n",
      "   120        0.3381             nan     0.1000   -0.0005\n",
      "   140        0.3019             nan     0.1000   -0.0012\n",
      "   160        0.2713             nan     0.1000   -0.0007\n",
      "   180        0.2456             nan     0.1000   -0.0005\n",
      "   200        0.2201             nan     0.1000   -0.0010\n",
      "   220        0.2005             nan     0.1000   -0.0009\n",
      "   240        0.1818             nan     0.1000   -0.0002\n",
      "   250        0.1736             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold09.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1116             nan     0.0100    0.0019\n",
      "     2        1.1080             nan     0.0100    0.0018\n",
      "     3        1.1044             nan     0.0100    0.0018\n",
      "     4        1.1012             nan     0.0100    0.0017\n",
      "     5        1.0976             nan     0.0100    0.0017\n",
      "     6        1.0942             nan     0.0100    0.0017\n",
      "     7        1.0906             nan     0.0100    0.0016\n",
      "     8        1.0873             nan     0.0100    0.0016\n",
      "     9        1.0841             nan     0.0100    0.0016\n",
      "    10        1.0808             nan     0.0100    0.0015\n",
      "    20        1.0527             nan     0.0100    0.0013\n",
      "    40        1.0091             nan     0.0100    0.0010\n",
      "    60        0.9748             nan     0.0100    0.0006\n",
      "    80        0.9448             nan     0.0100    0.0007\n",
      "   100        0.9206             nan     0.0100    0.0005\n",
      "   120        0.9003             nan     0.0100    0.0004\n",
      "   140        0.8828             nan     0.0100    0.0003\n",
      "   160        0.8679             nan     0.0100    0.0002\n",
      "   180        0.8545             nan     0.0100    0.0003\n",
      "   200        0.8419             nan     0.0100    0.0002\n",
      "   220        0.8304             nan     0.0100    0.0001\n",
      "   240        0.8201             nan     0.0100    0.0002\n",
      "   250        0.8156             nan     0.0100    0.0001\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1085             nan     0.0100    0.0032\n",
      "     2        1.1020             nan     0.0100    0.0031\n",
      "     3        1.0957             nan     0.0100    0.0029\n",
      "     4        1.0896             nan     0.0100    0.0027\n",
      "     5        1.0844             nan     0.0100    0.0024\n",
      "     6        1.0787             nan     0.0100    0.0029\n",
      "     7        1.0732             nan     0.0100    0.0026\n",
      "     8        1.0676             nan     0.0100    0.0029\n",
      "     9        1.0624             nan     0.0100    0.0024\n",
      "    10        1.0572             nan     0.0100    0.0025\n",
      "    20        1.0089             nan     0.0100    0.0020\n",
      "    40        0.9353             nan     0.0100    0.0016\n",
      "    60        0.8799             nan     0.0100    0.0011\n",
      "    80        0.8378             nan     0.0100    0.0008\n",
      "   100        0.8047             nan     0.0100    0.0004\n",
      "   120        0.7777             nan     0.0100    0.0005\n",
      "   140        0.7548             nan     0.0100    0.0004\n",
      "   160        0.7362             nan     0.0100    0.0003\n",
      "   180        0.7205             nan     0.0100    0.0002\n",
      "   200        0.7063             nan     0.0100    0.0001\n",
      "   220        0.6942             nan     0.0100    0.0001\n",
      "   240        0.6831             nan     0.0100    0.0000\n",
      "   250        0.6780             nan     0.0100    0.0000\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1083             nan     0.0100    0.0029\n",
      "     2        1.1008             nan     0.0100    0.0035\n",
      "     3        1.0941             nan     0.0100    0.0028\n",
      "     4        1.0877             nan     0.0100    0.0029\n",
      "     5        1.0816             nan     0.0100    0.0027\n",
      "     6        1.0750             nan     0.0100    0.0031\n",
      "     7        1.0685             nan     0.0100    0.0027\n",
      "     8        1.0622             nan     0.0100    0.0029\n",
      "     9        1.0559             nan     0.0100    0.0031\n",
      "    10        1.0498             nan     0.0100    0.0027\n",
      "    20        0.9959             nan     0.0100    0.0021\n",
      "    40        0.9141             nan     0.0100    0.0012\n",
      "    60        0.8530             nan     0.0100    0.0010\n",
      "    80        0.8058             nan     0.0100    0.0006\n",
      "   100        0.7670             nan     0.0100    0.0006\n",
      "   120        0.7367             nan     0.0100    0.0004\n",
      "   140        0.7109             nan     0.0100    0.0003\n",
      "   160        0.6892             nan     0.0100    0.0001\n",
      "   180        0.6694             nan     0.0100    0.0003\n",
      "   200        0.6528             nan     0.0100    0.0002\n",
      "   220        0.6380             nan     0.0100    0.0002\n",
      "   240        0.6243             nan     0.0100    0.0000\n",
      "   250        0.6178             nan     0.0100    0.0001\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0977             nan     0.0500    0.0091\n",
      "     2        1.0806             nan     0.0500    0.0083\n",
      "     3        1.0686             nan     0.0500    0.0055\n",
      "     4        1.0544             nan     0.0500    0.0074\n",
      "     5        1.0414             nan     0.0500    0.0067\n",
      "     6        1.0300             nan     0.0500    0.0061\n",
      "     7        1.0181             nan     0.0500    0.0055\n",
      "     8        1.0074             nan     0.0500    0.0050\n",
      "     9        0.9973             nan     0.0500    0.0041\n",
      "    10        0.9888             nan     0.0500    0.0035\n",
      "    20        0.9189             nan     0.0500    0.0025\n",
      "    40        0.8402             nan     0.0500    0.0012\n",
      "    60        0.7913             nan     0.0500    0.0007\n",
      "    80        0.7601             nan     0.0500    0.0003\n",
      "   100        0.7389             nan     0.0500    0.0002\n",
      "   120        0.7223             nan     0.0500    0.0002\n",
      "   140        0.7083             nan     0.0500   -0.0000\n",
      "   160        0.6979             nan     0.0500    0.0001\n",
      "   180        0.6895             nan     0.0500    0.0001\n",
      "   200        0.6812             nan     0.0500    0.0002\n",
      "   220        0.6740             nan     0.0500   -0.0000\n",
      "   240        0.6684             nan     0.0500   -0.0004\n",
      "   250        0.6659             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0812             nan     0.0500    0.0175\n",
      "     2        1.0531             nan     0.0500    0.0140\n",
      "     3        1.0283             nan     0.0500    0.0121\n",
      "     4        1.0055             nan     0.0500    0.0097\n",
      "     5        0.9842             nan     0.0500    0.0097\n",
      "     6        0.9659             nan     0.0500    0.0084\n",
      "     7        0.9477             nan     0.0500    0.0087\n",
      "     8        0.9327             nan     0.0500    0.0066\n",
      "     9        0.9176             nan     0.0500    0.0069\n",
      "    10        0.9022             nan     0.0500    0.0065\n",
      "    20        0.8022             nan     0.0500    0.0031\n",
      "    40        0.7102             nan     0.0500    0.0008\n",
      "    60        0.6563             nan     0.0500    0.0004\n",
      "    80        0.6240             nan     0.0500   -0.0003\n",
      "   100        0.5958             nan     0.0500   -0.0002\n",
      "   120        0.5723             nan     0.0500   -0.0001\n",
      "   140        0.5490             nan     0.0500    0.0001\n",
      "   160        0.5295             nan     0.0500   -0.0004\n",
      "   180        0.5129             nan     0.0500   -0.0003\n",
      "   200        0.4949             nan     0.0500   -0.0005\n",
      "   220        0.4801             nan     0.0500   -0.0004\n",
      "   240        0.4657             nan     0.0500   -0.0006\n",
      "   250        0.4587             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0800             nan     0.0500    0.0167\n",
      "     2        1.0480             nan     0.0500    0.0139\n",
      "     3        1.0194             nan     0.0500    0.0135\n",
      "     4        0.9959             nan     0.0500    0.0099\n",
      "     5        0.9723             nan     0.0500    0.0101\n",
      "     6        0.9516             nan     0.0500    0.0095\n",
      "     7        0.9302             nan     0.0500    0.0104\n",
      "     8        0.9128             nan     0.0500    0.0070\n",
      "     9        0.8961             nan     0.0500    0.0066\n",
      "    10        0.8805             nan     0.0500    0.0068\n",
      "    20        0.7734             nan     0.0500    0.0025\n",
      "    40        0.6589             nan     0.0500    0.0006\n",
      "    60        0.5949             nan     0.0500    0.0002\n",
      "    80        0.5498             nan     0.0500   -0.0002\n",
      "   100        0.5095             nan     0.0500   -0.0000\n",
      "   120        0.4766             nan     0.0500   -0.0003\n",
      "   140        0.4472             nan     0.0500   -0.0002\n",
      "   160        0.4208             nan     0.0500   -0.0001\n",
      "   180        0.3978             nan     0.0500   -0.0005\n",
      "   200        0.3745             nan     0.0500   -0.0005\n",
      "   220        0.3538             nan     0.0500   -0.0004\n",
      "   240        0.3348             nan     0.0500   -0.0002\n",
      "   250        0.3259             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0808             nan     0.1000    0.0178\n",
      "     2        1.0499             nan     0.1000    0.0144\n",
      "     3        1.0265             nan     0.1000    0.0118\n",
      "     4        1.0048             nan     0.1000    0.0093\n",
      "     5        0.9847             nan     0.1000    0.0100\n",
      "     6        0.9684             nan     0.1000    0.0074\n",
      "     7        0.9519             nan     0.1000    0.0080\n",
      "     8        0.9399             nan     0.1000    0.0066\n",
      "     9        0.9267             nan     0.1000    0.0060\n",
      "    10        0.9144             nan     0.1000    0.0058\n",
      "    20        0.8429             nan     0.1000    0.0021\n",
      "    40        0.7626             nan     0.1000    0.0010\n",
      "    60        0.7232             nan     0.1000    0.0005\n",
      "    80        0.7002             nan     0.1000   -0.0002\n",
      "   100        0.6829             nan     0.1000    0.0000\n",
      "   120        0.6695             nan     0.1000   -0.0001\n",
      "   140        0.6586             nan     0.1000   -0.0010\n",
      "   160        0.6503             nan     0.1000   -0.0005\n",
      "   180        0.6420             nan     0.1000    0.0003\n",
      "   200        0.6341             nan     0.1000    0.0002\n",
      "   220        0.6281             nan     0.1000   -0.0000\n",
      "   240        0.6230             nan     0.1000   -0.0004\n",
      "   250        0.6211             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0522             nan     0.1000    0.0283\n",
      "     2        1.0013             nan     0.1000    0.0216\n",
      "     3        0.9569             nan     0.1000    0.0208\n",
      "     4        0.9229             nan     0.1000    0.0160\n",
      "     5        0.8940             nan     0.1000    0.0136\n",
      "     6        0.8702             nan     0.1000    0.0101\n",
      "     7        0.8491             nan     0.1000    0.0087\n",
      "     8        0.8323             nan     0.1000    0.0069\n",
      "     9        0.8139             nan     0.1000    0.0074\n",
      "    10        0.8009             nan     0.1000    0.0036\n",
      "    20        0.7046             nan     0.1000    0.0028\n",
      "    40        0.6219             nan     0.1000   -0.0003\n",
      "    60        0.5730             nan     0.1000   -0.0009\n",
      "    80        0.5355             nan     0.1000   -0.0013\n",
      "   100        0.5004             nan     0.1000   -0.0005\n",
      "   120        0.4709             nan     0.1000   -0.0009\n",
      "   140        0.4451             nan     0.1000   -0.0003\n",
      "   160        0.4204             nan     0.1000   -0.0008\n",
      "   180        0.3966             nan     0.1000   -0.0004\n",
      "   200        0.3771             nan     0.1000    0.0001\n",
      "   220        0.3591             nan     0.1000   -0.0008\n",
      "   240        0.3423             nan     0.1000   -0.0006\n",
      "   250        0.3348             nan     0.1000   -0.0007\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0459             nan     0.1000    0.0320\n",
      "     2        0.9898             nan     0.1000    0.0265\n",
      "     3        0.9456             nan     0.1000    0.0192\n",
      "     4        0.9061             nan     0.1000    0.0157\n",
      "     5        0.8763             nan     0.1000    0.0121\n",
      "     6        0.8491             nan     0.1000    0.0112\n",
      "     7        0.8256             nan     0.1000    0.0086\n",
      "     8        0.8047             nan     0.1000    0.0075\n",
      "     9        0.7834             nan     0.1000    0.0091\n",
      "    10        0.7655             nan     0.1000    0.0056\n",
      "    20        0.6562             nan     0.1000    0.0025\n",
      "    40        0.5494             nan     0.1000   -0.0008\n",
      "    60        0.4782             nan     0.1000   -0.0003\n",
      "    80        0.4254             nan     0.1000   -0.0010\n",
      "   100        0.3815             nan     0.1000   -0.0012\n",
      "   120        0.3437             nan     0.1000   -0.0010\n",
      "   140        0.3090             nan     0.1000   -0.0007\n",
      "   160        0.2789             nan     0.1000   -0.0006\n",
      "   180        0.2507             nan     0.1000   -0.0005\n",
      "   200        0.2299             nan     0.1000   -0.0003\n",
      "   220        0.2097             nan     0.1000   -0.0003\n",
      "   240        0.1899             nan     0.1000   -0.0003\n",
      "   250        0.1809             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold10.Rep2: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1106             nan     0.0100    0.0018\n",
      "     2        1.1074             nan     0.0100    0.0017\n",
      "     3        1.1043             nan     0.0100    0.0017\n",
      "     4        1.1006             nan     0.0100    0.0016\n",
      "     5        1.0974             nan     0.0100    0.0016\n",
      "     6        1.0941             nan     0.0100    0.0016\n",
      "     7        1.0907             nan     0.0100    0.0015\n",
      "     8        1.0877             nan     0.0100    0.0015\n",
      "     9        1.0847             nan     0.0100    0.0015\n",
      "    10        1.0815             nan     0.0100    0.0014\n",
      "    20        1.0551             nan     0.0100    0.0013\n",
      "    40        1.0124             nan     0.0100    0.0008\n",
      "    60        0.9788             nan     0.0100    0.0008\n",
      "    80        0.9508             nan     0.0100    0.0006\n",
      "   100        0.9262             nan     0.0100    0.0006\n",
      "   120        0.9045             nan     0.0100    0.0004\n",
      "   140        0.8868             nan     0.0100    0.0002\n",
      "   160        0.8712             nan     0.0100    0.0004\n",
      "   180        0.8570             nan     0.0100    0.0003\n",
      "   200        0.8443             nan     0.0100    0.0003\n",
      "   220        0.8330             nan     0.0100    0.0002\n",
      "   240        0.8230             nan     0.0100    0.0001\n",
      "   250        0.8185             nan     0.0100    0.0001\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0032\n",
      "     2        1.1006             nan     0.0100    0.0033\n",
      "     3        1.0946             nan     0.0100    0.0029\n",
      "     4        1.0888             nan     0.0100    0.0029\n",
      "     5        1.0824             nan     0.0100    0.0033\n",
      "     6        1.0765             nan     0.0100    0.0027\n",
      "     7        1.0710             nan     0.0100    0.0026\n",
      "     8        1.0656             nan     0.0100    0.0030\n",
      "     9        1.0604             nan     0.0100    0.0023\n",
      "    10        1.0550             nan     0.0100    0.0026\n",
      "    20        1.0076             nan     0.0100    0.0020\n",
      "    40        0.9339             nan     0.0100    0.0013\n",
      "    60        0.8791             nan     0.0100    0.0011\n",
      "    80        0.8383             nan     0.0100    0.0007\n",
      "   100        0.8046             nan     0.0100    0.0005\n",
      "   120        0.7780             nan     0.0100    0.0006\n",
      "   140        0.7546             nan     0.0100    0.0004\n",
      "   160        0.7353             nan     0.0100    0.0003\n",
      "   180        0.7193             nan     0.0100    0.0002\n",
      "   200        0.7045             nan     0.0100    0.0002\n",
      "   220        0.6922             nan     0.0100    0.0001\n",
      "   240        0.6805             nan     0.0100    0.0001\n",
      "   250        0.6752             nan     0.0100    0.0001\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1070             nan     0.0100    0.0033\n",
      "     2        1.1000             nan     0.0100    0.0033\n",
      "     3        1.0926             nan     0.0100    0.0032\n",
      "     4        1.0864             nan     0.0100    0.0026\n",
      "     5        1.0795             nan     0.0100    0.0032\n",
      "     6        1.0728             nan     0.0100    0.0031\n",
      "     7        1.0664             nan     0.0100    0.0031\n",
      "     8        1.0606             nan     0.0100    0.0027\n",
      "     9        1.0544             nan     0.0100    0.0029\n",
      "    10        1.0481             nan     0.0100    0.0026\n",
      "    20        0.9944             nan     0.0100    0.0024\n",
      "    40        0.9130             nan     0.0100    0.0015\n",
      "    60        0.8532             nan     0.0100    0.0011\n",
      "    80        0.8062             nan     0.0100    0.0008\n",
      "   100        0.7683             nan     0.0100    0.0007\n",
      "   120        0.7368             nan     0.0100    0.0006\n",
      "   140        0.7104             nan     0.0100    0.0003\n",
      "   160        0.6884             nan     0.0100    0.0001\n",
      "   180        0.6685             nan     0.0100    0.0002\n",
      "   200        0.6517             nan     0.0100   -0.0000\n",
      "   220        0.6368             nan     0.0100    0.0002\n",
      "   240        0.6228             nan     0.0100    0.0001\n",
      "   250        0.6166             nan     0.0100   -0.0001\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0972             nan     0.0500    0.0087\n",
      "     2        1.0853             nan     0.0500    0.0049\n",
      "     3        1.0689             nan     0.0500    0.0076\n",
      "     4        1.0547             nan     0.0500    0.0069\n",
      "     5        1.0415             nan     0.0500    0.0062\n",
      "     6        1.0319             nan     0.0500    0.0037\n",
      "     7        1.0217             nan     0.0500    0.0051\n",
      "     8        1.0109             nan     0.0500    0.0055\n",
      "     9        0.9992             nan     0.0500    0.0049\n",
      "    10        0.9913             nan     0.0500    0.0037\n",
      "    20        0.9251             nan     0.0500    0.0023\n",
      "    40        0.8435             nan     0.0500    0.0010\n",
      "    60        0.7972             nan     0.0500    0.0006\n",
      "    80        0.7636             nan     0.0500    0.0004\n",
      "   100        0.7413             nan     0.0500    0.0002\n",
      "   120        0.7257             nan     0.0500    0.0002\n",
      "   140        0.7130             nan     0.0500    0.0003\n",
      "   160        0.7013             nan     0.0500    0.0000\n",
      "   180        0.6919             nan     0.0500   -0.0001\n",
      "   200        0.6836             nan     0.0500   -0.0001\n",
      "   220        0.6755             nan     0.0500   -0.0000\n",
      "   240        0.6678             nan     0.0500    0.0001\n",
      "   250        0.6652             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0830             nan     0.0500    0.0149\n",
      "     2        1.0553             nan     0.0500    0.0135\n",
      "     3        1.0307             nan     0.0500    0.0115\n",
      "     4        1.0080             nan     0.0500    0.0109\n",
      "     5        0.9865             nan     0.0500    0.0100\n",
      "     6        0.9661             nan     0.0500    0.0087\n",
      "     7        0.9469             nan     0.0500    0.0087\n",
      "     8        0.9315             nan     0.0500    0.0070\n",
      "     9        0.9151             nan     0.0500    0.0071\n",
      "    10        0.9019             nan     0.0500    0.0054\n",
      "    20        0.8020             nan     0.0500    0.0022\n",
      "    40        0.7054             nan     0.0500    0.0007\n",
      "    60        0.6554             nan     0.0500    0.0002\n",
      "    80        0.6211             nan     0.0500    0.0000\n",
      "   100        0.5932             nan     0.0500    0.0001\n",
      "   120        0.5695             nan     0.0500   -0.0002\n",
      "   140        0.5486             nan     0.0500   -0.0004\n",
      "   160        0.5287             nan     0.0500   -0.0002\n",
      "   180        0.5118             nan     0.0500   -0.0005\n",
      "   200        0.4933             nan     0.0500   -0.0005\n",
      "   220        0.4777             nan     0.0500   -0.0003\n",
      "   240        0.4630             nan     0.0500   -0.0003\n",
      "   250        0.4539             nan     0.0500    0.0000\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0796             nan     0.0500    0.0164\n",
      "     2        1.0458             nan     0.0500    0.0158\n",
      "     3        1.0180             nan     0.0500    0.0125\n",
      "     4        0.9915             nan     0.0500    0.0120\n",
      "     5        0.9681             nan     0.0500    0.0101\n",
      "     6        0.9475             nan     0.0500    0.0087\n",
      "     7        0.9292             nan     0.0500    0.0085\n",
      "     8        0.9128             nan     0.0500    0.0064\n",
      "     9        0.8956             nan     0.0500    0.0074\n",
      "    10        0.8812             nan     0.0500    0.0054\n",
      "    20        0.7677             nan     0.0500    0.0021\n",
      "    40        0.6528             nan     0.0500    0.0016\n",
      "    60        0.5874             nan     0.0500    0.0002\n",
      "    80        0.5411             nan     0.0500   -0.0001\n",
      "   100        0.5013             nan     0.0500   -0.0001\n",
      "   120        0.4676             nan     0.0500   -0.0002\n",
      "   140        0.4395             nan     0.0500   -0.0004\n",
      "   160        0.4114             nan     0.0500   -0.0003\n",
      "   180        0.3863             nan     0.0500   -0.0004\n",
      "   200        0.3645             nan     0.0500   -0.0005\n",
      "   220        0.3460             nan     0.0500   -0.0001\n",
      "   240        0.3270             nan     0.0500   -0.0001\n",
      "   250        0.3173             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0803             nan     0.1000    0.0168\n",
      "     2        1.0532             nan     0.1000    0.0137\n",
      "     3        1.0292             nan     0.1000    0.0110\n",
      "     4        1.0086             nan     0.1000    0.0082\n",
      "     5        0.9900             nan     0.1000    0.0077\n",
      "     6        0.9761             nan     0.1000    0.0065\n",
      "     7        0.9570             nan     0.1000    0.0080\n",
      "     8        0.9455             nan     0.1000    0.0046\n",
      "     9        0.9313             nan     0.1000    0.0066\n",
      "    10        0.9182             nan     0.1000    0.0060\n",
      "    20        0.8419             nan     0.1000    0.0022\n",
      "    40        0.7612             nan     0.1000    0.0010\n",
      "    60        0.7220             nan     0.1000    0.0000\n",
      "    80        0.6991             nan     0.1000   -0.0003\n",
      "   100        0.6787             nan     0.1000    0.0001\n",
      "   120        0.6651             nan     0.1000    0.0001\n",
      "   140        0.6558             nan     0.1000   -0.0007\n",
      "   160        0.6472             nan     0.1000   -0.0001\n",
      "   180        0.6402             nan     0.1000   -0.0006\n",
      "   200        0.6339             nan     0.1000   -0.0001\n",
      "   220        0.6279             nan     0.1000   -0.0006\n",
      "   240        0.6235             nan     0.1000   -0.0003\n",
      "   250        0.6215             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0524             nan     0.1000    0.0303\n",
      "     2        1.0009             nan     0.1000    0.0220\n",
      "     3        0.9609             nan     0.1000    0.0184\n",
      "     4        0.9267             nan     0.1000    0.0164\n",
      "     5        0.8998             nan     0.1000    0.0122\n",
      "     6        0.8780             nan     0.1000    0.0092\n",
      "     7        0.8548             nan     0.1000    0.0095\n",
      "     8        0.8360             nan     0.1000    0.0077\n",
      "     9        0.8186             nan     0.1000    0.0067\n",
      "    10        0.8037             nan     0.1000    0.0064\n",
      "    20        0.7050             nan     0.1000    0.0030\n",
      "    40        0.6186             nan     0.1000    0.0000\n",
      "    60        0.5646             nan     0.1000   -0.0011\n",
      "    80        0.5249             nan     0.1000   -0.0010\n",
      "   100        0.4906             nan     0.1000   -0.0007\n",
      "   120        0.4624             nan     0.1000   -0.0009\n",
      "   140        0.4356             nan     0.1000   -0.0011\n",
      "   160        0.4136             nan     0.1000   -0.0006\n",
      "   180        0.3908             nan     0.1000   -0.0007\n",
      "   200        0.3701             nan     0.1000    0.0002\n",
      "   220        0.3490             nan     0.1000   -0.0003\n",
      "   240        0.3306             nan     0.1000   -0.0006\n",
      "   250        0.3229             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0427             nan     0.1000    0.0313\n",
      "     2        0.9901             nan     0.1000    0.0228\n",
      "     3        0.9443             nan     0.1000    0.0180\n",
      "     4        0.9073             nan     0.1000    0.0157\n",
      "     5        0.8744             nan     0.1000    0.0141\n",
      "     6        0.8455             nan     0.1000    0.0111\n",
      "     7        0.8202             nan     0.1000    0.0109\n",
      "     8        0.8008             nan     0.1000    0.0058\n",
      "     9        0.7792             nan     0.1000    0.0090\n",
      "    10        0.7621             nan     0.1000    0.0063\n",
      "    20        0.6484             nan     0.1000    0.0000\n",
      "    40        0.5379             nan     0.1000   -0.0004\n",
      "    60        0.4703             nan     0.1000   -0.0004\n",
      "    80        0.4200             nan     0.1000   -0.0009\n",
      "   100        0.3766             nan     0.1000   -0.0012\n",
      "   120        0.3397             nan     0.1000   -0.0014\n",
      "   140        0.3052             nan     0.1000   -0.0007\n",
      "   160        0.2714             nan     0.1000   -0.0003\n",
      "   180        0.2446             nan     0.1000   -0.0007\n",
      "   200        0.2225             nan     0.1000   -0.0005\n",
      "   220        0.2025             nan     0.1000   -0.0009\n",
      "   240        0.1849             nan     0.1000   -0.0002\n",
      "   250        0.1758             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold01.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0019\n",
      "     2        1.1071             nan     0.0100    0.0019\n",
      "     3        1.1037             nan     0.0100    0.0018\n",
      "     4        1.1001             nan     0.0100    0.0018\n",
      "     5        1.0970             nan     0.0100    0.0017\n",
      "     6        1.0936             nan     0.0100    0.0017\n",
      "     7        1.0901             nan     0.0100    0.0017\n",
      "     8        1.0866             nan     0.0100    0.0016\n",
      "     9        1.0836             nan     0.0100    0.0016\n",
      "    10        1.0802             nan     0.0100    0.0016\n",
      "    20        1.0513             nan     0.0100    0.0013\n",
      "    40        1.0071             nan     0.0100    0.0011\n",
      "    60        0.9713             nan     0.0100    0.0009\n",
      "    80        0.9402             nan     0.0100    0.0007\n",
      "   100        0.9161             nan     0.0100    0.0006\n",
      "   120        0.8954             nan     0.0100    0.0005\n",
      "   140        0.8774             nan     0.0100    0.0002\n",
      "   160        0.8605             nan     0.0100    0.0004\n",
      "   180        0.8460             nan     0.0100    0.0002\n",
      "   200        0.8335             nan     0.0100    0.0003\n",
      "   220        0.8217             nan     0.0100    0.0002\n",
      "   240        0.8104             nan     0.0100    0.0002\n",
      "   250        0.8051             nan     0.0100    0.0002\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1077             nan     0.0100    0.0033\n",
      "     2        1.1014             nan     0.0100    0.0031\n",
      "     3        1.0949             nan     0.0100    0.0032\n",
      "     4        1.0885             nan     0.0100    0.0030\n",
      "     5        1.0825             nan     0.0100    0.0029\n",
      "     6        1.0765             nan     0.0100    0.0028\n",
      "     7        1.0710             nan     0.0100    0.0025\n",
      "     8        1.0653             nan     0.0100    0.0025\n",
      "     9        1.0602             nan     0.0100    0.0025\n",
      "    10        1.0548             nan     0.0100    0.0026\n",
      "    20        1.0063             nan     0.0100    0.0022\n",
      "    40        0.9314             nan     0.0100    0.0014\n",
      "    60        0.8754             nan     0.0100    0.0011\n",
      "    80        0.8333             nan     0.0100    0.0009\n",
      "   100        0.7990             nan     0.0100    0.0004\n",
      "   120        0.7715             nan     0.0100    0.0005\n",
      "   140        0.7485             nan     0.0100    0.0004\n",
      "   160        0.7298             nan     0.0100    0.0003\n",
      "   180        0.7140             nan     0.0100    0.0002\n",
      "   200        0.6998             nan     0.0100    0.0003\n",
      "   220        0.6872             nan     0.0100    0.0002\n",
      "   240        0.6761             nan     0.0100    0.0001\n",
      "   250        0.6706             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0033\n",
      "     2        1.0998             nan     0.0100    0.0033\n",
      "     3        1.0927             nan     0.0100    0.0032\n",
      "     4        1.0854             nan     0.0100    0.0031\n",
      "     5        1.0789             nan     0.0100    0.0031\n",
      "     6        1.0720             nan     0.0100    0.0034\n",
      "     7        1.0652             nan     0.0100    0.0032\n",
      "     8        1.0589             nan     0.0100    0.0030\n",
      "     9        1.0526             nan     0.0100    0.0028\n",
      "    10        1.0465             nan     0.0100    0.0028\n",
      "    20        0.9924             nan     0.0100    0.0021\n",
      "    40        0.9106             nan     0.0100    0.0015\n",
      "    60        0.8481             nan     0.0100    0.0012\n",
      "    80        0.8006             nan     0.0100    0.0007\n",
      "   100        0.7616             nan     0.0100    0.0007\n",
      "   120        0.7309             nan     0.0100    0.0006\n",
      "   140        0.7053             nan     0.0100    0.0003\n",
      "   160        0.6839             nan     0.0100    0.0002\n",
      "   180        0.6647             nan     0.0100    0.0002\n",
      "   200        0.6462             nan     0.0100    0.0003\n",
      "   220        0.6306             nan     0.0100    0.0001\n",
      "   240        0.6169             nan     0.0100    0.0001\n",
      "   250        0.6101             nan     0.0100    0.0001\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0952             nan     0.0500    0.0093\n",
      "     2        1.0778             nan     0.0500    0.0084\n",
      "     3        1.0624             nan     0.0500    0.0075\n",
      "     4        1.0489             nan     0.0500    0.0068\n",
      "     5        1.0380             nan     0.0500    0.0051\n",
      "     6        1.0259             nan     0.0500    0.0061\n",
      "     7        1.0146             nan     0.0500    0.0056\n",
      "     8        1.0061             nan     0.0500    0.0039\n",
      "     9        0.9958             nan     0.0500    0.0050\n",
      "    10        0.9864             nan     0.0500    0.0047\n",
      "    20        0.9165             nan     0.0500    0.0029\n",
      "    40        0.8317             nan     0.0500    0.0016\n",
      "    60        0.7813             nan     0.0500    0.0012\n",
      "    80        0.7487             nan     0.0500    0.0003\n",
      "   100        0.7239             nan     0.0500    0.0002\n",
      "   120        0.7092             nan     0.0500    0.0001\n",
      "   140        0.6956             nan     0.0500    0.0000\n",
      "   160        0.6859             nan     0.0500   -0.0000\n",
      "   180        0.6776             nan     0.0500   -0.0002\n",
      "   200        0.6692             nan     0.0500   -0.0000\n",
      "   220        0.6625             nan     0.0500   -0.0000\n",
      "   240        0.6566             nan     0.0500   -0.0000\n",
      "   250        0.6538             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0824             nan     0.0500    0.0151\n",
      "     2        1.0527             nan     0.0500    0.0138\n",
      "     3        1.0261             nan     0.0500    0.0121\n",
      "     4        1.0019             nan     0.0500    0.0108\n",
      "     5        0.9810             nan     0.0500    0.0093\n",
      "     6        0.9613             nan     0.0500    0.0088\n",
      "     7        0.9449             nan     0.0500    0.0083\n",
      "     8        0.9275             nan     0.0500    0.0085\n",
      "     9        0.9119             nan     0.0500    0.0074\n",
      "    10        0.8975             nan     0.0500    0.0064\n",
      "    20        0.7998             nan     0.0500    0.0031\n",
      "    40        0.7014             nan     0.0500    0.0015\n",
      "    60        0.6471             nan     0.0500    0.0006\n",
      "    80        0.6130             nan     0.0500    0.0001\n",
      "   100        0.5861             nan     0.0500    0.0001\n",
      "   120        0.5655             nan     0.0500   -0.0003\n",
      "   140        0.5451             nan     0.0500   -0.0007\n",
      "   160        0.5262             nan     0.0500   -0.0003\n",
      "   180        0.5066             nan     0.0500   -0.0007\n",
      "   200        0.4905             nan     0.0500   -0.0004\n",
      "   220        0.4756             nan     0.0500   -0.0003\n",
      "   240        0.4583             nan     0.0500   -0.0004\n",
      "   250        0.4506             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0792             nan     0.0500    0.0151\n",
      "     2        1.0471             nan     0.0500    0.0140\n",
      "     3        1.0179             nan     0.0500    0.0124\n",
      "     4        0.9928             nan     0.0500    0.0111\n",
      "     5        0.9696             nan     0.0500    0.0096\n",
      "     6        0.9482             nan     0.0500    0.0093\n",
      "     7        0.9265             nan     0.0500    0.0092\n",
      "     8        0.9075             nan     0.0500    0.0083\n",
      "     9        0.8919             nan     0.0500    0.0062\n",
      "    10        0.8762             nan     0.0500    0.0058\n",
      "    20        0.7660             nan     0.0500    0.0027\n",
      "    40        0.6463             nan     0.0500    0.0005\n",
      "    60        0.5823             nan     0.0500    0.0002\n",
      "    80        0.5380             nan     0.0500   -0.0006\n",
      "   100        0.4995             nan     0.0500   -0.0003\n",
      "   120        0.4667             nan     0.0500   -0.0006\n",
      "   140        0.4355             nan     0.0500   -0.0002\n",
      "   160        0.4099             nan     0.0500   -0.0005\n",
      "   180        0.3841             nan     0.0500   -0.0006\n",
      "   200        0.3627             nan     0.0500   -0.0001\n",
      "   220        0.3419             nan     0.0500   -0.0002\n",
      "   240        0.3228             nan     0.0500   -0.0002\n",
      "   250        0.3139             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0779             nan     0.1000    0.0182\n",
      "     2        1.0500             nan     0.1000    0.0148\n",
      "     3        1.0252             nan     0.1000    0.0122\n",
      "     4        1.0052             nan     0.1000    0.0089\n",
      "     5        0.9856             nan     0.1000    0.0097\n",
      "     6        0.9677             nan     0.1000    0.0088\n",
      "     7        0.9530             nan     0.1000    0.0066\n",
      "     8        0.9406             nan     0.1000    0.0050\n",
      "     9        0.9282             nan     0.1000    0.0049\n",
      "    10        0.9178             nan     0.1000    0.0036\n",
      "    20        0.8289             nan     0.1000    0.0028\n",
      "    40        0.7491             nan     0.1000    0.0007\n",
      "    60        0.7108             nan     0.1000    0.0007\n",
      "    80        0.6867             nan     0.1000    0.0003\n",
      "   100        0.6710             nan     0.1000   -0.0002\n",
      "   120        0.6586             nan     0.1000   -0.0003\n",
      "   140        0.6476             nan     0.1000   -0.0003\n",
      "   160        0.6401             nan     0.1000   -0.0004\n",
      "   180        0.6347             nan     0.1000   -0.0003\n",
      "   200        0.6272             nan     0.1000   -0.0002\n",
      "   220        0.6231             nan     0.1000   -0.0004\n",
      "   240        0.6182             nan     0.1000   -0.0005\n",
      "   250        0.6163             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0494             nan     0.1000    0.0304\n",
      "     2        0.9980             nan     0.1000    0.0249\n",
      "     3        0.9592             nan     0.1000    0.0170\n",
      "     4        0.9248             nan     0.1000    0.0149\n",
      "     5        0.8972             nan     0.1000    0.0103\n",
      "     6        0.8702             nan     0.1000    0.0123\n",
      "     7        0.8469             nan     0.1000    0.0101\n",
      "     8        0.8275             nan     0.1000    0.0089\n",
      "     9        0.8096             nan     0.1000    0.0081\n",
      "    10        0.7915             nan     0.1000    0.0084\n",
      "    20        0.6953             nan     0.1000    0.0013\n",
      "    40        0.6118             nan     0.1000   -0.0006\n",
      "    60        0.5605             nan     0.1000   -0.0009\n",
      "    80        0.5197             nan     0.1000   -0.0005\n",
      "   100        0.4892             nan     0.1000   -0.0007\n",
      "   120        0.4560             nan     0.1000   -0.0006\n",
      "   140        0.4301             nan     0.1000   -0.0007\n",
      "   160        0.4056             nan     0.1000   -0.0008\n",
      "   180        0.3849             nan     0.1000   -0.0005\n",
      "   200        0.3625             nan     0.1000   -0.0004\n",
      "   220        0.3431             nan     0.1000   -0.0005\n",
      "   240        0.3275             nan     0.1000   -0.0001\n",
      "   250        0.3180             nan     0.1000   -0.0008\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0455             nan     0.1000    0.0296\n",
      "     2        0.9911             nan     0.1000    0.0246\n",
      "     3        0.9471             nan     0.1000    0.0194\n",
      "     4        0.9062             nan     0.1000    0.0170\n",
      "     5        0.8715             nan     0.1000    0.0153\n",
      "     6        0.8448             nan     0.1000    0.0099\n",
      "     7        0.8167             nan     0.1000    0.0109\n",
      "     8        0.7951             nan     0.1000    0.0079\n",
      "     9        0.7754             nan     0.1000    0.0084\n",
      "    10        0.7574             nan     0.1000    0.0067\n",
      "    20        0.6459             nan     0.1000    0.0015\n",
      "    40        0.5356             nan     0.1000   -0.0012\n",
      "    60        0.4574             nan     0.1000   -0.0012\n",
      "    80        0.4037             nan     0.1000   -0.0008\n",
      "   100        0.3620             nan     0.1000   -0.0004\n",
      "   120        0.3253             nan     0.1000   -0.0008\n",
      "   140        0.2930             nan     0.1000   -0.0008\n",
      "   160        0.2646             nan     0.1000   -0.0004\n",
      "   180        0.2380             nan     0.1000   -0.0001\n",
      "   200        0.2174             nan     0.1000   -0.0006\n",
      "   220        0.1982             nan     0.1000   -0.0004\n",
      "   240        0.1808             nan     0.1000   -0.0007\n",
      "   250        0.1720             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold02.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1109             nan     0.0100    0.0018\n",
      "     2        1.1070             nan     0.0100    0.0018\n",
      "     3        1.1037             nan     0.0100    0.0017\n",
      "     4        1.1004             nan     0.0100    0.0017\n",
      "     5        1.0971             nan     0.0100    0.0017\n",
      "     6        1.0938             nan     0.0100    0.0016\n",
      "     7        1.0908             nan     0.0100    0.0016\n",
      "     8        1.0876             nan     0.0100    0.0016\n",
      "     9        1.0845             nan     0.0100    0.0015\n",
      "    10        1.0815             nan     0.0100    0.0015\n",
      "    20        1.0540             nan     0.0100    0.0013\n",
      "    40        1.0109             nan     0.0100    0.0010\n",
      "    60        0.9781             nan     0.0100    0.0007\n",
      "    80        0.9503             nan     0.0100    0.0005\n",
      "   100        0.9276             nan     0.0100    0.0006\n",
      "   120        0.9058             nan     0.0100    0.0004\n",
      "   140        0.8877             nan     0.0100    0.0002\n",
      "   160        0.8711             nan     0.0100    0.0003\n",
      "   180        0.8561             nan     0.0100    0.0003\n",
      "   200        0.8436             nan     0.0100    0.0002\n",
      "   220        0.8319             nan     0.0100    0.0002\n",
      "   240        0.8210             nan     0.0100    0.0002\n",
      "   250        0.8163             nan     0.0100    0.0001\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1078             nan     0.0100    0.0031\n",
      "     2        1.1015             nan     0.0100    0.0029\n",
      "     3        1.0957             nan     0.0100    0.0027\n",
      "     4        1.0893             nan     0.0100    0.0028\n",
      "     5        1.0835             nan     0.0100    0.0028\n",
      "     6        1.0780             nan     0.0100    0.0027\n",
      "     7        1.0721             nan     0.0100    0.0028\n",
      "     8        1.0668             nan     0.0100    0.0025\n",
      "     9        1.0614             nan     0.0100    0.0025\n",
      "    10        1.0561             nan     0.0100    0.0026\n",
      "    20        1.0087             nan     0.0100    0.0020\n",
      "    40        0.9357             nan     0.0100    0.0014\n",
      "    60        0.8824             nan     0.0100    0.0009\n",
      "    80        0.8406             nan     0.0100    0.0007\n",
      "   100        0.8066             nan     0.0100    0.0006\n",
      "   120        0.7806             nan     0.0100    0.0003\n",
      "   140        0.7579             nan     0.0100    0.0004\n",
      "   160        0.7392             nan     0.0100    0.0003\n",
      "   180        0.7229             nan     0.0100    0.0001\n",
      "   200        0.7090             nan     0.0100    0.0001\n",
      "   220        0.6963             nan     0.0100    0.0001\n",
      "   240        0.6856             nan     0.0100    0.0002\n",
      "   250        0.6799             nan     0.0100    0.0002\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0032\n",
      "     2        1.1005             nan     0.0100    0.0029\n",
      "     3        1.0937             nan     0.0100    0.0030\n",
      "     4        1.0874             nan     0.0100    0.0029\n",
      "     5        1.0802             nan     0.0100    0.0033\n",
      "     6        1.0738             nan     0.0100    0.0029\n",
      "     7        1.0674             nan     0.0100    0.0029\n",
      "     8        1.0614             nan     0.0100    0.0027\n",
      "     9        1.0555             nan     0.0100    0.0028\n",
      "    10        1.0495             nan     0.0100    0.0025\n",
      "    20        0.9960             nan     0.0100    0.0022\n",
      "    40        0.9145             nan     0.0100    0.0014\n",
      "    60        0.8531             nan     0.0100    0.0010\n",
      "    80        0.8068             nan     0.0100    0.0006\n",
      "   100        0.7704             nan     0.0100    0.0004\n",
      "   120        0.7392             nan     0.0100    0.0002\n",
      "   140        0.7131             nan     0.0100    0.0004\n",
      "   160        0.6906             nan     0.0100    0.0002\n",
      "   180        0.6720             nan     0.0100   -0.0000\n",
      "   200        0.6552             nan     0.0100   -0.0001\n",
      "   220        0.6393             nan     0.0100    0.0001\n",
      "   240        0.6260             nan     0.0100    0.0001\n",
      "   250        0.6194             nan     0.0100    0.0000\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0973             nan     0.0500    0.0087\n",
      "     2        1.0803             nan     0.0500    0.0080\n",
      "     3        1.0643             nan     0.0500    0.0071\n",
      "     4        1.0523             nan     0.0500    0.0062\n",
      "     5        1.0419             nan     0.0500    0.0044\n",
      "     6        1.0305             nan     0.0500    0.0058\n",
      "     7        1.0211             nan     0.0500    0.0036\n",
      "     8        1.0133             nan     0.0500    0.0039\n",
      "     9        1.0031             nan     0.0500    0.0052\n",
      "    10        0.9934             nan     0.0500    0.0047\n",
      "    20        0.9217             nan     0.0500    0.0021\n",
      "    40        0.8455             nan     0.0500    0.0019\n",
      "    60        0.7958             nan     0.0500    0.0010\n",
      "    80        0.7625             nan     0.0500    0.0006\n",
      "   100        0.7403             nan     0.0500    0.0004\n",
      "   120        0.7233             nan     0.0500    0.0002\n",
      "   140        0.7103             nan     0.0500   -0.0000\n",
      "   160        0.7003             nan     0.0500   -0.0004\n",
      "   180        0.6924             nan     0.0500   -0.0001\n",
      "   200        0.6846             nan     0.0500   -0.0001\n",
      "   220        0.6756             nan     0.0500    0.0000\n",
      "   240        0.6698             nan     0.0500   -0.0001\n",
      "   250        0.6669             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0863             nan     0.0500    0.0131\n",
      "     2        1.0557             nan     0.0500    0.0145\n",
      "     3        1.0296             nan     0.0500    0.0123\n",
      "     4        1.0068             nan     0.0500    0.0117\n",
      "     5        0.9864             nan     0.0500    0.0094\n",
      "     6        0.9669             nan     0.0500    0.0094\n",
      "     7        0.9485             nan     0.0500    0.0088\n",
      "     8        0.9330             nan     0.0500    0.0073\n",
      "     9        0.9182             nan     0.0500    0.0066\n",
      "    10        0.9042             nan     0.0500    0.0064\n",
      "    20        0.8066             nan     0.0500    0.0021\n",
      "    40        0.7092             nan     0.0500    0.0005\n",
      "    60        0.6582             nan     0.0500   -0.0001\n",
      "    80        0.6224             nan     0.0500   -0.0002\n",
      "   100        0.5942             nan     0.0500   -0.0001\n",
      "   120        0.5700             nan     0.0500   -0.0004\n",
      "   140        0.5492             nan     0.0500   -0.0005\n",
      "   160        0.5310             nan     0.0500   -0.0006\n",
      "   180        0.5154             nan     0.0500    0.0001\n",
      "   200        0.4982             nan     0.0500   -0.0002\n",
      "   220        0.4836             nan     0.0500   -0.0004\n",
      "   240        0.4669             nan     0.0500   -0.0003\n",
      "   250        0.4594             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0794             nan     0.0500    0.0174\n",
      "     2        1.0494             nan     0.0500    0.0139\n",
      "     3        1.0212             nan     0.0500    0.0110\n",
      "     4        0.9946             nan     0.0500    0.0116\n",
      "     5        0.9694             nan     0.0500    0.0116\n",
      "     6        0.9484             nan     0.0500    0.0094\n",
      "     7        0.9299             nan     0.0500    0.0078\n",
      "     8        0.9111             nan     0.0500    0.0071\n",
      "     9        0.8941             nan     0.0500    0.0076\n",
      "    10        0.8795             nan     0.0500    0.0062\n",
      "    20        0.7686             nan     0.0500    0.0026\n",
      "    40        0.6590             nan     0.0500    0.0009\n",
      "    60        0.5948             nan     0.0500   -0.0006\n",
      "    80        0.5492             nan     0.0500   -0.0005\n",
      "   100        0.5122             nan     0.0500   -0.0006\n",
      "   120        0.4796             nan     0.0500   -0.0003\n",
      "   140        0.4502             nan     0.0500   -0.0002\n",
      "   160        0.4216             nan     0.0500   -0.0004\n",
      "   180        0.3979             nan     0.0500   -0.0004\n",
      "   200        0.3766             nan     0.0500   -0.0003\n",
      "   220        0.3591             nan     0.0500   -0.0004\n",
      "   240        0.3407             nan     0.0500   -0.0003\n",
      "   250        0.3302             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0815             nan     0.1000    0.0173\n",
      "     2        1.0532             nan     0.1000    0.0141\n",
      "     3        1.0321             nan     0.1000    0.0092\n",
      "     4        1.0105             nan     0.1000    0.0113\n",
      "     5        0.9924             nan     0.1000    0.0092\n",
      "     6        0.9773             nan     0.1000    0.0078\n",
      "     7        0.9628             nan     0.1000    0.0062\n",
      "     8        0.9463             nan     0.1000    0.0073\n",
      "     9        0.9346             nan     0.1000    0.0053\n",
      "    10        0.9212             nan     0.1000    0.0057\n",
      "    20        0.8456             nan     0.1000    0.0018\n",
      "    40        0.7635             nan     0.1000    0.0011\n",
      "    60        0.7268             nan     0.1000    0.0005\n",
      "    80        0.7038             nan     0.1000   -0.0001\n",
      "   100        0.6862             nan     0.1000    0.0000\n",
      "   120        0.6716             nan     0.1000   -0.0000\n",
      "   140        0.6601             nan     0.1000   -0.0004\n",
      "   160        0.6518             nan     0.1000   -0.0002\n",
      "   180        0.6450             nan     0.1000   -0.0002\n",
      "   200        0.6382             nan     0.1000   -0.0000\n",
      "   220        0.6321             nan     0.1000   -0.0001\n",
      "   240        0.6272             nan     0.1000   -0.0005\n",
      "   250        0.6250             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0559             nan     0.1000    0.0292\n",
      "     2        1.0066             nan     0.1000    0.0239\n",
      "     3        0.9666             nan     0.1000    0.0180\n",
      "     4        0.9337             nan     0.1000    0.0150\n",
      "     5        0.9046             nan     0.1000    0.0125\n",
      "     6        0.8795             nan     0.1000    0.0115\n",
      "     7        0.8611             nan     0.1000    0.0065\n",
      "     8        0.8406             nan     0.1000    0.0093\n",
      "     9        0.8245             nan     0.1000    0.0068\n",
      "    10        0.8080             nan     0.1000    0.0070\n",
      "    20        0.7111             nan     0.1000    0.0022\n",
      "    40        0.6254             nan     0.1000   -0.0003\n",
      "    60        0.5755             nan     0.1000   -0.0008\n",
      "    80        0.5357             nan     0.1000   -0.0012\n",
      "   100        0.5018             nan     0.1000   -0.0006\n",
      "   120        0.4723             nan     0.1000   -0.0013\n",
      "   140        0.4443             nan     0.1000   -0.0002\n",
      "   160        0.4195             nan     0.1000   -0.0009\n",
      "   180        0.3964             nan     0.1000   -0.0006\n",
      "   200        0.3760             nan     0.1000   -0.0006\n",
      "   220        0.3572             nan     0.1000   -0.0005\n",
      "   240        0.3384             nan     0.1000   -0.0005\n",
      "   250        0.3302             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0510             nan     0.1000    0.0276\n",
      "     2        0.9993             nan     0.1000    0.0226\n",
      "     3        0.9542             nan     0.1000    0.0198\n",
      "     4        0.9166             nan     0.1000    0.0150\n",
      "     5        0.8836             nan     0.1000    0.0144\n",
      "     6        0.8595             nan     0.1000    0.0084\n",
      "     7        0.8367             nan     0.1000    0.0077\n",
      "     8        0.8130             nan     0.1000    0.0102\n",
      "     9        0.7943             nan     0.1000    0.0062\n",
      "    10        0.7769             nan     0.1000    0.0064\n",
      "    20        0.6621             nan     0.1000    0.0013\n",
      "    40        0.5546             nan     0.1000    0.0004\n",
      "    60        0.4857             nan     0.1000    0.0000\n",
      "    80        0.4295             nan     0.1000   -0.0007\n",
      "   100        0.3886             nan     0.1000   -0.0006\n",
      "   120        0.3479             nan     0.1000   -0.0004\n",
      "   140        0.3115             nan     0.1000   -0.0005\n",
      "   160        0.2836             nan     0.1000   -0.0007\n",
      "   180        0.2557             nan     0.1000   -0.0004\n",
      "   200        0.2323             nan     0.1000   -0.0007\n",
      "   220        0.2134             nan     0.1000   -0.0007\n",
      "   240        0.1929             nan     0.1000   -0.0007\n",
      "   250        0.1845             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold03.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1105             nan     0.0100    0.0018\n",
      "     2        1.1069             nan     0.0100    0.0018\n",
      "     3        1.1034             nan     0.0100    0.0018\n",
      "     4        1.1001             nan     0.0100    0.0017\n",
      "     5        1.0971             nan     0.0100    0.0017\n",
      "     6        1.0939             nan     0.0100    0.0017\n",
      "     7        1.0909             nan     0.0100    0.0016\n",
      "     8        1.0878             nan     0.0100    0.0016\n",
      "     9        1.0845             nan     0.0100    0.0016\n",
      "    10        1.0810             nan     0.0100    0.0015\n",
      "    20        1.0535             nan     0.0100    0.0013\n",
      "    40        1.0087             nan     0.0100    0.0009\n",
      "    60        0.9724             nan     0.0100    0.0009\n",
      "    80        0.9422             nan     0.0100    0.0005\n",
      "   100        0.9171             nan     0.0100    0.0004\n",
      "   120        0.8950             nan     0.0100    0.0004\n",
      "   140        0.8768             nan     0.0100    0.0003\n",
      "   160        0.8611             nan     0.0100    0.0003\n",
      "   180        0.8463             nan     0.0100    0.0002\n",
      "   200        0.8345             nan     0.0100    0.0002\n",
      "   220        0.8231             nan     0.0100    0.0003\n",
      "   240        0.8129             nan     0.0100    0.0001\n",
      "   250        0.8082             nan     0.0100    0.0002\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0035\n",
      "     2        1.1005             nan     0.0100    0.0031\n",
      "     3        1.0936             nan     0.0100    0.0031\n",
      "     4        1.0872             nan     0.0100    0.0030\n",
      "     5        1.0811             nan     0.0100    0.0028\n",
      "     6        1.0752             nan     0.0100    0.0027\n",
      "     7        1.0697             nan     0.0100    0.0025\n",
      "     8        1.0640             nan     0.0100    0.0027\n",
      "     9        1.0579             nan     0.0100    0.0027\n",
      "    10        1.0522             nan     0.0100    0.0028\n",
      "    20        1.0019             nan     0.0100    0.0022\n",
      "    40        0.9271             nan     0.0100    0.0012\n",
      "    60        0.8711             nan     0.0100    0.0009\n",
      "    80        0.8285             nan     0.0100    0.0009\n",
      "   100        0.7954             nan     0.0100    0.0005\n",
      "   120        0.7680             nan     0.0100    0.0005\n",
      "   140        0.7460             nan     0.0100    0.0003\n",
      "   160        0.7274             nan     0.0100    0.0003\n",
      "   180        0.7117             nan     0.0100    0.0002\n",
      "   200        0.6970             nan     0.0100    0.0001\n",
      "   220        0.6840             nan     0.0100    0.0001\n",
      "   240        0.6733             nan     0.0100    0.0000\n",
      "   250        0.6679             nan     0.0100    0.0002\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1065             nan     0.0100    0.0034\n",
      "     2        1.0992             nan     0.0100    0.0032\n",
      "     3        1.0920             nan     0.0100    0.0033\n",
      "     4        1.0848             nan     0.0100    0.0033\n",
      "     5        1.0778             nan     0.0100    0.0033\n",
      "     6        1.0713             nan     0.0100    0.0032\n",
      "     7        1.0649             nan     0.0100    0.0029\n",
      "     8        1.0583             nan     0.0100    0.0030\n",
      "     9        1.0520             nan     0.0100    0.0031\n",
      "    10        1.0457             nan     0.0100    0.0026\n",
      "    20        0.9908             nan     0.0100    0.0021\n",
      "    40        0.9070             nan     0.0100    0.0017\n",
      "    60        0.8457             nan     0.0100    0.0012\n",
      "    80        0.7979             nan     0.0100    0.0008\n",
      "   100        0.7602             nan     0.0100    0.0004\n",
      "   120        0.7279             nan     0.0100    0.0004\n",
      "   140        0.7013             nan     0.0100    0.0004\n",
      "   160        0.6788             nan     0.0100    0.0002\n",
      "   180        0.6592             nan     0.0100    0.0000\n",
      "   200        0.6429             nan     0.0100    0.0002\n",
      "   220        0.6270             nan     0.0100    0.0002\n",
      "   240        0.6132             nan     0.0100    0.0001\n",
      "   250        0.6066             nan     0.0100    0.0001\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0964             nan     0.0500    0.0091\n",
      "     2        1.0794             nan     0.0500    0.0082\n",
      "     3        1.0650             nan     0.0500    0.0073\n",
      "     4        1.0511             nan     0.0500    0.0066\n",
      "     5        1.0394             nan     0.0500    0.0064\n",
      "     6        1.0270             nan     0.0500    0.0059\n",
      "     7        1.0167             nan     0.0500    0.0049\n",
      "     8        1.0075             nan     0.0500    0.0039\n",
      "     9        0.9977             nan     0.0500    0.0041\n",
      "    10        0.9865             nan     0.0500    0.0052\n",
      "    20        0.9155             nan     0.0500    0.0021\n",
      "    40        0.8331             nan     0.0500    0.0013\n",
      "    60        0.7824             nan     0.0500    0.0008\n",
      "    80        0.7514             nan     0.0500    0.0003\n",
      "   100        0.7292             nan     0.0500    0.0004\n",
      "   120        0.7128             nan     0.0500    0.0002\n",
      "   140        0.7014             nan     0.0500    0.0001\n",
      "   160        0.6923             nan     0.0500    0.0002\n",
      "   180        0.6828             nan     0.0500    0.0000\n",
      "   200        0.6751             nan     0.0500   -0.0002\n",
      "   220        0.6676             nan     0.0500   -0.0002\n",
      "   240        0.6608             nan     0.0500   -0.0000\n",
      "   250        0.6575             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0828             nan     0.0500    0.0168\n",
      "     2        1.0525             nan     0.0500    0.0150\n",
      "     3        1.0259             nan     0.0500    0.0126\n",
      "     4        1.0022             nan     0.0500    0.0114\n",
      "     5        0.9811             nan     0.0500    0.0101\n",
      "     6        0.9624             nan     0.0500    0.0083\n",
      "     7        0.9441             nan     0.0500    0.0078\n",
      "     8        0.9273             nan     0.0500    0.0076\n",
      "     9        0.9129             nan     0.0500    0.0059\n",
      "    10        0.8993             nan     0.0500    0.0067\n",
      "    20        0.7967             nan     0.0500    0.0030\n",
      "    40        0.6946             nan     0.0500    0.0011\n",
      "    60        0.6447             nan     0.0500   -0.0000\n",
      "    80        0.6090             nan     0.0500   -0.0005\n",
      "   100        0.5834             nan     0.0500    0.0001\n",
      "   120        0.5606             nan     0.0500    0.0001\n",
      "   140        0.5388             nan     0.0500   -0.0007\n",
      "   160        0.5193             nan     0.0500   -0.0004\n",
      "   180        0.5018             nan     0.0500   -0.0005\n",
      "   200        0.4832             nan     0.0500    0.0000\n",
      "   220        0.4678             nan     0.0500   -0.0004\n",
      "   240        0.4541             nan     0.0500   -0.0006\n",
      "   250        0.4463             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0744             nan     0.0500    0.0174\n",
      "     2        1.0411             nan     0.0500    0.0140\n",
      "     3        1.0131             nan     0.0500    0.0113\n",
      "     4        0.9855             nan     0.0500    0.0114\n",
      "     5        0.9630             nan     0.0500    0.0096\n",
      "     6        0.9430             nan     0.0500    0.0082\n",
      "     7        0.9211             nan     0.0500    0.0086\n",
      "     8        0.9018             nan     0.0500    0.0081\n",
      "     9        0.8838             nan     0.0500    0.0077\n",
      "    10        0.8676             nan     0.0500    0.0070\n",
      "    20        0.7520             nan     0.0500    0.0025\n",
      "    40        0.6427             nan     0.0500    0.0001\n",
      "    60        0.5824             nan     0.0500   -0.0002\n",
      "    80        0.5384             nan     0.0500   -0.0000\n",
      "   100        0.5004             nan     0.0500   -0.0000\n",
      "   120        0.4666             nan     0.0500   -0.0007\n",
      "   140        0.4356             nan     0.0500   -0.0003\n",
      "   160        0.4097             nan     0.0500   -0.0006\n",
      "   180        0.3855             nan     0.0500   -0.0002\n",
      "   200        0.3633             nan     0.0500   -0.0002\n",
      "   220        0.3432             nan     0.0500   -0.0003\n",
      "   240        0.3245             nan     0.0500   -0.0004\n",
      "   250        0.3144             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0785             nan     0.1000    0.0176\n",
      "     2        1.0481             nan     0.1000    0.0140\n",
      "     3        1.0261             nan     0.1000    0.0094\n",
      "     4        1.0054             nan     0.1000    0.0089\n",
      "     5        0.9828             nan     0.1000    0.0108\n",
      "     6        0.9678             nan     0.1000    0.0069\n",
      "     7        0.9523             nan     0.1000    0.0061\n",
      "     8        0.9356             nan     0.1000    0.0085\n",
      "     9        0.9254             nan     0.1000    0.0048\n",
      "    10        0.9115             nan     0.1000    0.0069\n",
      "    20        0.8298             nan     0.1000    0.0024\n",
      "    40        0.7511             nan     0.1000    0.0009\n",
      "    60        0.7145             nan     0.1000    0.0000\n",
      "    80        0.6906             nan     0.1000    0.0004\n",
      "   100        0.6742             nan     0.1000    0.0002\n",
      "   120        0.6597             nan     0.1000    0.0001\n",
      "   140        0.6498             nan     0.1000   -0.0004\n",
      "   160        0.6412             nan     0.1000   -0.0001\n",
      "   180        0.6329             nan     0.1000   -0.0003\n",
      "   200        0.6267             nan     0.1000   -0.0001\n",
      "   220        0.6206             nan     0.1000   -0.0004\n",
      "   240        0.6155             nan     0.1000   -0.0006\n",
      "   250        0.6138             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0528             nan     0.1000    0.0298\n",
      "     2        1.0039             nan     0.1000    0.0212\n",
      "     3        0.9559             nan     0.1000    0.0237\n",
      "     4        0.9226             nan     0.1000    0.0156\n",
      "     5        0.8927             nan     0.1000    0.0122\n",
      "     6        0.8681             nan     0.1000    0.0101\n",
      "     7        0.8463             nan     0.1000    0.0106\n",
      "     8        0.8247             nan     0.1000    0.0092\n",
      "     9        0.8059             nan     0.1000    0.0062\n",
      "    10        0.7882             nan     0.1000    0.0063\n",
      "    20        0.6945             nan     0.1000    0.0014\n",
      "    40        0.6057             nan     0.1000    0.0004\n",
      "    60        0.5564             nan     0.1000   -0.0009\n",
      "    80        0.5215             nan     0.1000   -0.0005\n",
      "   100        0.4850             nan     0.1000   -0.0006\n",
      "   120        0.4526             nan     0.1000   -0.0002\n",
      "   140        0.4276             nan     0.1000   -0.0008\n",
      "   160        0.4020             nan     0.1000   -0.0004\n",
      "   180        0.3796             nan     0.1000   -0.0007\n",
      "   200        0.3578             nan     0.1000   -0.0007\n",
      "   220        0.3370             nan     0.1000   -0.0011\n",
      "   240        0.3200             nan     0.1000   -0.0002\n",
      "   250        0.3097             nan     0.1000   -0.0000\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0408             nan     0.1000    0.0331\n",
      "     2        0.9851             nan     0.1000    0.0231\n",
      "     3        0.9370             nan     0.1000    0.0224\n",
      "     4        0.8998             nan     0.1000    0.0163\n",
      "     5        0.8676             nan     0.1000    0.0119\n",
      "     6        0.8390             nan     0.1000    0.0127\n",
      "     7        0.8138             nan     0.1000    0.0091\n",
      "     8        0.7926             nan     0.1000    0.0075\n",
      "     9        0.7738             nan     0.1000    0.0062\n",
      "    10        0.7539             nan     0.1000    0.0069\n",
      "    20        0.6369             nan     0.1000    0.0014\n",
      "    40        0.5331             nan     0.1000   -0.0013\n",
      "    60        0.4627             nan     0.1000   -0.0009\n",
      "    80        0.4088             nan     0.1000   -0.0007\n",
      "   100        0.3639             nan     0.1000   -0.0013\n",
      "   120        0.3230             nan     0.1000   -0.0003\n",
      "   140        0.2889             nan     0.1000   -0.0004\n",
      "   160        0.2565             nan     0.1000   -0.0003\n",
      "   180        0.2312             nan     0.1000   -0.0005\n",
      "   200        0.2083             nan     0.1000   -0.0008\n",
      "   220        0.1899             nan     0.1000   -0.0003\n",
      "   240        0.1723             nan     0.1000   -0.0000\n",
      "   250        0.1643             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold04.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1102             nan     0.0100    0.0018\n",
      "     2        1.1067             nan     0.0100    0.0018\n",
      "     3        1.1032             nan     0.0100    0.0017\n",
      "     4        1.0994             nan     0.0100    0.0017\n",
      "     5        1.0961             nan     0.0100    0.0017\n",
      "     6        1.0924             nan     0.0100    0.0016\n",
      "     7        1.0893             nan     0.0100    0.0016\n",
      "     8        1.0863             nan     0.0100    0.0016\n",
      "     9        1.0833             nan     0.0100    0.0015\n",
      "    10        1.0803             nan     0.0100    0.0015\n",
      "    20        1.0538             nan     0.0100    0.0013\n",
      "    40        1.0111             nan     0.0100    0.0010\n",
      "    60        0.9765             nan     0.0100    0.0006\n",
      "    80        0.9466             nan     0.0100    0.0006\n",
      "   100        0.9221             nan     0.0100    0.0006\n",
      "   120        0.9016             nan     0.0100    0.0004\n",
      "   140        0.8843             nan     0.0100    0.0003\n",
      "   160        0.8685             nan     0.0100    0.0002\n",
      "   180        0.8540             nan     0.0100    0.0003\n",
      "   200        0.8410             nan     0.0100    0.0003\n",
      "   220        0.8290             nan     0.0100    0.0003\n",
      "   240        0.8184             nan     0.0100    0.0002\n",
      "   250        0.8138             nan     0.0100    0.0002\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1080             nan     0.0100    0.0027\n",
      "     2        1.1014             nan     0.0100    0.0027\n",
      "     3        1.0954             nan     0.0100    0.0027\n",
      "     4        1.0891             nan     0.0100    0.0031\n",
      "     5        1.0831             nan     0.0100    0.0029\n",
      "     6        1.0772             nan     0.0100    0.0024\n",
      "     7        1.0716             nan     0.0100    0.0027\n",
      "     8        1.0661             nan     0.0100    0.0026\n",
      "     9        1.0610             nan     0.0100    0.0025\n",
      "    10        1.0559             nan     0.0100    0.0025\n",
      "    20        1.0069             nan     0.0100    0.0021\n",
      "    40        0.9325             nan     0.0100    0.0016\n",
      "    60        0.8780             nan     0.0100    0.0010\n",
      "    80        0.8350             nan     0.0100    0.0008\n",
      "   100        0.8020             nan     0.0100    0.0006\n",
      "   120        0.7749             nan     0.0100    0.0006\n",
      "   140        0.7519             nan     0.0100    0.0005\n",
      "   160        0.7325             nan     0.0100    0.0003\n",
      "   180        0.7166             nan     0.0100    0.0002\n",
      "   200        0.7019             nan     0.0100    0.0003\n",
      "   220        0.6894             nan     0.0100    0.0003\n",
      "   240        0.6787             nan     0.0100    0.0001\n",
      "   250        0.6734             nan     0.0100    0.0001\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1065             nan     0.0100    0.0034\n",
      "     2        1.0993             nan     0.0100    0.0031\n",
      "     3        1.0921             nan     0.0100    0.0033\n",
      "     4        1.0859             nan     0.0100    0.0028\n",
      "     5        1.0794             nan     0.0100    0.0029\n",
      "     6        1.0732             nan     0.0100    0.0029\n",
      "     7        1.0663             nan     0.0100    0.0028\n",
      "     8        1.0602             nan     0.0100    0.0025\n",
      "     9        1.0540             nan     0.0100    0.0029\n",
      "    10        1.0479             nan     0.0100    0.0027\n",
      "    20        0.9928             nan     0.0100    0.0022\n",
      "    40        0.9125             nan     0.0100    0.0015\n",
      "    60        0.8522             nan     0.0100    0.0012\n",
      "    80        0.8046             nan     0.0100    0.0009\n",
      "   100        0.7662             nan     0.0100    0.0007\n",
      "   120        0.7352             nan     0.0100    0.0006\n",
      "   140        0.7087             nan     0.0100    0.0003\n",
      "   160        0.6860             nan     0.0100    0.0002\n",
      "   180        0.6660             nan     0.0100    0.0002\n",
      "   200        0.6499             nan     0.0100    0.0002\n",
      "   220        0.6346             nan     0.0100    0.0001\n",
      "   240        0.6205             nan     0.0100    0.0001\n",
      "   250        0.6141             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0976             nan     0.0500    0.0088\n",
      "     2        1.0824             nan     0.0500    0.0081\n",
      "     3        1.0682             nan     0.0500    0.0073\n",
      "     4        1.0567             nan     0.0500    0.0052\n",
      "     5        1.0433             nan     0.0500    0.0066\n",
      "     6        1.0303             nan     0.0500    0.0059\n",
      "     7        1.0191             nan     0.0500    0.0054\n",
      "     8        1.0087             nan     0.0500    0.0048\n",
      "     9        1.0001             nan     0.0500    0.0037\n",
      "    10        0.9928             nan     0.0500    0.0029\n",
      "    20        0.9237             nan     0.0500    0.0028\n",
      "    40        0.8379             nan     0.0500    0.0011\n",
      "    60        0.7906             nan     0.0500    0.0010\n",
      "    80        0.7571             nan     0.0500    0.0004\n",
      "   100        0.7345             nan     0.0500    0.0004\n",
      "   120        0.7160             nan     0.0500    0.0005\n",
      "   140        0.7027             nan     0.0500    0.0001\n",
      "   160        0.6918             nan     0.0500   -0.0000\n",
      "   180        0.6836             nan     0.0500    0.0000\n",
      "   200        0.6752             nan     0.0500    0.0000\n",
      "   220        0.6682             nan     0.0500   -0.0001\n",
      "   240        0.6624             nan     0.0500   -0.0000\n",
      "   250        0.6596             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0824             nan     0.0500    0.0162\n",
      "     2        1.0535             nan     0.0500    0.0135\n",
      "     3        1.0300             nan     0.0500    0.0113\n",
      "     4        1.0075             nan     0.0500    0.0111\n",
      "     5        0.9870             nan     0.0500    0.0086\n",
      "     6        0.9685             nan     0.0500    0.0081\n",
      "     7        0.9526             nan     0.0500    0.0074\n",
      "     8        0.9351             nan     0.0500    0.0077\n",
      "     9        0.9205             nan     0.0500    0.0060\n",
      "    10        0.9054             nan     0.0500    0.0065\n",
      "    20        0.8026             nan     0.0500    0.0038\n",
      "    40        0.7056             nan     0.0500    0.0009\n",
      "    60        0.6540             nan     0.0500    0.0005\n",
      "    80        0.6177             nan     0.0500   -0.0000\n",
      "   100        0.5896             nan     0.0500   -0.0003\n",
      "   120        0.5657             nan     0.0500   -0.0004\n",
      "   140        0.5444             nan     0.0500   -0.0002\n",
      "   160        0.5266             nan     0.0500    0.0000\n",
      "   180        0.5099             nan     0.0500   -0.0002\n",
      "   200        0.4930             nan     0.0500   -0.0001\n",
      "   220        0.4757             nan     0.0500   -0.0008\n",
      "   240        0.4616             nan     0.0500   -0.0007\n",
      "   250        0.4551             nan     0.0500   -0.0007\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0766             nan     0.0500    0.0164\n",
      "     2        1.0445             nan     0.0500    0.0159\n",
      "     3        1.0150             nan     0.0500    0.0138\n",
      "     4        0.9889             nan     0.0500    0.0115\n",
      "     5        0.9639             nan     0.0500    0.0105\n",
      "     6        0.9427             nan     0.0500    0.0092\n",
      "     7        0.9242             nan     0.0500    0.0074\n",
      "     8        0.9050             nan     0.0500    0.0081\n",
      "     9        0.8888             nan     0.0500    0.0069\n",
      "    10        0.8722             nan     0.0500    0.0059\n",
      "    20        0.7606             nan     0.0500    0.0018\n",
      "    40        0.6477             nan     0.0500    0.0004\n",
      "    60        0.5873             nan     0.0500   -0.0003\n",
      "    80        0.5432             nan     0.0500   -0.0001\n",
      "   100        0.5049             nan     0.0500   -0.0001\n",
      "   120        0.4702             nan     0.0500   -0.0003\n",
      "   140        0.4449             nan     0.0500   -0.0007\n",
      "   160        0.4195             nan     0.0500   -0.0005\n",
      "   180        0.3939             nan     0.0500   -0.0003\n",
      "   200        0.3723             nan     0.0500   -0.0005\n",
      "   220        0.3518             nan     0.0500   -0.0002\n",
      "   240        0.3319             nan     0.0500   -0.0002\n",
      "   250        0.3230             nan     0.0500   -0.0008\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0776             nan     0.1000    0.0170\n",
      "     2        1.0551             nan     0.1000    0.0108\n",
      "     3        1.0268             nan     0.1000    0.0133\n",
      "     4        1.0048             nan     0.1000    0.0108\n",
      "     5        0.9910             nan     0.1000    0.0060\n",
      "     6        0.9726             nan     0.1000    0.0086\n",
      "     7        0.9555             nan     0.1000    0.0079\n",
      "     8        0.9414             nan     0.1000    0.0070\n",
      "     9        0.9287             nan     0.1000    0.0061\n",
      "    10        0.9175             nan     0.1000    0.0051\n",
      "    20        0.8374             nan     0.1000    0.0023\n",
      "    40        0.7540             nan     0.1000    0.0012\n",
      "    60        0.7140             nan     0.1000    0.0002\n",
      "    80        0.6932             nan     0.1000   -0.0003\n",
      "   100        0.6760             nan     0.1000   -0.0005\n",
      "   120        0.6625             nan     0.1000   -0.0001\n",
      "   140        0.6516             nan     0.1000   -0.0005\n",
      "   160        0.6423             nan     0.1000   -0.0003\n",
      "   180        0.6353             nan     0.1000   -0.0006\n",
      "   200        0.6278             nan     0.1000   -0.0003\n",
      "   220        0.6222             nan     0.1000   -0.0005\n",
      "   240        0.6179             nan     0.1000   -0.0005\n",
      "   250        0.6154             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0498             nan     0.1000    0.0277\n",
      "     2        1.0034             nan     0.1000    0.0202\n",
      "     3        0.9621             nan     0.1000    0.0167\n",
      "     4        0.9285             nan     0.1000    0.0155\n",
      "     5        0.8993             nan     0.1000    0.0124\n",
      "     6        0.8712             nan     0.1000    0.0137\n",
      "     7        0.8482             nan     0.1000    0.0107\n",
      "     8        0.8297             nan     0.1000    0.0081\n",
      "     9        0.8120             nan     0.1000    0.0081\n",
      "    10        0.7957             nan     0.1000    0.0074\n",
      "    20        0.7004             nan     0.1000    0.0023\n",
      "    40        0.6174             nan     0.1000   -0.0001\n",
      "    60        0.5683             nan     0.1000   -0.0006\n",
      "    80        0.5320             nan     0.1000   -0.0013\n",
      "   100        0.4968             nan     0.1000   -0.0007\n",
      "   120        0.4681             nan     0.1000   -0.0013\n",
      "   140        0.4388             nan     0.1000   -0.0007\n",
      "   160        0.4159             nan     0.1000   -0.0009\n",
      "   180        0.3914             nan     0.1000   -0.0010\n",
      "   200        0.3702             nan     0.1000   -0.0008\n",
      "   220        0.3532             nan     0.1000   -0.0006\n",
      "   240        0.3376             nan     0.1000   -0.0007\n",
      "   250        0.3287             nan     0.1000   -0.0008\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0465             nan     0.1000    0.0313\n",
      "     2        0.9912             nan     0.1000    0.0244\n",
      "     3        0.9452             nan     0.1000    0.0207\n",
      "     4        0.9092             nan     0.1000    0.0155\n",
      "     5        0.8772             nan     0.1000    0.0121\n",
      "     6        0.8506             nan     0.1000    0.0104\n",
      "     7        0.8235             nan     0.1000    0.0109\n",
      "     8        0.8044             nan     0.1000    0.0070\n",
      "     9        0.7884             nan     0.1000    0.0054\n",
      "    10        0.7715             nan     0.1000    0.0058\n",
      "    20        0.6562             nan     0.1000    0.0008\n",
      "    40        0.5493             nan     0.1000    0.0006\n",
      "    60        0.4850             nan     0.1000   -0.0008\n",
      "    80        0.4319             nan     0.1000   -0.0003\n",
      "   100        0.3839             nan     0.1000   -0.0005\n",
      "   120        0.3444             nan     0.1000   -0.0008\n",
      "   140        0.3071             nan     0.1000   -0.0003\n",
      "   160        0.2780             nan     0.1000   -0.0008\n",
      "   180        0.2513             nan     0.1000   -0.0003\n",
      "   200        0.2282             nan     0.1000   -0.0008\n",
      "   220        0.2051             nan     0.1000   -0.0010\n",
      "   240        0.1859             nan     0.1000   -0.0003\n",
      "   250        0.1778             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold05.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1107             nan     0.0100    0.0018\n",
      "     2        1.1071             nan     0.0100    0.0018\n",
      "     3        1.1036             nan     0.0100    0.0018\n",
      "     4        1.1000             nan     0.0100    0.0017\n",
      "     5        1.0968             nan     0.0100    0.0017\n",
      "     6        1.0936             nan     0.0100    0.0017\n",
      "     7        1.0905             nan     0.0100    0.0016\n",
      "     8        1.0871             nan     0.0100    0.0016\n",
      "     9        1.0841             nan     0.0100    0.0016\n",
      "    10        1.0811             nan     0.0100    0.0015\n",
      "    20        1.0538             nan     0.0100    0.0011\n",
      "    40        1.0113             nan     0.0100    0.0007\n",
      "    60        0.9769             nan     0.0100    0.0005\n",
      "    80        0.9484             nan     0.0100    0.0007\n",
      "   100        0.9238             nan     0.0100    0.0006\n",
      "   120        0.9028             nan     0.0100    0.0004\n",
      "   140        0.8853             nan     0.0100    0.0003\n",
      "   160        0.8698             nan     0.0100    0.0004\n",
      "   180        0.8559             nan     0.0100    0.0003\n",
      "   200        0.8433             nan     0.0100    0.0002\n",
      "   220        0.8314             nan     0.0100    0.0002\n",
      "   240        0.8209             nan     0.0100    0.0002\n",
      "   250        0.8162             nan     0.0100    0.0002\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1082             nan     0.0100    0.0032\n",
      "     2        1.1019             nan     0.0100    0.0031\n",
      "     3        1.0956             nan     0.0100    0.0026\n",
      "     4        1.0896             nan     0.0100    0.0027\n",
      "     5        1.0837             nan     0.0100    0.0026\n",
      "     6        1.0780             nan     0.0100    0.0026\n",
      "     7        1.0724             nan     0.0100    0.0028\n",
      "     8        1.0668             nan     0.0100    0.0029\n",
      "     9        1.0609             nan     0.0100    0.0030\n",
      "    10        1.0552             nan     0.0100    0.0025\n",
      "    20        1.0084             nan     0.0100    0.0019\n",
      "    40        0.9349             nan     0.0100    0.0013\n",
      "    60        0.8798             nan     0.0100    0.0009\n",
      "    80        0.8375             nan     0.0100    0.0009\n",
      "   100        0.8052             nan     0.0100    0.0005\n",
      "   120        0.7786             nan     0.0100    0.0004\n",
      "   140        0.7573             nan     0.0100    0.0003\n",
      "   160        0.7385             nan     0.0100    0.0001\n",
      "   180        0.7223             nan     0.0100    0.0002\n",
      "   200        0.7090             nan     0.0100    0.0001\n",
      "   220        0.6959             nan     0.0100    0.0002\n",
      "   240        0.6840             nan     0.0100    0.0002\n",
      "   250        0.6791             nan     0.0100    0.0000\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1072             nan     0.0100    0.0030\n",
      "     2        1.0999             nan     0.0100    0.0037\n",
      "     3        1.0932             nan     0.0100    0.0031\n",
      "     4        1.0861             nan     0.0100    0.0031\n",
      "     5        1.0795             nan     0.0100    0.0028\n",
      "     6        1.0732             nan     0.0100    0.0031\n",
      "     7        1.0671             nan     0.0100    0.0030\n",
      "     8        1.0608             nan     0.0100    0.0026\n",
      "     9        1.0551             nan     0.0100    0.0025\n",
      "    10        1.0490             nan     0.0100    0.0026\n",
      "    20        0.9967             nan     0.0100    0.0020\n",
      "    40        0.9142             nan     0.0100    0.0015\n",
      "    60        0.8531             nan     0.0100    0.0011\n",
      "    80        0.8065             nan     0.0100    0.0006\n",
      "   100        0.7700             nan     0.0100    0.0006\n",
      "   120        0.7398             nan     0.0100    0.0002\n",
      "   140        0.7130             nan     0.0100    0.0003\n",
      "   160        0.6905             nan     0.0100    0.0001\n",
      "   180        0.6717             nan     0.0100    0.0001\n",
      "   200        0.6549             nan     0.0100    0.0002\n",
      "   220        0.6398             nan     0.0100    0.0002\n",
      "   240        0.6251             nan     0.0100    0.0001\n",
      "   250        0.6187             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0968             nan     0.0500    0.0089\n",
      "     2        1.0798             nan     0.0500    0.0081\n",
      "     3        1.0681             nan     0.0500    0.0049\n",
      "     4        1.0545             nan     0.0500    0.0072\n",
      "     5        1.0413             nan     0.0500    0.0065\n",
      "     6        1.0291             nan     0.0500    0.0060\n",
      "     7        1.0184             nan     0.0500    0.0054\n",
      "     8        1.0087             nan     0.0500    0.0049\n",
      "     9        0.9997             nan     0.0500    0.0045\n",
      "    10        0.9907             nan     0.0500    0.0040\n",
      "    20        0.9210             nan     0.0500    0.0021\n",
      "    40        0.8389             nan     0.0500    0.0012\n",
      "    60        0.7934             nan     0.0500    0.0005\n",
      "    80        0.7623             nan     0.0500    0.0004\n",
      "   100        0.7408             nan     0.0500    0.0002\n",
      "   120        0.7234             nan     0.0500    0.0002\n",
      "   140        0.7122             nan     0.0500   -0.0002\n",
      "   160        0.7008             nan     0.0500   -0.0002\n",
      "   180        0.6923             nan     0.0500    0.0001\n",
      "   200        0.6839             nan     0.0500   -0.0000\n",
      "   220        0.6775             nan     0.0500   -0.0001\n",
      "   240        0.6721             nan     0.0500   -0.0000\n",
      "   250        0.6689             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0842             nan     0.0500    0.0151\n",
      "     2        1.0580             nan     0.0500    0.0128\n",
      "     3        1.0319             nan     0.0500    0.0114\n",
      "     4        1.0066             nan     0.0500    0.0124\n",
      "     5        0.9867             nan     0.0500    0.0081\n",
      "     6        0.9680             nan     0.0500    0.0087\n",
      "     7        0.9507             nan     0.0500    0.0072\n",
      "     8        0.9333             nan     0.0500    0.0084\n",
      "     9        0.9180             nan     0.0500    0.0071\n",
      "    10        0.9045             nan     0.0500    0.0058\n",
      "    20        0.8030             nan     0.0500    0.0032\n",
      "    40        0.7044             nan     0.0500    0.0003\n",
      "    60        0.6545             nan     0.0500   -0.0002\n",
      "    80        0.6203             nan     0.0500    0.0003\n",
      "   100        0.5911             nan     0.0500    0.0001\n",
      "   120        0.5675             nan     0.0500   -0.0006\n",
      "   140        0.5469             nan     0.0500   -0.0001\n",
      "   160        0.5289             nan     0.0500   -0.0007\n",
      "   180        0.5103             nan     0.0500   -0.0002\n",
      "   200        0.4948             nan     0.0500   -0.0004\n",
      "   220        0.4804             nan     0.0500   -0.0005\n",
      "   240        0.4675             nan     0.0500   -0.0003\n",
      "   250        0.4609             nan     0.0500   -0.0006\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0794             nan     0.0500    0.0158\n",
      "     2        1.0467             nan     0.0500    0.0150\n",
      "     3        1.0150             nan     0.0500    0.0144\n",
      "     4        0.9901             nan     0.0500    0.0108\n",
      "     5        0.9660             nan     0.0500    0.0102\n",
      "     6        0.9464             nan     0.0500    0.0082\n",
      "     7        0.9260             nan     0.0500    0.0081\n",
      "     8        0.9073             nan     0.0500    0.0087\n",
      "     9        0.8913             nan     0.0500    0.0073\n",
      "    10        0.8763             nan     0.0500    0.0053\n",
      "    20        0.7648             nan     0.0500    0.0029\n",
      "    40        0.6514             nan     0.0500    0.0011\n",
      "    60        0.5928             nan     0.0500    0.0002\n",
      "    80        0.5467             nan     0.0500    0.0000\n",
      "   100        0.5113             nan     0.0500   -0.0004\n",
      "   120        0.4791             nan     0.0500   -0.0001\n",
      "   140        0.4494             nan     0.0500   -0.0005\n",
      "   160        0.4212             nan     0.0500   -0.0002\n",
      "   180        0.3968             nan     0.0500   -0.0004\n",
      "   200        0.3753             nan     0.0500    0.0000\n",
      "   220        0.3542             nan     0.0500   -0.0006\n",
      "   240        0.3353             nan     0.0500   -0.0006\n",
      "   250        0.3266             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0776             nan     0.1000    0.0172\n",
      "     2        1.0497             nan     0.1000    0.0141\n",
      "     3        1.0262             nan     0.1000    0.0114\n",
      "     4        1.0070             nan     0.1000    0.0093\n",
      "     5        0.9877             nan     0.1000    0.0111\n",
      "     6        0.9700             nan     0.1000    0.0084\n",
      "     7        0.9539             nan     0.1000    0.0071\n",
      "     8        0.9417             nan     0.1000    0.0067\n",
      "     9        0.9278             nan     0.1000    0.0062\n",
      "    10        0.9165             nan     0.1000    0.0048\n",
      "    20        0.8388             nan     0.1000    0.0023\n",
      "    40        0.7601             nan     0.1000    0.0006\n",
      "    60        0.7239             nan     0.1000    0.0000\n",
      "    80        0.7011             nan     0.1000   -0.0000\n",
      "   100        0.6853             nan     0.1000   -0.0007\n",
      "   120        0.6742             nan     0.1000    0.0000\n",
      "   140        0.6622             nan     0.1000   -0.0000\n",
      "   160        0.6520             nan     0.1000   -0.0001\n",
      "   180        0.6453             nan     0.1000   -0.0003\n",
      "   200        0.6376             nan     0.1000   -0.0003\n",
      "   220        0.6332             nan     0.1000   -0.0003\n",
      "   240        0.6264             nan     0.1000   -0.0004\n",
      "   250        0.6240             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0501             nan     0.1000    0.0269\n",
      "     2        1.0055             nan     0.1000    0.0195\n",
      "     3        0.9642             nan     0.1000    0.0191\n",
      "     4        0.9280             nan     0.1000    0.0176\n",
      "     5        0.8996             nan     0.1000    0.0115\n",
      "     6        0.8732             nan     0.1000    0.0111\n",
      "     7        0.8532             nan     0.1000    0.0093\n",
      "     8        0.8385             nan     0.1000    0.0052\n",
      "     9        0.8220             nan     0.1000    0.0069\n",
      "    10        0.8051             nan     0.1000    0.0081\n",
      "    20        0.7104             nan     0.1000    0.0006\n",
      "    40        0.6258             nan     0.1000   -0.0007\n",
      "    60        0.5750             nan     0.1000   -0.0007\n",
      "    80        0.5324             nan     0.1000   -0.0005\n",
      "   100        0.4984             nan     0.1000   -0.0012\n",
      "   120        0.4686             nan     0.1000   -0.0010\n",
      "   140        0.4415             nan     0.1000   -0.0010\n",
      "   160        0.4162             nan     0.1000   -0.0009\n",
      "   180        0.3938             nan     0.1000   -0.0008\n",
      "   200        0.3718             nan     0.1000   -0.0000\n",
      "   220        0.3532             nan     0.1000   -0.0012\n",
      "   240        0.3346             nan     0.1000   -0.0005\n",
      "   250        0.3252             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 55: x57 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0458             nan     0.1000    0.0279\n",
      "     2        0.9906             nan     0.1000    0.0219\n",
      "     3        0.9440             nan     0.1000    0.0200\n",
      "     4        0.9055             nan     0.1000    0.0161\n",
      "     5        0.8791             nan     0.1000    0.0108\n",
      "     6        0.8519             nan     0.1000    0.0107\n",
      "     7        0.8268             nan     0.1000    0.0100\n",
      "     8        0.8050             nan     0.1000    0.0084\n",
      "     9        0.7849             nan     0.1000    0.0068\n",
      "    10        0.7683             nan     0.1000    0.0038\n",
      "    20        0.6580             nan     0.1000    0.0010\n",
      "    40        0.5519             nan     0.1000   -0.0005\n",
      "    60        0.4806             nan     0.1000   -0.0003\n",
      "    80        0.4266             nan     0.1000   -0.0008\n",
      "   100        0.3778             nan     0.1000   -0.0005\n",
      "   120        0.3357             nan     0.1000   -0.0003\n",
      "   140        0.2985             nan     0.1000   -0.0006\n",
      "   160        0.2700             nan     0.1000   -0.0003\n",
      "   180        0.2441             nan     0.1000   -0.0004\n",
      "   200        0.2204             nan     0.1000   -0.0005\n",
      "   220        0.2003             nan     0.1000   -0.0007\n",
      "   240        0.1813             nan     0.1000   -0.0005\n",
      "   250        0.1734             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold06.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0019\n",
      "     2        1.1072             nan     0.0100    0.0019\n",
      "     3        1.1037             nan     0.0100    0.0018\n",
      "     4        1.0999             nan     0.0100    0.0018\n",
      "     5        1.0966             nan     0.0100    0.0017\n",
      "     6        1.0936             nan     0.0100    0.0017\n",
      "     7        1.0902             nan     0.0100    0.0017\n",
      "     8        1.0869             nan     0.0100    0.0017\n",
      "     9        1.0836             nan     0.0100    0.0016\n",
      "    10        1.0804             nan     0.0100    0.0016\n",
      "    20        1.0516             nan     0.0100    0.0011\n",
      "    40        1.0069             nan     0.0100    0.0010\n",
      "    60        0.9728             nan     0.0100    0.0008\n",
      "    80        0.9452             nan     0.0100    0.0005\n",
      "   100        0.9215             nan     0.0100    0.0004\n",
      "   120        0.9025             nan     0.0100    0.0004\n",
      "   140        0.8845             nan     0.0100    0.0003\n",
      "   160        0.8688             nan     0.0100    0.0002\n",
      "   180        0.8556             nan     0.0100    0.0002\n",
      "   200        0.8427             nan     0.0100    0.0002\n",
      "   220        0.8324             nan     0.0100    0.0002\n",
      "   240        0.8218             nan     0.0100    0.0002\n",
      "   250        0.8170             nan     0.0100    0.0001\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1076             nan     0.0100    0.0031\n",
      "     2        1.1011             nan     0.0100    0.0030\n",
      "     3        1.0947             nan     0.0100    0.0029\n",
      "     4        1.0891             nan     0.0100    0.0028\n",
      "     5        1.0833             nan     0.0100    0.0026\n",
      "     6        1.0774             nan     0.0100    0.0027\n",
      "     7        1.0714             nan     0.0100    0.0030\n",
      "     8        1.0658             nan     0.0100    0.0027\n",
      "     9        1.0600             nan     0.0100    0.0028\n",
      "    10        1.0547             nan     0.0100    0.0025\n",
      "    20        1.0066             nan     0.0100    0.0017\n",
      "    40        0.9330             nan     0.0100    0.0017\n",
      "    60        0.8794             nan     0.0100    0.0009\n",
      "    80        0.8379             nan     0.0100    0.0009\n",
      "   100        0.8066             nan     0.0100    0.0005\n",
      "   120        0.7810             nan     0.0100    0.0004\n",
      "   140        0.7588             nan     0.0100    0.0004\n",
      "   160        0.7403             nan     0.0100    0.0002\n",
      "   180        0.7242             nan     0.0100    0.0003\n",
      "   200        0.7105             nan     0.0100    0.0001\n",
      "   220        0.6983             nan     0.0100    0.0002\n",
      "   240        0.6876             nan     0.0100   -0.0000\n",
      "   250        0.6825             nan     0.0100    0.0001\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0035\n",
      "     2        1.0998             nan     0.0100    0.0037\n",
      "     3        1.0927             nan     0.0100    0.0032\n",
      "     4        1.0862             nan     0.0100    0.0030\n",
      "     5        1.0794             nan     0.0100    0.0031\n",
      "     6        1.0731             nan     0.0100    0.0028\n",
      "     7        1.0669             nan     0.0100    0.0031\n",
      "     8        1.0607             nan     0.0100    0.0027\n",
      "     9        1.0544             nan     0.0100    0.0028\n",
      "    10        1.0486             nan     0.0100    0.0025\n",
      "    20        0.9956             nan     0.0100    0.0021\n",
      "    40        0.9139             nan     0.0100    0.0015\n",
      "    60        0.8544             nan     0.0100    0.0009\n",
      "    80        0.8082             nan     0.0100    0.0006\n",
      "   100        0.7706             nan     0.0100    0.0005\n",
      "   120        0.7402             nan     0.0100    0.0003\n",
      "   140        0.7140             nan     0.0100    0.0003\n",
      "   160        0.6922             nan     0.0100    0.0001\n",
      "   180        0.6742             nan     0.0100    0.0002\n",
      "   200        0.6572             nan     0.0100    0.0001\n",
      "   220        0.6420             nan     0.0100    0.0002\n",
      "   240        0.6281             nan     0.0100   -0.0000\n",
      "   250        0.6218             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0959             nan     0.0500    0.0093\n",
      "     2        1.0783             nan     0.0500    0.0084\n",
      "     3        1.0631             nan     0.0500    0.0076\n",
      "     4        1.0521             nan     0.0500    0.0042\n",
      "     5        1.0428             nan     0.0500    0.0038\n",
      "     6        1.0279             nan     0.0500    0.0065\n",
      "     7        1.0160             nan     0.0500    0.0060\n",
      "     8        1.0049             nan     0.0500    0.0055\n",
      "     9        0.9954             nan     0.0500    0.0049\n",
      "    10        0.9883             nan     0.0500    0.0030\n",
      "    20        0.9181             nan     0.0500    0.0023\n",
      "    40        0.8415             nan     0.0500    0.0008\n",
      "    60        0.7946             nan     0.0500    0.0008\n",
      "    80        0.7619             nan     0.0500    0.0006\n",
      "   100        0.7403             nan     0.0500    0.0002\n",
      "   120        0.7257             nan     0.0500   -0.0000\n",
      "   140        0.7117             nan     0.0500    0.0003\n",
      "   160        0.7007             nan     0.0500    0.0001\n",
      "   180        0.6923             nan     0.0500   -0.0001\n",
      "   200        0.6845             nan     0.0500   -0.0000\n",
      "   220        0.6780             nan     0.0500   -0.0001\n",
      "   240        0.6722             nan     0.0500   -0.0001\n",
      "   250        0.6692             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0835             nan     0.0500    0.0141\n",
      "     2        1.0542             nan     0.0500    0.0142\n",
      "     3        1.0290             nan     0.0500    0.0116\n",
      "     4        1.0059             nan     0.0500    0.0111\n",
      "     5        0.9858             nan     0.0500    0.0086\n",
      "     6        0.9654             nan     0.0500    0.0086\n",
      "     7        0.9479             nan     0.0500    0.0086\n",
      "     8        0.9303             nan     0.0500    0.0080\n",
      "     9        0.9141             nan     0.0500    0.0079\n",
      "    10        0.9008             nan     0.0500    0.0053\n",
      "    20        0.8008             nan     0.0500    0.0028\n",
      "    40        0.7087             nan     0.0500    0.0004\n",
      "    60        0.6588             nan     0.0500    0.0000\n",
      "    80        0.6246             nan     0.0500    0.0000\n",
      "   100        0.5992             nan     0.0500   -0.0003\n",
      "   120        0.5751             nan     0.0500   -0.0001\n",
      "   140        0.5543             nan     0.0500   -0.0004\n",
      "   160        0.5355             nan     0.0500   -0.0003\n",
      "   180        0.5166             nan     0.0500    0.0000\n",
      "   200        0.4994             nan     0.0500   -0.0002\n",
      "   220        0.4824             nan     0.0500   -0.0006\n",
      "   240        0.4662             nan     0.0500   -0.0004\n",
      "   250        0.4599             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0793             nan     0.0500    0.0163\n",
      "     2        1.0472             nan     0.0500    0.0147\n",
      "     3        1.0201             nan     0.0500    0.0117\n",
      "     4        0.9937             nan     0.0500    0.0116\n",
      "     5        0.9697             nan     0.0500    0.0104\n",
      "     6        0.9475             nan     0.0500    0.0104\n",
      "     7        0.9291             nan     0.0500    0.0072\n",
      "     8        0.9121             nan     0.0500    0.0060\n",
      "     9        0.8953             nan     0.0500    0.0072\n",
      "    10        0.8798             nan     0.0500    0.0057\n",
      "    20        0.7687             nan     0.0500    0.0030\n",
      "    40        0.6558             nan     0.0500   -0.0000\n",
      "    60        0.5971             nan     0.0500    0.0001\n",
      "    80        0.5491             nan     0.0500   -0.0004\n",
      "   100        0.5136             nan     0.0500   -0.0007\n",
      "   120        0.4791             nan     0.0500   -0.0000\n",
      "   140        0.4501             nan     0.0500   -0.0002\n",
      "   160        0.4251             nan     0.0500   -0.0003\n",
      "   180        0.4020             nan     0.0500   -0.0005\n",
      "   200        0.3801             nan     0.0500   -0.0003\n",
      "   220        0.3599             nan     0.0500   -0.0002\n",
      "   240        0.3392             nan     0.0500   -0.0005\n",
      "   250        0.3293             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0783             nan     0.1000    0.0181\n",
      "     2        1.0500             nan     0.1000    0.0147\n",
      "     3        1.0296             nan     0.1000    0.0076\n",
      "     4        1.0075             nan     0.1000    0.0119\n",
      "     5        0.9871             nan     0.1000    0.0099\n",
      "     6        0.9717             nan     0.1000    0.0069\n",
      "     7        0.9597             nan     0.1000    0.0054\n",
      "     8        0.9439             nan     0.1000    0.0079\n",
      "     9        0.9339             nan     0.1000    0.0041\n",
      "    10        0.9255             nan     0.1000    0.0036\n",
      "    20        0.8423             nan     0.1000    0.0030\n",
      "    40        0.7610             nan     0.1000    0.0006\n",
      "    60        0.7228             nan     0.1000   -0.0002\n",
      "    80        0.7023             nan     0.1000    0.0002\n",
      "   100        0.6868             nan     0.1000   -0.0002\n",
      "   120        0.6728             nan     0.1000   -0.0000\n",
      "   140        0.6626             nan     0.1000   -0.0000\n",
      "   160        0.6528             nan     0.1000   -0.0002\n",
      "   180        0.6464             nan     0.1000   -0.0001\n",
      "   200        0.6394             nan     0.1000   -0.0003\n",
      "   220        0.6342             nan     0.1000   -0.0001\n",
      "   240        0.6298             nan     0.1000   -0.0006\n",
      "   250        0.6274             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0528             nan     0.1000    0.0289\n",
      "     2        1.0090             nan     0.1000    0.0212\n",
      "     3        0.9636             nan     0.1000    0.0179\n",
      "     4        0.9282             nan     0.1000    0.0167\n",
      "     5        0.9003             nan     0.1000    0.0108\n",
      "     6        0.8781             nan     0.1000    0.0094\n",
      "     7        0.8551             nan     0.1000    0.0090\n",
      "     8        0.8332             nan     0.1000    0.0105\n",
      "     9        0.8153             nan     0.1000    0.0077\n",
      "    10        0.8024             nan     0.1000    0.0047\n",
      "    20        0.7096             nan     0.1000    0.0027\n",
      "    40        0.6261             nan     0.1000    0.0005\n",
      "    60        0.5748             nan     0.1000   -0.0008\n",
      "    80        0.5351             nan     0.1000   -0.0013\n",
      "   100        0.5010             nan     0.1000   -0.0003\n",
      "   120        0.4716             nan     0.1000   -0.0005\n",
      "   140        0.4433             nan     0.1000   -0.0011\n",
      "   160        0.4167             nan     0.1000   -0.0005\n",
      "   180        0.3916             nan     0.1000   -0.0009\n",
      "   200        0.3726             nan     0.1000    0.0002\n",
      "   220        0.3545             nan     0.1000   -0.0006\n",
      "   240        0.3364             nan     0.1000   -0.0005\n",
      "   250        0.3265             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0480             nan     0.1000    0.0307\n",
      "     2        0.9948             nan     0.1000    0.0237\n",
      "     3        0.9497             nan     0.1000    0.0205\n",
      "     4        0.9151             nan     0.1000    0.0149\n",
      "     5        0.8835             nan     0.1000    0.0145\n",
      "     6        0.8529             nan     0.1000    0.0122\n",
      "     7        0.8266             nan     0.1000    0.0126\n",
      "     8        0.8061             nan     0.1000    0.0081\n",
      "     9        0.7904             nan     0.1000    0.0043\n",
      "    10        0.7712             nan     0.1000    0.0067\n",
      "    20        0.6626             nan     0.1000   -0.0000\n",
      "    40        0.5570             nan     0.1000   -0.0006\n",
      "    60        0.4823             nan     0.1000   -0.0004\n",
      "    80        0.4279             nan     0.1000   -0.0003\n",
      "   100        0.3805             nan     0.1000   -0.0012\n",
      "   120        0.3387             nan     0.1000   -0.0005\n",
      "   140        0.3038             nan     0.1000   -0.0006\n",
      "   160        0.2747             nan     0.1000   -0.0002\n",
      "   180        0.2474             nan     0.1000   -0.0005\n",
      "   200        0.2255             nan     0.1000   -0.0001\n",
      "   220        0.2049             nan     0.1000   -0.0002\n",
      "   240        0.1861             nan     0.1000   -0.0004\n",
      "   250        0.1780             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold07.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1114             nan     0.0100    0.0018\n",
      "     2        1.1080             nan     0.0100    0.0018\n",
      "     3        1.1047             nan     0.0100    0.0017\n",
      "     4        1.1013             nan     0.0100    0.0017\n",
      "     5        1.0978             nan     0.0100    0.0017\n",
      "     6        1.0945             nan     0.0100    0.0016\n",
      "     7        1.0916             nan     0.0100    0.0016\n",
      "     8        1.0883             nan     0.0100    0.0016\n",
      "     9        1.0852             nan     0.0100    0.0015\n",
      "    10        1.0822             nan     0.0100    0.0015\n",
      "    20        1.0556             nan     0.0100    0.0010\n",
      "    40        1.0131             nan     0.0100    0.0007\n",
      "    60        0.9788             nan     0.0100    0.0008\n",
      "    80        0.9500             nan     0.0100    0.0007\n",
      "   100        0.9256             nan     0.0100    0.0004\n",
      "   120        0.9043             nan     0.0100    0.0004\n",
      "   140        0.8865             nan     0.0100    0.0004\n",
      "   160        0.8712             nan     0.0100    0.0004\n",
      "   180        0.8572             nan     0.0100    0.0002\n",
      "   200        0.8453             nan     0.0100    0.0002\n",
      "   220        0.8336             nan     0.0100    0.0003\n",
      "   240        0.8232             nan     0.0100    0.0002\n",
      "   250        0.8185             nan     0.0100    0.0002\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1094             nan     0.0100    0.0029\n",
      "     2        1.1031             nan     0.0100    0.0028\n",
      "     3        1.0971             nan     0.0100    0.0028\n",
      "     4        1.0911             nan     0.0100    0.0029\n",
      "     5        1.0854             nan     0.0100    0.0026\n",
      "     6        1.0796             nan     0.0100    0.0028\n",
      "     7        1.0741             nan     0.0100    0.0024\n",
      "     8        1.0686             nan     0.0100    0.0027\n",
      "     9        1.0629             nan     0.0100    0.0027\n",
      "    10        1.0578             nan     0.0100    0.0022\n",
      "    20        1.0097             nan     0.0100    0.0018\n",
      "    40        0.9355             nan     0.0100    0.0016\n",
      "    60        0.8817             nan     0.0100    0.0011\n",
      "    80        0.8401             nan     0.0100    0.0008\n",
      "   100        0.8091             nan     0.0100    0.0005\n",
      "   120        0.7834             nan     0.0100    0.0006\n",
      "   140        0.7611             nan     0.0100    0.0003\n",
      "   160        0.7428             nan     0.0100    0.0002\n",
      "   180        0.7266             nan     0.0100    0.0002\n",
      "   200        0.7119             nan     0.0100    0.0002\n",
      "   220        0.7000             nan     0.0100    0.0001\n",
      "   240        0.6888             nan     0.0100    0.0000\n",
      "   250        0.6834             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1082             nan     0.0100    0.0033\n",
      "     2        1.1014             nan     0.0100    0.0034\n",
      "     3        1.0948             nan     0.0100    0.0030\n",
      "     4        1.0882             nan     0.0100    0.0033\n",
      "     5        1.0814             nan     0.0100    0.0032\n",
      "     6        1.0749             nan     0.0100    0.0028\n",
      "     7        1.0691             nan     0.0100    0.0026\n",
      "     8        1.0633             nan     0.0100    0.0029\n",
      "     9        1.0580             nan     0.0100    0.0025\n",
      "    10        1.0520             nan     0.0100    0.0029\n",
      "    20        0.9988             nan     0.0100    0.0021\n",
      "    40        0.9181             nan     0.0100    0.0015\n",
      "    60        0.8575             nan     0.0100    0.0009\n",
      "    80        0.8121             nan     0.0100    0.0007\n",
      "   100        0.7734             nan     0.0100    0.0006\n",
      "   120        0.7426             nan     0.0100    0.0004\n",
      "   140        0.7166             nan     0.0100    0.0003\n",
      "   160        0.6945             nan     0.0100    0.0001\n",
      "   180        0.6750             nan     0.0100    0.0000\n",
      "   200        0.6582             nan     0.0100    0.0001\n",
      "   220        0.6434             nan     0.0100    0.0001\n",
      "   240        0.6294             nan     0.0100    0.0001\n",
      "   250        0.6227             nan     0.0100    0.0001\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0988             nan     0.0500    0.0089\n",
      "     2        1.0819             nan     0.0500    0.0080\n",
      "     3        1.0677             nan     0.0500    0.0073\n",
      "     4        1.0562             nan     0.0500    0.0057\n",
      "     5        1.0428             nan     0.0500    0.0064\n",
      "     6        1.0302             nan     0.0500    0.0058\n",
      "     7        1.0199             nan     0.0500    0.0053\n",
      "     8        1.0096             nan     0.0500    0.0044\n",
      "     9        0.9994             nan     0.0500    0.0047\n",
      "    10        0.9899             nan     0.0500    0.0042\n",
      "    20        0.9219             nan     0.0500    0.0024\n",
      "    40        0.8447             nan     0.0500    0.0011\n",
      "    60        0.7964             nan     0.0500    0.0012\n",
      "    80        0.7625             nan     0.0500    0.0002\n",
      "   100        0.7403             nan     0.0500    0.0004\n",
      "   120        0.7256             nan     0.0500    0.0001\n",
      "   140        0.7128             nan     0.0500   -0.0000\n",
      "   160        0.7033             nan     0.0500    0.0001\n",
      "   180        0.6943             nan     0.0500    0.0000\n",
      "   200        0.6862             nan     0.0500   -0.0002\n",
      "   220        0.6780             nan     0.0500    0.0001\n",
      "   240        0.6721             nan     0.0500   -0.0001\n",
      "   250        0.6700             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0849             nan     0.0500    0.0145\n",
      "     2        1.0559             nan     0.0500    0.0136\n",
      "     3        1.0307             nan     0.0500    0.0118\n",
      "     4        1.0087             nan     0.0500    0.0102\n",
      "     5        0.9889             nan     0.0500    0.0087\n",
      "     6        0.9709             nan     0.0500    0.0089\n",
      "     7        0.9532             nan     0.0500    0.0087\n",
      "     8        0.9374             nan     0.0500    0.0068\n",
      "     9        0.9230             nan     0.0500    0.0064\n",
      "    10        0.9095             nan     0.0500    0.0062\n",
      "    20        0.8092             nan     0.0500    0.0027\n",
      "    40        0.7105             nan     0.0500    0.0007\n",
      "    60        0.6601             nan     0.0500    0.0007\n",
      "    80        0.6243             nan     0.0500   -0.0004\n",
      "   100        0.5947             nan     0.0500   -0.0001\n",
      "   120        0.5726             nan     0.0500   -0.0001\n",
      "   140        0.5524             nan     0.0500   -0.0004\n",
      "   160        0.5307             nan     0.0500   -0.0002\n",
      "   180        0.5116             nan     0.0500   -0.0004\n",
      "   200        0.4964             nan     0.0500   -0.0003\n",
      "   220        0.4815             nan     0.0500   -0.0003\n",
      "   240        0.4660             nan     0.0500    0.0001\n",
      "   250        0.4581             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0831             nan     0.0500    0.0155\n",
      "     2        1.0526             nan     0.0500    0.0134\n",
      "     3        1.0237             nan     0.0500    0.0128\n",
      "     4        0.9978             nan     0.0500    0.0122\n",
      "     5        0.9758             nan     0.0500    0.0105\n",
      "     6        0.9551             nan     0.0500    0.0091\n",
      "     7        0.9373             nan     0.0500    0.0075\n",
      "     8        0.9209             nan     0.0500    0.0065\n",
      "     9        0.9043             nan     0.0500    0.0070\n",
      "    10        0.8891             nan     0.0500    0.0056\n",
      "    20        0.7708             nan     0.0500    0.0039\n",
      "    40        0.6548             nan     0.0500    0.0003\n",
      "    60        0.5923             nan     0.0500   -0.0003\n",
      "    80        0.5492             nan     0.0500   -0.0003\n",
      "   100        0.5117             nan     0.0500   -0.0009\n",
      "   120        0.4765             nan     0.0500   -0.0006\n",
      "   140        0.4471             nan     0.0500   -0.0003\n",
      "   160        0.4205             nan     0.0500   -0.0005\n",
      "   180        0.3982             nan     0.0500   -0.0001\n",
      "   200        0.3764             nan     0.0500   -0.0004\n",
      "   220        0.3556             nan     0.0500   -0.0005\n",
      "   240        0.3341             nan     0.0500    0.0000\n",
      "   250        0.3244             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0814             nan     0.1000    0.0174\n",
      "     2        1.0545             nan     0.1000    0.0141\n",
      "     3        1.0325             nan     0.1000    0.0088\n",
      "     4        1.0096             nan     0.1000    0.0108\n",
      "     5        0.9925             nan     0.1000    0.0084\n",
      "     6        0.9769             nan     0.1000    0.0063\n",
      "     7        0.9644             nan     0.1000    0.0047\n",
      "     8        0.9455             nan     0.1000    0.0085\n",
      "     9        0.9314             nan     0.1000    0.0070\n",
      "    10        0.9226             nan     0.1000    0.0041\n",
      "    20        0.8391             nan     0.1000    0.0022\n",
      "    40        0.7611             nan     0.1000    0.0009\n",
      "    60        0.7271             nan     0.1000    0.0001\n",
      "    80        0.7041             nan     0.1000   -0.0001\n",
      "   100        0.6855             nan     0.1000    0.0001\n",
      "   120        0.6743             nan     0.1000   -0.0004\n",
      "   140        0.6621             nan     0.1000   -0.0007\n",
      "   160        0.6533             nan     0.1000    0.0000\n",
      "   180        0.6455             nan     0.1000    0.0001\n",
      "   200        0.6385             nan     0.1000   -0.0004\n",
      "   220        0.6333             nan     0.1000   -0.0002\n",
      "   240        0.6284             nan     0.1000   -0.0004\n",
      "   250        0.6255             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0538             nan     0.1000    0.0302\n",
      "     2        1.0114             nan     0.1000    0.0193\n",
      "     3        0.9727             nan     0.1000    0.0162\n",
      "     4        0.9368             nan     0.1000    0.0169\n",
      "     5        0.9069             nan     0.1000    0.0121\n",
      "     6        0.8824             nan     0.1000    0.0103\n",
      "     7        0.8627             nan     0.1000    0.0090\n",
      "     8        0.8435             nan     0.1000    0.0075\n",
      "     9        0.8259             nan     0.1000    0.0088\n",
      "    10        0.8108             nan     0.1000    0.0046\n",
      "    20        0.7133             nan     0.1000    0.0024\n",
      "    40        0.6311             nan     0.1000    0.0001\n",
      "    60        0.5791             nan     0.1000   -0.0002\n",
      "    80        0.5348             nan     0.1000   -0.0001\n",
      "   100        0.4995             nan     0.1000   -0.0008\n",
      "   120        0.4712             nan     0.1000   -0.0007\n",
      "   140        0.4457             nan     0.1000   -0.0001\n",
      "   160        0.4222             nan     0.1000   -0.0004\n",
      "   180        0.3996             nan     0.1000   -0.0005\n",
      "   200        0.3789             nan     0.1000   -0.0012\n",
      "   220        0.3588             nan     0.1000   -0.0002\n",
      "   240        0.3381             nan     0.1000   -0.0004\n",
      "   250        0.3307             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0453             nan     0.1000    0.0316\n",
      "     2        0.9934             nan     0.1000    0.0232\n",
      "     3        0.9515             nan     0.1000    0.0187\n",
      "     4        0.9132             nan     0.1000    0.0180\n",
      "     5        0.8866             nan     0.1000    0.0112\n",
      "     6        0.8543             nan     0.1000    0.0143\n",
      "     7        0.8343             nan     0.1000    0.0067\n",
      "     8        0.8106             nan     0.1000    0.0074\n",
      "     9        0.7914             nan     0.1000    0.0062\n",
      "    10        0.7735             nan     0.1000    0.0057\n",
      "    20        0.6650             nan     0.1000    0.0017\n",
      "    40        0.5575             nan     0.1000   -0.0013\n",
      "    60        0.4878             nan     0.1000   -0.0011\n",
      "    80        0.4330             nan     0.1000   -0.0005\n",
      "   100        0.3858             nan     0.1000   -0.0005\n",
      "   120        0.3446             nan     0.1000   -0.0005\n",
      "   140        0.3095             nan     0.1000   -0.0003\n",
      "   160        0.2802             nan     0.1000   -0.0011\n",
      "   180        0.2534             nan     0.1000   -0.0005\n",
      "   200        0.2291             nan     0.1000   -0.0004\n",
      "   220        0.2075             nan     0.1000   -0.0003\n",
      "   240        0.1889             nan     0.1000   -0.0002\n",
      "   250        0.1787             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold08.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1111             nan     0.0100    0.0017\n",
      "     2        1.1077             nan     0.0100    0.0017\n",
      "     3        1.1046             nan     0.0100    0.0016\n",
      "     4        1.1014             nan     0.0100    0.0016\n",
      "     5        1.0983             nan     0.0100    0.0016\n",
      "     6        1.0955             nan     0.0100    0.0015\n",
      "     7        1.0925             nan     0.0100    0.0015\n",
      "     8        1.0896             nan     0.0100    0.0015\n",
      "     9        1.0864             nan     0.0100    0.0014\n",
      "    10        1.0835             nan     0.0100    0.0014\n",
      "    20        1.0580             nan     0.0100    0.0010\n",
      "    40        1.0164             nan     0.0100    0.0010\n",
      "    60        0.9829             nan     0.0100    0.0008\n",
      "    80        0.9543             nan     0.0100    0.0007\n",
      "   100        0.9307             nan     0.0100    0.0004\n",
      "   120        0.9109             nan     0.0100    0.0003\n",
      "   140        0.8931             nan     0.0100    0.0004\n",
      "   160        0.8775             nan     0.0100    0.0003\n",
      "   180        0.8634             nan     0.0100    0.0002\n",
      "   200        0.8518             nan     0.0100    0.0003\n",
      "   220        0.8402             nan     0.0100    0.0002\n",
      "   240        0.8296             nan     0.0100    0.0001\n",
      "   250        0.8247             nan     0.0100    0.0002\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1082             nan     0.0100    0.0029\n",
      "     2        1.1019             nan     0.0100    0.0029\n",
      "     3        1.0955             nan     0.0100    0.0029\n",
      "     4        1.0894             nan     0.0100    0.0028\n",
      "     5        1.0838             nan     0.0100    0.0025\n",
      "     6        1.0783             nan     0.0100    0.0029\n",
      "     7        1.0729             nan     0.0100    0.0027\n",
      "     8        1.0679             nan     0.0100    0.0025\n",
      "     9        1.0624             nan     0.0100    0.0026\n",
      "    10        1.0574             nan     0.0100    0.0021\n",
      "    20        1.0107             nan     0.0100    0.0019\n",
      "    40        0.9394             nan     0.0100    0.0015\n",
      "    60        0.8860             nan     0.0100    0.0011\n",
      "    80        0.8462             nan     0.0100    0.0006\n",
      "   100        0.8138             nan     0.0100    0.0006\n",
      "   120        0.7881             nan     0.0100    0.0006\n",
      "   140        0.7670             nan     0.0100    0.0004\n",
      "   160        0.7477             nan     0.0100    0.0001\n",
      "   180        0.7313             nan     0.0100    0.0001\n",
      "   200        0.7177             nan     0.0100    0.0001\n",
      "   220        0.7053             nan     0.0100    0.0001\n",
      "   240        0.6937             nan     0.0100    0.0002\n",
      "   250        0.6888             nan     0.0100    0.0002\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1077             nan     0.0100    0.0031\n",
      "     2        1.1011             nan     0.0100    0.0032\n",
      "     3        1.0941             nan     0.0100    0.0031\n",
      "     4        1.0877             nan     0.0100    0.0028\n",
      "     5        1.0814             nan     0.0100    0.0030\n",
      "     6        1.0755             nan     0.0100    0.0026\n",
      "     7        1.0693             nan     0.0100    0.0030\n",
      "     8        1.0631             nan     0.0100    0.0027\n",
      "     9        1.0572             nan     0.0100    0.0028\n",
      "    10        1.0513             nan     0.0100    0.0027\n",
      "    20        1.0000             nan     0.0100    0.0018\n",
      "    40        0.9210             nan     0.0100    0.0013\n",
      "    60        0.8610             nan     0.0100    0.0010\n",
      "    80        0.8146             nan     0.0100    0.0007\n",
      "   100        0.7774             nan     0.0100    0.0005\n",
      "   120        0.7465             nan     0.0100    0.0002\n",
      "   140        0.7206             nan     0.0100    0.0002\n",
      "   160        0.6997             nan     0.0100    0.0001\n",
      "   180        0.6798             nan     0.0100    0.0002\n",
      "   200        0.6631             nan     0.0100    0.0001\n",
      "   220        0.6484             nan     0.0100    0.0002\n",
      "   240        0.6355             nan     0.0100   -0.0000\n",
      "   250        0.6291             nan     0.0100    0.0001\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0983             nan     0.0500    0.0084\n",
      "     2        1.0838             nan     0.0500    0.0075\n",
      "     3        1.0700             nan     0.0500    0.0069\n",
      "     4        1.0588             nan     0.0500    0.0052\n",
      "     5        1.0454             nan     0.0500    0.0061\n",
      "     6        1.0356             nan     0.0500    0.0044\n",
      "     7        1.0272             nan     0.0500    0.0032\n",
      "     8        1.0169             nan     0.0500    0.0054\n",
      "     9        1.0063             nan     0.0500    0.0049\n",
      "    10        0.9970             nan     0.0500    0.0044\n",
      "    20        0.9261             nan     0.0500    0.0025\n",
      "    40        0.8487             nan     0.0500    0.0009\n",
      "    60        0.8011             nan     0.0500    0.0011\n",
      "    80        0.7707             nan     0.0500    0.0002\n",
      "   100        0.7500             nan     0.0500    0.0004\n",
      "   120        0.7355             nan     0.0500   -0.0000\n",
      "   140        0.7220             nan     0.0500    0.0000\n",
      "   160        0.7132             nan     0.0500    0.0001\n",
      "   180        0.7037             nan     0.0500   -0.0001\n",
      "   200        0.6963             nan     0.0500   -0.0001\n",
      "   220        0.6888             nan     0.0500   -0.0001\n",
      "   240        0.6829             nan     0.0500   -0.0001\n",
      "   250        0.6801             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0819             nan     0.0500    0.0150\n",
      "     2        1.0545             nan     0.0500    0.0127\n",
      "     3        1.0289             nan     0.0500    0.0123\n",
      "     4        1.0084             nan     0.0500    0.0095\n",
      "     5        0.9885             nan     0.0500    0.0091\n",
      "     6        0.9700             nan     0.0500    0.0074\n",
      "     7        0.9507             nan     0.0500    0.0078\n",
      "     8        0.9332             nan     0.0500    0.0087\n",
      "     9        0.9186             nan     0.0500    0.0073\n",
      "    10        0.9044             nan     0.0500    0.0061\n",
      "    20        0.8099             nan     0.0500    0.0032\n",
      "    40        0.7152             nan     0.0500    0.0009\n",
      "    60        0.6626             nan     0.0500   -0.0003\n",
      "    80        0.6307             nan     0.0500   -0.0005\n",
      "   100        0.6029             nan     0.0500   -0.0005\n",
      "   120        0.5803             nan     0.0500   -0.0001\n",
      "   140        0.5580             nan     0.0500   -0.0004\n",
      "   160        0.5398             nan     0.0500   -0.0004\n",
      "   180        0.5231             nan     0.0500   -0.0004\n",
      "   200        0.5074             nan     0.0500   -0.0007\n",
      "   220        0.4926             nan     0.0500   -0.0001\n",
      "   240        0.4777             nan     0.0500   -0.0003\n",
      "   250        0.4700             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0815             nan     0.0500    0.0149\n",
      "     2        1.0511             nan     0.0500    0.0140\n",
      "     3        1.0241             nan     0.0500    0.0118\n",
      "     4        0.9979             nan     0.0500    0.0110\n",
      "     5        0.9728             nan     0.0500    0.0115\n",
      "     6        0.9520             nan     0.0500    0.0089\n",
      "     7        0.9345             nan     0.0500    0.0086\n",
      "     8        0.9170             nan     0.0500    0.0079\n",
      "     9        0.9002             nan     0.0500    0.0061\n",
      "    10        0.8853             nan     0.0500    0.0056\n",
      "    20        0.7778             nan     0.0500    0.0021\n",
      "    40        0.6667             nan     0.0500    0.0016\n",
      "    60        0.6019             nan     0.0500    0.0003\n",
      "    80        0.5566             nan     0.0500   -0.0001\n",
      "   100        0.5202             nan     0.0500   -0.0004\n",
      "   120        0.4850             nan     0.0500   -0.0003\n",
      "   140        0.4570             nan     0.0500   -0.0003\n",
      "   160        0.4288             nan     0.0500   -0.0005\n",
      "   180        0.4057             nan     0.0500   -0.0005\n",
      "   200        0.3838             nan     0.0500   -0.0003\n",
      "   220        0.3623             nan     0.0500   -0.0004\n",
      "   240        0.3429             nan     0.0500   -0.0002\n",
      "   250        0.3335             nan     0.0500   -0.0006\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0827             nan     0.1000    0.0163\n",
      "     2        1.0566             nan     0.1000    0.0132\n",
      "     3        1.0331             nan     0.1000    0.0106\n",
      "     4        1.0131             nan     0.1000    0.0098\n",
      "     5        0.9960             nan     0.1000    0.0072\n",
      "     6        0.9792             nan     0.1000    0.0083\n",
      "     7        0.9640             nan     0.1000    0.0070\n",
      "     8        0.9519             nan     0.1000    0.0056\n",
      "     9        0.9425             nan     0.1000    0.0038\n",
      "    10        0.9288             nan     0.1000    0.0066\n",
      "    20        0.8481             nan     0.1000    0.0028\n",
      "    40        0.7707             nan     0.1000    0.0009\n",
      "    60        0.7324             nan     0.1000    0.0001\n",
      "    80        0.7117             nan     0.1000   -0.0001\n",
      "   100        0.6953             nan     0.1000    0.0001\n",
      "   120        0.6829             nan     0.1000   -0.0000\n",
      "   140        0.6716             nan     0.1000   -0.0000\n",
      "   160        0.6620             nan     0.1000   -0.0002\n",
      "   180        0.6538             nan     0.1000   -0.0004\n",
      "   200        0.6488             nan     0.1000   -0.0003\n",
      "   220        0.6430             nan     0.1000   -0.0002\n",
      "   240        0.6384             nan     0.1000   -0.0004\n",
      "   250        0.6358             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0519             nan     0.1000    0.0293\n",
      "     2        1.0018             nan     0.1000    0.0215\n",
      "     3        0.9614             nan     0.1000    0.0195\n",
      "     4        0.9312             nan     0.1000    0.0138\n",
      "     5        0.9035             nan     0.1000    0.0126\n",
      "     6        0.8777             nan     0.1000    0.0110\n",
      "     7        0.8597             nan     0.1000    0.0074\n",
      "     8        0.8404             nan     0.1000    0.0076\n",
      "     9        0.8225             nan     0.1000    0.0076\n",
      "    10        0.8083             nan     0.1000    0.0045\n",
      "    20        0.7228             nan     0.1000    0.0021\n",
      "    40        0.6379             nan     0.1000   -0.0003\n",
      "    60        0.5868             nan     0.1000   -0.0006\n",
      "    80        0.5456             nan     0.1000   -0.0000\n",
      "   100        0.5126             nan     0.1000   -0.0007\n",
      "   120        0.4815             nan     0.1000   -0.0004\n",
      "   140        0.4524             nan     0.1000   -0.0012\n",
      "   160        0.4269             nan     0.1000   -0.0006\n",
      "   180        0.4025             nan     0.1000   -0.0009\n",
      "   200        0.3799             nan     0.1000    0.0001\n",
      "   220        0.3591             nan     0.1000   -0.0007\n",
      "   240        0.3400             nan     0.1000   -0.0006\n",
      "   250        0.3316             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0466             nan     0.1000    0.0329\n",
      "     2        0.9967             nan     0.1000    0.0233\n",
      "     3        0.9545             nan     0.1000    0.0184\n",
      "     4        0.9203             nan     0.1000    0.0152\n",
      "     5        0.8870             nan     0.1000    0.0146\n",
      "     6        0.8610             nan     0.1000    0.0107\n",
      "     7        0.8397             nan     0.1000    0.0084\n",
      "     8        0.8160             nan     0.1000    0.0096\n",
      "     9        0.7999             nan     0.1000    0.0053\n",
      "    10        0.7839             nan     0.1000    0.0054\n",
      "    20        0.6685             nan     0.1000   -0.0004\n",
      "    40        0.5596             nan     0.1000   -0.0004\n",
      "    60        0.4944             nan     0.1000   -0.0012\n",
      "    80        0.4411             nan     0.1000   -0.0005\n",
      "   100        0.3939             nan     0.1000   -0.0002\n",
      "   120        0.3549             nan     0.1000   -0.0007\n",
      "   140        0.3166             nan     0.1000   -0.0002\n",
      "   160        0.2860             nan     0.1000   -0.0006\n",
      "   180        0.2573             nan     0.1000   -0.0006\n",
      "   200        0.2328             nan     0.1000   -0.0004\n",
      "   220        0.2112             nan     0.1000   -0.0005\n",
      "   240        0.1905             nan     0.1000   -0.0005\n",
      "   250        0.1804             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold09.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1105             nan     0.0100    0.0018\n",
      "     2        1.1067             nan     0.0100    0.0018\n",
      "     3        1.1032             nan     0.0100    0.0017\n",
      "     4        1.0998             nan     0.0100    0.0017\n",
      "     5        1.0965             nan     0.0100    0.0017\n",
      "     6        1.0933             nan     0.0100    0.0016\n",
      "     7        1.0896             nan     0.0100    0.0015\n",
      "     8        1.0865             nan     0.0100    0.0016\n",
      "     9        1.0833             nan     0.0100    0.0015\n",
      "    10        1.0803             nan     0.0100    0.0015\n",
      "    20        1.0528             nan     0.0100    0.0010\n",
      "    40        1.0097             nan     0.0100    0.0007\n",
      "    60        0.9752             nan     0.0100    0.0007\n",
      "    80        0.9459             nan     0.0100    0.0007\n",
      "   100        0.9222             nan     0.0100    0.0004\n",
      "   120        0.9018             nan     0.0100    0.0005\n",
      "   140        0.8831             nan     0.0100    0.0004\n",
      "   160        0.8676             nan     0.0100    0.0004\n",
      "   180        0.8531             nan     0.0100    0.0002\n",
      "   200        0.8413             nan     0.0100    0.0002\n",
      "   220        0.8299             nan     0.0100    0.0002\n",
      "   240        0.8197             nan     0.0100    0.0002\n",
      "   250        0.8146             nan     0.0100    0.0003\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1081             nan     0.0100    0.0029\n",
      "     2        1.1020             nan     0.0100    0.0026\n",
      "     3        1.0956             nan     0.0100    0.0031\n",
      "     4        1.0893             nan     0.0100    0.0029\n",
      "     5        1.0832             nan     0.0100    0.0029\n",
      "     6        1.0774             nan     0.0100    0.0028\n",
      "     7        1.0722             nan     0.0100    0.0025\n",
      "     8        1.0663             nan     0.0100    0.0027\n",
      "     9        1.0609             nan     0.0100    0.0024\n",
      "    10        1.0555             nan     0.0100    0.0025\n",
      "    20        1.0075             nan     0.0100    0.0022\n",
      "    40        0.9333             nan     0.0100    0.0014\n",
      "    60        0.8778             nan     0.0100    0.0008\n",
      "    80        0.8363             nan     0.0100    0.0006\n",
      "   100        0.8017             nan     0.0100    0.0007\n",
      "   120        0.7751             nan     0.0100    0.0006\n",
      "   140        0.7525             nan     0.0100    0.0002\n",
      "   160        0.7348             nan     0.0100    0.0003\n",
      "   180        0.7177             nan     0.0100    0.0001\n",
      "   200        0.7036             nan     0.0100    0.0001\n",
      "   220        0.6912             nan     0.0100    0.0001\n",
      "   240        0.6808             nan     0.0100    0.0001\n",
      "   250        0.6755             nan     0.0100    0.0001\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1070             nan     0.0100    0.0031\n",
      "     2        1.0996             nan     0.0100    0.0035\n",
      "     3        1.0929             nan     0.0100    0.0029\n",
      "     4        1.0866             nan     0.0100    0.0027\n",
      "     5        1.0800             nan     0.0100    0.0030\n",
      "     6        1.0735             nan     0.0100    0.0031\n",
      "     7        1.0668             nan     0.0100    0.0029\n",
      "     8        1.0608             nan     0.0100    0.0026\n",
      "     9        1.0545             nan     0.0100    0.0027\n",
      "    10        1.0484             nan     0.0100    0.0026\n",
      "    20        0.9938             nan     0.0100    0.0022\n",
      "    40        0.9115             nan     0.0100    0.0013\n",
      "    60        0.8501             nan     0.0100    0.0011\n",
      "    80        0.8036             nan     0.0100    0.0008\n",
      "   100        0.7660             nan     0.0100    0.0006\n",
      "   120        0.7354             nan     0.0100    0.0002\n",
      "   140        0.7088             nan     0.0100    0.0004\n",
      "   160        0.6868             nan     0.0100    0.0003\n",
      "   180        0.6673             nan     0.0100    0.0003\n",
      "   200        0.6498             nan     0.0100    0.0002\n",
      "   220        0.6345             nan     0.0100    0.0001\n",
      "   240        0.6202             nan     0.0100    0.0001\n",
      "   250        0.6140             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0969             nan     0.0500    0.0089\n",
      "     2        1.0849             nan     0.0500    0.0049\n",
      "     3        1.0691             nan     0.0500    0.0080\n",
      "     4        1.0544             nan     0.0500    0.0072\n",
      "     5        1.0408             nan     0.0500    0.0065\n",
      "     6        1.0282             nan     0.0500    0.0059\n",
      "     7        1.0194             nan     0.0500    0.0033\n",
      "     8        1.0087             nan     0.0500    0.0052\n",
      "     9        1.0013             nan     0.0500    0.0031\n",
      "    10        0.9925             nan     0.0500    0.0043\n",
      "    20        0.9237             nan     0.0500    0.0027\n",
      "    40        0.8394             nan     0.0500    0.0011\n",
      "    60        0.7898             nan     0.0500    0.0007\n",
      "    80        0.7594             nan     0.0500    0.0001\n",
      "   100        0.7349             nan     0.0500    0.0003\n",
      "   120        0.7190             nan     0.0500   -0.0000\n",
      "   140        0.7070             nan     0.0500    0.0002\n",
      "   160        0.6968             nan     0.0500   -0.0001\n",
      "   180        0.6877             nan     0.0500   -0.0000\n",
      "   200        0.6799             nan     0.0500    0.0001\n",
      "   220        0.6732             nan     0.0500   -0.0001\n",
      "   240        0.6660             nan     0.0500   -0.0003\n",
      "   250        0.6635             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0817             nan     0.0500    0.0136\n",
      "     2        1.0536             nan     0.0500    0.0121\n",
      "     3        1.0290             nan     0.0500    0.0134\n",
      "     4        1.0057             nan     0.0500    0.0110\n",
      "     5        0.9845             nan     0.0500    0.0095\n",
      "     6        0.9640             nan     0.0500    0.0086\n",
      "     7        0.9454             nan     0.0500    0.0089\n",
      "     8        0.9294             nan     0.0500    0.0074\n",
      "     9        0.9149             nan     0.0500    0.0071\n",
      "    10        0.9019             nan     0.0500    0.0057\n",
      "    20        0.8029             nan     0.0500    0.0030\n",
      "    40        0.7021             nan     0.0500    0.0013\n",
      "    60        0.6521             nan     0.0500    0.0006\n",
      "    80        0.6156             nan     0.0500   -0.0001\n",
      "   100        0.5868             nan     0.0500   -0.0002\n",
      "   120        0.5623             nan     0.0500   -0.0006\n",
      "   140        0.5401             nan     0.0500   -0.0005\n",
      "   160        0.5212             nan     0.0500   -0.0006\n",
      "   180        0.5062             nan     0.0500   -0.0004\n",
      "   200        0.4908             nan     0.0500   -0.0006\n",
      "   220        0.4764             nan     0.0500   -0.0003\n",
      "   240        0.4622             nan     0.0500   -0.0004\n",
      "   250        0.4566             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0781             nan     0.0500    0.0168\n",
      "     2        1.0455             nan     0.0500    0.0149\n",
      "     3        1.0170             nan     0.0500    0.0134\n",
      "     4        0.9919             nan     0.0500    0.0115\n",
      "     5        0.9681             nan     0.0500    0.0106\n",
      "     6        0.9475             nan     0.0500    0.0094\n",
      "     7        0.9290             nan     0.0500    0.0083\n",
      "     8        0.9104             nan     0.0500    0.0075\n",
      "     9        0.8927             nan     0.0500    0.0082\n",
      "    10        0.8793             nan     0.0500    0.0045\n",
      "    20        0.7656             nan     0.0500    0.0030\n",
      "    40        0.6489             nan     0.0500    0.0004\n",
      "    60        0.5819             nan     0.0500    0.0002\n",
      "    80        0.5375             nan     0.0500   -0.0002\n",
      "   100        0.5004             nan     0.0500   -0.0005\n",
      "   120        0.4685             nan     0.0500   -0.0007\n",
      "   140        0.4395             nan     0.0500   -0.0003\n",
      "   160        0.4146             nan     0.0500   -0.0004\n",
      "   180        0.3900             nan     0.0500   -0.0002\n",
      "   200        0.3700             nan     0.0500   -0.0003\n",
      "   220        0.3508             nan     0.0500   -0.0006\n",
      "   240        0.3322             nan     0.0500   -0.0004\n",
      "   250        0.3245             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0787             nan     0.1000    0.0173\n",
      "     2        1.0495             nan     0.1000    0.0140\n",
      "     3        1.0288             nan     0.1000    0.0103\n",
      "     4        1.0068             nan     0.1000    0.0111\n",
      "     5        0.9880             nan     0.1000    0.0090\n",
      "     6        0.9715             nan     0.1000    0.0080\n",
      "     7        0.9581             nan     0.1000    0.0057\n",
      "     8        0.9433             nan     0.1000    0.0071\n",
      "     9        0.9299             nan     0.1000    0.0062\n",
      "    10        0.9206             nan     0.1000    0.0036\n",
      "    20        0.8381             nan     0.1000    0.0015\n",
      "    40        0.7561             nan     0.1000    0.0009\n",
      "    60        0.7188             nan     0.1000    0.0011\n",
      "    80        0.6960             nan     0.1000   -0.0001\n",
      "   100        0.6796             nan     0.1000   -0.0002\n",
      "   120        0.6670             nan     0.1000   -0.0002\n",
      "   140        0.6562             nan     0.1000   -0.0001\n",
      "   160        0.6458             nan     0.1000   -0.0001\n",
      "   180        0.6377             nan     0.1000   -0.0006\n",
      "   200        0.6314             nan     0.1000   -0.0005\n",
      "   220        0.6244             nan     0.1000   -0.0005\n",
      "   240        0.6188             nan     0.1000   -0.0001\n",
      "   250        0.6160             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0545             nan     0.1000    0.0307\n",
      "     2        1.0078             nan     0.1000    0.0229\n",
      "     3        0.9668             nan     0.1000    0.0213\n",
      "     4        0.9332             nan     0.1000    0.0151\n",
      "     5        0.9048             nan     0.1000    0.0122\n",
      "     6        0.8761             nan     0.1000    0.0124\n",
      "     7        0.8506             nan     0.1000    0.0123\n",
      "     8        0.8341             nan     0.1000    0.0059\n",
      "     9        0.8167             nan     0.1000    0.0065\n",
      "    10        0.8013             nan     0.1000    0.0054\n",
      "    20        0.7037             nan     0.1000    0.0022\n",
      "    40        0.6165             nan     0.1000   -0.0006\n",
      "    60        0.5701             nan     0.1000   -0.0008\n",
      "    80        0.5346             nan     0.1000   -0.0002\n",
      "   100        0.4988             nan     0.1000   -0.0009\n",
      "   120        0.4708             nan     0.1000   -0.0011\n",
      "   140        0.4428             nan     0.1000   -0.0007\n",
      "   160        0.4192             nan     0.1000   -0.0006\n",
      "   180        0.3953             nan     0.1000   -0.0006\n",
      "   200        0.3738             nan     0.1000   -0.0008\n",
      "   220        0.3570             nan     0.1000   -0.0006\n",
      "   240        0.3357             nan     0.1000   -0.0004\n",
      "   250        0.3283             nan     0.1000   -0.0007\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0418             nan     0.1000    0.0320\n",
      "     2        0.9858             nan     0.1000    0.0236\n",
      "     3        0.9410             nan     0.1000    0.0185\n",
      "     4        0.9057             nan     0.1000    0.0142\n",
      "     5        0.8745             nan     0.1000    0.0129\n",
      "     6        0.8462             nan     0.1000    0.0118\n",
      "     7        0.8205             nan     0.1000    0.0097\n",
      "     8        0.8001             nan     0.1000    0.0071\n",
      "     9        0.7812             nan     0.1000    0.0068\n",
      "    10        0.7626             nan     0.1000    0.0064\n",
      "    20        0.6538             nan     0.1000    0.0019\n",
      "    40        0.5443             nan     0.1000   -0.0006\n",
      "    60        0.4756             nan     0.1000   -0.0013\n",
      "    80        0.4193             nan     0.1000    0.0003\n",
      "   100        0.3753             nan     0.1000   -0.0010\n",
      "   120        0.3394             nan     0.1000   -0.0006\n",
      "   140        0.3059             nan     0.1000   -0.0010\n",
      "   160        0.2758             nan     0.1000   -0.0009\n",
      "   180        0.2499             nan     0.1000   -0.0012\n",
      "   200        0.2254             nan     0.1000   -0.0006\n",
      "   220        0.2053             nan     0.1000   -0.0008\n",
      "   240        0.1877             nan     0.1000   -0.0004\n",
      "   250        0.1774             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold10.Rep3: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1115             nan     0.0100    0.0019\n",
      "     2        1.1078             nan     0.0100    0.0018\n",
      "     3        1.1041             nan     0.0100    0.0018\n",
      "     4        1.1007             nan     0.0100    0.0017\n",
      "     5        1.0972             nan     0.0100    0.0017\n",
      "     6        1.0938             nan     0.0100    0.0017\n",
      "     7        1.0904             nan     0.0100    0.0016\n",
      "     8        1.0873             nan     0.0100    0.0016\n",
      "     9        1.0844             nan     0.0100    0.0016\n",
      "    10        1.0813             nan     0.0100    0.0016\n",
      "    20        1.0527             nan     0.0100    0.0013\n",
      "    40        1.0096             nan     0.0100    0.0010\n",
      "    60        0.9751             nan     0.0100    0.0008\n",
      "    80        0.9469             nan     0.0100    0.0005\n",
      "   100        0.9235             nan     0.0100    0.0004\n",
      "   120        0.9029             nan     0.0100    0.0005\n",
      "   140        0.8843             nan     0.0100    0.0004\n",
      "   160        0.8686             nan     0.0100    0.0003\n",
      "   180        0.8546             nan     0.0100    0.0003\n",
      "   200        0.8429             nan     0.0100    0.0002\n",
      "   220        0.8318             nan     0.0100    0.0003\n",
      "   240        0.8218             nan     0.0100    0.0002\n",
      "   250        0.8169             nan     0.0100    0.0002\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1086             nan     0.0100    0.0031\n",
      "     2        1.1025             nan     0.0100    0.0028\n",
      "     3        1.0960             nan     0.0100    0.0027\n",
      "     4        1.0894             nan     0.0100    0.0029\n",
      "     5        1.0839             nan     0.0100    0.0025\n",
      "     6        1.0778             nan     0.0100    0.0028\n",
      "     7        1.0719             nan     0.0100    0.0027\n",
      "     8        1.0667             nan     0.0100    0.0025\n",
      "     9        1.0611             nan     0.0100    0.0026\n",
      "    10        1.0554             nan     0.0100    0.0025\n",
      "    20        1.0087             nan     0.0100    0.0022\n",
      "    40        0.9367             nan     0.0100    0.0015\n",
      "    60        0.8824             nan     0.0100    0.0009\n",
      "    80        0.8407             nan     0.0100    0.0007\n",
      "   100        0.8084             nan     0.0100    0.0005\n",
      "   120        0.7816             nan     0.0100    0.0004\n",
      "   140        0.7588             nan     0.0100    0.0003\n",
      "   160        0.7401             nan     0.0100    0.0003\n",
      "   180        0.7230             nan     0.0100    0.0003\n",
      "   200        0.7090             nan     0.0100    0.0002\n",
      "   220        0.6962             nan     0.0100    0.0003\n",
      "   240        0.6855             nan     0.0100    0.0001\n",
      "   250        0.6795             nan     0.0100    0.0000\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1077             nan     0.0100    0.0034\n",
      "     2        1.1004             nan     0.0100    0.0033\n",
      "     3        1.0932             nan     0.0100    0.0028\n",
      "     4        1.0861             nan     0.0100    0.0031\n",
      "     5        1.0794             nan     0.0100    0.0030\n",
      "     6        1.0729             nan     0.0100    0.0028\n",
      "     7        1.0666             nan     0.0100    0.0028\n",
      "     8        1.0601             nan     0.0100    0.0029\n",
      "     9        1.0537             nan     0.0100    0.0026\n",
      "    10        1.0480             nan     0.0100    0.0028\n",
      "    20        0.9953             nan     0.0100    0.0021\n",
      "    40        0.9135             nan     0.0100    0.0016\n",
      "    60        0.8545             nan     0.0100    0.0012\n",
      "    80        0.8073             nan     0.0100    0.0007\n",
      "   100        0.7695             nan     0.0100    0.0005\n",
      "   120        0.7387             nan     0.0100    0.0004\n",
      "   140        0.7131             nan     0.0100    0.0002\n",
      "   160        0.6910             nan     0.0100    0.0004\n",
      "   180        0.6713             nan     0.0100    0.0002\n",
      "   200        0.6544             nan     0.0100    0.0002\n",
      "   220        0.6396             nan     0.0100    0.0002\n",
      "   240        0.6255             nan     0.0100    0.0001\n",
      "   250        0.6185             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0967             nan     0.0500    0.0091\n",
      "     2        1.0797             nan     0.0500    0.0082\n",
      "     3        1.0642             nan     0.0500    0.0074\n",
      "     4        1.0495             nan     0.0500    0.0065\n",
      "     5        1.0379             nan     0.0500    0.0060\n",
      "     6        1.0275             nan     0.0500    0.0051\n",
      "     7        1.0162             nan     0.0500    0.0053\n",
      "     8        1.0081             nan     0.0500    0.0038\n",
      "     9        0.9974             nan     0.0500    0.0046\n",
      "    10        0.9881             nan     0.0500    0.0044\n",
      "    20        0.9202             nan     0.0500    0.0023\n",
      "    40        0.8416             nan     0.0500    0.0015\n",
      "    60        0.7936             nan     0.0500    0.0009\n",
      "    80        0.7632             nan     0.0500    0.0003\n",
      "   100        0.7411             nan     0.0500    0.0002\n",
      "   120        0.7241             nan     0.0500    0.0004\n",
      "   140        0.7134             nan     0.0500    0.0002\n",
      "   160        0.7027             nan     0.0500   -0.0002\n",
      "   180        0.6930             nan     0.0500    0.0002\n",
      "   200        0.6858             nan     0.0500   -0.0001\n",
      "   220        0.6795             nan     0.0500    0.0000\n",
      "   240        0.6731             nan     0.0500   -0.0000\n",
      "   250        0.6698             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0817             nan     0.0500    0.0164\n",
      "     2        1.0531             nan     0.0500    0.0136\n",
      "     3        1.0274             nan     0.0500    0.0113\n",
      "     4        1.0045             nan     0.0500    0.0112\n",
      "     5        0.9837             nan     0.0500    0.0103\n",
      "     6        0.9665             nan     0.0500    0.0082\n",
      "     7        0.9499             nan     0.0500    0.0077\n",
      "     8        0.9332             nan     0.0500    0.0077\n",
      "     9        0.9177             nan     0.0500    0.0080\n",
      "    10        0.9040             nan     0.0500    0.0060\n",
      "    20        0.8095             nan     0.0500    0.0023\n",
      "    40        0.7085             nan     0.0500    0.0008\n",
      "    60        0.6587             nan     0.0500    0.0004\n",
      "    80        0.6228             nan     0.0500    0.0001\n",
      "   100        0.5950             nan     0.0500   -0.0010\n",
      "   120        0.5731             nan     0.0500   -0.0001\n",
      "   140        0.5507             nan     0.0500   -0.0003\n",
      "   160        0.5348             nan     0.0500   -0.0006\n",
      "   180        0.5169             nan     0.0500   -0.0001\n",
      "   200        0.4994             nan     0.0500   -0.0003\n",
      "   220        0.4834             nan     0.0500   -0.0002\n",
      "   240        0.4687             nan     0.0500   -0.0001\n",
      "   250        0.4618             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0790             nan     0.0500    0.0161\n",
      "     2        1.0462             nan     0.0500    0.0149\n",
      "     3        1.0191             nan     0.0500    0.0127\n",
      "     4        0.9946             nan     0.0500    0.0108\n",
      "     5        0.9711             nan     0.0500    0.0093\n",
      "     6        0.9516             nan     0.0500    0.0090\n",
      "     7        0.9319             nan     0.0500    0.0084\n",
      "     8        0.9127             nan     0.0500    0.0081\n",
      "     9        0.8961             nan     0.0500    0.0066\n",
      "    10        0.8798             nan     0.0500    0.0069\n",
      "    20        0.7667             nan     0.0500    0.0037\n",
      "    40        0.6546             nan     0.0500    0.0004\n",
      "    60        0.5925             nan     0.0500   -0.0006\n",
      "    80        0.5484             nan     0.0500   -0.0003\n",
      "   100        0.5112             nan     0.0500   -0.0006\n",
      "   120        0.4752             nan     0.0500   -0.0002\n",
      "   140        0.4483             nan     0.0500   -0.0004\n",
      "   160        0.4228             nan     0.0500   -0.0001\n",
      "   180        0.3995             nan     0.0500   -0.0001\n",
      "   200        0.3770             nan     0.0500   -0.0002\n",
      "   220        0.3564             nan     0.0500   -0.0004\n",
      "   240        0.3373             nan     0.0500   -0.0004\n",
      "   250        0.3288             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0823             nan     0.1000    0.0178\n",
      "     2        1.0516             nan     0.1000    0.0145\n",
      "     3        1.0277             nan     0.1000    0.0118\n",
      "     4        1.0095             nan     0.1000    0.0086\n",
      "     5        0.9892             nan     0.1000    0.0093\n",
      "     6        0.9728             nan     0.1000    0.0076\n",
      "     7        0.9610             nan     0.1000    0.0069\n",
      "     8        0.9448             nan     0.1000    0.0080\n",
      "     9        0.9312             nan     0.1000    0.0061\n",
      "    10        0.9218             nan     0.1000    0.0048\n",
      "    20        0.8416             nan     0.1000    0.0019\n",
      "    40        0.7636             nan     0.1000    0.0006\n",
      "    60        0.7251             nan     0.1000    0.0002\n",
      "    80        0.7030             nan     0.1000   -0.0002\n",
      "   100        0.6861             nan     0.1000   -0.0001\n",
      "   120        0.6732             nan     0.1000   -0.0003\n",
      "   140        0.6622             nan     0.1000   -0.0001\n",
      "   160        0.6517             nan     0.1000   -0.0001\n",
      "   180        0.6442             nan     0.1000   -0.0006\n",
      "   200        0.6389             nan     0.1000   -0.0005\n",
      "   220        0.6339             nan     0.1000   -0.0003\n",
      "   240        0.6271             nan     0.1000   -0.0003\n",
      "   250        0.6252             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0499             nan     0.1000    0.0343\n",
      "     2        1.0005             nan     0.1000    0.0249\n",
      "     3        0.9607             nan     0.1000    0.0192\n",
      "     4        0.9275             nan     0.1000    0.0133\n",
      "     5        0.8990             nan     0.1000    0.0139\n",
      "     6        0.8731             nan     0.1000    0.0109\n",
      "     7        0.8504             nan     0.1000    0.0102\n",
      "     8        0.8325             nan     0.1000    0.0076\n",
      "     9        0.8163             nan     0.1000    0.0066\n",
      "    10        0.8008             nan     0.1000    0.0065\n",
      "    20        0.7072             nan     0.1000    0.0010\n",
      "    40        0.6221             nan     0.1000    0.0000\n",
      "    60        0.5728             nan     0.1000   -0.0012\n",
      "    80        0.5322             nan     0.1000   -0.0004\n",
      "   100        0.4974             nan     0.1000   -0.0004\n",
      "   120        0.4675             nan     0.1000   -0.0010\n",
      "   140        0.4410             nan     0.1000   -0.0007\n",
      "   160        0.4150             nan     0.1000   -0.0004\n",
      "   180        0.3929             nan     0.1000   -0.0004\n",
      "   200        0.3712             nan     0.1000   -0.0009\n",
      "   220        0.3513             nan     0.1000   -0.0003\n",
      "   240        0.3330             nan     0.1000   -0.0004\n",
      "   250        0.3243             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0465             nan     0.1000    0.0315\n",
      "     2        0.9901             nan     0.1000    0.0251\n",
      "     3        0.9463             nan     0.1000    0.0203\n",
      "     4        0.9072             nan     0.1000    0.0157\n",
      "     5        0.8718             nan     0.1000    0.0148\n",
      "     6        0.8457             nan     0.1000    0.0105\n",
      "     7        0.8202             nan     0.1000    0.0100\n",
      "     8        0.7978             nan     0.1000    0.0089\n",
      "     9        0.7789             nan     0.1000    0.0060\n",
      "    10        0.7623             nan     0.1000    0.0057\n",
      "    20        0.6517             nan     0.1000    0.0004\n",
      "    40        0.5414             nan     0.1000   -0.0002\n",
      "    60        0.4736             nan     0.1000   -0.0002\n",
      "    80        0.4185             nan     0.1000   -0.0011\n",
      "   100        0.3699             nan     0.1000   -0.0000\n",
      "   120        0.3301             nan     0.1000   -0.0007\n",
      "   140        0.2964             nan     0.1000   -0.0004\n",
      "   160        0.2680             nan     0.1000   -0.0004\n",
      "   180        0.2452             nan     0.1000   -0.0011\n",
      "   200        0.2228             nan     0.1000   -0.0001\n",
      "   220        0.2021             nan     0.1000   -0.0005\n",
      "   240        0.1843             nan     0.1000   -0.0004\n",
      "   250        0.1773             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold01.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1104             nan     0.0100    0.0018\n",
      "     2        1.1066             nan     0.0100    0.0018\n",
      "     3        1.1030             nan     0.0100    0.0017\n",
      "     4        1.0995             nan     0.0100    0.0017\n",
      "     5        1.0962             nan     0.0100    0.0017\n",
      "     6        1.0929             nan     0.0100    0.0016\n",
      "     7        1.0896             nan     0.0100    0.0016\n",
      "     8        1.0866             nan     0.0100    0.0016\n",
      "     9        1.0837             nan     0.0100    0.0015\n",
      "    10        1.0808             nan     0.0100    0.0015\n",
      "    20        1.0547             nan     0.0100    0.0010\n",
      "    40        1.0112             nan     0.0100    0.0007\n",
      "    60        0.9752             nan     0.0100    0.0008\n",
      "    80        0.9461             nan     0.0100    0.0006\n",
      "   100        0.9220             nan     0.0100    0.0003\n",
      "   120        0.9021             nan     0.0100    0.0004\n",
      "   140        0.8827             nan     0.0100    0.0002\n",
      "   160        0.8663             nan     0.0100    0.0004\n",
      "   180        0.8520             nan     0.0100    0.0004\n",
      "   200        0.8392             nan     0.0100    0.0003\n",
      "   220        0.8271             nan     0.0100    0.0002\n",
      "   240        0.8159             nan     0.0100    0.0001\n",
      "   250        0.8109             nan     0.0100    0.0001\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1077             nan     0.0100    0.0033\n",
      "     2        1.1013             nan     0.0100    0.0032\n",
      "     3        1.0954             nan     0.0100    0.0029\n",
      "     4        1.0893             nan     0.0100    0.0029\n",
      "     5        1.0833             nan     0.0100    0.0029\n",
      "     6        1.0775             nan     0.0100    0.0027\n",
      "     7        1.0717             nan     0.0100    0.0028\n",
      "     8        1.0662             nan     0.0100    0.0027\n",
      "     9        1.0612             nan     0.0100    0.0022\n",
      "    10        1.0558             nan     0.0100    0.0025\n",
      "    20        1.0064             nan     0.0100    0.0021\n",
      "    40        0.9309             nan     0.0100    0.0016\n",
      "    60        0.8754             nan     0.0100    0.0009\n",
      "    80        0.8330             nan     0.0100    0.0007\n",
      "   100        0.8003             nan     0.0100    0.0008\n",
      "   120        0.7732             nan     0.0100    0.0005\n",
      "   140        0.7501             nan     0.0100    0.0004\n",
      "   160        0.7308             nan     0.0100    0.0003\n",
      "   180        0.7150             nan     0.0100    0.0001\n",
      "   200        0.7010             nan     0.0100    0.0001\n",
      "   220        0.6877             nan     0.0100    0.0001\n",
      "   240        0.6766             nan     0.0100    0.0001\n",
      "   250        0.6716             nan     0.0100    0.0001\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1070             nan     0.0100    0.0034\n",
      "     2        1.1000             nan     0.0100    0.0031\n",
      "     3        1.0928             nan     0.0100    0.0035\n",
      "     4        1.0858             nan     0.0100    0.0033\n",
      "     5        1.0786             nan     0.0100    0.0032\n",
      "     6        1.0720             nan     0.0100    0.0029\n",
      "     7        1.0655             nan     0.0100    0.0028\n",
      "     8        1.0594             nan     0.0100    0.0027\n",
      "     9        1.0529             nan     0.0100    0.0029\n",
      "    10        1.0469             nan     0.0100    0.0026\n",
      "    20        0.9921             nan     0.0100    0.0021\n",
      "    40        0.9098             nan     0.0100    0.0013\n",
      "    60        0.8466             nan     0.0100    0.0011\n",
      "    80        0.8006             nan     0.0100    0.0008\n",
      "   100        0.7624             nan     0.0100    0.0007\n",
      "   120        0.7313             nan     0.0100    0.0004\n",
      "   140        0.7051             nan     0.0100    0.0003\n",
      "   160        0.6824             nan     0.0100    0.0002\n",
      "   180        0.6630             nan     0.0100    0.0001\n",
      "   200        0.6461             nan     0.0100    0.0001\n",
      "   220        0.6313             nan     0.0100    0.0002\n",
      "   240        0.6169             nan     0.0100    0.0001\n",
      "   250        0.6103             nan     0.0100    0.0000\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0970             nan     0.0500    0.0089\n",
      "     2        1.0793             nan     0.0500    0.0078\n",
      "     3        1.0650             nan     0.0500    0.0072\n",
      "     4        1.0536             nan     0.0500    0.0048\n",
      "     5        1.0408             nan     0.0500    0.0064\n",
      "     6        1.0282             nan     0.0500    0.0057\n",
      "     7        1.0188             nan     0.0500    0.0046\n",
      "     8        1.0075             nan     0.0500    0.0051\n",
      "     9        0.9969             nan     0.0500    0.0045\n",
      "    10        0.9882             nan     0.0500    0.0042\n",
      "    20        0.9211             nan     0.0500    0.0023\n",
      "    40        0.8354             nan     0.0500    0.0012\n",
      "    60        0.7871             nan     0.0500    0.0010\n",
      "    80        0.7547             nan     0.0500    0.0004\n",
      "   100        0.7313             nan     0.0500    0.0003\n",
      "   120        0.7148             nan     0.0500   -0.0001\n",
      "   140        0.7041             nan     0.0500   -0.0001\n",
      "   160        0.6931             nan     0.0500   -0.0001\n",
      "   180        0.6844             nan     0.0500   -0.0000\n",
      "   200        0.6757             nan     0.0500   -0.0002\n",
      "   220        0.6685             nan     0.0500    0.0001\n",
      "   240        0.6613             nan     0.0500   -0.0000\n",
      "   250        0.6587             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0830             nan     0.0500    0.0151\n",
      "     2        1.0528             nan     0.0500    0.0139\n",
      "     3        1.0280             nan     0.0500    0.0118\n",
      "     4        1.0059             nan     0.0500    0.0102\n",
      "     5        0.9837             nan     0.0500    0.0101\n",
      "     6        0.9648             nan     0.0500    0.0081\n",
      "     7        0.9478             nan     0.0500    0.0077\n",
      "     8        0.9310             nan     0.0500    0.0074\n",
      "     9        0.9151             nan     0.0500    0.0076\n",
      "    10        0.8991             nan     0.0500    0.0076\n",
      "    20        0.7975             nan     0.0500    0.0023\n",
      "    40        0.6998             nan     0.0500    0.0007\n",
      "    60        0.6485             nan     0.0500   -0.0002\n",
      "    80        0.6122             nan     0.0500   -0.0001\n",
      "   100        0.5833             nan     0.0500    0.0000\n",
      "   120        0.5621             nan     0.0500   -0.0002\n",
      "   140        0.5411             nan     0.0500   -0.0001\n",
      "   160        0.5223             nan     0.0500   -0.0004\n",
      "   180        0.5054             nan     0.0500    0.0000\n",
      "   200        0.4915             nan     0.0500   -0.0002\n",
      "   220        0.4734             nan     0.0500   -0.0004\n",
      "   240        0.4571             nan     0.0500   -0.0003\n",
      "   250        0.4507             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0791             nan     0.0500    0.0161\n",
      "     2        1.0472             nan     0.0500    0.0142\n",
      "     3        1.0175             nan     0.0500    0.0140\n",
      "     4        0.9923             nan     0.0500    0.0109\n",
      "     5        0.9711             nan     0.0500    0.0098\n",
      "     6        0.9500             nan     0.0500    0.0091\n",
      "     7        0.9294             nan     0.0500    0.0093\n",
      "     8        0.9104             nan     0.0500    0.0082\n",
      "     9        0.8929             nan     0.0500    0.0073\n",
      "    10        0.8777             nan     0.0500    0.0058\n",
      "    20        0.7619             nan     0.0500    0.0023\n",
      "    40        0.6488             nan     0.0500    0.0003\n",
      "    60        0.5858             nan     0.0500    0.0002\n",
      "    80        0.5401             nan     0.0500   -0.0006\n",
      "   100        0.5017             nan     0.0500   -0.0004\n",
      "   120        0.4679             nan     0.0500   -0.0006\n",
      "   140        0.4399             nan     0.0500   -0.0007\n",
      "   160        0.4123             nan     0.0500   -0.0006\n",
      "   180        0.3882             nan     0.0500   -0.0006\n",
      "   200        0.3684             nan     0.0500   -0.0004\n",
      "   220        0.3484             nan     0.0500   -0.0002\n",
      "   240        0.3302             nan     0.0500   -0.0003\n",
      "   250        0.3209             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0786             nan     0.1000    0.0173\n",
      "     2        1.0511             nan     0.1000    0.0139\n",
      "     3        1.0289             nan     0.1000    0.0101\n",
      "     4        1.0066             nan     0.1000    0.0110\n",
      "     5        0.9860             nan     0.1000    0.0082\n",
      "     6        0.9726             nan     0.1000    0.0050\n",
      "     7        0.9621             nan     0.1000    0.0038\n",
      "     8        0.9451             nan     0.1000    0.0086\n",
      "     9        0.9369             nan     0.1000    0.0034\n",
      "    10        0.9242             nan     0.1000    0.0058\n",
      "    20        0.8343             nan     0.1000    0.0019\n",
      "    40        0.7537             nan     0.1000    0.0007\n",
      "    60        0.7148             nan     0.1000    0.0003\n",
      "    80        0.6937             nan     0.1000   -0.0000\n",
      "   100        0.6784             nan     0.1000    0.0002\n",
      "   120        0.6629             nan     0.1000   -0.0001\n",
      "   140        0.6521             nan     0.1000   -0.0002\n",
      "   160        0.6433             nan     0.1000   -0.0005\n",
      "   180        0.6361             nan     0.1000    0.0001\n",
      "   200        0.6305             nan     0.1000   -0.0005\n",
      "   220        0.6243             nan     0.1000   -0.0002\n",
      "   240        0.6180             nan     0.1000   -0.0001\n",
      "   250        0.6158             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0552             nan     0.1000    0.0298\n",
      "     2        1.0023             nan     0.1000    0.0248\n",
      "     3        0.9602             nan     0.1000    0.0182\n",
      "     4        0.9244             nan     0.1000    0.0167\n",
      "     5        0.8975             nan     0.1000    0.0132\n",
      "     6        0.8750             nan     0.1000    0.0081\n",
      "     7        0.8549             nan     0.1000    0.0060\n",
      "     8        0.8348             nan     0.1000    0.0086\n",
      "     9        0.8172             nan     0.1000    0.0081\n",
      "    10        0.8009             nan     0.1000    0.0066\n",
      "    20        0.7031             nan     0.1000    0.0017\n",
      "    40        0.6199             nan     0.1000    0.0001\n",
      "    60        0.5693             nan     0.1000    0.0000\n",
      "    80        0.5329             nan     0.1000   -0.0007\n",
      "   100        0.4988             nan     0.1000   -0.0012\n",
      "   120        0.4637             nan     0.1000   -0.0005\n",
      "   140        0.4343             nan     0.1000   -0.0012\n",
      "   160        0.4088             nan     0.1000   -0.0004\n",
      "   180        0.3832             nan     0.1000   -0.0005\n",
      "   200        0.3618             nan     0.1000   -0.0007\n",
      "   220        0.3438             nan     0.1000   -0.0008\n",
      "   240        0.3258             nan     0.1000   -0.0004\n",
      "   250        0.3179             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0482             nan     0.1000    0.0302\n",
      "     2        0.9953             nan     0.1000    0.0213\n",
      "     3        0.9492             nan     0.1000    0.0192\n",
      "     4        0.9092             nan     0.1000    0.0192\n",
      "     5        0.8790             nan     0.1000    0.0125\n",
      "     6        0.8504             nan     0.1000    0.0116\n",
      "     7        0.8240             nan     0.1000    0.0113\n",
      "     8        0.8010             nan     0.1000    0.0093\n",
      "     9        0.7827             nan     0.1000    0.0046\n",
      "    10        0.7636             nan     0.1000    0.0070\n",
      "    20        0.6516             nan     0.1000    0.0015\n",
      "    40        0.5442             nan     0.1000   -0.0005\n",
      "    60        0.4719             nan     0.1000   -0.0008\n",
      "    80        0.4095             nan     0.1000   -0.0012\n",
      "   100        0.3690             nan     0.1000   -0.0015\n",
      "   120        0.3298             nan     0.1000   -0.0010\n",
      "   140        0.3002             nan     0.1000   -0.0008\n",
      "   160        0.2689             nan     0.1000   -0.0000\n",
      "   180        0.2427             nan     0.1000   -0.0006\n",
      "   200        0.2188             nan     0.1000   -0.0004\n",
      "   220        0.1967             nan     0.1000   -0.0004\n",
      "   240        0.1798             nan     0.1000   -0.0003\n",
      "   250        0.1712             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold02.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0018\n",
      "     2        1.1072             nan     0.0100    0.0018\n",
      "     3        1.1038             nan     0.0100    0.0017\n",
      "     4        1.1002             nan     0.0100    0.0017\n",
      "     5        1.0971             nan     0.0100    0.0017\n",
      "     6        1.0939             nan     0.0100    0.0016\n",
      "     7        1.0906             nan     0.0100    0.0016\n",
      "     8        1.0877             nan     0.0100    0.0016\n",
      "     9        1.0846             nan     0.0100    0.0016\n",
      "    10        1.0816             nan     0.0100    0.0015\n",
      "    20        1.0545             nan     0.0100    0.0013\n",
      "    40        1.0101             nan     0.0100    0.0008\n",
      "    60        0.9753             nan     0.0100    0.0006\n",
      "    80        0.9477             nan     0.0100    0.0006\n",
      "   100        0.9226             nan     0.0100    0.0005\n",
      "   120        0.9018             nan     0.0100    0.0004\n",
      "   140        0.8831             nan     0.0100    0.0003\n",
      "   160        0.8678             nan     0.0100    0.0003\n",
      "   180        0.8533             nan     0.0100    0.0003\n",
      "   200        0.8404             nan     0.0100    0.0003\n",
      "   220        0.8289             nan     0.0100    0.0002\n",
      "   240        0.8185             nan     0.0100    0.0002\n",
      "   250        0.8140             nan     0.0100    0.0002\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1072             nan     0.0100    0.0031\n",
      "     2        1.1009             nan     0.0100    0.0031\n",
      "     3        1.0946             nan     0.0100    0.0032\n",
      "     4        1.0884             nan     0.0100    0.0028\n",
      "     5        1.0825             nan     0.0100    0.0030\n",
      "     6        1.0769             nan     0.0100    0.0028\n",
      "     7        1.0711             nan     0.0100    0.0027\n",
      "     8        1.0651             nan     0.0100    0.0027\n",
      "     9        1.0597             nan     0.0100    0.0027\n",
      "    10        1.0542             nan     0.0100    0.0028\n",
      "    20        1.0058             nan     0.0100    0.0021\n",
      "    40        0.9328             nan     0.0100    0.0014\n",
      "    60        0.8778             nan     0.0100    0.0012\n",
      "    80        0.8365             nan     0.0100    0.0007\n",
      "   100        0.8030             nan     0.0100    0.0006\n",
      "   120        0.7749             nan     0.0100    0.0006\n",
      "   140        0.7529             nan     0.0100    0.0002\n",
      "   160        0.7341             nan     0.0100    0.0002\n",
      "   180        0.7180             nan     0.0100    0.0002\n",
      "   200        0.7041             nan     0.0100    0.0002\n",
      "   220        0.6917             nan     0.0100    0.0002\n",
      "   240        0.6812             nan     0.0100    0.0002\n",
      "   250        0.6761             nan     0.0100    0.0001\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1067             nan     0.0100    0.0033\n",
      "     2        1.0995             nan     0.0100    0.0030\n",
      "     3        1.0922             nan     0.0100    0.0037\n",
      "     4        1.0856             nan     0.0100    0.0030\n",
      "     5        1.0785             nan     0.0100    0.0033\n",
      "     6        1.0719             nan     0.0100    0.0029\n",
      "     7        1.0652             nan     0.0100    0.0031\n",
      "     8        1.0587             nan     0.0100    0.0030\n",
      "     9        1.0528             nan     0.0100    0.0026\n",
      "    10        1.0465             nan     0.0100    0.0029\n",
      "    20        0.9923             nan     0.0100    0.0018\n",
      "    40        0.9104             nan     0.0100    0.0016\n",
      "    60        0.8491             nan     0.0100    0.0011\n",
      "    80        0.8027             nan     0.0100    0.0009\n",
      "   100        0.7646             nan     0.0100    0.0004\n",
      "   120        0.7340             nan     0.0100    0.0005\n",
      "   140        0.7079             nan     0.0100    0.0003\n",
      "   160        0.6866             nan     0.0100    0.0003\n",
      "   180        0.6682             nan     0.0100    0.0002\n",
      "   200        0.6514             nan     0.0100    0.0002\n",
      "   220        0.6358             nan     0.0100    0.0001\n",
      "   240        0.6217             nan     0.0100   -0.0000\n",
      "   250        0.6150             nan     0.0100    0.0000\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0971             nan     0.0500    0.0089\n",
      "     2        1.0824             nan     0.0500    0.0080\n",
      "     3        1.0676             nan     0.0500    0.0074\n",
      "     4        1.0566             nan     0.0500    0.0048\n",
      "     5        1.0427             nan     0.0500    0.0065\n",
      "     6        1.0328             nan     0.0500    0.0045\n",
      "     7        1.0239             nan     0.0500    0.0039\n",
      "     8        1.0129             nan     0.0500    0.0058\n",
      "     9        1.0054             nan     0.0500    0.0038\n",
      "    10        0.9954             nan     0.0500    0.0053\n",
      "    20        0.9207             nan     0.0500    0.0020\n",
      "    40        0.8412             nan     0.0500    0.0011\n",
      "    60        0.7898             nan     0.0500    0.0010\n",
      "    80        0.7572             nan     0.0500    0.0009\n",
      "   100        0.7367             nan     0.0500   -0.0000\n",
      "   120        0.7195             nan     0.0500   -0.0001\n",
      "   140        0.7085             nan     0.0500   -0.0000\n",
      "   160        0.6981             nan     0.0500    0.0000\n",
      "   180        0.6894             nan     0.0500   -0.0001\n",
      "   200        0.6818             nan     0.0500    0.0002\n",
      "   220        0.6752             nan     0.0500   -0.0002\n",
      "   240        0.6686             nan     0.0500   -0.0002\n",
      "   250        0.6657             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0831             nan     0.0500    0.0150\n",
      "     2        1.0542             nan     0.0500    0.0140\n",
      "     3        1.0302             nan     0.0500    0.0115\n",
      "     4        1.0051             nan     0.0500    0.0120\n",
      "     5        0.9849             nan     0.0500    0.0100\n",
      "     6        0.9647             nan     0.0500    0.0089\n",
      "     7        0.9464             nan     0.0500    0.0084\n",
      "     8        0.9294             nan     0.0500    0.0071\n",
      "     9        0.9129             nan     0.0500    0.0073\n",
      "    10        0.8995             nan     0.0500    0.0065\n",
      "    20        0.7990             nan     0.0500    0.0022\n",
      "    40        0.7007             nan     0.0500    0.0002\n",
      "    60        0.6495             nan     0.0500    0.0004\n",
      "    80        0.6163             nan     0.0500    0.0003\n",
      "   100        0.5885             nan     0.0500   -0.0004\n",
      "   120        0.5641             nan     0.0500   -0.0004\n",
      "   140        0.5446             nan     0.0500   -0.0003\n",
      "   160        0.5269             nan     0.0500   -0.0009\n",
      "   180        0.5076             nan     0.0500   -0.0006\n",
      "   200        0.4919             nan     0.0500   -0.0000\n",
      "   220        0.4753             nan     0.0500   -0.0002\n",
      "   240        0.4605             nan     0.0500   -0.0004\n",
      "   250        0.4536             nan     0.0500    0.0001\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0770             nan     0.0500    0.0169\n",
      "     2        1.0471             nan     0.0500    0.0140\n",
      "     3        1.0201             nan     0.0500    0.0115\n",
      "     4        0.9958             nan     0.0500    0.0100\n",
      "     5        0.9702             nan     0.0500    0.0105\n",
      "     6        0.9491             nan     0.0500    0.0096\n",
      "     7        0.9303             nan     0.0500    0.0074\n",
      "     8        0.9121             nan     0.0500    0.0062\n",
      "     9        0.8949             nan     0.0500    0.0074\n",
      "    10        0.8791             nan     0.0500    0.0064\n",
      "    20        0.7672             nan     0.0500    0.0029\n",
      "    40        0.6525             nan     0.0500    0.0009\n",
      "    60        0.5887             nan     0.0500   -0.0001\n",
      "    80        0.5439             nan     0.0500   -0.0000\n",
      "   100        0.5068             nan     0.0500   -0.0003\n",
      "   120        0.4749             nan     0.0500   -0.0007\n",
      "   140        0.4441             nan     0.0500   -0.0001\n",
      "   160        0.4181             nan     0.0500   -0.0002\n",
      "   180        0.3920             nan     0.0500    0.0000\n",
      "   200        0.3688             nan     0.0500   -0.0003\n",
      "   220        0.3493             nan     0.0500   -0.0002\n",
      "   240        0.3297             nan     0.0500   -0.0002\n",
      "   250        0.3214             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0795             nan     0.1000    0.0174\n",
      "     2        1.0507             nan     0.1000    0.0139\n",
      "     3        1.0266             nan     0.1000    0.0113\n",
      "     4        1.0065             nan     0.1000    0.0101\n",
      "     5        0.9883             nan     0.1000    0.0087\n",
      "     6        0.9686             nan     0.1000    0.0088\n",
      "     7        0.9539             nan     0.1000    0.0067\n",
      "     8        0.9378             nan     0.1000    0.0069\n",
      "     9        0.9261             nan     0.1000    0.0058\n",
      "    10        0.9129             nan     0.1000    0.0051\n",
      "    20        0.8358             nan     0.1000    0.0029\n",
      "    40        0.7560             nan     0.1000    0.0011\n",
      "    60        0.7213             nan     0.1000    0.0002\n",
      "    80        0.6948             nan     0.1000    0.0001\n",
      "   100        0.6764             nan     0.1000   -0.0001\n",
      "   120        0.6660             nan     0.1000   -0.0004\n",
      "   140        0.6562             nan     0.1000   -0.0005\n",
      "   160        0.6484             nan     0.1000   -0.0004\n",
      "   180        0.6409             nan     0.1000   -0.0002\n",
      "   200        0.6341             nan     0.1000   -0.0005\n",
      "   220        0.6294             nan     0.1000   -0.0003\n",
      "   240        0.6241             nan     0.1000   -0.0004\n",
      "   250        0.6215             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0501             nan     0.1000    0.0289\n",
      "     2        0.9985             nan     0.1000    0.0242\n",
      "     3        0.9593             nan     0.1000    0.0178\n",
      "     4        0.9252             nan     0.1000    0.0140\n",
      "     5        0.8983             nan     0.1000    0.0111\n",
      "     6        0.8741             nan     0.1000    0.0089\n",
      "     7        0.8545             nan     0.1000    0.0082\n",
      "     8        0.8340             nan     0.1000    0.0081\n",
      "     9        0.8160             nan     0.1000    0.0078\n",
      "    10        0.8007             nan     0.1000    0.0060\n",
      "    20        0.7004             nan     0.1000    0.0013\n",
      "    40        0.6192             nan     0.1000   -0.0000\n",
      "    60        0.5681             nan     0.1000   -0.0008\n",
      "    80        0.5283             nan     0.1000   -0.0002\n",
      "   100        0.4961             nan     0.1000   -0.0008\n",
      "   120        0.4658             nan     0.1000   -0.0002\n",
      "   140        0.4373             nan     0.1000   -0.0002\n",
      "   160        0.4125             nan     0.1000   -0.0006\n",
      "   180        0.3915             nan     0.1000   -0.0007\n",
      "   200        0.3702             nan     0.1000   -0.0003\n",
      "   220        0.3528             nan     0.1000   -0.0005\n",
      "   240        0.3365             nan     0.1000   -0.0006\n",
      "   250        0.3282             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0472             nan     0.1000    0.0337\n",
      "     2        0.9919             nan     0.1000    0.0269\n",
      "     3        0.9457             nan     0.1000    0.0210\n",
      "     4        0.9052             nan     0.1000    0.0150\n",
      "     5        0.8730             nan     0.1000    0.0132\n",
      "     6        0.8476             nan     0.1000    0.0111\n",
      "     7        0.8245             nan     0.1000    0.0091\n",
      "     8        0.8023             nan     0.1000    0.0085\n",
      "     9        0.7844             nan     0.1000    0.0072\n",
      "    10        0.7662             nan     0.1000    0.0059\n",
      "    20        0.6549             nan     0.1000    0.0023\n",
      "    40        0.5482             nan     0.1000    0.0004\n",
      "    60        0.4796             nan     0.1000   -0.0006\n",
      "    80        0.4216             nan     0.1000   -0.0009\n",
      "   100        0.3724             nan     0.1000   -0.0007\n",
      "   120        0.3361             nan     0.1000   -0.0007\n",
      "   140        0.3012             nan     0.1000   -0.0004\n",
      "   160        0.2723             nan     0.1000   -0.0003\n",
      "   180        0.2470             nan     0.1000   -0.0007\n",
      "   200        0.2229             nan     0.1000   -0.0010\n",
      "   220        0.2027             nan     0.1000   -0.0003\n",
      "   240        0.1857             nan     0.1000   -0.0005\n",
      "   250        0.1773             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold03.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0018\n",
      "     2        1.1070             nan     0.0100    0.0018\n",
      "     3        1.1037             nan     0.0100    0.0017\n",
      "     4        1.1004             nan     0.0100    0.0017\n",
      "     5        1.0973             nan     0.0100    0.0016\n",
      "     6        1.0939             nan     0.0100    0.0016\n",
      "     7        1.0908             nan     0.0100    0.0016\n",
      "     8        1.0878             nan     0.0100    0.0016\n",
      "     9        1.0847             nan     0.0100    0.0015\n",
      "    10        1.0815             nan     0.0100    0.0015\n",
      "    20        1.0545             nan     0.0100    0.0011\n",
      "    40        1.0119             nan     0.0100    0.0006\n",
      "    60        0.9778             nan     0.0100    0.0007\n",
      "    80        0.9501             nan     0.0100    0.0006\n",
      "   100        0.9262             nan     0.0100    0.0005\n",
      "   120        0.9050             nan     0.0100    0.0005\n",
      "   140        0.8874             nan     0.0100    0.0005\n",
      "   160        0.8716             nan     0.0100    0.0004\n",
      "   180        0.8578             nan     0.0100    0.0003\n",
      "   200        0.8456             nan     0.0100    0.0002\n",
      "   220        0.8336             nan     0.0100    0.0003\n",
      "   240        0.8232             nan     0.0100    0.0003\n",
      "   250        0.8185             nan     0.0100    0.0002\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1081             nan     0.0100    0.0030\n",
      "     2        1.1018             nan     0.0100    0.0029\n",
      "     3        1.0957             nan     0.0100    0.0031\n",
      "     4        1.0899             nan     0.0100    0.0028\n",
      "     5        1.0838             nan     0.0100    0.0029\n",
      "     6        1.0778             nan     0.0100    0.0028\n",
      "     7        1.0724             nan     0.0100    0.0027\n",
      "     8        1.0666             nan     0.0100    0.0026\n",
      "     9        1.0615             nan     0.0100    0.0025\n",
      "    10        1.0561             nan     0.0100    0.0027\n",
      "    20        1.0101             nan     0.0100    0.0019\n",
      "    40        0.9367             nan     0.0100    0.0011\n",
      "    60        0.8819             nan     0.0100    0.0011\n",
      "    80        0.8406             nan     0.0100    0.0007\n",
      "   100        0.8079             nan     0.0100    0.0006\n",
      "   120        0.7817             nan     0.0100    0.0005\n",
      "   140        0.7600             nan     0.0100    0.0003\n",
      "   160        0.7414             nan     0.0100    0.0003\n",
      "   180        0.7254             nan     0.0100    0.0002\n",
      "   200        0.7112             nan     0.0100    0.0003\n",
      "   220        0.6991             nan     0.0100    0.0002\n",
      "   240        0.6883             nan     0.0100    0.0001\n",
      "   250        0.6831             nan     0.0100    0.0001\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1068             nan     0.0100    0.0033\n",
      "     2        1.0999             nan     0.0100    0.0031\n",
      "     3        1.0926             nan     0.0100    0.0031\n",
      "     4        1.0860             nan     0.0100    0.0029\n",
      "     5        1.0800             nan     0.0100    0.0028\n",
      "     6        1.0735             nan     0.0100    0.0030\n",
      "     7        1.0672             nan     0.0100    0.0029\n",
      "     8        1.0613             nan     0.0100    0.0026\n",
      "     9        1.0551             nan     0.0100    0.0026\n",
      "    10        1.0497             nan     0.0100    0.0025\n",
      "    20        0.9981             nan     0.0100    0.0024\n",
      "    40        0.9152             nan     0.0100    0.0014\n",
      "    60        0.8554             nan     0.0100    0.0012\n",
      "    80        0.8076             nan     0.0100    0.0008\n",
      "   100        0.7695             nan     0.0100    0.0005\n",
      "   120        0.7399             nan     0.0100    0.0005\n",
      "   140        0.7147             nan     0.0100    0.0003\n",
      "   160        0.6930             nan     0.0100    0.0003\n",
      "   180        0.6732             nan     0.0100    0.0001\n",
      "   200        0.6559             nan     0.0100    0.0002\n",
      "   220        0.6407             nan     0.0100    0.0001\n",
      "   240        0.6270             nan     0.0100    0.0000\n",
      "   250        0.6207             nan     0.0100    0.0000\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0959             nan     0.0500    0.0088\n",
      "     2        1.0802             nan     0.0500    0.0080\n",
      "     3        1.0658             nan     0.0500    0.0072\n",
      "     4        1.0549             nan     0.0500    0.0053\n",
      "     5        1.0425             nan     0.0500    0.0064\n",
      "     6        1.0315             nan     0.0500    0.0058\n",
      "     7        1.0213             nan     0.0500    0.0042\n",
      "     8        1.0105             nan     0.0500    0.0053\n",
      "     9        1.0004             nan     0.0500    0.0048\n",
      "    10        0.9920             nan     0.0500    0.0041\n",
      "    20        0.9242             nan     0.0500    0.0022\n",
      "    40        0.8434             nan     0.0500    0.0014\n",
      "    60        0.7945             nan     0.0500    0.0005\n",
      "    80        0.7630             nan     0.0500    0.0003\n",
      "   100        0.7408             nan     0.0500    0.0002\n",
      "   120        0.7255             nan     0.0500   -0.0000\n",
      "   140        0.7128             nan     0.0500    0.0001\n",
      "   160        0.7037             nan     0.0500    0.0002\n",
      "   180        0.6951             nan     0.0500   -0.0001\n",
      "   200        0.6880             nan     0.0500   -0.0001\n",
      "   220        0.6820             nan     0.0500   -0.0001\n",
      "   240        0.6757             nan     0.0500   -0.0000\n",
      "   250        0.6722             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0798             nan     0.0500    0.0139\n",
      "     2        1.0528             nan     0.0500    0.0131\n",
      "     3        1.0282             nan     0.0500    0.0126\n",
      "     4        1.0058             nan     0.0500    0.0096\n",
      "     5        0.9834             nan     0.0500    0.0091\n",
      "     6        0.9635             nan     0.0500    0.0082\n",
      "     7        0.9456             nan     0.0500    0.0079\n",
      "     8        0.9306             nan     0.0500    0.0068\n",
      "     9        0.9167             nan     0.0500    0.0065\n",
      "    10        0.9023             nan     0.0500    0.0063\n",
      "    20        0.8055             nan     0.0500    0.0025\n",
      "    40        0.7112             nan     0.0500    0.0009\n",
      "    60        0.6640             nan     0.0500   -0.0000\n",
      "    80        0.6308             nan     0.0500   -0.0001\n",
      "   100        0.6033             nan     0.0500   -0.0003\n",
      "   120        0.5813             nan     0.0500   -0.0001\n",
      "   140        0.5588             nan     0.0500   -0.0002\n",
      "   160        0.5388             nan     0.0500   -0.0003\n",
      "   180        0.5204             nan     0.0500   -0.0003\n",
      "   200        0.5056             nan     0.0500   -0.0002\n",
      "   220        0.4906             nan     0.0500   -0.0002\n",
      "   240        0.4746             nan     0.0500   -0.0001\n",
      "   250        0.4669             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0784             nan     0.0500    0.0157\n",
      "     2        1.0479             nan     0.0500    0.0139\n",
      "     3        1.0187             nan     0.0500    0.0129\n",
      "     4        0.9931             nan     0.0500    0.0121\n",
      "     5        0.9688             nan     0.0500    0.0099\n",
      "     6        0.9472             nan     0.0500    0.0089\n",
      "     7        0.9268             nan     0.0500    0.0092\n",
      "     8        0.9091             nan     0.0500    0.0081\n",
      "     9        0.8927             nan     0.0500    0.0064\n",
      "    10        0.8788             nan     0.0500    0.0055\n",
      "    20        0.7699             nan     0.0500    0.0029\n",
      "    40        0.6586             nan     0.0500    0.0007\n",
      "    60        0.5971             nan     0.0500    0.0000\n",
      "    80        0.5490             nan     0.0500    0.0002\n",
      "   100        0.5121             nan     0.0500   -0.0002\n",
      "   120        0.4800             nan     0.0500   -0.0001\n",
      "   140        0.4503             nan     0.0500   -0.0003\n",
      "   160        0.4245             nan     0.0500   -0.0004\n",
      "   180        0.3993             nan     0.0500   -0.0003\n",
      "   200        0.3770             nan     0.0500   -0.0003\n",
      "   220        0.3553             nan     0.0500   -0.0003\n",
      "   240        0.3376             nan     0.0500   -0.0004\n",
      "   250        0.3277             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0803             nan     0.1000    0.0172\n",
      "     2        1.0575             nan     0.1000    0.0090\n",
      "     3        1.0284             nan     0.1000    0.0136\n",
      "     4        1.0138             nan     0.1000    0.0051\n",
      "     5        0.9920             nan     0.1000    0.0109\n",
      "     6        0.9740             nan     0.1000    0.0090\n",
      "     7        0.9566             nan     0.1000    0.0074\n",
      "     8        0.9441             nan     0.1000    0.0058\n",
      "     9        0.9294             nan     0.1000    0.0072\n",
      "    10        0.9208             nan     0.1000    0.0042\n",
      "    20        0.8402             nan     0.1000    0.0016\n",
      "    40        0.7651             nan     0.1000    0.0010\n",
      "    60        0.7267             nan     0.1000    0.0004\n",
      "    80        0.7052             nan     0.1000   -0.0000\n",
      "   100        0.6891             nan     0.1000    0.0000\n",
      "   120        0.6743             nan     0.1000    0.0001\n",
      "   140        0.6629             nan     0.1000   -0.0007\n",
      "   160        0.6547             nan     0.1000    0.0001\n",
      "   180        0.6477             nan     0.1000   -0.0004\n",
      "   200        0.6405             nan     0.1000   -0.0003\n",
      "   220        0.6351             nan     0.1000   -0.0001\n",
      "   240        0.6303             nan     0.1000   -0.0007\n",
      "   250        0.6283             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0508             nan     0.1000    0.0285\n",
      "     2        1.0030             nan     0.1000    0.0207\n",
      "     3        0.9656             nan     0.1000    0.0159\n",
      "     4        0.9298             nan     0.1000    0.0163\n",
      "     5        0.9026             nan     0.1000    0.0120\n",
      "     6        0.8807             nan     0.1000    0.0092\n",
      "     7        0.8575             nan     0.1000    0.0114\n",
      "     8        0.8386             nan     0.1000    0.0085\n",
      "     9        0.8217             nan     0.1000    0.0062\n",
      "    10        0.8084             nan     0.1000    0.0055\n",
      "    20        0.7101             nan     0.1000    0.0011\n",
      "    40        0.6271             nan     0.1000   -0.0002\n",
      "    60        0.5759             nan     0.1000   -0.0004\n",
      "    80        0.5372             nan     0.1000   -0.0008\n",
      "   100        0.5026             nan     0.1000   -0.0009\n",
      "   120        0.4706             nan     0.1000   -0.0013\n",
      "   140        0.4457             nan     0.1000   -0.0004\n",
      "   160        0.4195             nan     0.1000   -0.0007\n",
      "   180        0.3965             nan     0.1000   -0.0010\n",
      "   200        0.3744             nan     0.1000   -0.0007\n",
      "   220        0.3506             nan     0.1000   -0.0009\n",
      "   240        0.3326             nan     0.1000   -0.0006\n",
      "   250        0.3248             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0425             nan     0.1000    0.0333\n",
      "     2        0.9896             nan     0.1000    0.0230\n",
      "     3        0.9416             nan     0.1000    0.0203\n",
      "     4        0.9091             nan     0.1000    0.0129\n",
      "     5        0.8801             nan     0.1000    0.0123\n",
      "     6        0.8517             nan     0.1000    0.0107\n",
      "     7        0.8275             nan     0.1000    0.0109\n",
      "     8        0.8048             nan     0.1000    0.0081\n",
      "     9        0.7853             nan     0.1000    0.0065\n",
      "    10        0.7695             nan     0.1000    0.0047\n",
      "    20        0.6634             nan     0.1000    0.0007\n",
      "    40        0.5506             nan     0.1000   -0.0004\n",
      "    60        0.4787             nan     0.1000    0.0006\n",
      "    80        0.4253             nan     0.1000   -0.0014\n",
      "   100        0.3807             nan     0.1000   -0.0003\n",
      "   120        0.3433             nan     0.1000   -0.0010\n",
      "   140        0.3080             nan     0.1000   -0.0003\n",
      "   160        0.2790             nan     0.1000   -0.0007\n",
      "   180        0.2520             nan     0.1000   -0.0002\n",
      "   200        0.2297             nan     0.1000   -0.0006\n",
      "   220        0.2090             nan     0.1000   -0.0006\n",
      "   240        0.1897             nan     0.1000   -0.0003\n",
      "   250        0.1810             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold04.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1107             nan     0.0100    0.0018\n",
      "     2        1.1071             nan     0.0100    0.0017\n",
      "     3        1.1037             nan     0.0100    0.0017\n",
      "     4        1.1006             nan     0.0100    0.0017\n",
      "     5        1.0974             nan     0.0100    0.0016\n",
      "     6        1.0940             nan     0.0100    0.0016\n",
      "     7        1.0907             nan     0.0100    0.0016\n",
      "     8        1.0880             nan     0.0100    0.0015\n",
      "     9        1.0849             nan     0.0100    0.0015\n",
      "    10        1.0818             nan     0.0100    0.0015\n",
      "    20        1.0544             nan     0.0100    0.0012\n",
      "    40        1.0138             nan     0.0100    0.0010\n",
      "    60        0.9802             nan     0.0100    0.0007\n",
      "    80        0.9518             nan     0.0100    0.0005\n",
      "   100        0.9280             nan     0.0100    0.0004\n",
      "   120        0.9076             nan     0.0100    0.0005\n",
      "   140        0.8898             nan     0.0100    0.0003\n",
      "   160        0.8740             nan     0.0100    0.0004\n",
      "   180        0.8598             nan     0.0100    0.0003\n",
      "   200        0.8477             nan     0.0100    0.0002\n",
      "   220        0.8359             nan     0.0100    0.0002\n",
      "   240        0.8252             nan     0.0100    0.0001\n",
      "   250        0.8205             nan     0.0100    0.0002\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1081             nan     0.0100    0.0032\n",
      "     2        1.1019             nan     0.0100    0.0031\n",
      "     3        1.0955             nan     0.0100    0.0027\n",
      "     4        1.0893             nan     0.0100    0.0028\n",
      "     5        1.0833             nan     0.0100    0.0027\n",
      "     6        1.0776             nan     0.0100    0.0025\n",
      "     7        1.0721             nan     0.0100    0.0024\n",
      "     8        1.0665             nan     0.0100    0.0025\n",
      "     9        1.0613             nan     0.0100    0.0024\n",
      "    10        1.0560             nan     0.0100    0.0024\n",
      "    20        1.0078             nan     0.0100    0.0019\n",
      "    40        0.9343             nan     0.0100    0.0015\n",
      "    60        0.8813             nan     0.0100    0.0010\n",
      "    80        0.8394             nan     0.0100    0.0009\n",
      "   100        0.8069             nan     0.0100    0.0004\n",
      "   120        0.7807             nan     0.0100    0.0004\n",
      "   140        0.7584             nan     0.0100    0.0003\n",
      "   160        0.7392             nan     0.0100    0.0002\n",
      "   180        0.7229             nan     0.0100    0.0001\n",
      "   200        0.7087             nan     0.0100    0.0002\n",
      "   220        0.6964             nan     0.0100    0.0001\n",
      "   240        0.6849             nan     0.0100    0.0001\n",
      "   250        0.6796             nan     0.0100    0.0001\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1072             nan     0.0100    0.0032\n",
      "     2        1.1004             nan     0.0100    0.0030\n",
      "     3        1.0935             nan     0.0100    0.0032\n",
      "     4        1.0866             nan     0.0100    0.0031\n",
      "     5        1.0799             nan     0.0100    0.0028\n",
      "     6        1.0732             nan     0.0100    0.0032\n",
      "     7        1.0667             nan     0.0100    0.0031\n",
      "     8        1.0608             nan     0.0100    0.0026\n",
      "     9        1.0548             nan     0.0100    0.0028\n",
      "    10        1.0488             nan     0.0100    0.0027\n",
      "    20        0.9968             nan     0.0100    0.0018\n",
      "    40        0.9153             nan     0.0100    0.0015\n",
      "    60        0.8551             nan     0.0100    0.0011\n",
      "    80        0.8088             nan     0.0100    0.0008\n",
      "   100        0.7722             nan     0.0100    0.0006\n",
      "   120        0.7406             nan     0.0100    0.0005\n",
      "   140        0.7143             nan     0.0100    0.0003\n",
      "   160        0.6922             nan     0.0100    0.0002\n",
      "   180        0.6737             nan     0.0100    0.0001\n",
      "   200        0.6565             nan     0.0100    0.0001\n",
      "   220        0.6413             nan     0.0100    0.0001\n",
      "   240        0.6280             nan     0.0100    0.0001\n",
      "   250        0.6214             nan     0.0100    0.0001\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0973             nan     0.0500    0.0087\n",
      "     2        1.0814             nan     0.0500    0.0079\n",
      "     3        1.0669             nan     0.0500    0.0071\n",
      "     4        1.0527             nan     0.0500    0.0063\n",
      "     5        1.0420             nan     0.0500    0.0046\n",
      "     6        1.0289             nan     0.0500    0.0055\n",
      "     7        1.0183             nan     0.0500    0.0050\n",
      "     8        1.0095             nan     0.0500    0.0044\n",
      "     9        1.0001             nan     0.0500    0.0045\n",
      "    10        0.9913             nan     0.0500    0.0039\n",
      "    20        0.9251             nan     0.0500    0.0024\n",
      "    40        0.8454             nan     0.0500    0.0015\n",
      "    60        0.7959             nan     0.0500    0.0006\n",
      "    80        0.7655             nan     0.0500    0.0003\n",
      "   100        0.7430             nan     0.0500    0.0004\n",
      "   120        0.7267             nan     0.0500    0.0002\n",
      "   140        0.7141             nan     0.0500    0.0002\n",
      "   160        0.7032             nan     0.0500    0.0001\n",
      "   180        0.6936             nan     0.0500   -0.0001\n",
      "   200        0.6851             nan     0.0500   -0.0001\n",
      "   220        0.6781             nan     0.0500   -0.0001\n",
      "   240        0.6717             nan     0.0500    0.0000\n",
      "   250        0.6685             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0793             nan     0.0500    0.0151\n",
      "     2        1.0520             nan     0.0500    0.0118\n",
      "     3        1.0287             nan     0.0500    0.0114\n",
      "     4        1.0044             nan     0.0500    0.0117\n",
      "     5        0.9839             nan     0.0500    0.0101\n",
      "     6        0.9645             nan     0.0500    0.0081\n",
      "     7        0.9482             nan     0.0500    0.0074\n",
      "     8        0.9318             nan     0.0500    0.0075\n",
      "     9        0.9175             nan     0.0500    0.0063\n",
      "    10        0.9019             nan     0.0500    0.0060\n",
      "    20        0.8048             nan     0.0500    0.0028\n",
      "    40        0.7065             nan     0.0500    0.0011\n",
      "    60        0.6571             nan     0.0500    0.0007\n",
      "    80        0.6205             nan     0.0500   -0.0001\n",
      "   100        0.5921             nan     0.0500   -0.0006\n",
      "   120        0.5645             nan     0.0500   -0.0001\n",
      "   140        0.5420             nan     0.0500   -0.0004\n",
      "   160        0.5248             nan     0.0500   -0.0000\n",
      "   180        0.5063             nan     0.0500   -0.0005\n",
      "   200        0.4899             nan     0.0500   -0.0008\n",
      "   220        0.4754             nan     0.0500    0.0001\n",
      "   240        0.4607             nan     0.0500    0.0001\n",
      "   250        0.4534             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0776             nan     0.0500    0.0162\n",
      "     2        1.0460             nan     0.0500    0.0145\n",
      "     3        1.0189             nan     0.0500    0.0120\n",
      "     4        0.9946             nan     0.0500    0.0104\n",
      "     5        0.9716             nan     0.0500    0.0105\n",
      "     6        0.9519             nan     0.0500    0.0083\n",
      "     7        0.9318             nan     0.0500    0.0087\n",
      "     8        0.9148             nan     0.0500    0.0066\n",
      "     9        0.8965             nan     0.0500    0.0079\n",
      "    10        0.8815             nan     0.0500    0.0057\n",
      "    20        0.7700             nan     0.0500    0.0037\n",
      "    40        0.6539             nan     0.0500    0.0008\n",
      "    60        0.5906             nan     0.0500    0.0001\n",
      "    80        0.5449             nan     0.0500   -0.0007\n",
      "   100        0.5062             nan     0.0500   -0.0004\n",
      "   120        0.4741             nan     0.0500   -0.0004\n",
      "   140        0.4436             nan     0.0500   -0.0005\n",
      "   160        0.4161             nan     0.0500   -0.0003\n",
      "   180        0.3918             nan     0.0500   -0.0004\n",
      "   200        0.3703             nan     0.0500   -0.0004\n",
      "   220        0.3489             nan     0.0500   -0.0001\n",
      "   240        0.3308             nan     0.0500   -0.0003\n",
      "   250        0.3210             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0763             nan     0.1000    0.0164\n",
      "     2        1.0487             nan     0.1000    0.0132\n",
      "     3        1.0267             nan     0.1000    0.0108\n",
      "     4        1.0095             nan     0.1000    0.0089\n",
      "     5        0.9910             nan     0.1000    0.0083\n",
      "     6        0.9761             nan     0.1000    0.0073\n",
      "     7        0.9621             nan     0.1000    0.0073\n",
      "     8        0.9481             nan     0.1000    0.0068\n",
      "     9        0.9348             nan     0.1000    0.0061\n",
      "    10        0.9254             nan     0.1000    0.0044\n",
      "    20        0.8441             nan     0.1000    0.0021\n",
      "    40        0.7623             nan     0.1000    0.0009\n",
      "    60        0.7254             nan     0.1000    0.0004\n",
      "    80        0.7023             nan     0.1000   -0.0000\n",
      "   100        0.6852             nan     0.1000   -0.0002\n",
      "   120        0.6715             nan     0.1000   -0.0001\n",
      "   140        0.6600             nan     0.1000   -0.0001\n",
      "   160        0.6512             nan     0.1000   -0.0005\n",
      "   180        0.6428             nan     0.1000   -0.0001\n",
      "   200        0.6364             nan     0.1000   -0.0002\n",
      "   220        0.6315             nan     0.1000   -0.0006\n",
      "   240        0.6256             nan     0.1000   -0.0001\n",
      "   250        0.6229             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0494             nan     0.1000    0.0317\n",
      "     2        1.0027             nan     0.1000    0.0220\n",
      "     3        0.9608             nan     0.1000    0.0205\n",
      "     4        0.9281             nan     0.1000    0.0160\n",
      "     5        0.8983             nan     0.1000    0.0140\n",
      "     6        0.8734             nan     0.1000    0.0111\n",
      "     7        0.8509             nan     0.1000    0.0087\n",
      "     8        0.8320             nan     0.1000    0.0075\n",
      "     9        0.8188             nan     0.1000    0.0042\n",
      "    10        0.8056             nan     0.1000    0.0054\n",
      "    20        0.7098             nan     0.1000    0.0009\n",
      "    40        0.6246             nan     0.1000   -0.0005\n",
      "    60        0.5704             nan     0.1000   -0.0007\n",
      "    80        0.5261             nan     0.1000   -0.0010\n",
      "   100        0.4936             nan     0.1000   -0.0009\n",
      "   120        0.4614             nan     0.1000   -0.0004\n",
      "   140        0.4341             nan     0.1000   -0.0008\n",
      "   160        0.4062             nan     0.1000   -0.0003\n",
      "   180        0.3853             nan     0.1000   -0.0002\n",
      "   200        0.3628             nan     0.1000   -0.0005\n",
      "   220        0.3440             nan     0.1000   -0.0003\n",
      "   240        0.3263             nan     0.1000   -0.0008\n",
      "   250        0.3182             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0449             nan     0.1000    0.0312\n",
      "     2        0.9924             nan     0.1000    0.0226\n",
      "     3        0.9496             nan     0.1000    0.0180\n",
      "     4        0.9137             nan     0.1000    0.0153\n",
      "     5        0.8816             nan     0.1000    0.0134\n",
      "     6        0.8540             nan     0.1000    0.0096\n",
      "     7        0.8292             nan     0.1000    0.0090\n",
      "     8        0.8065             nan     0.1000    0.0086\n",
      "     9        0.7892             nan     0.1000    0.0051\n",
      "    10        0.7701             nan     0.1000    0.0079\n",
      "    20        0.6577             nan     0.1000   -0.0001\n",
      "    40        0.5459             nan     0.1000    0.0003\n",
      "    60        0.4715             nan     0.1000   -0.0006\n",
      "    80        0.4117             nan     0.1000   -0.0010\n",
      "   100        0.3652             nan     0.1000   -0.0011\n",
      "   120        0.3234             nan     0.1000   -0.0003\n",
      "   140        0.2908             nan     0.1000   -0.0007\n",
      "   160        0.2635             nan     0.1000   -0.0005\n",
      "   180        0.2398             nan     0.1000   -0.0004\n",
      "   200        0.2178             nan     0.1000   -0.0006\n",
      "   220        0.1978             nan     0.1000   -0.0001\n",
      "   240        0.1806             nan     0.1000   -0.0007\n",
      "   250        0.1715             nan     0.1000   -0.0007\n",
      "\n",
      "- Fold05.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1107             nan     0.0100    0.0018\n",
      "     2        1.1070             nan     0.0100    0.0018\n",
      "     3        1.1032             nan     0.0100    0.0018\n",
      "     4        1.1000             nan     0.0100    0.0017\n",
      "     5        1.0965             nan     0.0100    0.0017\n",
      "     6        1.0932             nan     0.0100    0.0017\n",
      "     7        1.0899             nan     0.0100    0.0016\n",
      "     8        1.0867             nan     0.0100    0.0016\n",
      "     9        1.0833             nan     0.0100    0.0016\n",
      "    10        1.0799             nan     0.0100    0.0015\n",
      "    20        1.0524             nan     0.0100    0.0008\n",
      "    40        1.0083             nan     0.0100    0.0010\n",
      "    60        0.9733             nan     0.0100    0.0009\n",
      "    80        0.9454             nan     0.0100    0.0007\n",
      "   100        0.9215             nan     0.0100    0.0006\n",
      "   120        0.9002             nan     0.0100    0.0005\n",
      "   140        0.8826             nan     0.0100    0.0004\n",
      "   160        0.8668             nan     0.0100    0.0002\n",
      "   180        0.8532             nan     0.0100    0.0004\n",
      "   200        0.8405             nan     0.0100    0.0002\n",
      "   220        0.8282             nan     0.0100    0.0002\n",
      "   240        0.8180             nan     0.0100    0.0002\n",
      "   250        0.8136             nan     0.0100    0.0002\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1076             nan     0.0100    0.0030\n",
      "     2        1.1010             nan     0.0100    0.0031\n",
      "     3        1.0949             nan     0.0100    0.0029\n",
      "     4        1.0889             nan     0.0100    0.0031\n",
      "     5        1.0829             nan     0.0100    0.0032\n",
      "     6        1.0772             nan     0.0100    0.0027\n",
      "     7        1.0717             nan     0.0100    0.0026\n",
      "     8        1.0663             nan     0.0100    0.0025\n",
      "     9        1.0607             nan     0.0100    0.0025\n",
      "    10        1.0553             nan     0.0100    0.0024\n",
      "    20        1.0070             nan     0.0100    0.0020\n",
      "    40        0.9322             nan     0.0100    0.0014\n",
      "    60        0.8777             nan     0.0100    0.0010\n",
      "    80        0.8356             nan     0.0100    0.0009\n",
      "   100        0.8032             nan     0.0100    0.0006\n",
      "   120        0.7770             nan     0.0100    0.0004\n",
      "   140        0.7540             nan     0.0100    0.0004\n",
      "   160        0.7350             nan     0.0100    0.0002\n",
      "   180        0.7197             nan     0.0100    0.0002\n",
      "   200        0.7054             nan     0.0100    0.0001\n",
      "   220        0.6930             nan     0.0100    0.0000\n",
      "   240        0.6817             nan     0.0100    0.0002\n",
      "   250        0.6765             nan     0.0100    0.0001\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1070             nan     0.0100    0.0035\n",
      "     2        1.1001             nan     0.0100    0.0029\n",
      "     3        1.0929             nan     0.0100    0.0034\n",
      "     4        1.0859             nan     0.0100    0.0028\n",
      "     5        1.0797             nan     0.0100    0.0027\n",
      "     6        1.0735             nan     0.0100    0.0028\n",
      "     7        1.0668             nan     0.0100    0.0032\n",
      "     8        1.0608             nan     0.0100    0.0026\n",
      "     9        1.0543             nan     0.0100    0.0024\n",
      "    10        1.0483             nan     0.0100    0.0030\n",
      "    20        0.9956             nan     0.0100    0.0021\n",
      "    40        0.9140             nan     0.0100    0.0014\n",
      "    60        0.8510             nan     0.0100    0.0009\n",
      "    80        0.8024             nan     0.0100    0.0007\n",
      "   100        0.7646             nan     0.0100    0.0007\n",
      "   120        0.7346             nan     0.0100    0.0005\n",
      "   140        0.7080             nan     0.0100    0.0004\n",
      "   160        0.6858             nan     0.0100    0.0002\n",
      "   180        0.6667             nan     0.0100    0.0001\n",
      "   200        0.6495             nan     0.0100    0.0001\n",
      "   220        0.6339             nan     0.0100    0.0001\n",
      "   240        0.6209             nan     0.0100   -0.0000\n",
      "   250        0.6145             nan     0.0100    0.0000\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0958             nan     0.0500    0.0090\n",
      "     2        1.0797             nan     0.0500    0.0081\n",
      "     3        1.0649             nan     0.0500    0.0074\n",
      "     4        1.0510             nan     0.0500    0.0066\n",
      "     5        1.0384             nan     0.0500    0.0060\n",
      "     6        1.0278             nan     0.0500    0.0048\n",
      "     7        1.0163             nan     0.0500    0.0053\n",
      "     8        1.0059             nan     0.0500    0.0044\n",
      "     9        0.9979             nan     0.0500    0.0041\n",
      "    10        0.9882             nan     0.0500    0.0047\n",
      "    20        0.9208             nan     0.0500    0.0031\n",
      "    40        0.8415             nan     0.0500    0.0012\n",
      "    60        0.7936             nan     0.0500    0.0001\n",
      "    80        0.7611             nan     0.0500    0.0009\n",
      "   100        0.7377             nan     0.0500    0.0001\n",
      "   120        0.7199             nan     0.0500    0.0004\n",
      "   140        0.7068             nan     0.0500    0.0001\n",
      "   160        0.6946             nan     0.0500   -0.0001\n",
      "   180        0.6851             nan     0.0500   -0.0000\n",
      "   200        0.6779             nan     0.0500   -0.0001\n",
      "   220        0.6708             nan     0.0500   -0.0000\n",
      "   240        0.6639             nan     0.0500   -0.0000\n",
      "   250        0.6610             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0809             nan     0.0500    0.0162\n",
      "     2        1.0539             nan     0.0500    0.0126\n",
      "     3        1.0277             nan     0.0500    0.0116\n",
      "     4        1.0040             nan     0.0500    0.0101\n",
      "     5        0.9827             nan     0.0500    0.0104\n",
      "     6        0.9645             nan     0.0500    0.0082\n",
      "     7        0.9478             nan     0.0500    0.0078\n",
      "     8        0.9318             nan     0.0500    0.0076\n",
      "     9        0.9164             nan     0.0500    0.0068\n",
      "    10        0.9011             nan     0.0500    0.0074\n",
      "    20        0.8023             nan     0.0500    0.0015\n",
      "    40        0.7061             nan     0.0500    0.0010\n",
      "    60        0.6535             nan     0.0500    0.0001\n",
      "    80        0.6203             nan     0.0500   -0.0002\n",
      "   100        0.5921             nan     0.0500    0.0000\n",
      "   120        0.5681             nan     0.0500   -0.0005\n",
      "   140        0.5465             nan     0.0500   -0.0001\n",
      "   160        0.5263             nan     0.0500   -0.0005\n",
      "   180        0.5072             nan     0.0500   -0.0005\n",
      "   200        0.4897             nan     0.0500   -0.0002\n",
      "   220        0.4739             nan     0.0500   -0.0001\n",
      "   240        0.4581             nan     0.0500   -0.0004\n",
      "   250        0.4502             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0783             nan     0.0500    0.0167\n",
      "     2        1.0456             nan     0.0500    0.0139\n",
      "     3        1.0155             nan     0.0500    0.0137\n",
      "     4        0.9914             nan     0.0500    0.0101\n",
      "     5        0.9680             nan     0.0500    0.0093\n",
      "     6        0.9469             nan     0.0500    0.0084\n",
      "     7        0.9295             nan     0.0500    0.0071\n",
      "     8        0.9102             nan     0.0500    0.0092\n",
      "     9        0.8934             nan     0.0500    0.0069\n",
      "    10        0.8770             nan     0.0500    0.0066\n",
      "    20        0.7627             nan     0.0500    0.0031\n",
      "    40        0.6481             nan     0.0500    0.0002\n",
      "    60        0.5836             nan     0.0500   -0.0002\n",
      "    80        0.5391             nan     0.0500    0.0003\n",
      "   100        0.5022             nan     0.0500   -0.0005\n",
      "   120        0.4689             nan     0.0500   -0.0002\n",
      "   140        0.4381             nan     0.0500   -0.0001\n",
      "   160        0.4129             nan     0.0500   -0.0003\n",
      "   180        0.3885             nan     0.0500   -0.0001\n",
      "   200        0.3667             nan     0.0500   -0.0001\n",
      "   220        0.3452             nan     0.0500   -0.0005\n",
      "   240        0.3254             nan     0.0500   -0.0003\n",
      "   250        0.3154             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0796             nan     0.1000    0.0177\n",
      "     2        1.0520             nan     0.1000    0.0143\n",
      "     3        1.0316             nan     0.1000    0.0085\n",
      "     4        1.0114             nan     0.1000    0.0079\n",
      "     5        0.9881             nan     0.1000    0.0112\n",
      "     6        0.9698             nan     0.1000    0.0093\n",
      "     7        0.9562             nan     0.1000    0.0065\n",
      "     8        0.9422             nan     0.1000    0.0057\n",
      "     9        0.9259             nan     0.1000    0.0065\n",
      "    10        0.9138             nan     0.1000    0.0059\n",
      "    20        0.8374             nan     0.1000    0.0039\n",
      "    40        0.7570             nan     0.1000    0.0008\n",
      "    60        0.7214             nan     0.1000   -0.0003\n",
      "    80        0.6976             nan     0.1000   -0.0005\n",
      "   100        0.6778             nan     0.1000    0.0000\n",
      "   120        0.6645             nan     0.1000   -0.0002\n",
      "   140        0.6538             nan     0.1000    0.0000\n",
      "   160        0.6426             nan     0.1000   -0.0002\n",
      "   180        0.6340             nan     0.1000   -0.0001\n",
      "   200        0.6291             nan     0.1000   -0.0004\n",
      "   220        0.6233             nan     0.1000   -0.0004\n",
      "   240        0.6181             nan     0.1000   -0.0005\n",
      "   250        0.6154             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0560             nan     0.1000    0.0253\n",
      "     2        1.0099             nan     0.1000    0.0209\n",
      "     3        0.9733             nan     0.1000    0.0171\n",
      "     4        0.9356             nan     0.1000    0.0159\n",
      "     5        0.9020             nan     0.1000    0.0162\n",
      "     6        0.8754             nan     0.1000    0.0128\n",
      "     7        0.8538             nan     0.1000    0.0100\n",
      "     8        0.8333             nan     0.1000    0.0082\n",
      "     9        0.8146             nan     0.1000    0.0075\n",
      "    10        0.7983             nan     0.1000    0.0063\n",
      "    20        0.7012             nan     0.1000    0.0023\n",
      "    40        0.6159             nan     0.1000   -0.0010\n",
      "    60        0.5678             nan     0.1000   -0.0002\n",
      "    80        0.5300             nan     0.1000    0.0001\n",
      "   100        0.4961             nan     0.1000   -0.0008\n",
      "   120        0.4643             nan     0.1000   -0.0005\n",
      "   140        0.4353             nan     0.1000   -0.0012\n",
      "   160        0.4148             nan     0.1000   -0.0015\n",
      "   180        0.3937             nan     0.1000   -0.0004\n",
      "   200        0.3736             nan     0.1000   -0.0005\n",
      "   220        0.3538             nan     0.1000   -0.0007\n",
      "   240        0.3331             nan     0.1000   -0.0003\n",
      "   250        0.3259             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0450             nan     0.1000    0.0311\n",
      "     2        0.9913             nan     0.1000    0.0257\n",
      "     3        0.9447             nan     0.1000    0.0204\n",
      "     4        0.9072             nan     0.1000    0.0159\n",
      "     5        0.8761             nan     0.1000    0.0130\n",
      "     6        0.8486             nan     0.1000    0.0110\n",
      "     7        0.8261             nan     0.1000    0.0080\n",
      "     8        0.8031             nan     0.1000    0.0089\n",
      "     9        0.7862             nan     0.1000    0.0062\n",
      "    10        0.7704             nan     0.1000    0.0051\n",
      "    20        0.6555             nan     0.1000    0.0010\n",
      "    40        0.5494             nan     0.1000   -0.0010\n",
      "    60        0.4772             nan     0.1000   -0.0002\n",
      "    80        0.4218             nan     0.1000   -0.0005\n",
      "   100        0.3728             nan     0.1000    0.0000\n",
      "   120        0.3313             nan     0.1000   -0.0004\n",
      "   140        0.2963             nan     0.1000   -0.0005\n",
      "   160        0.2665             nan     0.1000   -0.0004\n",
      "   180        0.2414             nan     0.1000   -0.0007\n",
      "   200        0.2174             nan     0.1000   -0.0004\n",
      "   220        0.1958             nan     0.1000   -0.0006\n",
      "   240        0.1796             nan     0.1000   -0.0004\n",
      "   250        0.1703             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold06.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1108             nan     0.0100    0.0018\n",
      "     2        1.1071             nan     0.0100    0.0018\n",
      "     3        1.1037             nan     0.0100    0.0018\n",
      "     4        1.1003             nan     0.0100    0.0017\n",
      "     5        1.0970             nan     0.0100    0.0017\n",
      "     6        1.0935             nan     0.0100    0.0016\n",
      "     7        1.0905             nan     0.0100    0.0016\n",
      "     8        1.0873             nan     0.0100    0.0016\n",
      "     9        1.0843             nan     0.0100    0.0016\n",
      "    10        1.0814             nan     0.0100    0.0015\n",
      "    20        1.0541             nan     0.0100    0.0013\n",
      "    40        1.0109             nan     0.0100    0.0009\n",
      "    60        0.9751             nan     0.0100    0.0008\n",
      "    80        0.9455             nan     0.0100    0.0006\n",
      "   100        0.9207             nan     0.0100    0.0006\n",
      "   120        0.8995             nan     0.0100    0.0004\n",
      "   140        0.8808             nan     0.0100    0.0004\n",
      "   160        0.8656             nan     0.0100    0.0004\n",
      "   180        0.8520             nan     0.0100    0.0003\n",
      "   200        0.8399             nan     0.0100    0.0002\n",
      "   220        0.8280             nan     0.0100    0.0002\n",
      "   240        0.8175             nan     0.0100    0.0001\n",
      "   250        0.8120             nan     0.0100    0.0001\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1082             nan     0.0100    0.0032\n",
      "     2        1.1019             nan     0.0100    0.0030\n",
      "     3        1.0955             nan     0.0100    0.0031\n",
      "     4        1.0900             nan     0.0100    0.0027\n",
      "     5        1.0836             nan     0.0100    0.0032\n",
      "     6        1.0778             nan     0.0100    0.0027\n",
      "     7        1.0721             nan     0.0100    0.0025\n",
      "     8        1.0666             nan     0.0100    0.0027\n",
      "     9        1.0612             nan     0.0100    0.0022\n",
      "    10        1.0560             nan     0.0100    0.0023\n",
      "    20        1.0087             nan     0.0100    0.0019\n",
      "    40        0.9326             nan     0.0100    0.0016\n",
      "    60        0.8774             nan     0.0100    0.0011\n",
      "    80        0.8355             nan     0.0100    0.0008\n",
      "   100        0.8008             nan     0.0100    0.0003\n",
      "   120        0.7742             nan     0.0100    0.0004\n",
      "   140        0.7521             nan     0.0100    0.0002\n",
      "   160        0.7332             nan     0.0100    0.0003\n",
      "   180        0.7165             nan     0.0100    0.0001\n",
      "   200        0.7026             nan     0.0100    0.0001\n",
      "   220        0.6898             nan     0.0100    0.0001\n",
      "   240        0.6781             nan     0.0100    0.0001\n",
      "   250        0.6730             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0035\n",
      "     2        1.0998             nan     0.0100    0.0033\n",
      "     3        1.0924             nan     0.0100    0.0030\n",
      "     4        1.0860             nan     0.0100    0.0028\n",
      "     5        1.0794             nan     0.0100    0.0027\n",
      "     6        1.0724             nan     0.0100    0.0032\n",
      "     7        1.0666             nan     0.0100    0.0025\n",
      "     8        1.0600             nan     0.0100    0.0030\n",
      "     9        1.0542             nan     0.0100    0.0027\n",
      "    10        1.0482             nan     0.0100    0.0027\n",
      "    20        0.9943             nan     0.0100    0.0023\n",
      "    40        0.9110             nan     0.0100    0.0016\n",
      "    60        0.8489             nan     0.0100    0.0010\n",
      "    80        0.8006             nan     0.0100    0.0008\n",
      "   100        0.7627             nan     0.0100    0.0007\n",
      "   120        0.7315             nan     0.0100    0.0003\n",
      "   140        0.7051             nan     0.0100    0.0004\n",
      "   160        0.6829             nan     0.0100    0.0003\n",
      "   180        0.6641             nan     0.0100    0.0001\n",
      "   200        0.6477             nan     0.0100    0.0002\n",
      "   220        0.6329             nan     0.0100    0.0002\n",
      "   240        0.6188             nan     0.0100   -0.0000\n",
      "   250        0.6128             nan     0.0100   -0.0001\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0978             nan     0.0500    0.0090\n",
      "     2        1.0819             nan     0.0500    0.0081\n",
      "     3        1.0662             nan     0.0500    0.0073\n",
      "     4        1.0547             nan     0.0500    0.0051\n",
      "     5        1.0415             nan     0.0500    0.0065\n",
      "     6        1.0321             nan     0.0500    0.0043\n",
      "     7        1.0205             nan     0.0500    0.0058\n",
      "     8        1.0097             nan     0.0500    0.0053\n",
      "     9        1.0001             nan     0.0500    0.0048\n",
      "    10        0.9899             nan     0.0500    0.0042\n",
      "    20        0.9212             nan     0.0500    0.0028\n",
      "    40        0.8401             nan     0.0500    0.0006\n",
      "    60        0.7897             nan     0.0500    0.0007\n",
      "    80        0.7569             nan     0.0500    0.0005\n",
      "   100        0.7326             nan     0.0500    0.0003\n",
      "   120        0.7177             nan     0.0500    0.0003\n",
      "   140        0.7060             nan     0.0500    0.0000\n",
      "   160        0.6941             nan     0.0500    0.0000\n",
      "   180        0.6861             nan     0.0500    0.0001\n",
      "   200        0.6786             nan     0.0500   -0.0000\n",
      "   220        0.6726             nan     0.0500    0.0000\n",
      "   240        0.6663             nan     0.0500   -0.0001\n",
      "   250        0.6628             nan     0.0500    0.0000\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0829             nan     0.0500    0.0163\n",
      "     2        1.0538             nan     0.0500    0.0126\n",
      "     3        1.0295             nan     0.0500    0.0116\n",
      "     4        1.0033             nan     0.0500    0.0127\n",
      "     5        0.9825             nan     0.0500    0.0097\n",
      "     6        0.9639             nan     0.0500    0.0076\n",
      "     7        0.9445             nan     0.0500    0.0091\n",
      "     8        0.9276             nan     0.0500    0.0067\n",
      "     9        0.9128             nan     0.0500    0.0065\n",
      "    10        0.8983             nan     0.0500    0.0065\n",
      "    20        0.7968             nan     0.0500    0.0038\n",
      "    40        0.7016             nan     0.0500    0.0013\n",
      "    60        0.6518             nan     0.0500    0.0002\n",
      "    80        0.6182             nan     0.0500   -0.0001\n",
      "   100        0.5902             nan     0.0500   -0.0004\n",
      "   120        0.5648             nan     0.0500   -0.0007\n",
      "   140        0.5409             nan     0.0500   -0.0004\n",
      "   160        0.5233             nan     0.0500   -0.0005\n",
      "   180        0.5052             nan     0.0500    0.0001\n",
      "   200        0.4888             nan     0.0500   -0.0006\n",
      "   220        0.4740             nan     0.0500   -0.0005\n",
      "   240        0.4592             nan     0.0500   -0.0002\n",
      "   250        0.4529             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0782             nan     0.0500    0.0153\n",
      "     2        1.0481             nan     0.0500    0.0147\n",
      "     3        1.0185             nan     0.0500    0.0147\n",
      "     4        0.9946             nan     0.0500    0.0106\n",
      "     5        0.9712             nan     0.0500    0.0101\n",
      "     6        0.9501             nan     0.0500    0.0091\n",
      "     7        0.9300             nan     0.0500    0.0087\n",
      "     8        0.9127             nan     0.0500    0.0068\n",
      "     9        0.8964             nan     0.0500    0.0064\n",
      "    10        0.8781             nan     0.0500    0.0082\n",
      "    20        0.7635             nan     0.0500    0.0024\n",
      "    40        0.6481             nan     0.0500    0.0008\n",
      "    60        0.5843             nan     0.0500    0.0004\n",
      "    80        0.5399             nan     0.0500   -0.0001\n",
      "   100        0.4994             nan     0.0500   -0.0001\n",
      "   120        0.4696             nan     0.0500   -0.0002\n",
      "   140        0.4413             nan     0.0500   -0.0003\n",
      "   160        0.4158             nan     0.0500   -0.0000\n",
      "   180        0.3876             nan     0.0500   -0.0001\n",
      "   200        0.3672             nan     0.0500   -0.0002\n",
      "   220        0.3485             nan     0.0500   -0.0003\n",
      "   240        0.3304             nan     0.0500   -0.0002\n",
      "   250        0.3219             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0798             nan     0.1000    0.0175\n",
      "     2        1.0521             nan     0.1000    0.0142\n",
      "     3        1.0305             nan     0.1000    0.0093\n",
      "     4        1.0153             nan     0.1000    0.0059\n",
      "     5        0.9930             nan     0.1000    0.0112\n",
      "     6        0.9738             nan     0.1000    0.0092\n",
      "     7        0.9576             nan     0.1000    0.0073\n",
      "     8        0.9408             nan     0.1000    0.0078\n",
      "     9        0.9286             nan     0.1000    0.0038\n",
      "    10        0.9196             nan     0.1000    0.0045\n",
      "    20        0.8390             nan     0.1000    0.0027\n",
      "    40        0.7550             nan     0.1000    0.0009\n",
      "    60        0.7146             nan     0.1000    0.0004\n",
      "    80        0.6903             nan     0.1000   -0.0000\n",
      "   100        0.6755             nan     0.1000   -0.0003\n",
      "   120        0.6620             nan     0.1000   -0.0000\n",
      "   140        0.6525             nan     0.1000    0.0001\n",
      "   160        0.6432             nan     0.1000   -0.0005\n",
      "   180        0.6368             nan     0.1000   -0.0001\n",
      "   200        0.6301             nan     0.1000   -0.0006\n",
      "   220        0.6252             nan     0.1000   -0.0006\n",
      "   240        0.6204             nan     0.1000   -0.0002\n",
      "   250        0.6163             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0507             nan     0.1000    0.0292\n",
      "     2        1.0024             nan     0.1000    0.0226\n",
      "     3        0.9626             nan     0.1000    0.0163\n",
      "     4        0.9302             nan     0.1000    0.0145\n",
      "     5        0.8956             nan     0.1000    0.0142\n",
      "     6        0.8692             nan     0.1000    0.0129\n",
      "     7        0.8502             nan     0.1000    0.0070\n",
      "     8        0.8306             nan     0.1000    0.0085\n",
      "     9        0.8145             nan     0.1000    0.0073\n",
      "    10        0.7971             nan     0.1000    0.0071\n",
      "    20        0.7017             nan     0.1000    0.0019\n",
      "    40        0.6145             nan     0.1000   -0.0003\n",
      "    60        0.5678             nan     0.1000   -0.0012\n",
      "    80        0.5281             nan     0.1000   -0.0006\n",
      "   100        0.4953             nan     0.1000   -0.0006\n",
      "   120        0.4637             nan     0.1000   -0.0002\n",
      "   140        0.4385             nan     0.1000   -0.0005\n",
      "   160        0.4120             nan     0.1000   -0.0006\n",
      "   180        0.3865             nan     0.1000   -0.0004\n",
      "   200        0.3645             nan     0.1000   -0.0003\n",
      "   220        0.3437             nan     0.1000   -0.0002\n",
      "   240        0.3264             nan     0.1000   -0.0006\n",
      "   250        0.3180             nan     0.1000   -0.0007\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0422             nan     0.1000    0.0299\n",
      "     2        0.9897             nan     0.1000    0.0243\n",
      "     3        0.9470             nan     0.1000    0.0202\n",
      "     4        0.9077             nan     0.1000    0.0153\n",
      "     5        0.8753             nan     0.1000    0.0129\n",
      "     6        0.8475             nan     0.1000    0.0109\n",
      "     7        0.8226             nan     0.1000    0.0095\n",
      "     8        0.7990             nan     0.1000    0.0076\n",
      "     9        0.7789             nan     0.1000    0.0066\n",
      "    10        0.7620             nan     0.1000    0.0057\n",
      "    20        0.6535             nan     0.1000    0.0036\n",
      "    40        0.5403             nan     0.1000    0.0004\n",
      "    60        0.4698             nan     0.1000   -0.0004\n",
      "    80        0.4183             nan     0.1000   -0.0010\n",
      "   100        0.3709             nan     0.1000   -0.0001\n",
      "   120        0.3306             nan     0.1000   -0.0006\n",
      "   140        0.2995             nan     0.1000   -0.0007\n",
      "   160        0.2680             nan     0.1000   -0.0004\n",
      "   180        0.2401             nan     0.1000    0.0002\n",
      "   200        0.2158             nan     0.1000   -0.0001\n",
      "   220        0.1955             nan     0.1000   -0.0007\n",
      "   240        0.1786             nan     0.1000   -0.0003\n",
      "   250        0.1709             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold07.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1109             nan     0.0100    0.0018\n",
      "     2        1.1074             nan     0.0100    0.0018\n",
      "     3        1.1038             nan     0.0100    0.0017\n",
      "     4        1.1007             nan     0.0100    0.0016\n",
      "     5        1.0975             nan     0.0100    0.0017\n",
      "     6        1.0944             nan     0.0100    0.0016\n",
      "     7        1.0912             nan     0.0100    0.0016\n",
      "     8        1.0882             nan     0.0100    0.0016\n",
      "     9        1.0851             nan     0.0100    0.0015\n",
      "    10        1.0819             nan     0.0100    0.0015\n",
      "    20        1.0552             nan     0.0100    0.0013\n",
      "    40        1.0127             nan     0.0100    0.0009\n",
      "    60        0.9771             nan     0.0100    0.0008\n",
      "    80        0.9495             nan     0.0100    0.0007\n",
      "   100        0.9262             nan     0.0100    0.0004\n",
      "   120        0.9051             nan     0.0100    0.0004\n",
      "   140        0.8875             nan     0.0100    0.0004\n",
      "   160        0.8726             nan     0.0100    0.0002\n",
      "   180        0.8584             nan     0.0100    0.0002\n",
      "   200        0.8447             nan     0.0100    0.0003\n",
      "   220        0.8325             nan     0.0100    0.0003\n",
      "   240        0.8222             nan     0.0100    0.0002\n",
      "   250        0.8171             nan     0.0100    0.0002\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1078             nan     0.0100    0.0031\n",
      "     2        1.1011             nan     0.0100    0.0033\n",
      "     3        1.0950             nan     0.0100    0.0029\n",
      "     4        1.0892             nan     0.0100    0.0026\n",
      "     5        1.0832             nan     0.0100    0.0027\n",
      "     6        1.0777             nan     0.0100    0.0027\n",
      "     7        1.0721             nan     0.0100    0.0027\n",
      "     8        1.0665             nan     0.0100    0.0023\n",
      "     9        1.0610             nan     0.0100    0.0026\n",
      "    10        1.0556             nan     0.0100    0.0027\n",
      "    20        1.0066             nan     0.0100    0.0021\n",
      "    40        0.9330             nan     0.0100    0.0017\n",
      "    60        0.8786             nan     0.0100    0.0012\n",
      "    80        0.8350             nan     0.0100    0.0007\n",
      "   100        0.8022             nan     0.0100    0.0006\n",
      "   120        0.7756             nan     0.0100    0.0005\n",
      "   140        0.7527             nan     0.0100    0.0003\n",
      "   160        0.7341             nan     0.0100    0.0002\n",
      "   180        0.7180             nan     0.0100    0.0002\n",
      "   200        0.7040             nan     0.0100    0.0001\n",
      "   220        0.6911             nan     0.0100    0.0001\n",
      "   240        0.6798             nan     0.0100    0.0001\n",
      "   250        0.6749             nan     0.0100    0.0002\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1072             nan     0.0100    0.0032\n",
      "     2        1.0997             nan     0.0100    0.0036\n",
      "     3        1.0928             nan     0.0100    0.0032\n",
      "     4        1.0861             nan     0.0100    0.0028\n",
      "     5        1.0799             nan     0.0100    0.0029\n",
      "     6        1.0738             nan     0.0100    0.0026\n",
      "     7        1.0676             nan     0.0100    0.0029\n",
      "     8        1.0611             nan     0.0100    0.0030\n",
      "     9        1.0554             nan     0.0100    0.0027\n",
      "    10        1.0493             nan     0.0100    0.0029\n",
      "    20        0.9953             nan     0.0100    0.0021\n",
      "    40        0.9130             nan     0.0100    0.0016\n",
      "    60        0.8521             nan     0.0100    0.0010\n",
      "    80        0.8049             nan     0.0100    0.0008\n",
      "   100        0.7680             nan     0.0100    0.0007\n",
      "   120        0.7375             nan     0.0100    0.0004\n",
      "   140        0.7113             nan     0.0100    0.0004\n",
      "   160        0.6899             nan     0.0100    0.0003\n",
      "   180        0.6708             nan     0.0100    0.0002\n",
      "   200        0.6535             nan     0.0100    0.0002\n",
      "   220        0.6389             nan     0.0100    0.0001\n",
      "   240        0.6247             nan     0.0100    0.0001\n",
      "   250        0.6183             nan     0.0100    0.0001\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0982             nan     0.0500    0.0088\n",
      "     2        1.0827             nan     0.0500    0.0080\n",
      "     3        1.0685             nan     0.0500    0.0073\n",
      "     4        1.0546             nan     0.0500    0.0065\n",
      "     5        1.0436             nan     0.0500    0.0056\n",
      "     6        1.0323             nan     0.0500    0.0059\n",
      "     7        1.0222             nan     0.0500    0.0045\n",
      "     8        1.0139             nan     0.0500    0.0036\n",
      "     9        1.0063             nan     0.0500    0.0024\n",
      "    10        0.9963             nan     0.0500    0.0051\n",
      "    20        0.9244             nan     0.0500    0.0029\n",
      "    40        0.8411             nan     0.0500    0.0016\n",
      "    60        0.7926             nan     0.0500    0.0010\n",
      "    80        0.7609             nan     0.0500    0.0003\n",
      "   100        0.7377             nan     0.0500    0.0002\n",
      "   120        0.7198             nan     0.0500    0.0006\n",
      "   140        0.7068             nan     0.0500   -0.0000\n",
      "   160        0.6976             nan     0.0500    0.0001\n",
      "   180        0.6865             nan     0.0500    0.0002\n",
      "   200        0.6780             nan     0.0500   -0.0001\n",
      "   220        0.6715             nan     0.0500   -0.0000\n",
      "   240        0.6663             nan     0.0500    0.0000\n",
      "   250        0.6637             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0813             nan     0.0500    0.0165\n",
      "     2        1.0538             nan     0.0500    0.0131\n",
      "     3        1.0288             nan     0.0500    0.0113\n",
      "     4        1.0054             nan     0.0500    0.0113\n",
      "     5        0.9841             nan     0.0500    0.0097\n",
      "     6        0.9635             nan     0.0500    0.0094\n",
      "     7        0.9471             nan     0.0500    0.0074\n",
      "     8        0.9311             nan     0.0500    0.0070\n",
      "     9        0.9158             nan     0.0500    0.0074\n",
      "    10        0.9028             nan     0.0500    0.0058\n",
      "    20        0.8058             nan     0.0500    0.0027\n",
      "    40        0.7089             nan     0.0500    0.0002\n",
      "    60        0.6554             nan     0.0500    0.0002\n",
      "    80        0.6239             nan     0.0500   -0.0001\n",
      "   100        0.5946             nan     0.0500    0.0003\n",
      "   120        0.5729             nan     0.0500   -0.0003\n",
      "   140        0.5526             nan     0.0500   -0.0001\n",
      "   160        0.5324             nan     0.0500   -0.0000\n",
      "   180        0.5154             nan     0.0500   -0.0001\n",
      "   200        0.4981             nan     0.0500   -0.0002\n",
      "   220        0.4824             nan     0.0500   -0.0001\n",
      "   240        0.4659             nan     0.0500   -0.0004\n",
      "   250        0.4585             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0773             nan     0.0500    0.0158\n",
      "     2        1.0452             nan     0.0500    0.0133\n",
      "     3        1.0195             nan     0.0500    0.0114\n",
      "     4        0.9925             nan     0.0500    0.0115\n",
      "     5        0.9677             nan     0.0500    0.0119\n",
      "     6        0.9465             nan     0.0500    0.0089\n",
      "     7        0.9245             nan     0.0500    0.0103\n",
      "     8        0.9056             nan     0.0500    0.0083\n",
      "     9        0.8903             nan     0.0500    0.0063\n",
      "    10        0.8756             nan     0.0500    0.0062\n",
      "    20        0.7633             nan     0.0500    0.0026\n",
      "    40        0.6557             nan     0.0500    0.0003\n",
      "    60        0.5896             nan     0.0500   -0.0004\n",
      "    80        0.5422             nan     0.0500   -0.0001\n",
      "   100        0.5029             nan     0.0500   -0.0004\n",
      "   120        0.4721             nan     0.0500   -0.0007\n",
      "   140        0.4425             nan     0.0500   -0.0004\n",
      "   160        0.4162             nan     0.0500   -0.0003\n",
      "   180        0.3941             nan     0.0500   -0.0007\n",
      "   200        0.3726             nan     0.0500   -0.0001\n",
      "   220        0.3520             nan     0.0500   -0.0003\n",
      "   240        0.3339             nan     0.0500   -0.0002\n",
      "   250        0.3251             nan     0.0500   -0.0007\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0820             nan     0.1000    0.0172\n",
      "     2        1.0545             nan     0.1000    0.0140\n",
      "     3        1.0329             nan     0.1000    0.0116\n",
      "     4        1.0122             nan     0.1000    0.0098\n",
      "     5        0.9933             nan     0.1000    0.0094\n",
      "     6        0.9797             nan     0.1000    0.0066\n",
      "     7        0.9617             nan     0.1000    0.0078\n",
      "     8        0.9508             nan     0.1000    0.0034\n",
      "     9        0.9352             nan     0.1000    0.0071\n",
      "    10        0.9235             nan     0.1000    0.0056\n",
      "    20        0.8440             nan     0.1000    0.0024\n",
      "    40        0.7574             nan     0.1000    0.0006\n",
      "    60        0.7194             nan     0.1000   -0.0003\n",
      "    80        0.6958             nan     0.1000   -0.0000\n",
      "   100        0.6800             nan     0.1000   -0.0005\n",
      "   120        0.6677             nan     0.1000   -0.0001\n",
      "   140        0.6567             nan     0.1000   -0.0002\n",
      "   160        0.6487             nan     0.1000   -0.0005\n",
      "   180        0.6419             nan     0.1000   -0.0002\n",
      "   200        0.6365             nan     0.1000   -0.0003\n",
      "   220        0.6326             nan     0.1000   -0.0005\n",
      "   240        0.6290             nan     0.1000   -0.0006\n",
      "   250        0.6274             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0489             nan     0.1000    0.0297\n",
      "     2        0.9943             nan     0.1000    0.0268\n",
      "     3        0.9517             nan     0.1000    0.0190\n",
      "     4        0.9208             nan     0.1000    0.0145\n",
      "     5        0.8912             nan     0.1000    0.0143\n",
      "     6        0.8662             nan     0.1000    0.0107\n",
      "     7        0.8451             nan     0.1000    0.0094\n",
      "     8        0.8287             nan     0.1000    0.0061\n",
      "     9        0.8118             nan     0.1000    0.0074\n",
      "    10        0.7964             nan     0.1000    0.0045\n",
      "    20        0.7019             nan     0.1000    0.0024\n",
      "    40        0.6182             nan     0.1000    0.0006\n",
      "    60        0.5695             nan     0.1000   -0.0005\n",
      "    80        0.5329             nan     0.1000   -0.0008\n",
      "   100        0.5002             nan     0.1000   -0.0018\n",
      "   120        0.4652             nan     0.1000   -0.0006\n",
      "   140        0.4372             nan     0.1000   -0.0006\n",
      "   160        0.4120             nan     0.1000   -0.0002\n",
      "   180        0.3889             nan     0.1000   -0.0003\n",
      "   200        0.3676             nan     0.1000   -0.0001\n",
      "   220        0.3487             nan     0.1000   -0.0006\n",
      "   240        0.3297             nan     0.1000   -0.0010\n",
      "   250        0.3197             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0477             nan     0.1000    0.0305\n",
      "     2        0.9910             nan     0.1000    0.0276\n",
      "     3        0.9455             nan     0.1000    0.0196\n",
      "     4        0.9081             nan     0.1000    0.0157\n",
      "     5        0.8791             nan     0.1000    0.0109\n",
      "     6        0.8515             nan     0.1000    0.0112\n",
      "     7        0.8281             nan     0.1000    0.0088\n",
      "     8        0.8041             nan     0.1000    0.0104\n",
      "     9        0.7846             nan     0.1000    0.0086\n",
      "    10        0.7672             nan     0.1000    0.0054\n",
      "    20        0.6558             nan     0.1000    0.0007\n",
      "    40        0.5466             nan     0.1000    0.0009\n",
      "    60        0.4697             nan     0.1000   -0.0008\n",
      "    80        0.4155             nan     0.1000   -0.0017\n",
      "   100        0.3712             nan     0.1000   -0.0006\n",
      "   120        0.3353             nan     0.1000   -0.0009\n",
      "   140        0.2981             nan     0.1000   -0.0002\n",
      "   160        0.2687             nan     0.1000   -0.0008\n",
      "   180        0.2418             nan     0.1000   -0.0007\n",
      "   200        0.2191             nan     0.1000   -0.0006\n",
      "   220        0.1984             nan     0.1000   -0.0006\n",
      "   240        0.1818             nan     0.1000   -0.0004\n",
      "   250        0.1735             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold08.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1109             nan     0.0100    0.0018\n",
      "     2        1.1076             nan     0.0100    0.0017\n",
      "     3        1.1040             nan     0.0100    0.0018\n",
      "     4        1.1005             nan     0.0100    0.0017\n",
      "     5        1.0972             nan     0.0100    0.0017\n",
      "     6        1.0939             nan     0.0100    0.0016\n",
      "     7        1.0906             nan     0.0100    0.0016\n",
      "     8        1.0876             nan     0.0100    0.0016\n",
      "     9        1.0846             nan     0.0100    0.0016\n",
      "    10        1.0816             nan     0.0100    0.0015\n",
      "    20        1.0545             nan     0.0100    0.0013\n",
      "    40        1.0120             nan     0.0100    0.0010\n",
      "    60        0.9781             nan     0.0100    0.0008\n",
      "    80        0.9488             nan     0.0100    0.0007\n",
      "   100        0.9247             nan     0.0100    0.0004\n",
      "   120        0.9034             nan     0.0100    0.0003\n",
      "   140        0.8857             nan     0.0100    0.0003\n",
      "   160        0.8705             nan     0.0100    0.0003\n",
      "   180        0.8566             nan     0.0100    0.0003\n",
      "   200        0.8451             nan     0.0100    0.0002\n",
      "   220        0.8340             nan     0.0100    0.0003\n",
      "   240        0.8243             nan     0.0100    0.0002\n",
      "   250        0.8192             nan     0.0100    0.0002\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1082             nan     0.0100    0.0031\n",
      "     2        1.1012             nan     0.0100    0.0032\n",
      "     3        1.0948             nan     0.0100    0.0030\n",
      "     4        1.0886             nan     0.0100    0.0030\n",
      "     5        1.0825             nan     0.0100    0.0027\n",
      "     6        1.0766             nan     0.0100    0.0029\n",
      "     7        1.0714             nan     0.0100    0.0025\n",
      "     8        1.0659             nan     0.0100    0.0024\n",
      "     9        1.0608             nan     0.0100    0.0025\n",
      "    10        1.0554             nan     0.0100    0.0025\n",
      "    20        1.0091             nan     0.0100    0.0023\n",
      "    40        0.9384             nan     0.0100    0.0013\n",
      "    60        0.8860             nan     0.0100    0.0009\n",
      "    80        0.8448             nan     0.0100    0.0008\n",
      "   100        0.8130             nan     0.0100    0.0005\n",
      "   120        0.7856             nan     0.0100    0.0005\n",
      "   140        0.7644             nan     0.0100    0.0003\n",
      "   160        0.7456             nan     0.0100    0.0002\n",
      "   180        0.7293             nan     0.0100    0.0002\n",
      "   200        0.7153             nan     0.0100    0.0001\n",
      "   220        0.7027             nan     0.0100    0.0000\n",
      "   240        0.6915             nan     0.0100    0.0001\n",
      "   250        0.6863             nan     0.0100    0.0000\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1071             nan     0.0100    0.0032\n",
      "     2        1.1001             nan     0.0100    0.0033\n",
      "     3        1.0933             nan     0.0100    0.0031\n",
      "     4        1.0865             nan     0.0100    0.0033\n",
      "     5        1.0803             nan     0.0100    0.0027\n",
      "     6        1.0743             nan     0.0100    0.0027\n",
      "     7        1.0681             nan     0.0100    0.0030\n",
      "     8        1.0622             nan     0.0100    0.0030\n",
      "     9        1.0563             nan     0.0100    0.0029\n",
      "    10        1.0503             nan     0.0100    0.0026\n",
      "    20        0.9978             nan     0.0100    0.0018\n",
      "    40        0.9169             nan     0.0100    0.0013\n",
      "    60        0.8575             nan     0.0100    0.0010\n",
      "    80        0.8125             nan     0.0100    0.0008\n",
      "   100        0.7766             nan     0.0100    0.0005\n",
      "   120        0.7457             nan     0.0100    0.0003\n",
      "   140        0.7202             nan     0.0100    0.0003\n",
      "   160        0.6987             nan     0.0100    0.0004\n",
      "   180        0.6796             nan     0.0100    0.0001\n",
      "   200        0.6629             nan     0.0100    0.0001\n",
      "   220        0.6478             nan     0.0100   -0.0000\n",
      "   240        0.6339             nan     0.0100    0.0001\n",
      "   250        0.6279             nan     0.0100    0.0000\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0964             nan     0.0500    0.0089\n",
      "     2        1.0806             nan     0.0500    0.0080\n",
      "     3        1.0651             nan     0.0500    0.0072\n",
      "     4        1.0509             nan     0.0500    0.0064\n",
      "     5        1.0384             nan     0.0500    0.0058\n",
      "     6        1.0282             nan     0.0500    0.0053\n",
      "     7        1.0178             nan     0.0500    0.0048\n",
      "     8        1.0084             nan     0.0500    0.0046\n",
      "     9        0.9992             nan     0.0500    0.0047\n",
      "    10        0.9905             nan     0.0500    0.0038\n",
      "    20        0.9207             nan     0.0500    0.0029\n",
      "    40        0.8436             nan     0.0500    0.0011\n",
      "    60        0.7954             nan     0.0500    0.0008\n",
      "    80        0.7661             nan     0.0500    0.0008\n",
      "   100        0.7435             nan     0.0500    0.0006\n",
      "   120        0.7287             nan     0.0500    0.0001\n",
      "   140        0.7173             nan     0.0500   -0.0000\n",
      "   160        0.7068             nan     0.0500   -0.0002\n",
      "   180        0.6979             nan     0.0500    0.0000\n",
      "   200        0.6910             nan     0.0500   -0.0001\n",
      "   220        0.6843             nan     0.0500    0.0000\n",
      "   240        0.6776             nan     0.0500   -0.0000\n",
      "   250        0.6744             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0836             nan     0.0500    0.0144\n",
      "     2        1.0557             nan     0.0500    0.0142\n",
      "     3        1.0291             nan     0.0500    0.0120\n",
      "     4        1.0058             nan     0.0500    0.0108\n",
      "     5        0.9860             nan     0.0500    0.0098\n",
      "     6        0.9676             nan     0.0500    0.0087\n",
      "     7        0.9503             nan     0.0500    0.0066\n",
      "     8        0.9337             nan     0.0500    0.0072\n",
      "     9        0.9191             nan     0.0500    0.0058\n",
      "    10        0.9066             nan     0.0500    0.0055\n",
      "    20        0.8109             nan     0.0500    0.0027\n",
      "    40        0.7158             nan     0.0500    0.0009\n",
      "    60        0.6637             nan     0.0500    0.0002\n",
      "    80        0.6305             nan     0.0500    0.0001\n",
      "   100        0.6035             nan     0.0500   -0.0005\n",
      "   120        0.5789             nan     0.0500   -0.0001\n",
      "   140        0.5586             nan     0.0500   -0.0004\n",
      "   160        0.5391             nan     0.0500   -0.0002\n",
      "   180        0.5206             nan     0.0500   -0.0003\n",
      "   200        0.5026             nan     0.0500   -0.0003\n",
      "   220        0.4873             nan     0.0500    0.0001\n",
      "   240        0.4736             nan     0.0500   -0.0005\n",
      "   250        0.4666             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0807             nan     0.0500    0.0142\n",
      "     2        1.0489             nan     0.0500    0.0131\n",
      "     3        1.0206             nan     0.0500    0.0128\n",
      "     4        0.9970             nan     0.0500    0.0104\n",
      "     5        0.9748             nan     0.0500    0.0099\n",
      "     6        0.9525             nan     0.0500    0.0086\n",
      "     7        0.9328             nan     0.0500    0.0084\n",
      "     8        0.9148             nan     0.0500    0.0071\n",
      "     9        0.8984             nan     0.0500    0.0069\n",
      "    10        0.8836             nan     0.0500    0.0057\n",
      "    20        0.7697             nan     0.0500    0.0032\n",
      "    40        0.6639             nan     0.0500   -0.0002\n",
      "    60        0.5991             nan     0.0500    0.0005\n",
      "    80        0.5530             nan     0.0500   -0.0005\n",
      "   100        0.5152             nan     0.0500   -0.0002\n",
      "   120        0.4818             nan     0.0500   -0.0005\n",
      "   140        0.4516             nan     0.0500   -0.0002\n",
      "   160        0.4265             nan     0.0500   -0.0003\n",
      "   180        0.4051             nan     0.0500   -0.0003\n",
      "   200        0.3806             nan     0.0500   -0.0003\n",
      "   220        0.3593             nan     0.0500   -0.0001\n",
      "   240        0.3401             nan     0.0500   -0.0003\n",
      "   250        0.3308             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0835             nan     0.1000    0.0173\n",
      "     2        1.0565             nan     0.1000    0.0145\n",
      "     3        1.0344             nan     0.1000    0.0094\n",
      "     4        1.0099             nan     0.1000    0.0117\n",
      "     5        0.9898             nan     0.1000    0.0082\n",
      "     6        0.9679             nan     0.1000    0.0088\n",
      "     7        0.9550             nan     0.1000    0.0072\n",
      "     8        0.9398             nan     0.1000    0.0073\n",
      "     9        0.9305             nan     0.1000    0.0035\n",
      "    10        0.9177             nan     0.1000    0.0058\n",
      "    20        0.8400             nan     0.1000    0.0023\n",
      "    40        0.7663             nan     0.1000    0.0007\n",
      "    60        0.7318             nan     0.1000    0.0003\n",
      "    80        0.7053             nan     0.1000    0.0005\n",
      "   100        0.6887             nan     0.1000   -0.0003\n",
      "   120        0.6754             nan     0.1000   -0.0001\n",
      "   140        0.6651             nan     0.1000   -0.0001\n",
      "   160        0.6558             nan     0.1000   -0.0004\n",
      "   180        0.6471             nan     0.1000   -0.0003\n",
      "   200        0.6413             nan     0.1000   -0.0004\n",
      "   220        0.6367             nan     0.1000   -0.0003\n",
      "   240        0.6323             nan     0.1000   -0.0001\n",
      "   250        0.6302             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0484             nan     0.1000    0.0284\n",
      "     2        1.0027             nan     0.1000    0.0225\n",
      "     3        0.9643             nan     0.1000    0.0167\n",
      "     4        0.9293             nan     0.1000    0.0154\n",
      "     5        0.9015             nan     0.1000    0.0134\n",
      "     6        0.8773             nan     0.1000    0.0105\n",
      "     7        0.8596             nan     0.1000    0.0086\n",
      "     8        0.8399             nan     0.1000    0.0078\n",
      "     9        0.8235             nan     0.1000    0.0071\n",
      "    10        0.8121             nan     0.1000    0.0033\n",
      "    20        0.7129             nan     0.1000    0.0029\n",
      "    40        0.6321             nan     0.1000    0.0005\n",
      "    60        0.5816             nan     0.1000   -0.0010\n",
      "    80        0.5404             nan     0.1000   -0.0006\n",
      "   100        0.5027             nan     0.1000   -0.0010\n",
      "   120        0.4717             nan     0.1000   -0.0005\n",
      "   140        0.4406             nan     0.1000   -0.0002\n",
      "   160        0.4187             nan     0.1000   -0.0003\n",
      "   180        0.3958             nan     0.1000   -0.0004\n",
      "   200        0.3776             nan     0.1000   -0.0013\n",
      "   220        0.3561             nan     0.1000   -0.0007\n",
      "   240        0.3370             nan     0.1000   -0.0007\n",
      "   250        0.3283             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0539             nan     0.1000    0.0274\n",
      "     2        1.0021             nan     0.1000    0.0249\n",
      "     3        0.9548             nan     0.1000    0.0204\n",
      "     4        0.9170             nan     0.1000    0.0172\n",
      "     5        0.8859             nan     0.1000    0.0108\n",
      "     6        0.8563             nan     0.1000    0.0132\n",
      "     7        0.8324             nan     0.1000    0.0099\n",
      "     8        0.8089             nan     0.1000    0.0089\n",
      "     9        0.7925             nan     0.1000    0.0054\n",
      "    10        0.7757             nan     0.1000    0.0049\n",
      "    20        0.6626             nan     0.1000    0.0022\n",
      "    40        0.5556             nan     0.1000   -0.0001\n",
      "    60        0.4832             nan     0.1000   -0.0005\n",
      "    80        0.4300             nan     0.1000   -0.0009\n",
      "   100        0.3841             nan     0.1000   -0.0006\n",
      "   120        0.3478             nan     0.1000   -0.0005\n",
      "   140        0.3123             nan     0.1000   -0.0001\n",
      "   160        0.2835             nan     0.1000   -0.0006\n",
      "   180        0.2562             nan     0.1000   -0.0001\n",
      "   200        0.2344             nan     0.1000   -0.0010\n",
      "   220        0.2115             nan     0.1000   -0.0000\n",
      "   240        0.1925             nan     0.1000   -0.0005\n",
      "   250        0.1828             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold09.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1109             nan     0.0100    0.0018\n",
      "     2        1.1082             nan     0.0100    0.0010\n",
      "     3        1.1046             nan     0.0100    0.0018\n",
      "     4        1.1009             nan     0.0100    0.0017\n",
      "     5        1.0975             nan     0.0100    0.0017\n",
      "     6        1.0942             nan     0.0100    0.0017\n",
      "     7        1.0909             nan     0.0100    0.0016\n",
      "     8        1.0877             nan     0.0100    0.0016\n",
      "     9        1.0849             nan     0.0100    0.0016\n",
      "    10        1.0817             nan     0.0100    0.0015\n",
      "    20        1.0529             nan     0.0100    0.0013\n",
      "    40        1.0098             nan     0.0100    0.0009\n",
      "    60        0.9758             nan     0.0100    0.0007\n",
      "    80        0.9455             nan     0.0100    0.0006\n",
      "   100        0.9204             nan     0.0100    0.0004\n",
      "   120        0.8998             nan     0.0100    0.0003\n",
      "   140        0.8809             nan     0.0100    0.0004\n",
      "   160        0.8659             nan     0.0100    0.0002\n",
      "   180        0.8514             nan     0.0100    0.0002\n",
      "   200        0.8383             nan     0.0100    0.0002\n",
      "   220        0.8262             nan     0.0100    0.0002\n",
      "   240        0.8157             nan     0.0100    0.0002\n",
      "   250        0.8108             nan     0.0100    0.0002\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1077             nan     0.0100    0.0031\n",
      "     2        1.1013             nan     0.0100    0.0030\n",
      "     3        1.0949             nan     0.0100    0.0031\n",
      "     4        1.0888             nan     0.0100    0.0028\n",
      "     5        1.0828             nan     0.0100    0.0029\n",
      "     6        1.0771             nan     0.0100    0.0029\n",
      "     7        1.0712             nan     0.0100    0.0027\n",
      "     8        1.0654             nan     0.0100    0.0026\n",
      "     9        1.0601             nan     0.0100    0.0026\n",
      "    10        1.0547             nan     0.0100    0.0025\n",
      "    20        1.0067             nan     0.0100    0.0019\n",
      "    40        0.9325             nan     0.0100    0.0016\n",
      "    60        0.8778             nan     0.0100    0.0010\n",
      "    80        0.8360             nan     0.0100    0.0009\n",
      "   100        0.8032             nan     0.0100    0.0007\n",
      "   120        0.7743             nan     0.0100    0.0005\n",
      "   140        0.7521             nan     0.0100    0.0003\n",
      "   160        0.7331             nan     0.0100    0.0004\n",
      "   180        0.7168             nan     0.0100    0.0002\n",
      "   200        0.7030             nan     0.0100    0.0002\n",
      "   220        0.6905             nan     0.0100    0.0002\n",
      "   240        0.6791             nan     0.0100    0.0001\n",
      "   250        0.6739             nan     0.0100    0.0000\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1070             nan     0.0100    0.0035\n",
      "     2        1.0992             nan     0.0100    0.0033\n",
      "     3        1.0922             nan     0.0100    0.0031\n",
      "     4        1.0854             nan     0.0100    0.0034\n",
      "     5        1.0783             nan     0.0100    0.0034\n",
      "     6        1.0712             nan     0.0100    0.0032\n",
      "     7        1.0649             nan     0.0100    0.0028\n",
      "     8        1.0586             nan     0.0100    0.0026\n",
      "     9        1.0527             nan     0.0100    0.0024\n",
      "    10        1.0469             nan     0.0100    0.0027\n",
      "    20        0.9927             nan     0.0100    0.0019\n",
      "    40        0.9119             nan     0.0100    0.0013\n",
      "    60        0.8522             nan     0.0100    0.0009\n",
      "    80        0.8040             nan     0.0100    0.0007\n",
      "   100        0.7661             nan     0.0100    0.0006\n",
      "   120        0.7367             nan     0.0100    0.0003\n",
      "   140        0.7106             nan     0.0100    0.0000\n",
      "   160        0.6882             nan     0.0100    0.0002\n",
      "   180        0.6685             nan     0.0100    0.0003\n",
      "   200        0.6512             nan     0.0100    0.0001\n",
      "   220        0.6364             nan     0.0100    0.0001\n",
      "   240        0.6228             nan     0.0100    0.0002\n",
      "   250        0.6167             nan     0.0100   -0.0001\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0971             nan     0.0500    0.0089\n",
      "     2        1.0810             nan     0.0500    0.0081\n",
      "     3        1.0665             nan     0.0500    0.0073\n",
      "     4        1.0527             nan     0.0500    0.0066\n",
      "     5        1.0410             nan     0.0500    0.0060\n",
      "     6        1.0301             nan     0.0500    0.0054\n",
      "     7        1.0205             nan     0.0500    0.0039\n",
      "     8        1.0097             nan     0.0500    0.0053\n",
      "     9        0.9996             nan     0.0500    0.0048\n",
      "    10        0.9923             nan     0.0500    0.0036\n",
      "    20        0.9191             nan     0.0500    0.0030\n",
      "    40        0.8371             nan     0.0500    0.0012\n",
      "    60        0.7893             nan     0.0500    0.0005\n",
      "    80        0.7558             nan     0.0500    0.0002\n",
      "   100        0.7338             nan     0.0500    0.0002\n",
      "   120        0.7177             nan     0.0500   -0.0001\n",
      "   140        0.7040             nan     0.0500    0.0002\n",
      "   160        0.6934             nan     0.0500    0.0000\n",
      "   180        0.6848             nan     0.0500   -0.0000\n",
      "   200        0.6766             nan     0.0500   -0.0001\n",
      "   220        0.6695             nan     0.0500   -0.0002\n",
      "   240        0.6642             nan     0.0500   -0.0002\n",
      "   250        0.6610             nan     0.0500    0.0001\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0819             nan     0.0500    0.0153\n",
      "     2        1.0550             nan     0.0500    0.0129\n",
      "     3        1.0283             nan     0.0500    0.0120\n",
      "     4        1.0056             nan     0.0500    0.0105\n",
      "     5        0.9860             nan     0.0500    0.0094\n",
      "     6        0.9665             nan     0.0500    0.0087\n",
      "     7        0.9482             nan     0.0500    0.0079\n",
      "     8        0.9330             nan     0.0500    0.0072\n",
      "     9        0.9168             nan     0.0500    0.0073\n",
      "    10        0.9030             nan     0.0500    0.0065\n",
      "    20        0.8012             nan     0.0500    0.0020\n",
      "    40        0.7058             nan     0.0500    0.0005\n",
      "    60        0.6546             nan     0.0500   -0.0000\n",
      "    80        0.6184             nan     0.0500   -0.0005\n",
      "   100        0.5908             nan     0.0500   -0.0004\n",
      "   120        0.5687             nan     0.0500   -0.0000\n",
      "   140        0.5484             nan     0.0500   -0.0002\n",
      "   160        0.5305             nan     0.0500   -0.0003\n",
      "   180        0.5133             nan     0.0500   -0.0002\n",
      "   200        0.4977             nan     0.0500   -0.0004\n",
      "   220        0.4803             nan     0.0500    0.0001\n",
      "   240        0.4643             nan     0.0500   -0.0005\n",
      "   250        0.4557             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0799             nan     0.0500    0.0168\n",
      "     2        1.0489             nan     0.0500    0.0146\n",
      "     3        1.0189             nan     0.0500    0.0144\n",
      "     4        0.9914             nan     0.0500    0.0114\n",
      "     5        0.9694             nan     0.0500    0.0093\n",
      "     6        0.9493             nan     0.0500    0.0080\n",
      "     7        0.9305             nan     0.0500    0.0085\n",
      "     8        0.9114             nan     0.0500    0.0084\n",
      "     9        0.8947             nan     0.0500    0.0070\n",
      "    10        0.8769             nan     0.0500    0.0075\n",
      "    20        0.7592             nan     0.0500    0.0032\n",
      "    40        0.6478             nan     0.0500    0.0007\n",
      "    60        0.5856             nan     0.0500   -0.0000\n",
      "    80        0.5403             nan     0.0500   -0.0004\n",
      "   100        0.5045             nan     0.0500   -0.0008\n",
      "   120        0.4743             nan     0.0500   -0.0003\n",
      "   140        0.4458             nan     0.0500   -0.0003\n",
      "   160        0.4171             nan     0.0500   -0.0003\n",
      "   180        0.3931             nan     0.0500   -0.0002\n",
      "   200        0.3727             nan     0.0500   -0.0003\n",
      "   220        0.3525             nan     0.0500   -0.0004\n",
      "   240        0.3331             nan     0.0500   -0.0005\n",
      "   250        0.3237             nan     0.0500   -0.0006\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0795             nan     0.1000    0.0173\n",
      "     2        1.0533             nan     0.1000    0.0141\n",
      "     3        1.0315             nan     0.1000    0.0117\n",
      "     4        1.0101             nan     0.1000    0.0097\n",
      "     5        0.9899             nan     0.1000    0.0105\n",
      "     6        0.9735             nan     0.1000    0.0085\n",
      "     7        0.9602             nan     0.1000    0.0051\n",
      "     8        0.9452             nan     0.1000    0.0064\n",
      "     9        0.9301             nan     0.1000    0.0072\n",
      "    10        0.9176             nan     0.1000    0.0060\n",
      "    20        0.8345             nan     0.1000    0.0021\n",
      "    40        0.7548             nan     0.1000    0.0014\n",
      "    60        0.7169             nan     0.1000    0.0004\n",
      "    80        0.6916             nan     0.1000    0.0001\n",
      "   100        0.6759             nan     0.1000    0.0002\n",
      "   120        0.6626             nan     0.1000   -0.0001\n",
      "   140        0.6537             nan     0.1000    0.0000\n",
      "   160        0.6449             nan     0.1000   -0.0000\n",
      "   180        0.6367             nan     0.1000   -0.0002\n",
      "   200        0.6320             nan     0.1000   -0.0004\n",
      "   220        0.6265             nan     0.1000   -0.0005\n",
      "   240        0.6199             nan     0.1000   -0.0007\n",
      "   250        0.6166             nan     0.1000   -0.0008\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0479             nan     0.1000    0.0314\n",
      "     2        0.9998             nan     0.1000    0.0243\n",
      "     3        0.9583             nan     0.1000    0.0195\n",
      "     4        0.9244             nan     0.1000    0.0153\n",
      "     5        0.8989             nan     0.1000    0.0106\n",
      "     6        0.8736             nan     0.1000    0.0099\n",
      "     7        0.8547             nan     0.1000    0.0085\n",
      "     8        0.8315             nan     0.1000    0.0094\n",
      "     9        0.8157             nan     0.1000    0.0055\n",
      "    10        0.7981             nan     0.1000    0.0067\n",
      "    20        0.6972             nan     0.1000    0.0013\n",
      "    40        0.6132             nan     0.1000   -0.0002\n",
      "    60        0.5648             nan     0.1000   -0.0003\n",
      "    80        0.5273             nan     0.1000   -0.0005\n",
      "   100        0.4978             nan     0.1000   -0.0001\n",
      "   120        0.4684             nan     0.1000   -0.0004\n",
      "   140        0.4409             nan     0.1000   -0.0007\n",
      "   160        0.4166             nan     0.1000   -0.0002\n",
      "   180        0.3954             nan     0.1000   -0.0013\n",
      "   200        0.3726             nan     0.1000   -0.0004\n",
      "   220        0.3533             nan     0.1000   -0.0006\n",
      "   240        0.3342             nan     0.1000   -0.0006\n",
      "   250        0.3241             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0424             nan     0.1000    0.0331\n",
      "     2        0.9900             nan     0.1000    0.0241\n",
      "     3        0.9431             nan     0.1000    0.0200\n",
      "     4        0.9088             nan     0.1000    0.0146\n",
      "     5        0.8741             nan     0.1000    0.0146\n",
      "     6        0.8479             nan     0.1000    0.0109\n",
      "     7        0.8219             nan     0.1000    0.0095\n",
      "     8        0.7994             nan     0.1000    0.0087\n",
      "     9        0.7802             nan     0.1000    0.0060\n",
      "    10        0.7641             nan     0.1000    0.0060\n",
      "    20        0.6529             nan     0.1000    0.0013\n",
      "    40        0.5435             nan     0.1000   -0.0004\n",
      "    60        0.4748             nan     0.1000   -0.0007\n",
      "    80        0.4180             nan     0.1000   -0.0006\n",
      "   100        0.3707             nan     0.1000   -0.0007\n",
      "   120        0.3333             nan     0.1000   -0.0005\n",
      "   140        0.3006             nan     0.1000   -0.0004\n",
      "   160        0.2704             nan     0.1000   -0.0005\n",
      "   180        0.2425             nan     0.1000   -0.0005\n",
      "   200        0.2200             nan     0.1000   -0.0006\n",
      "   220        0.1990             nan     0.1000   -0.0003\n",
      "   240        0.1821             nan     0.1000   -0.0004\n",
      "   250        0.1741             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold10.Rep4: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1114             nan     0.0100    0.0018\n",
      "     2        1.1081             nan     0.0100    0.0018\n",
      "     3        1.1048             nan     0.0100    0.0017\n",
      "     4        1.1014             nan     0.0100    0.0017\n",
      "     5        1.0981             nan     0.0100    0.0017\n",
      "     6        1.0949             nan     0.0100    0.0016\n",
      "     7        1.0916             nan     0.0100    0.0016\n",
      "     8        1.0883             nan     0.0100    0.0016\n",
      "     9        1.0850             nan     0.0100    0.0015\n",
      "    10        1.0825             nan     0.0100    0.0012\n",
      "    20        1.0550             nan     0.0100    0.0013\n",
      "    40        1.0118             nan     0.0100    0.0008\n",
      "    60        0.9763             nan     0.0100    0.0006\n",
      "    80        0.9465             nan     0.0100    0.0006\n",
      "   100        0.9197             nan     0.0100    0.0005\n",
      "   120        0.9002             nan     0.0100    0.0004\n",
      "   140        0.8816             nan     0.0100    0.0004\n",
      "   160        0.8669             nan     0.0100    0.0003\n",
      "   180        0.8523             nan     0.0100    0.0003\n",
      "   200        0.8405             nan     0.0100    0.0002\n",
      "   220        0.8293             nan     0.0100    0.0003\n",
      "   240        0.8190             nan     0.0100    0.0001\n",
      "   250        0.8146             nan     0.0100    0.0001\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1086             nan     0.0100    0.0030\n",
      "     2        1.1024             nan     0.0100    0.0030\n",
      "     3        1.0962             nan     0.0100    0.0030\n",
      "     4        1.0898             nan     0.0100    0.0029\n",
      "     5        1.0840             nan     0.0100    0.0027\n",
      "     6        1.0782             nan     0.0100    0.0028\n",
      "     7        1.0725             nan     0.0100    0.0026\n",
      "     8        1.0666             nan     0.0100    0.0027\n",
      "     9        1.0610             nan     0.0100    0.0028\n",
      "    10        1.0555             nan     0.0100    0.0025\n",
      "    20        1.0079             nan     0.0100    0.0021\n",
      "    40        0.9341             nan     0.0100    0.0015\n",
      "    60        0.8795             nan     0.0100    0.0013\n",
      "    80        0.8365             nan     0.0100    0.0009\n",
      "   100        0.8022             nan     0.0100    0.0008\n",
      "   120        0.7752             nan     0.0100    0.0005\n",
      "   140        0.7524             nan     0.0100    0.0004\n",
      "   160        0.7336             nan     0.0100    0.0003\n",
      "   180        0.7172             nan     0.0100    0.0002\n",
      "   200        0.7028             nan     0.0100    0.0002\n",
      "   220        0.6905             nan     0.0100    0.0003\n",
      "   240        0.6796             nan     0.0100    0.0002\n",
      "   250        0.6742             nan     0.0100    0.0001\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0033\n",
      "     2        1.1002             nan     0.0100    0.0034\n",
      "     3        1.0931             nan     0.0100    0.0033\n",
      "     4        1.0865             nan     0.0100    0.0030\n",
      "     5        1.0796             nan     0.0100    0.0027\n",
      "     6        1.0731             nan     0.0100    0.0030\n",
      "     7        1.0669             nan     0.0100    0.0026\n",
      "     8        1.0607             nan     0.0100    0.0026\n",
      "     9        1.0549             nan     0.0100    0.0025\n",
      "    10        1.0492             nan     0.0100    0.0024\n",
      "    20        0.9958             nan     0.0100    0.0022\n",
      "    40        0.9118             nan     0.0100    0.0014\n",
      "    60        0.8492             nan     0.0100    0.0011\n",
      "    80        0.8023             nan     0.0100    0.0007\n",
      "   100        0.7628             nan     0.0100    0.0006\n",
      "   120        0.7316             nan     0.0100    0.0004\n",
      "   140        0.7047             nan     0.0100    0.0004\n",
      "   160        0.6834             nan     0.0100    0.0004\n",
      "   180        0.6635             nan     0.0100    0.0002\n",
      "   200        0.6466             nan     0.0100    0.0001\n",
      "   220        0.6315             nan     0.0100    0.0002\n",
      "   240        0.6180             nan     0.0100    0.0001\n",
      "   250        0.6121             nan     0.0100    0.0000\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0979             nan     0.0500    0.0089\n",
      "     2        1.0832             nan     0.0500    0.0080\n",
      "     3        1.0693             nan     0.0500    0.0073\n",
      "     4        1.0579             nan     0.0500    0.0060\n",
      "     5        1.0460             nan     0.0500    0.0065\n",
      "     6        1.0340             nan     0.0500    0.0060\n",
      "     7        1.0221             nan     0.0500    0.0054\n",
      "     8        1.0124             nan     0.0500    0.0045\n",
      "     9        1.0036             nan     0.0500    0.0047\n",
      "    10        0.9939             nan     0.0500    0.0048\n",
      "    20        0.9266             nan     0.0500    0.0016\n",
      "    40        0.8409             nan     0.0500    0.0014\n",
      "    60        0.7925             nan     0.0500    0.0008\n",
      "    80        0.7596             nan     0.0500    0.0005\n",
      "   100        0.7368             nan     0.0500    0.0001\n",
      "   120        0.7208             nan     0.0500   -0.0000\n",
      "   140        0.7059             nan     0.0500   -0.0001\n",
      "   160        0.6955             nan     0.0500   -0.0001\n",
      "   180        0.6875             nan     0.0500    0.0000\n",
      "   200        0.6804             nan     0.0500   -0.0001\n",
      "   220        0.6724             nan     0.0500   -0.0002\n",
      "   240        0.6669             nan     0.0500    0.0000\n",
      "   250        0.6644             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0822             nan     0.0500    0.0160\n",
      "     2        1.0544             nan     0.0500    0.0124\n",
      "     3        1.0275             nan     0.0500    0.0126\n",
      "     4        1.0038             nan     0.0500    0.0108\n",
      "     5        0.9829             nan     0.0500    0.0096\n",
      "     6        0.9652             nan     0.0500    0.0069\n",
      "     7        0.9481             nan     0.0500    0.0072\n",
      "     8        0.9331             nan     0.0500    0.0066\n",
      "     9        0.9184             nan     0.0500    0.0059\n",
      "    10        0.9041             nan     0.0500    0.0068\n",
      "    20        0.8045             nan     0.0500    0.0038\n",
      "    40        0.7013             nan     0.0500    0.0012\n",
      "    60        0.6509             nan     0.0500    0.0004\n",
      "    80        0.6168             nan     0.0500    0.0001\n",
      "   100        0.5899             nan     0.0500   -0.0002\n",
      "   120        0.5653             nan     0.0500   -0.0002\n",
      "   140        0.5447             nan     0.0500   -0.0002\n",
      "   160        0.5263             nan     0.0500   -0.0006\n",
      "   180        0.5099             nan     0.0500   -0.0002\n",
      "   200        0.4932             nan     0.0500   -0.0004\n",
      "   220        0.4776             nan     0.0500   -0.0001\n",
      "   240        0.4624             nan     0.0500   -0.0005\n",
      "   250        0.4554             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0794             nan     0.0500    0.0175\n",
      "     2        1.0472             nan     0.0500    0.0147\n",
      "     3        1.0174             nan     0.0500    0.0156\n",
      "     4        0.9923             nan     0.0500    0.0106\n",
      "     5        0.9689             nan     0.0500    0.0105\n",
      "     6        0.9484             nan     0.0500    0.0086\n",
      "     7        0.9275             nan     0.0500    0.0086\n",
      "     8        0.9087             nan     0.0500    0.0079\n",
      "     9        0.8926             nan     0.0500    0.0063\n",
      "    10        0.8784             nan     0.0500    0.0052\n",
      "    20        0.7608             nan     0.0500    0.0037\n",
      "    40        0.6470             nan     0.0500    0.0003\n",
      "    60        0.5839             nan     0.0500    0.0000\n",
      "    80        0.5394             nan     0.0500   -0.0004\n",
      "   100        0.5032             nan     0.0500   -0.0002\n",
      "   120        0.4680             nan     0.0500   -0.0002\n",
      "   140        0.4404             nan     0.0500   -0.0001\n",
      "   160        0.4151             nan     0.0500   -0.0002\n",
      "   180        0.3930             nan     0.0500   -0.0004\n",
      "   200        0.3702             nan     0.0500   -0.0005\n",
      "   220        0.3494             nan     0.0500   -0.0003\n",
      "   240        0.3329             nan     0.0500   -0.0002\n",
      "   250        0.3235             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0799             nan     0.1000    0.0172\n",
      "     2        1.0512             nan     0.1000    0.0139\n",
      "     3        1.0294             nan     0.1000    0.0087\n",
      "     4        1.0063             nan     0.1000    0.0110\n",
      "     5        0.9862             nan     0.1000    0.0087\n",
      "     6        0.9717             nan     0.1000    0.0061\n",
      "     7        0.9546             nan     0.1000    0.0087\n",
      "     8        0.9380             nan     0.1000    0.0067\n",
      "     9        0.9254             nan     0.1000    0.0056\n",
      "    10        0.9152             nan     0.1000    0.0036\n",
      "    20        0.8334             nan     0.1000    0.0033\n",
      "    40        0.7567             nan     0.1000    0.0005\n",
      "    60        0.7187             nan     0.1000    0.0001\n",
      "    80        0.6959             nan     0.1000   -0.0001\n",
      "   100        0.6779             nan     0.1000   -0.0001\n",
      "   120        0.6654             nan     0.1000   -0.0004\n",
      "   140        0.6566             nan     0.1000   -0.0003\n",
      "   160        0.6470             nan     0.1000   -0.0002\n",
      "   180        0.6374             nan     0.1000   -0.0002\n",
      "   200        0.6314             nan     0.1000    0.0001\n",
      "   220        0.6248             nan     0.1000   -0.0005\n",
      "   240        0.6194             nan     0.1000   -0.0005\n",
      "   250        0.6170             nan     0.1000    0.0000\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0517             nan     0.1000    0.0304\n",
      "     2        0.9995             nan     0.1000    0.0271\n",
      "     3        0.9609             nan     0.1000    0.0189\n",
      "     4        0.9291             nan     0.1000    0.0152\n",
      "     5        0.8995             nan     0.1000    0.0141\n",
      "     6        0.8790             nan     0.1000    0.0084\n",
      "     7        0.8558             nan     0.1000    0.0117\n",
      "     8        0.8345             nan     0.1000    0.0104\n",
      "     9        0.8179             nan     0.1000    0.0068\n",
      "    10        0.7987             nan     0.1000    0.0076\n",
      "    20        0.7021             nan     0.1000    0.0007\n",
      "    40        0.6184             nan     0.1000   -0.0001\n",
      "    60        0.5697             nan     0.1000   -0.0009\n",
      "    80        0.5310             nan     0.1000   -0.0013\n",
      "   100        0.4941             nan     0.1000   -0.0010\n",
      "   120        0.4617             nan     0.1000   -0.0011\n",
      "   140        0.4356             nan     0.1000   -0.0009\n",
      "   160        0.4070             nan     0.1000   -0.0008\n",
      "   180        0.3851             nan     0.1000   -0.0002\n",
      "   200        0.3632             nan     0.1000   -0.0004\n",
      "   220        0.3431             nan     0.1000   -0.0004\n",
      "   240        0.3266             nan     0.1000   -0.0005\n",
      "   250        0.3181             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold01.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0437             nan     0.1000    0.0306\n",
      "     2        0.9918             nan     0.1000    0.0240\n",
      "     3        0.9482             nan     0.1000    0.0198\n",
      "     4        0.9121             nan     0.1000    0.0151\n",
      "     5        0.8774             nan     0.1000    0.0158\n",
      "     6        0.8490             nan     0.1000    0.0103\n",
      "     7        0.8261             nan     0.1000    0.0082\n",
      "     8        0.8011             nan     0.1000    0.0098\n",
      "     9        0.7802             nan     0.1000    0.0067\n",
      "    10        0.7641             nan     0.1000    0.0052\n",
      "    20        0.6485             nan     0.1000    0.0008\n",
      "    40        0.5389             nan     0.1000   -0.0004\n",
      "    60        0.4703             nan     0.1000   -0.0005\n",
      "    80        0.4178             nan     0.1000   -0.0006\n",
      "   100        0.3721             nan     0.1000   -0.0008\n",
      "   120        0.3354             nan     0.1000   -0.0009\n",
      "   140        0.3042             nan     0.1000   -0.0007\n",
      "   160        0.2739             nan     0.1000   -0.0003\n",
      "   180        0.2464             nan     0.1000   -0.0005\n",
      "   200        0.2235             nan     0.1000   -0.0007\n",
      "   220        0.2045             nan     0.1000   -0.0006\n",
      "   240        0.1849             nan     0.1000   -0.0004\n",
      "   250        0.1765             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold01.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1110             nan     0.0100    0.0018\n",
      "     2        1.1072             nan     0.0100    0.0018\n",
      "     3        1.1031             nan     0.0100    0.0017\n",
      "     4        1.0992             nan     0.0100    0.0017\n",
      "     5        1.0958             nan     0.0100    0.0017\n",
      "     6        1.0927             nan     0.0100    0.0016\n",
      "     7        1.0894             nan     0.0100    0.0016\n",
      "     8        1.0862             nan     0.0100    0.0016\n",
      "     9        1.0833             nan     0.0100    0.0016\n",
      "    10        1.0802             nan     0.0100    0.0015\n",
      "    20        1.0530             nan     0.0100    0.0013\n",
      "    40        1.0096             nan     0.0100    0.0007\n",
      "    60        0.9741             nan     0.0100    0.0007\n",
      "    80        0.9457             nan     0.0100    0.0006\n",
      "   100        0.9207             nan     0.0100    0.0005\n",
      "   120        0.9007             nan     0.0100    0.0003\n",
      "   140        0.8822             nan     0.0100    0.0003\n",
      "   160        0.8657             nan     0.0100    0.0003\n",
      "   180        0.8522             nan     0.0100    0.0004\n",
      "   200        0.8394             nan     0.0100    0.0003\n",
      "   220        0.8277             nan     0.0100    0.0003\n",
      "   240        0.8171             nan     0.0100    0.0002\n",
      "   250        0.8118             nan     0.0100    0.0002\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1078             nan     0.0100    0.0033\n",
      "     2        1.1014             nan     0.0100    0.0030\n",
      "     3        1.0950             nan     0.0100    0.0030\n",
      "     4        1.0887             nan     0.0100    0.0030\n",
      "     5        1.0828             nan     0.0100    0.0029\n",
      "     6        1.0769             nan     0.0100    0.0029\n",
      "     7        1.0708             nan     0.0100    0.0027\n",
      "     8        1.0652             nan     0.0100    0.0026\n",
      "     9        1.0597             nan     0.0100    0.0025\n",
      "    10        1.0541             nan     0.0100    0.0027\n",
      "    20        1.0068             nan     0.0100    0.0019\n",
      "    40        0.9314             nan     0.0100    0.0017\n",
      "    60        0.8754             nan     0.0100    0.0010\n",
      "    80        0.8319             nan     0.0100    0.0008\n",
      "   100        0.7993             nan     0.0100    0.0007\n",
      "   120        0.7737             nan     0.0100    0.0004\n",
      "   140        0.7518             nan     0.0100    0.0005\n",
      "   160        0.7316             nan     0.0100    0.0003\n",
      "   180        0.7153             nan     0.0100    0.0002\n",
      "   200        0.7012             nan     0.0100    0.0002\n",
      "   220        0.6885             nan     0.0100    0.0001\n",
      "   240        0.6773             nan     0.0100    0.0001\n",
      "   250        0.6722             nan     0.0100    0.0001\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0034\n",
      "     2        1.1005             nan     0.0100    0.0033\n",
      "     3        1.0936             nan     0.0100    0.0031\n",
      "     4        1.0870             nan     0.0100    0.0029\n",
      "     5        1.0795             nan     0.0100    0.0037\n",
      "     6        1.0728             nan     0.0100    0.0032\n",
      "     7        1.0665             nan     0.0100    0.0030\n",
      "     8        1.0596             nan     0.0100    0.0031\n",
      "     9        1.0532             nan     0.0100    0.0031\n",
      "    10        1.0469             nan     0.0100    0.0027\n",
      "    20        0.9914             nan     0.0100    0.0021\n",
      "    40        0.9094             nan     0.0100    0.0013\n",
      "    60        0.8479             nan     0.0100    0.0011\n",
      "    80        0.7993             nan     0.0100    0.0010\n",
      "   100        0.7620             nan     0.0100    0.0006\n",
      "   120        0.7325             nan     0.0100    0.0004\n",
      "   140        0.7066             nan     0.0100    0.0004\n",
      "   160        0.6843             nan     0.0100    0.0002\n",
      "   180        0.6646             nan     0.0100    0.0001\n",
      "   200        0.6475             nan     0.0100    0.0002\n",
      "   220        0.6325             nan     0.0100    0.0001\n",
      "   240        0.6186             nan     0.0100    0.0002\n",
      "   250        0.6119             nan     0.0100    0.0001\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0973             nan     0.0500    0.0091\n",
      "     2        1.0806             nan     0.0500    0.0083\n",
      "     3        1.0684             nan     0.0500    0.0055\n",
      "     4        1.0531             nan     0.0500    0.0073\n",
      "     5        1.0426             nan     0.0500    0.0049\n",
      "     6        1.0296             nan     0.0500    0.0066\n",
      "     7        1.0182             nan     0.0500    0.0059\n",
      "     8        1.0077             nan     0.0500    0.0054\n",
      "     9        0.9991             nan     0.0500    0.0035\n",
      "    10        0.9911             nan     0.0500    0.0035\n",
      "    20        0.9170             nan     0.0500    0.0025\n",
      "    40        0.8357             nan     0.0500    0.0012\n",
      "    60        0.7902             nan     0.0500    0.0005\n",
      "    80        0.7556             nan     0.0500    0.0005\n",
      "   100        0.7327             nan     0.0500   -0.0002\n",
      "   120        0.7161             nan     0.0500    0.0003\n",
      "   140        0.7036             nan     0.0500    0.0002\n",
      "   160        0.6946             nan     0.0500   -0.0000\n",
      "   180        0.6872             nan     0.0500    0.0000\n",
      "   200        0.6797             nan     0.0500   -0.0002\n",
      "   220        0.6728             nan     0.0500   -0.0001\n",
      "   240        0.6671             nan     0.0500   -0.0001\n",
      "   250        0.6639             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0828             nan     0.0500    0.0139\n",
      "     2        1.0555             nan     0.0500    0.0125\n",
      "     3        1.0296             nan     0.0500    0.0127\n",
      "     4        1.0049             nan     0.0500    0.0106\n",
      "     5        0.9857             nan     0.0500    0.0090\n",
      "     6        0.9639             nan     0.0500    0.0109\n",
      "     7        0.9445             nan     0.0500    0.0085\n",
      "     8        0.9290             nan     0.0500    0.0069\n",
      "     9        0.9132             nan     0.0500    0.0072\n",
      "    10        0.8989             nan     0.0500    0.0064\n",
      "    20        0.8003             nan     0.0500    0.0029\n",
      "    40        0.7026             nan     0.0500    0.0003\n",
      "    60        0.6530             nan     0.0500   -0.0002\n",
      "    80        0.6212             nan     0.0500    0.0001\n",
      "   100        0.5942             nan     0.0500   -0.0003\n",
      "   120        0.5705             nan     0.0500   -0.0002\n",
      "   140        0.5508             nan     0.0500   -0.0002\n",
      "   160        0.5316             nan     0.0500   -0.0002\n",
      "   180        0.5117             nan     0.0500   -0.0002\n",
      "   200        0.4969             nan     0.0500   -0.0006\n",
      "   220        0.4790             nan     0.0500   -0.0003\n",
      "   240        0.4640             nan     0.0500   -0.0002\n",
      "   250        0.4566             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0758             nan     0.0500    0.0176\n",
      "     2        1.0447             nan     0.0500    0.0135\n",
      "     3        1.0184             nan     0.0500    0.0113\n",
      "     4        0.9910             nan     0.0500    0.0127\n",
      "     5        0.9661             nan     0.0500    0.0111\n",
      "     6        0.9443             nan     0.0500    0.0105\n",
      "     7        0.9253             nan     0.0500    0.0082\n",
      "     8        0.9072             nan     0.0500    0.0070\n",
      "     9        0.8892             nan     0.0500    0.0081\n",
      "    10        0.8732             nan     0.0500    0.0073\n",
      "    20        0.7642             nan     0.0500    0.0015\n",
      "    40        0.6505             nan     0.0500    0.0002\n",
      "    60        0.5870             nan     0.0500    0.0006\n",
      "    80        0.5407             nan     0.0500   -0.0005\n",
      "   100        0.5024             nan     0.0500   -0.0007\n",
      "   120        0.4712             nan     0.0500   -0.0002\n",
      "   140        0.4421             nan     0.0500   -0.0004\n",
      "   160        0.4162             nan     0.0500   -0.0008\n",
      "   180        0.3923             nan     0.0500   -0.0005\n",
      "   200        0.3711             nan     0.0500   -0.0003\n",
      "   220        0.3504             nan     0.0500   -0.0004\n",
      "   240        0.3334             nan     0.0500   -0.0005\n",
      "   250        0.3236             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0776             nan     0.1000    0.0171\n",
      "     2        1.0510             nan     0.1000    0.0141\n",
      "     3        1.0281             nan     0.1000    0.0104\n",
      "     4        1.0058             nan     0.1000    0.0115\n",
      "     5        0.9910             nan     0.1000    0.0055\n",
      "     6        0.9743             nan     0.1000    0.0070\n",
      "     7        0.9565             nan     0.1000    0.0091\n",
      "     8        0.9416             nan     0.1000    0.0076\n",
      "     9        0.9318             nan     0.1000    0.0046\n",
      "    10        0.9194             nan     0.1000    0.0061\n",
      "    20        0.8364             nan     0.1000    0.0023\n",
      "    40        0.7527             nan     0.1000    0.0009\n",
      "    60        0.7170             nan     0.1000    0.0003\n",
      "    80        0.6955             nan     0.1000   -0.0003\n",
      "   100        0.6757             nan     0.1000   -0.0001\n",
      "   120        0.6641             nan     0.1000   -0.0001\n",
      "   140        0.6549             nan     0.1000   -0.0002\n",
      "   160        0.6458             nan     0.1000   -0.0003\n",
      "   180        0.6398             nan     0.1000   -0.0003\n",
      "   200        0.6336             nan     0.1000   -0.0001\n",
      "   220        0.6278             nan     0.1000   -0.0003\n",
      "   240        0.6229             nan     0.1000   -0.0002\n",
      "   250        0.6208             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0496             nan     0.1000    0.0309\n",
      "     2        0.9982             nan     0.1000    0.0243\n",
      "     3        0.9541             nan     0.1000    0.0182\n",
      "     4        0.9189             nan     0.1000    0.0141\n",
      "     5        0.8907             nan     0.1000    0.0125\n",
      "     6        0.8673             nan     0.1000    0.0094\n",
      "     7        0.8479             nan     0.1000    0.0085\n",
      "     8        0.8294             nan     0.1000    0.0086\n",
      "     9        0.8110             nan     0.1000    0.0087\n",
      "    10        0.7950             nan     0.1000    0.0066\n",
      "    20        0.6980             nan     0.1000    0.0019\n",
      "    40        0.6181             nan     0.1000   -0.0003\n",
      "    60        0.5670             nan     0.1000   -0.0012\n",
      "    80        0.5284             nan     0.1000   -0.0004\n",
      "   100        0.4937             nan     0.1000    0.0002\n",
      "   120        0.4615             nan     0.1000   -0.0006\n",
      "   140        0.4348             nan     0.1000   -0.0011\n",
      "   160        0.4074             nan     0.1000   -0.0002\n",
      "   180        0.3807             nan     0.1000   -0.0005\n",
      "   200        0.3595             nan     0.1000   -0.0001\n",
      "   220        0.3383             nan     0.1000   -0.0008\n",
      "   240        0.3204             nan     0.1000   -0.0005\n",
      "   250        0.3096             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold02.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0435             nan     0.1000    0.0333\n",
      "     2        0.9845             nan     0.1000    0.0269\n",
      "     3        0.9404             nan     0.1000    0.0208\n",
      "     4        0.9036             nan     0.1000    0.0154\n",
      "     5        0.8696             nan     0.1000    0.0148\n",
      "     6        0.8404             nan     0.1000    0.0127\n",
      "     7        0.8169             nan     0.1000    0.0097\n",
      "     8        0.7944             nan     0.1000    0.0079\n",
      "     9        0.7731             nan     0.1000    0.0082\n",
      "    10        0.7569             nan     0.1000    0.0062\n",
      "    20        0.6528             nan     0.1000    0.0025\n",
      "    40        0.5476             nan     0.1000   -0.0011\n",
      "    60        0.4784             nan     0.1000   -0.0001\n",
      "    80        0.4243             nan     0.1000   -0.0007\n",
      "   100        0.3766             nan     0.1000    0.0002\n",
      "   120        0.3339             nan     0.1000   -0.0004\n",
      "   140        0.2990             nan     0.1000   -0.0005\n",
      "   160        0.2713             nan     0.1000   -0.0008\n",
      "   180        0.2472             nan     0.1000   -0.0006\n",
      "   200        0.2239             nan     0.1000   -0.0005\n",
      "   220        0.2017             nan     0.1000   -0.0007\n",
      "   240        0.1829             nan     0.1000   -0.0002\n",
      "   250        0.1738             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold02.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1104             nan     0.0100    0.0019\n",
      "     2        1.1071             nan     0.0100    0.0018\n",
      "     3        1.1036             nan     0.0100    0.0018\n",
      "     4        1.1001             nan     0.0100    0.0018\n",
      "     5        1.0964             nan     0.0100    0.0017\n",
      "     6        1.0929             nan     0.0100    0.0017\n",
      "     7        1.0895             nan     0.0100    0.0017\n",
      "     8        1.0862             nan     0.0100    0.0016\n",
      "     9        1.0832             nan     0.0100    0.0016\n",
      "    10        1.0800             nan     0.0100    0.0016\n",
      "    20        1.0529             nan     0.0100    0.0008\n",
      "    40        1.0073             nan     0.0100    0.0010\n",
      "    60        0.9732             nan     0.0100    0.0007\n",
      "    80        0.9438             nan     0.0100    0.0007\n",
      "   100        0.9206             nan     0.0100    0.0006\n",
      "   120        0.8996             nan     0.0100    0.0003\n",
      "   140        0.8822             nan     0.0100    0.0004\n",
      "   160        0.8657             nan     0.0100    0.0003\n",
      "   180        0.8513             nan     0.0100    0.0003\n",
      "   200        0.8392             nan     0.0100    0.0002\n",
      "   220        0.8274             nan     0.0100    0.0003\n",
      "   240        0.8168             nan     0.0100    0.0002\n",
      "   250        0.8125             nan     0.0100    0.0002\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1074             nan     0.0100    0.0034\n",
      "     2        1.1006             nan     0.0100    0.0028\n",
      "     3        1.0948             nan     0.0100    0.0027\n",
      "     4        1.0885             nan     0.0100    0.0030\n",
      "     5        1.0827             nan     0.0100    0.0028\n",
      "     6        1.0770             nan     0.0100    0.0028\n",
      "     7        1.0711             nan     0.0100    0.0027\n",
      "     8        1.0656             nan     0.0100    0.0025\n",
      "     9        1.0602             nan     0.0100    0.0025\n",
      "    10        1.0547             nan     0.0100    0.0025\n",
      "    20        1.0073             nan     0.0100    0.0020\n",
      "    40        0.9336             nan     0.0100    0.0013\n",
      "    60        0.8783             nan     0.0100    0.0011\n",
      "    80        0.8363             nan     0.0100    0.0009\n",
      "   100        0.8030             nan     0.0100    0.0005\n",
      "   120        0.7763             nan     0.0100    0.0004\n",
      "   140        0.7538             nan     0.0100    0.0004\n",
      "   160        0.7352             nan     0.0100    0.0002\n",
      "   180        0.7185             nan     0.0100    0.0001\n",
      "   200        0.7040             nan     0.0100    0.0002\n",
      "   220        0.6910             nan     0.0100    0.0000\n",
      "   240        0.6798             nan     0.0100    0.0002\n",
      "   250        0.6746             nan     0.0100    0.0001\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1075             nan     0.0100    0.0030\n",
      "     2        1.1003             nan     0.0100    0.0035\n",
      "     3        1.0931             nan     0.0100    0.0034\n",
      "     4        1.0865             nan     0.0100    0.0028\n",
      "     5        1.0799             nan     0.0100    0.0028\n",
      "     6        1.0735             nan     0.0100    0.0028\n",
      "     7        1.0672             nan     0.0100    0.0029\n",
      "     8        1.0608             nan     0.0100    0.0028\n",
      "     9        1.0544             nan     0.0100    0.0029\n",
      "    10        1.0481             nan     0.0100    0.0032\n",
      "    20        0.9945             nan     0.0100    0.0021\n",
      "    40        0.9121             nan     0.0100    0.0013\n",
      "    60        0.8507             nan     0.0100    0.0013\n",
      "    80        0.8042             nan     0.0100    0.0008\n",
      "   100        0.7650             nan     0.0100    0.0007\n",
      "   120        0.7347             nan     0.0100    0.0005\n",
      "   140        0.7087             nan     0.0100    0.0003\n",
      "   160        0.6862             nan     0.0100    0.0004\n",
      "   180        0.6671             nan     0.0100    0.0002\n",
      "   200        0.6503             nan     0.0100    0.0001\n",
      "   220        0.6353             nan     0.0100    0.0000\n",
      "   240        0.6216             nan     0.0100    0.0002\n",
      "   250        0.6152             nan     0.0100   -0.0000\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0950             nan     0.0500    0.0091\n",
      "     2        1.0793             nan     0.0500    0.0082\n",
      "     3        1.0651             nan     0.0500    0.0073\n",
      "     4        1.0513             nan     0.0500    0.0068\n",
      "     5        1.0382             nan     0.0500    0.0060\n",
      "     6        1.0276             nan     0.0500    0.0045\n",
      "     7        1.0167             nan     0.0500    0.0055\n",
      "     8        1.0079             nan     0.0500    0.0035\n",
      "     9        0.9987             nan     0.0500    0.0040\n",
      "    10        0.9909             nan     0.0500    0.0034\n",
      "    20        0.9190             nan     0.0500    0.0023\n",
      "    40        0.8382             nan     0.0500    0.0009\n",
      "    60        0.7897             nan     0.0500    0.0003\n",
      "    80        0.7552             nan     0.0500    0.0005\n",
      "   100        0.7329             nan     0.0500    0.0002\n",
      "   120        0.7187             nan     0.0500    0.0000\n",
      "   140        0.7050             nan     0.0500    0.0002\n",
      "   160        0.6950             nan     0.0500   -0.0001\n",
      "   180        0.6860             nan     0.0500    0.0001\n",
      "   200        0.6787             nan     0.0500    0.0001\n",
      "   220        0.6714             nan     0.0500   -0.0002\n",
      "   240        0.6657             nan     0.0500    0.0000\n",
      "   250        0.6627             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0822             nan     0.0500    0.0162\n",
      "     2        1.0552             nan     0.0500    0.0131\n",
      "     3        1.0301             nan     0.0500    0.0123\n",
      "     4        1.0064             nan     0.0500    0.0104\n",
      "     5        0.9860             nan     0.0500    0.0090\n",
      "     6        0.9687             nan     0.0500    0.0081\n",
      "     7        0.9500             nan     0.0500    0.0095\n",
      "     8        0.9347             nan     0.0500    0.0072\n",
      "     9        0.9184             nan     0.0500    0.0069\n",
      "    10        0.9039             nan     0.0500    0.0064\n",
      "    20        0.8019             nan     0.0500    0.0036\n",
      "    40        0.7053             nan     0.0500    0.0008\n",
      "    60        0.6549             nan     0.0500   -0.0001\n",
      "    80        0.6201             nan     0.0500   -0.0002\n",
      "   100        0.5914             nan     0.0500    0.0002\n",
      "   120        0.5666             nan     0.0500   -0.0000\n",
      "   140        0.5443             nan     0.0500   -0.0001\n",
      "   160        0.5266             nan     0.0500   -0.0003\n",
      "   180        0.5045             nan     0.0500   -0.0002\n",
      "   200        0.4889             nan     0.0500   -0.0003\n",
      "   220        0.4714             nan     0.0500   -0.0002\n",
      "   240        0.4557             nan     0.0500   -0.0000\n",
      "   250        0.4484             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0803             nan     0.0500    0.0155\n",
      "     2        1.0505             nan     0.0500    0.0137\n",
      "     3        1.0218             nan     0.0500    0.0126\n",
      "     4        0.9945             nan     0.0500    0.0127\n",
      "     5        0.9696             nan     0.0500    0.0114\n",
      "     6        0.9486             nan     0.0500    0.0077\n",
      "     7        0.9306             nan     0.0500    0.0074\n",
      "     8        0.9124             nan     0.0500    0.0070\n",
      "     9        0.8948             nan     0.0500    0.0071\n",
      "    10        0.8804             nan     0.0500    0.0056\n",
      "    20        0.7653             nan     0.0500    0.0029\n",
      "    40        0.6510             nan     0.0500    0.0004\n",
      "    60        0.5868             nan     0.0500    0.0001\n",
      "    80        0.5383             nan     0.0500   -0.0007\n",
      "   100        0.4987             nan     0.0500   -0.0007\n",
      "   120        0.4650             nan     0.0500   -0.0002\n",
      "   140        0.4367             nan     0.0500   -0.0003\n",
      "   160        0.4086             nan     0.0500    0.0000\n",
      "   180        0.3861             nan     0.0500   -0.0004\n",
      "   200        0.3646             nan     0.0500   -0.0005\n",
      "   220        0.3449             nan     0.0500   -0.0005\n",
      "   240        0.3244             nan     0.0500   -0.0003\n",
      "   250        0.3160             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0785             nan     0.1000    0.0179\n",
      "     2        1.0486             nan     0.1000    0.0143\n",
      "     3        1.0240             nan     0.1000    0.0115\n",
      "     4        1.0044             nan     0.1000    0.0076\n",
      "     5        0.9836             nan     0.1000    0.0088\n",
      "     6        0.9673             nan     0.1000    0.0071\n",
      "     7        0.9510             nan     0.1000    0.0070\n",
      "     8        0.9381             nan     0.1000    0.0072\n",
      "     9        0.9280             nan     0.1000    0.0047\n",
      "    10        0.9176             nan     0.1000    0.0058\n",
      "    20        0.8356             nan     0.1000    0.0030\n",
      "    40        0.7555             nan     0.1000    0.0012\n",
      "    60        0.7165             nan     0.1000    0.0002\n",
      "    80        0.6955             nan     0.1000   -0.0005\n",
      "   100        0.6779             nan     0.1000   -0.0000\n",
      "   120        0.6655             nan     0.1000    0.0002\n",
      "   140        0.6547             nan     0.1000   -0.0006\n",
      "   160        0.6474             nan     0.1000   -0.0006\n",
      "   180        0.6398             nan     0.1000    0.0000\n",
      "   200        0.6316             nan     0.1000   -0.0001\n",
      "   220        0.6262             nan     0.1000   -0.0003\n",
      "   240        0.6206             nan     0.1000   -0.0004\n",
      "   250        0.6174             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0545             nan     0.1000    0.0291\n",
      "     2        1.0047             nan     0.1000    0.0208\n",
      "     3        0.9635             nan     0.1000    0.0206\n",
      "     4        0.9297             nan     0.1000    0.0148\n",
      "     5        0.8989             nan     0.1000    0.0133\n",
      "     6        0.8717             nan     0.1000    0.0127\n",
      "     7        0.8523             nan     0.1000    0.0078\n",
      "     8        0.8319             nan     0.1000    0.0084\n",
      "     9        0.8126             nan     0.1000    0.0081\n",
      "    10        0.8004             nan     0.1000    0.0044\n",
      "    20        0.7045             nan     0.1000    0.0026\n",
      "    40        0.6165             nan     0.1000    0.0001\n",
      "    60        0.5709             nan     0.1000   -0.0005\n",
      "    80        0.5304             nan     0.1000   -0.0010\n",
      "   100        0.4962             nan     0.1000   -0.0006\n",
      "   120        0.4675             nan     0.1000   -0.0010\n",
      "   140        0.4380             nan     0.1000   -0.0008\n",
      "   160        0.4152             nan     0.1000   -0.0012\n",
      "   180        0.3884             nan     0.1000   -0.0003\n",
      "   200        0.3672             nan     0.1000   -0.0010\n",
      "   220        0.3480             nan     0.1000   -0.0006\n",
      "   240        0.3277             nan     0.1000   -0.0002\n",
      "   250        0.3189             nan     0.1000   -0.0008\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold03.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in (function (x, y, offset = NULL, misc = NULL, distribution = \"bernoulli\", :\n",
      "\"variable 37: x37 has no variation.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0480             nan     0.1000    0.0330\n",
      "     2        0.9903             nan     0.1000    0.0252\n",
      "     3        0.9457             nan     0.1000    0.0203\n",
      "     4        0.9081             nan     0.1000    0.0157\n",
      "     5        0.8773             nan     0.1000    0.0130\n",
      "     6        0.8516             nan     0.1000    0.0105\n",
      "     7        0.8286             nan     0.1000    0.0080\n",
      "     8        0.8039             nan     0.1000    0.0083\n",
      "     9        0.7827             nan     0.1000    0.0071\n",
      "    10        0.7633             nan     0.1000    0.0069\n",
      "    20        0.6500             nan     0.1000    0.0003\n",
      "    40        0.5433             nan     0.1000   -0.0008\n",
      "    60        0.4686             nan     0.1000   -0.0003\n",
      "    80        0.4123             nan     0.1000   -0.0008\n",
      "   100        0.3650             nan     0.1000   -0.0013\n",
      "   120        0.3269             nan     0.1000   -0.0008\n",
      "   140        0.2937             nan     0.1000   -0.0007\n",
      "   160        0.2648             nan     0.1000   -0.0005\n",
      "   180        0.2391             nan     0.1000   -0.0004\n",
      "   200        0.2167             nan     0.1000   -0.0006\n",
      "   220        0.1950             nan     0.1000   -0.0005\n",
      "   240        0.1753             nan     0.1000   -0.0007\n",
      "   250        0.1672             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold03.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1107             nan     0.0100    0.0018\n",
      "     2        1.1073             nan     0.0100    0.0017\n",
      "     3        1.1039             nan     0.0100    0.0017\n",
      "     4        1.1005             nan     0.0100    0.0017\n",
      "     5        1.0970             nan     0.0100    0.0016\n",
      "     6        1.0945             nan     0.0100    0.0010\n",
      "     7        1.0919             nan     0.0100    0.0010\n",
      "     8        1.0888             nan     0.0100    0.0016\n",
      "     9        1.0858             nan     0.0100    0.0016\n",
      "    10        1.0828             nan     0.0100    0.0015\n",
      "    20        1.0555             nan     0.0100    0.0013\n",
      "    40        1.0123             nan     0.0100    0.0009\n",
      "    60        0.9780             nan     0.0100    0.0006\n",
      "    80        0.9496             nan     0.0100    0.0006\n",
      "   100        0.9237             nan     0.0100    0.0004\n",
      "   120        0.9025             nan     0.0100    0.0004\n",
      "   140        0.8846             nan     0.0100    0.0004\n",
      "   160        0.8682             nan     0.0100    0.0004\n",
      "   180        0.8529             nan     0.0100    0.0004\n",
      "   200        0.8397             nan     0.0100    0.0002\n",
      "   220        0.8276             nan     0.0100    0.0003\n",
      "   240        0.8172             nan     0.0100    0.0002\n",
      "   250        0.8117             nan     0.0100    0.0002\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1076             nan     0.0100    0.0032\n",
      "     2        1.1011             nan     0.0100    0.0031\n",
      "     3        1.0950             nan     0.0100    0.0030\n",
      "     4        1.0893             nan     0.0100    0.0026\n",
      "     5        1.0836             nan     0.0100    0.0027\n",
      "     6        1.0781             nan     0.0100    0.0027\n",
      "     7        1.0723             nan     0.0100    0.0027\n",
      "     8        1.0669             nan     0.0100    0.0024\n",
      "     9        1.0613             nan     0.0100    0.0028\n",
      "    10        1.0559             nan     0.0100    0.0026\n",
      "    20        1.0082             nan     0.0100    0.0021\n",
      "    40        0.9320             nan     0.0100    0.0014\n",
      "    60        0.8763             nan     0.0100    0.0008\n",
      "    80        0.8352             nan     0.0100    0.0007\n",
      "   100        0.8016             nan     0.0100    0.0005\n",
      "   120        0.7736             nan     0.0100    0.0006\n",
      "   140        0.7510             nan     0.0100    0.0003\n",
      "   160        0.7319             nan     0.0100    0.0002\n",
      "   180        0.7157             nan     0.0100    0.0002\n",
      "   200        0.7011             nan     0.0100    0.0003\n",
      "   220        0.6881             nan     0.0100    0.0002\n",
      "   240        0.6768             nan     0.0100    0.0000\n",
      "   250        0.6717             nan     0.0100    0.0000\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1068             nan     0.0100    0.0033\n",
      "     2        1.0995             nan     0.0100    0.0034\n",
      "     3        1.0925             nan     0.0100    0.0034\n",
      "     4        1.0854             nan     0.0100    0.0031\n",
      "     5        1.0789             nan     0.0100    0.0029\n",
      "     6        1.0721             nan     0.0100    0.0032\n",
      "     7        1.0655             nan     0.0100    0.0029\n",
      "     8        1.0592             nan     0.0100    0.0031\n",
      "     9        1.0535             nan     0.0100    0.0026\n",
      "    10        1.0476             nan     0.0100    0.0028\n",
      "    20        0.9937             nan     0.0100    0.0020\n",
      "    40        0.9107             nan     0.0100    0.0013\n",
      "    60        0.8487             nan     0.0100    0.0011\n",
      "    80        0.8021             nan     0.0100    0.0007\n",
      "   100        0.7642             nan     0.0100    0.0005\n",
      "   120        0.7344             nan     0.0100    0.0003\n",
      "   140        0.7090             nan     0.0100    0.0004\n",
      "   160        0.6872             nan     0.0100    0.0002\n",
      "   180        0.6678             nan     0.0100    0.0001\n",
      "   200        0.6502             nan     0.0100    0.0002\n",
      "   220        0.6350             nan     0.0100    0.0001\n",
      "   240        0.6202             nan     0.0100    0.0001\n",
      "   250        0.6140             nan     0.0100    0.0000\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0965             nan     0.0500    0.0087\n",
      "     2        1.0806             nan     0.0500    0.0078\n",
      "     3        1.0658             nan     0.0500    0.0071\n",
      "     4        1.0526             nan     0.0500    0.0063\n",
      "     5        1.0412             nan     0.0500    0.0050\n",
      "     6        1.0299             nan     0.0500    0.0057\n",
      "     7        1.0182             nan     0.0500    0.0049\n",
      "     8        1.0077             nan     0.0500    0.0051\n",
      "     9        0.9983             nan     0.0500    0.0041\n",
      "    10        0.9890             nan     0.0500    0.0045\n",
      "    20        0.9213             nan     0.0500    0.0031\n",
      "    40        0.8392             nan     0.0500    0.0011\n",
      "    60        0.7883             nan     0.0500    0.0007\n",
      "    80        0.7568             nan     0.0500    0.0006\n",
      "   100        0.7340             nan     0.0500    0.0004\n",
      "   120        0.7161             nan     0.0500    0.0001\n",
      "   140        0.7023             nan     0.0500    0.0002\n",
      "   160        0.6922             nan     0.0500   -0.0003\n",
      "   180        0.6825             nan     0.0500    0.0000\n",
      "   200        0.6750             nan     0.0500   -0.0000\n",
      "   220        0.6680             nan     0.0500    0.0000\n",
      "   240        0.6626             nan     0.0500   -0.0001\n",
      "   250        0.6599             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0811             nan     0.0500    0.0150\n",
      "     2        1.0528             nan     0.0500    0.0135\n",
      "     3        1.0260             nan     0.0500    0.0129\n",
      "     4        1.0029             nan     0.0500    0.0117\n",
      "     5        0.9822             nan     0.0500    0.0099\n",
      "     6        0.9630             nan     0.0500    0.0092\n",
      "     7        0.9464             nan     0.0500    0.0076\n",
      "     8        0.9309             nan     0.0500    0.0063\n",
      "     9        0.9158             nan     0.0500    0.0069\n",
      "    10        0.9007             nan     0.0500    0.0070\n",
      "    20        0.8016             nan     0.0500    0.0027\n",
      "    40        0.7049             nan     0.0500    0.0004\n",
      "    60        0.6501             nan     0.0500   -0.0001\n",
      "    80        0.6149             nan     0.0500    0.0004\n",
      "   100        0.5878             nan     0.0500   -0.0001\n",
      "   120        0.5639             nan     0.0500   -0.0003\n",
      "   140        0.5426             nan     0.0500   -0.0007\n",
      "   160        0.5233             nan     0.0500   -0.0003\n",
      "   180        0.5060             nan     0.0500   -0.0005\n",
      "   200        0.4906             nan     0.0500   -0.0003\n",
      "   220        0.4757             nan     0.0500   -0.0005\n",
      "   240        0.4605             nan     0.0500   -0.0005\n",
      "   250        0.4535             nan     0.0500   -0.0000\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0779             nan     0.0500    0.0175\n",
      "     2        1.0462             nan     0.0500    0.0136\n",
      "     3        1.0193             nan     0.0500    0.0123\n",
      "     4        0.9953             nan     0.0500    0.0103\n",
      "     5        0.9712             nan     0.0500    0.0103\n",
      "     6        0.9514             nan     0.0500    0.0090\n",
      "     7        0.9320             nan     0.0500    0.0080\n",
      "     8        0.9140             nan     0.0500    0.0072\n",
      "     9        0.8960             nan     0.0500    0.0079\n",
      "    10        0.8810             nan     0.0500    0.0063\n",
      "    20        0.7677             nan     0.0500    0.0024\n",
      "    40        0.6526             nan     0.0500    0.0003\n",
      "    60        0.5868             nan     0.0500   -0.0000\n",
      "    80        0.5412             nan     0.0500   -0.0001\n",
      "   100        0.5028             nan     0.0500   -0.0003\n",
      "   120        0.4693             nan     0.0500   -0.0004\n",
      "   140        0.4414             nan     0.0500   -0.0004\n",
      "   160        0.4139             nan     0.0500   -0.0001\n",
      "   180        0.3908             nan     0.0500   -0.0001\n",
      "   200        0.3663             nan     0.0500   -0.0002\n",
      "   220        0.3444             nan     0.0500   -0.0004\n",
      "   240        0.3271             nan     0.0500   -0.0002\n",
      "   250        0.3176             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0803             nan     0.1000    0.0170\n",
      "     2        1.0575             nan     0.1000    0.0111\n",
      "     3        1.0302             nan     0.1000    0.0133\n",
      "     4        1.0098             nan     0.1000    0.0108\n",
      "     5        0.9896             nan     0.1000    0.0091\n",
      "     6        0.9755             nan     0.1000    0.0055\n",
      "     7        0.9569             nan     0.1000    0.0085\n",
      "     8        0.9416             nan     0.1000    0.0069\n",
      "     9        0.9276             nan     0.1000    0.0060\n",
      "    10        0.9161             nan     0.1000    0.0053\n",
      "    20        0.8325             nan     0.1000    0.0033\n",
      "    40        0.7554             nan     0.1000    0.0006\n",
      "    60        0.7145             nan     0.1000    0.0003\n",
      "    80        0.6911             nan     0.1000    0.0001\n",
      "   100        0.6726             nan     0.1000    0.0001\n",
      "   120        0.6602             nan     0.1000   -0.0002\n",
      "   140        0.6517             nan     0.1000   -0.0000\n",
      "   160        0.6411             nan     0.1000   -0.0002\n",
      "   180        0.6337             nan     0.1000   -0.0008\n",
      "   200        0.6280             nan     0.1000   -0.0002\n",
      "   220        0.6230             nan     0.1000   -0.0002\n",
      "   240        0.6176             nan     0.1000   -0.0004\n",
      "   250        0.6161             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0489             nan     0.1000    0.0309\n",
      "     2        1.0027             nan     0.1000    0.0221\n",
      "     3        0.9612             nan     0.1000    0.0199\n",
      "     4        0.9276             nan     0.1000    0.0136\n",
      "     5        0.9037             nan     0.1000    0.0117\n",
      "     6        0.8762             nan     0.1000    0.0120\n",
      "     7        0.8533             nan     0.1000    0.0102\n",
      "     8        0.8321             nan     0.1000    0.0081\n",
      "     9        0.8148             nan     0.1000    0.0063\n",
      "    10        0.7997             nan     0.1000    0.0055\n",
      "    20        0.6981             nan     0.1000    0.0012\n",
      "    40        0.6161             nan     0.1000    0.0001\n",
      "    60        0.5639             nan     0.1000   -0.0001\n",
      "    80        0.5230             nan     0.1000   -0.0006\n",
      "   100        0.4879             nan     0.1000   -0.0004\n",
      "   120        0.4580             nan     0.1000   -0.0004\n",
      "   140        0.4331             nan     0.1000   -0.0006\n",
      "   160        0.4095             nan     0.1000   -0.0010\n",
      "   180        0.3865             nan     0.1000   -0.0006\n",
      "   200        0.3659             nan     0.1000   -0.0004\n",
      "   220        0.3465             nan     0.1000   -0.0003\n",
      "   240        0.3295             nan     0.1000   -0.0010\n",
      "   250        0.3212             nan     0.1000   -0.0011\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold04.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0508             nan     0.1000    0.0271\n",
      "     2        0.9921             nan     0.1000    0.0267\n",
      "     3        0.9472             nan     0.1000    0.0197\n",
      "     4        0.9081             nan     0.1000    0.0136\n",
      "     5        0.8774             nan     0.1000    0.0121\n",
      "     6        0.8470             nan     0.1000    0.0125\n",
      "     7        0.8204             nan     0.1000    0.0090\n",
      "     8        0.8019             nan     0.1000    0.0061\n",
      "     9        0.7807             nan     0.1000    0.0086\n",
      "    10        0.7603             nan     0.1000    0.0084\n",
      "    20        0.6467             nan     0.1000    0.0017\n",
      "    40        0.5391             nan     0.1000   -0.0006\n",
      "    60        0.4690             nan     0.1000    0.0000\n",
      "    80        0.4162             nan     0.1000   -0.0007\n",
      "   100        0.3719             nan     0.1000   -0.0009\n",
      "   120        0.3346             nan     0.1000   -0.0005\n",
      "   140        0.3012             nan     0.1000   -0.0010\n",
      "   160        0.2711             nan     0.1000   -0.0010\n",
      "   180        0.2448             nan     0.1000   -0.0005\n",
      "   200        0.2200             nan     0.1000   -0.0003\n",
      "   220        0.1998             nan     0.1000   -0.0001\n",
      "   240        0.1821             nan     0.1000   -0.0003\n",
      "   250        0.1740             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold04.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1112             nan     0.0100    0.0018\n",
      "     2        1.1078             nan     0.0100    0.0018\n",
      "     3        1.1042             nan     0.0100    0.0017\n",
      "     4        1.1005             nan     0.0100    0.0017\n",
      "     5        1.0974             nan     0.0100    0.0016\n",
      "     6        1.0940             nan     0.0100    0.0016\n",
      "     7        1.0907             nan     0.0100    0.0015\n",
      "     8        1.0877             nan     0.0100    0.0015\n",
      "     9        1.0843             nan     0.0100    0.0015\n",
      "    10        1.0813             nan     0.0100    0.0015\n",
      "    20        1.0560             nan     0.0100    0.0009\n",
      "    40        1.0136             nan     0.0100    0.0007\n",
      "    60        0.9786             nan     0.0100    0.0007\n",
      "    80        0.9496             nan     0.0100    0.0006\n",
      "   100        0.9251             nan     0.0100    0.0005\n",
      "   120        0.9061             nan     0.0100    0.0004\n",
      "   140        0.8875             nan     0.0100    0.0005\n",
      "   160        0.8719             nan     0.0100    0.0003\n",
      "   180        0.8574             nan     0.0100    0.0003\n",
      "   200        0.8450             nan     0.0100    0.0003\n",
      "   220        0.8338             nan     0.0100    0.0003\n",
      "   240        0.8233             nan     0.0100    0.0003\n",
      "   250        0.8184             nan     0.0100    0.0001\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1081             nan     0.0100    0.0030\n",
      "     2        1.1017             nan     0.0100    0.0030\n",
      "     3        1.0956             nan     0.0100    0.0029\n",
      "     4        1.0897             nan     0.0100    0.0027\n",
      "     5        1.0839             nan     0.0100    0.0027\n",
      "     6        1.0783             nan     0.0100    0.0027\n",
      "     7        1.0725             nan     0.0100    0.0026\n",
      "     8        1.0668             nan     0.0100    0.0026\n",
      "     9        1.0615             nan     0.0100    0.0021\n",
      "    10        1.0560             nan     0.0100    0.0027\n",
      "    20        1.0097             nan     0.0100    0.0021\n",
      "    40        0.9370             nan     0.0100    0.0015\n",
      "    60        0.8836             nan     0.0100    0.0011\n",
      "    80        0.8408             nan     0.0100    0.0007\n",
      "   100        0.8085             nan     0.0100    0.0007\n",
      "   120        0.7815             nan     0.0100    0.0005\n",
      "   140        0.7595             nan     0.0100    0.0003\n",
      "   160        0.7407             nan     0.0100    0.0003\n",
      "   180        0.7247             nan     0.0100    0.0001\n",
      "   200        0.7100             nan     0.0100    0.0000\n",
      "   220        0.6969             nan     0.0100    0.0001\n",
      "   240        0.6860             nan     0.0100    0.0001\n",
      "   250        0.6808             nan     0.0100    0.0002\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1074             nan     0.0100    0.0034\n",
      "     2        1.1006             nan     0.0100    0.0029\n",
      "     3        1.0936             nan     0.0100    0.0031\n",
      "     4        1.0873             nan     0.0100    0.0032\n",
      "     5        1.0806             nan     0.0100    0.0031\n",
      "     6        1.0741             nan     0.0100    0.0029\n",
      "     7        1.0680             nan     0.0100    0.0028\n",
      "     8        1.0621             nan     0.0100    0.0026\n",
      "     9        1.0560             nan     0.0100    0.0028\n",
      "    10        1.0504             nan     0.0100    0.0026\n",
      "    20        0.9979             nan     0.0100    0.0020\n",
      "    40        0.9150             nan     0.0100    0.0015\n",
      "    60        0.8554             nan     0.0100    0.0009\n",
      "    80        0.8098             nan     0.0100    0.0010\n",
      "   100        0.7733             nan     0.0100    0.0006\n",
      "   120        0.7426             nan     0.0100    0.0004\n",
      "   140        0.7170             nan     0.0100    0.0006\n",
      "   160        0.6955             nan     0.0100    0.0002\n",
      "   180        0.6760             nan     0.0100    0.0002\n",
      "   200        0.6580             nan     0.0100    0.0000\n",
      "   220        0.6435             nan     0.0100   -0.0000\n",
      "   240        0.6304             nan     0.0100    0.0001\n",
      "   250        0.6240             nan     0.0100    0.0001\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0969             nan     0.0500    0.0087\n",
      "     2        1.0811             nan     0.0500    0.0078\n",
      "     3        1.0681             nan     0.0500    0.0069\n",
      "     4        1.0577             nan     0.0500    0.0045\n",
      "     5        1.0444             nan     0.0500    0.0064\n",
      "     6        1.0342             nan     0.0500    0.0052\n",
      "     7        1.0231             nan     0.0500    0.0058\n",
      "     8        1.0118             nan     0.0500    0.0052\n",
      "     9        1.0025             nan     0.0500    0.0041\n",
      "    10        0.9946             nan     0.0500    0.0039\n",
      "    20        0.9245             nan     0.0500    0.0026\n",
      "    40        0.8447             nan     0.0500    0.0012\n",
      "    60        0.7981             nan     0.0500    0.0006\n",
      "    80        0.7642             nan     0.0500    0.0005\n",
      "   100        0.7438             nan     0.0500    0.0005\n",
      "   120        0.7284             nan     0.0500    0.0006\n",
      "   140        0.7139             nan     0.0500    0.0003\n",
      "   160        0.7038             nan     0.0500   -0.0003\n",
      "   180        0.6959             nan     0.0500   -0.0000\n",
      "   200        0.6883             nan     0.0500   -0.0002\n",
      "   220        0.6815             nan     0.0500   -0.0001\n",
      "   240        0.6752             nan     0.0500   -0.0000\n",
      "   250        0.6724             nan     0.0500    0.0000\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0841             nan     0.0500    0.0144\n",
      "     2        1.0567             nan     0.0500    0.0133\n",
      "     3        1.0324             nan     0.0500    0.0116\n",
      "     4        1.0089             nan     0.0500    0.0109\n",
      "     5        0.9897             nan     0.0500    0.0094\n",
      "     6        0.9709             nan     0.0500    0.0089\n",
      "     7        0.9534             nan     0.0500    0.0069\n",
      "     8        0.9362             nan     0.0500    0.0079\n",
      "     9        0.9214             nan     0.0500    0.0062\n",
      "    10        0.9062             nan     0.0500    0.0068\n",
      "    20        0.8079             nan     0.0500    0.0020\n",
      "    40        0.7120             nan     0.0500    0.0011\n",
      "    60        0.6612             nan     0.0500    0.0004\n",
      "    80        0.6265             nan     0.0500    0.0000\n",
      "   100        0.5988             nan     0.0500   -0.0002\n",
      "   120        0.5769             nan     0.0500   -0.0001\n",
      "   140        0.5547             nan     0.0500   -0.0008\n",
      "   160        0.5366             nan     0.0500   -0.0004\n",
      "   180        0.5189             nan     0.0500   -0.0005\n",
      "   200        0.5028             nan     0.0500   -0.0002\n",
      "   220        0.4863             nan     0.0500   -0.0003\n",
      "   240        0.4744             nan     0.0500   -0.0002\n",
      "   250        0.4673             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0783             nan     0.0500    0.0161\n",
      "     2        1.0457             nan     0.0500    0.0151\n",
      "     3        1.0168             nan     0.0500    0.0133\n",
      "     4        0.9934             nan     0.0500    0.0100\n",
      "     5        0.9705             nan     0.0500    0.0100\n",
      "     6        0.9504             nan     0.0500    0.0096\n",
      "     7        0.9325             nan     0.0500    0.0075\n",
      "     8        0.9150             nan     0.0500    0.0082\n",
      "     9        0.8988             nan     0.0500    0.0074\n",
      "    10        0.8826             nan     0.0500    0.0062\n",
      "    20        0.7709             nan     0.0500    0.0032\n",
      "    40        0.6593             nan     0.0500    0.0007\n",
      "    60        0.5957             nan     0.0500   -0.0000\n",
      "    80        0.5531             nan     0.0500    0.0001\n",
      "   100        0.5159             nan     0.0500   -0.0005\n",
      "   120        0.4835             nan     0.0500   -0.0003\n",
      "   140        0.4537             nan     0.0500   -0.0003\n",
      "   160        0.4253             nan     0.0500   -0.0004\n",
      "   180        0.4006             nan     0.0500   -0.0004\n",
      "   200        0.3775             nan     0.0500   -0.0002\n",
      "   220        0.3579             nan     0.0500   -0.0003\n",
      "   240        0.3397             nan     0.0500   -0.0004\n",
      "   250        0.3315             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0807             nan     0.1000    0.0170\n",
      "     2        1.0578             nan     0.1000    0.0095\n",
      "     3        1.0320             nan     0.1000    0.0135\n",
      "     4        1.0081             nan     0.1000    0.0109\n",
      "     5        0.9910             nan     0.1000    0.0088\n",
      "     6        0.9742             nan     0.1000    0.0079\n",
      "     7        0.9618             nan     0.1000    0.0052\n",
      "     8        0.9474             nan     0.1000    0.0060\n",
      "     9        0.9379             nan     0.1000    0.0050\n",
      "    10        0.9242             nan     0.1000    0.0070\n",
      "    20        0.8465             nan     0.1000    0.0023\n",
      "    40        0.7636             nan     0.1000    0.0009\n",
      "    60        0.7272             nan     0.1000    0.0006\n",
      "    80        0.7036             nan     0.1000   -0.0004\n",
      "   100        0.6874             nan     0.1000   -0.0001\n",
      "   120        0.6742             nan     0.1000   -0.0002\n",
      "   140        0.6632             nan     0.1000   -0.0005\n",
      "   160        0.6550             nan     0.1000   -0.0002\n",
      "   180        0.6470             nan     0.1000   -0.0001\n",
      "   200        0.6402             nan     0.1000   -0.0004\n",
      "   220        0.6343             nan     0.1000   -0.0004\n",
      "   240        0.6293             nan     0.1000   -0.0002\n",
      "   250        0.6266             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0528             nan     0.1000    0.0294\n",
      "     2        1.0060             nan     0.1000    0.0226\n",
      "     3        0.9632             nan     0.1000    0.0206\n",
      "     4        0.9294             nan     0.1000    0.0159\n",
      "     5        0.9024             nan     0.1000    0.0112\n",
      "     6        0.8798             nan     0.1000    0.0099\n",
      "     7        0.8589             nan     0.1000    0.0101\n",
      "     8        0.8369             nan     0.1000    0.0073\n",
      "     9        0.8185             nan     0.1000    0.0079\n",
      "    10        0.8034             nan     0.1000    0.0058\n",
      "    20        0.7058             nan     0.1000    0.0024\n",
      "    40        0.6302             nan     0.1000    0.0003\n",
      "    60        0.5790             nan     0.1000   -0.0011\n",
      "    80        0.5414             nan     0.1000   -0.0006\n",
      "   100        0.5044             nan     0.1000   -0.0006\n",
      "   120        0.4738             nan     0.1000   -0.0005\n",
      "   140        0.4477             nan     0.1000   -0.0011\n",
      "   160        0.4229             nan     0.1000   -0.0007\n",
      "   180        0.4029             nan     0.1000   -0.0006\n",
      "   200        0.3829             nan     0.1000   -0.0001\n",
      "   220        0.3576             nan     0.1000   -0.0005\n",
      "   240        0.3395             nan     0.1000   -0.0007\n",
      "   250        0.3303             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold05.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0492             nan     0.1000    0.0278\n",
      "     2        0.9980             nan     0.1000    0.0238\n",
      "     3        0.9579             nan     0.1000    0.0176\n",
      "     4        0.9185             nan     0.1000    0.0160\n",
      "     5        0.8843             nan     0.1000    0.0148\n",
      "     6        0.8558             nan     0.1000    0.0097\n",
      "     7        0.8300             nan     0.1000    0.0102\n",
      "     8        0.8111             nan     0.1000    0.0050\n",
      "     9        0.7926             nan     0.1000    0.0065\n",
      "    10        0.7715             nan     0.1000    0.0071\n",
      "    20        0.6569             nan     0.1000    0.0016\n",
      "    40        0.5478             nan     0.1000   -0.0012\n",
      "    60        0.4778             nan     0.1000   -0.0002\n",
      "    80        0.4217             nan     0.1000    0.0000\n",
      "   100        0.3791             nan     0.1000   -0.0003\n",
      "   120        0.3385             nan     0.1000   -0.0009\n",
      "   140        0.3049             nan     0.1000   -0.0010\n",
      "   160        0.2731             nan     0.1000   -0.0005\n",
      "   180        0.2456             nan     0.1000   -0.0005\n",
      "   200        0.2241             nan     0.1000   -0.0004\n",
      "   220        0.2028             nan     0.1000   -0.0004\n",
      "   240        0.1847             nan     0.1000   -0.0005\n",
      "   250        0.1758             nan     0.1000   -0.0005\n",
      "\n",
      "- Fold05.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1110             nan     0.0100    0.0018\n",
      "     2        1.1074             nan     0.0100    0.0018\n",
      "     3        1.1038             nan     0.0100    0.0018\n",
      "     4        1.1002             nan     0.0100    0.0017\n",
      "     5        1.0967             nan     0.0100    0.0017\n",
      "     6        1.0933             nan     0.0100    0.0016\n",
      "     7        1.0902             nan     0.0100    0.0016\n",
      "     8        1.0871             nan     0.0100    0.0016\n",
      "     9        1.0841             nan     0.0100    0.0016\n",
      "    10        1.0813             nan     0.0100    0.0015\n",
      "    20        1.0538             nan     0.0100    0.0013\n",
      "    40        1.0103             nan     0.0100    0.0009\n",
      "    60        0.9763             nan     0.0100    0.0007\n",
      "    80        0.9485             nan     0.0100    0.0005\n",
      "   100        0.9247             nan     0.0100    0.0006\n",
      "   120        0.9045             nan     0.0100    0.0003\n",
      "   140        0.8863             nan     0.0100    0.0003\n",
      "   160        0.8701             nan     0.0100    0.0003\n",
      "   180        0.8562             nan     0.0100    0.0003\n",
      "   200        0.8440             nan     0.0100    0.0002\n",
      "   220        0.8326             nan     0.0100    0.0002\n",
      "   240        0.8223             nan     0.0100    0.0002\n",
      "   250        0.8171             nan     0.0100    0.0002\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1081             nan     0.0100    0.0031\n",
      "     2        1.1020             nan     0.0100    0.0029\n",
      "     3        1.0953             nan     0.0100    0.0031\n",
      "     4        1.0895             nan     0.0100    0.0028\n",
      "     5        1.0835             nan     0.0100    0.0030\n",
      "     6        1.0776             nan     0.0100    0.0029\n",
      "     7        1.0724             nan     0.0100    0.0025\n",
      "     8        1.0671             nan     0.0100    0.0025\n",
      "     9        1.0612             nan     0.0100    0.0025\n",
      "    10        1.0562             nan     0.0100    0.0024\n",
      "    20        1.0088             nan     0.0100    0.0019\n",
      "    40        0.9369             nan     0.0100    0.0014\n",
      "    60        0.8822             nan     0.0100    0.0010\n",
      "    80        0.8404             nan     0.0100    0.0008\n",
      "   100        0.8078             nan     0.0100    0.0006\n",
      "   120        0.7809             nan     0.0100    0.0003\n",
      "   140        0.7579             nan     0.0100    0.0002\n",
      "   160        0.7390             nan     0.0100    0.0003\n",
      "   180        0.7220             nan     0.0100    0.0004\n",
      "   200        0.7076             nan     0.0100    0.0001\n",
      "   220        0.6947             nan     0.0100    0.0002\n",
      "   240        0.6831             nan     0.0100    0.0001\n",
      "   250        0.6783             nan     0.0100    0.0001\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1076             nan     0.0100    0.0034\n",
      "     2        1.1002             nan     0.0100    0.0033\n",
      "     3        1.0938             nan     0.0100    0.0027\n",
      "     4        1.0870             nan     0.0100    0.0031\n",
      "     5        1.0801             nan     0.0100    0.0031\n",
      "     6        1.0737             nan     0.0100    0.0030\n",
      "     7        1.0674             nan     0.0100    0.0028\n",
      "     8        1.0611             nan     0.0100    0.0028\n",
      "     9        1.0551             nan     0.0100    0.0026\n",
      "    10        1.0493             nan     0.0100    0.0024\n",
      "    20        0.9972             nan     0.0100    0.0021\n",
      "    40        0.9161             nan     0.0100    0.0014\n",
      "    60        0.8548             nan     0.0100    0.0012\n",
      "    80        0.8074             nan     0.0100    0.0007\n",
      "   100        0.7697             nan     0.0100    0.0006\n",
      "   120        0.7380             nan     0.0100    0.0003\n",
      "   140        0.7133             nan     0.0100    0.0002\n",
      "   160        0.6907             nan     0.0100    0.0003\n",
      "   180        0.6718             nan     0.0100    0.0002\n",
      "   200        0.6549             nan     0.0100    0.0001\n",
      "   220        0.6392             nan     0.0100    0.0002\n",
      "   240        0.6248             nan     0.0100   -0.0000\n",
      "   250        0.6185             nan     0.0100    0.0000\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0967             nan     0.0500    0.0090\n",
      "     2        1.0802             nan     0.0500    0.0081\n",
      "     3        1.0652             nan     0.0500    0.0073\n",
      "     4        1.0504             nan     0.0500    0.0064\n",
      "     5        1.0396             nan     0.0500    0.0048\n",
      "     6        1.0291             nan     0.0500    0.0045\n",
      "     7        1.0172             nan     0.0500    0.0058\n",
      "     8        1.0082             nan     0.0500    0.0041\n",
      "     9        0.9973             nan     0.0500    0.0052\n",
      "    10        0.9902             nan     0.0500    0.0028\n",
      "    20        0.9234             nan     0.0500    0.0018\n",
      "    40        0.8424             nan     0.0500    0.0014\n",
      "    60        0.7939             nan     0.0500    0.0007\n",
      "    80        0.7622             nan     0.0500    0.0006\n",
      "   100        0.7402             nan     0.0500    0.0006\n",
      "   120        0.7250             nan     0.0500    0.0001\n",
      "   140        0.7122             nan     0.0500   -0.0001\n",
      "   160        0.7010             nan     0.0500    0.0000\n",
      "   180        0.6913             nan     0.0500    0.0001\n",
      "   200        0.6835             nan     0.0500   -0.0002\n",
      "   220        0.6774             nan     0.0500   -0.0001\n",
      "   240        0.6712             nan     0.0500   -0.0001\n",
      "   250        0.6684             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0807             nan     0.0500    0.0147\n",
      "     2        1.0537             nan     0.0500    0.0117\n",
      "     3        1.0275             nan     0.0500    0.0114\n",
      "     4        1.0040             nan     0.0500    0.0098\n",
      "     5        0.9845             nan     0.0500    0.0085\n",
      "     6        0.9653             nan     0.0500    0.0090\n",
      "     7        0.9481             nan     0.0500    0.0079\n",
      "     8        0.9310             nan     0.0500    0.0076\n",
      "     9        0.9159             nan     0.0500    0.0067\n",
      "    10        0.9024             nan     0.0500    0.0061\n",
      "    20        0.8048             nan     0.0500    0.0034\n",
      "    40        0.7064             nan     0.0500    0.0009\n",
      "    60        0.6541             nan     0.0500   -0.0001\n",
      "    80        0.6193             nan     0.0500   -0.0000\n",
      "   100        0.5923             nan     0.0500   -0.0005\n",
      "   120        0.5686             nan     0.0500   -0.0002\n",
      "   140        0.5483             nan     0.0500   -0.0004\n",
      "   160        0.5304             nan     0.0500    0.0001\n",
      "   180        0.5138             nan     0.0500   -0.0004\n",
      "   200        0.4979             nan     0.0500   -0.0002\n",
      "   220        0.4825             nan     0.0500   -0.0003\n",
      "   240        0.4672             nan     0.0500   -0.0005\n",
      "   250        0.4590             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0792             nan     0.0500    0.0158\n",
      "     2        1.0463             nan     0.0500    0.0152\n",
      "     3        1.0195             nan     0.0500    0.0108\n",
      "     4        0.9936             nan     0.0500    0.0121\n",
      "     5        0.9700             nan     0.0500    0.0098\n",
      "     6        0.9499             nan     0.0500    0.0083\n",
      "     7        0.9310             nan     0.0500    0.0076\n",
      "     8        0.9136             nan     0.0500    0.0073\n",
      "     9        0.8967             nan     0.0500    0.0072\n",
      "    10        0.8822             nan     0.0500    0.0060\n",
      "    20        0.7701             nan     0.0500    0.0026\n",
      "    40        0.6547             nan     0.0500    0.0001\n",
      "    60        0.5913             nan     0.0500    0.0002\n",
      "    80        0.5465             nan     0.0500   -0.0003\n",
      "   100        0.5108             nan     0.0500   -0.0004\n",
      "   120        0.4792             nan     0.0500   -0.0006\n",
      "   140        0.4482             nan     0.0500   -0.0003\n",
      "   160        0.4224             nan     0.0500   -0.0001\n",
      "   180        0.3983             nan     0.0500   -0.0003\n",
      "   200        0.3743             nan     0.0500   -0.0004\n",
      "   220        0.3536             nan     0.0500   -0.0005\n",
      "   240        0.3359             nan     0.0500   -0.0004\n",
      "   250        0.3267             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0810             nan     0.1000    0.0176\n",
      "     2        1.0502             nan     0.1000    0.0141\n",
      "     3        1.0306             nan     0.1000    0.0091\n",
      "     4        1.0132             nan     0.1000    0.0067\n",
      "     5        0.9905             nan     0.1000    0.0110\n",
      "     6        0.9716             nan     0.1000    0.0089\n",
      "     7        0.9593             nan     0.1000    0.0047\n",
      "     8        0.9476             nan     0.1000    0.0060\n",
      "     9        0.9335             nan     0.1000    0.0068\n",
      "    10        0.9245             nan     0.1000    0.0044\n",
      "    20        0.8400             nan     0.1000    0.0032\n",
      "    40        0.7644             nan     0.1000    0.0004\n",
      "    60        0.7254             nan     0.1000   -0.0001\n",
      "    80        0.6983             nan     0.1000    0.0002\n",
      "   100        0.6826             nan     0.1000   -0.0003\n",
      "   120        0.6705             nan     0.1000   -0.0002\n",
      "   140        0.6585             nan     0.1000   -0.0002\n",
      "   160        0.6482             nan     0.1000   -0.0002\n",
      "   180        0.6413             nan     0.1000   -0.0003\n",
      "   200        0.6345             nan     0.1000   -0.0006\n",
      "   220        0.6289             nan     0.1000    0.0001\n",
      "   240        0.6238             nan     0.1000   -0.0002\n",
      "   250        0.6210             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0474             nan     0.1000    0.0307\n",
      "     2        1.0005             nan     0.1000    0.0210\n",
      "     3        0.9648             nan     0.1000    0.0156\n",
      "     4        0.9321             nan     0.1000    0.0145\n",
      "     5        0.9067             nan     0.1000    0.0109\n",
      "     6        0.8799             nan     0.1000    0.0127\n",
      "     7        0.8566             nan     0.1000    0.0099\n",
      "     8        0.8369             nan     0.1000    0.0072\n",
      "     9        0.8194             nan     0.1000    0.0071\n",
      "    10        0.8017             nan     0.1000    0.0085\n",
      "    20        0.7029             nan     0.1000    0.0020\n",
      "    40        0.6183             nan     0.1000    0.0004\n",
      "    60        0.5706             nan     0.1000   -0.0011\n",
      "    80        0.5324             nan     0.1000   -0.0006\n",
      "   100        0.5024             nan     0.1000   -0.0003\n",
      "   120        0.4747             nan     0.1000   -0.0006\n",
      "   140        0.4478             nan     0.1000   -0.0008\n",
      "   160        0.4215             nan     0.1000   -0.0004\n",
      "   180        0.3944             nan     0.1000   -0.0006\n",
      "   200        0.3726             nan     0.1000    0.0000\n",
      "   220        0.3543             nan     0.1000   -0.0002\n",
      "   240        0.3354             nan     0.1000   -0.0012\n",
      "   250        0.3258             nan     0.1000   -0.0002\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold06.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0430             nan     0.1000    0.0301\n",
      "     2        0.9916             nan     0.1000    0.0229\n",
      "     3        0.9475             nan     0.1000    0.0189\n",
      "     4        0.9085             nan     0.1000    0.0178\n",
      "     5        0.8778             nan     0.1000    0.0115\n",
      "     6        0.8495             nan     0.1000    0.0104\n",
      "     7        0.8259             nan     0.1000    0.0084\n",
      "     8        0.8021             nan     0.1000    0.0090\n",
      "     9        0.7842             nan     0.1000    0.0056\n",
      "    10        0.7640             nan     0.1000    0.0076\n",
      "    20        0.6541             nan     0.1000    0.0008\n",
      "    40        0.5498             nan     0.1000    0.0007\n",
      "    60        0.4768             nan     0.1000   -0.0007\n",
      "    80        0.4224             nan     0.1000   -0.0012\n",
      "   100        0.3774             nan     0.1000   -0.0010\n",
      "   120        0.3383             nan     0.1000   -0.0004\n",
      "   140        0.3052             nan     0.1000   -0.0008\n",
      "   160        0.2756             nan     0.1000   -0.0003\n",
      "   180        0.2486             nan     0.1000   -0.0009\n",
      "   200        0.2289             nan     0.1000   -0.0002\n",
      "   220        0.2067             nan     0.1000   -0.0007\n",
      "   240        0.1868             nan     0.1000   -0.0004\n",
      "   250        0.1785             nan     0.1000   -0.0007\n",
      "\n",
      "- Fold06.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1106             nan     0.0100    0.0017\n",
      "     2        1.1072             nan     0.0100    0.0017\n",
      "     3        1.1038             nan     0.0100    0.0017\n",
      "     4        1.1005             nan     0.0100    0.0016\n",
      "     5        1.0972             nan     0.0100    0.0016\n",
      "     6        1.0948             nan     0.0100    0.0009\n",
      "     7        1.0916             nan     0.0100    0.0016\n",
      "     8        1.0885             nan     0.0100    0.0015\n",
      "     9        1.0851             nan     0.0100    0.0015\n",
      "    10        1.0821             nan     0.0100    0.0015\n",
      "    20        1.0550             nan     0.0100    0.0012\n",
      "    40        1.0126             nan     0.0100    0.0009\n",
      "    60        0.9802             nan     0.0100    0.0007\n",
      "    80        0.9526             nan     0.0100    0.0004\n",
      "   100        0.9291             nan     0.0100    0.0005\n",
      "   120        0.9094             nan     0.0100    0.0003\n",
      "   140        0.8915             nan     0.0100    0.0002\n",
      "   160        0.8746             nan     0.0100    0.0004\n",
      "   180        0.8605             nan     0.0100    0.0003\n",
      "   200        0.8471             nan     0.0100    0.0003\n",
      "   220        0.8353             nan     0.0100    0.0002\n",
      "   240        0.8253             nan     0.0100    0.0002\n",
      "   250        0.8206             nan     0.0100    0.0002\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1080             nan     0.0100    0.0033\n",
      "     2        1.1023             nan     0.0100    0.0029\n",
      "     3        1.0963             nan     0.0100    0.0028\n",
      "     4        1.0898             nan     0.0100    0.0030\n",
      "     5        1.0840             nan     0.0100    0.0024\n",
      "     6        1.0780             nan     0.0100    0.0028\n",
      "     7        1.0726             nan     0.0100    0.0025\n",
      "     8        1.0668             nan     0.0100    0.0026\n",
      "     9        1.0614             nan     0.0100    0.0025\n",
      "    10        1.0562             nan     0.0100    0.0026\n",
      "    20        1.0093             nan     0.0100    0.0020\n",
      "    40        0.9365             nan     0.0100    0.0012\n",
      "    60        0.8818             nan     0.0100    0.0008\n",
      "    80        0.8411             nan     0.0100    0.0006\n",
      "   100        0.8087             nan     0.0100    0.0006\n",
      "   120        0.7831             nan     0.0100    0.0005\n",
      "   140        0.7612             nan     0.0100    0.0003\n",
      "   160        0.7428             nan     0.0100    0.0002\n",
      "   180        0.7265             nan     0.0100    0.0003\n",
      "   200        0.7121             nan     0.0100    0.0002\n",
      "   220        0.6992             nan     0.0100    0.0002\n",
      "   240        0.6877             nan     0.0100    0.0001\n",
      "   250        0.6826             nan     0.0100    0.0001\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1078             nan     0.0100    0.0034\n",
      "     2        1.1007             nan     0.0100    0.0030\n",
      "     3        1.0936             nan     0.0100    0.0032\n",
      "     4        1.0869             nan     0.0100    0.0027\n",
      "     5        1.0805             nan     0.0100    0.0028\n",
      "     6        1.0744             nan     0.0100    0.0028\n",
      "     7        1.0684             nan     0.0100    0.0027\n",
      "     8        1.0625             nan     0.0100    0.0027\n",
      "     9        1.0568             nan     0.0100    0.0023\n",
      "    10        1.0504             nan     0.0100    0.0026\n",
      "    20        0.9971             nan     0.0100    0.0022\n",
      "    40        0.9157             nan     0.0100    0.0014\n",
      "    60        0.8561             nan     0.0100    0.0009\n",
      "    80        0.8095             nan     0.0100    0.0006\n",
      "   100        0.7716             nan     0.0100    0.0006\n",
      "   120        0.7406             nan     0.0100    0.0005\n",
      "   140        0.7161             nan     0.0100    0.0003\n",
      "   160        0.6928             nan     0.0100    0.0002\n",
      "   180        0.6741             nan     0.0100    0.0001\n",
      "   200        0.6569             nan     0.0100    0.0000\n",
      "   220        0.6414             nan     0.0100    0.0000\n",
      "   240        0.6275             nan     0.0100    0.0001\n",
      "   250        0.6209             nan     0.0100    0.0000\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0969             nan     0.0500    0.0086\n",
      "     2        1.0808             nan     0.0500    0.0076\n",
      "     3        1.0665             nan     0.0500    0.0069\n",
      "     4        1.0553             nan     0.0500    0.0044\n",
      "     5        1.0433             nan     0.0500    0.0062\n",
      "     6        1.0344             nan     0.0500    0.0041\n",
      "     7        1.0232             nan     0.0500    0.0056\n",
      "     8        1.0141             nan     0.0500    0.0043\n",
      "     9        1.0066             nan     0.0500    0.0034\n",
      "    10        0.9974             nan     0.0500    0.0049\n",
      "    20        0.9260             nan     0.0500    0.0021\n",
      "    40        0.8458             nan     0.0500    0.0014\n",
      "    60        0.7979             nan     0.0500    0.0007\n",
      "    80        0.7659             nan     0.0500    0.0007\n",
      "   100        0.7443             nan     0.0500    0.0001\n",
      "   120        0.7290             nan     0.0500    0.0001\n",
      "   140        0.7168             nan     0.0500    0.0000\n",
      "   160        0.7051             nan     0.0500    0.0000\n",
      "   180        0.6960             nan     0.0500   -0.0001\n",
      "   200        0.6874             nan     0.0500   -0.0001\n",
      "   220        0.6809             nan     0.0500    0.0000\n",
      "   240        0.6743             nan     0.0500   -0.0000\n",
      "   250        0.6717             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0828             nan     0.0500    0.0157\n",
      "     2        1.0539             nan     0.0500    0.0129\n",
      "     3        1.0289             nan     0.0500    0.0111\n",
      "     4        1.0051             nan     0.0500    0.0108\n",
      "     5        0.9850             nan     0.0500    0.0089\n",
      "     6        0.9683             nan     0.0500    0.0069\n",
      "     7        0.9513             nan     0.0500    0.0082\n",
      "     8        0.9357             nan     0.0500    0.0066\n",
      "     9        0.9219             nan     0.0500    0.0064\n",
      "    10        0.9086             nan     0.0500    0.0059\n",
      "    20        0.8088             nan     0.0500    0.0034\n",
      "    40        0.7105             nan     0.0500    0.0011\n",
      "    60        0.6594             nan     0.0500   -0.0002\n",
      "    80        0.6249             nan     0.0500   -0.0003\n",
      "   100        0.5968             nan     0.0500   -0.0002\n",
      "   120        0.5738             nan     0.0500   -0.0001\n",
      "   140        0.5550             nan     0.0500   -0.0006\n",
      "   160        0.5373             nan     0.0500   -0.0004\n",
      "   180        0.5194             nan     0.0500   -0.0003\n",
      "   200        0.5032             nan     0.0500   -0.0002\n",
      "   220        0.4863             nan     0.0500   -0.0003\n",
      "   240        0.4721             nan     0.0500   -0.0003\n",
      "   250        0.4639             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0772             nan     0.0500    0.0165\n",
      "     2        1.0482             nan     0.0500    0.0139\n",
      "     3        1.0204             nan     0.0500    0.0120\n",
      "     4        0.9948             nan     0.0500    0.0120\n",
      "     5        0.9714             nan     0.0500    0.0100\n",
      "     6        0.9493             nan     0.0500    0.0101\n",
      "     7        0.9293             nan     0.0500    0.0089\n",
      "     8        0.9121             nan     0.0500    0.0072\n",
      "     9        0.8944             nan     0.0500    0.0070\n",
      "    10        0.8802             nan     0.0500    0.0056\n",
      "    20        0.7672             nan     0.0500    0.0028\n",
      "    40        0.6556             nan     0.0500    0.0016\n",
      "    60        0.5961             nan     0.0500    0.0002\n",
      "    80        0.5518             nan     0.0500   -0.0005\n",
      "   100        0.5124             nan     0.0500   -0.0001\n",
      "   120        0.4785             nan     0.0500    0.0001\n",
      "   140        0.4497             nan     0.0500   -0.0005\n",
      "   160        0.4229             nan     0.0500   -0.0004\n",
      "   180        0.3999             nan     0.0500   -0.0001\n",
      "   200        0.3775             nan     0.0500   -0.0003\n",
      "   220        0.3584             nan     0.0500   -0.0003\n",
      "   240        0.3393             nan     0.0500    0.0000\n",
      "   250        0.3289             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0823             nan     0.1000    0.0168\n",
      "     2        1.0550             nan     0.1000    0.0138\n",
      "     3        1.0347             nan     0.1000    0.0090\n",
      "     4        1.0160             nan     0.1000    0.0083\n",
      "     5        0.9946             nan     0.1000    0.0108\n",
      "     6        0.9814             nan     0.1000    0.0069\n",
      "     7        0.9629             nan     0.1000    0.0087\n",
      "     8        0.9523             nan     0.1000    0.0042\n",
      "     9        0.9378             nan     0.1000    0.0069\n",
      "    10        0.9298             nan     0.1000    0.0032\n",
      "    20        0.8439             nan     0.1000    0.0034\n",
      "    40        0.7628             nan     0.1000    0.0007\n",
      "    60        0.7266             nan     0.1000   -0.0000\n",
      "    80        0.7060             nan     0.1000   -0.0001\n",
      "   100        0.6886             nan     0.1000    0.0001\n",
      "   120        0.6740             nan     0.1000   -0.0001\n",
      "   140        0.6616             nan     0.1000    0.0000\n",
      "   160        0.6533             nan     0.1000   -0.0004\n",
      "   180        0.6461             nan     0.1000    0.0000\n",
      "   200        0.6402             nan     0.1000   -0.0004\n",
      "   220        0.6339             nan     0.1000   -0.0004\n",
      "   240        0.6291             nan     0.1000   -0.0002\n",
      "   250        0.6267             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0540             nan     0.1000    0.0265\n",
      "     2        1.0017             nan     0.1000    0.0231\n",
      "     3        0.9621             nan     0.1000    0.0195\n",
      "     4        0.9284             nan     0.1000    0.0144\n",
      "     5        0.8984             nan     0.1000    0.0123\n",
      "     6        0.8756             nan     0.1000    0.0100\n",
      "     7        0.8546             nan     0.1000    0.0085\n",
      "     8        0.8359             nan     0.1000    0.0081\n",
      "     9        0.8209             nan     0.1000    0.0062\n",
      "    10        0.8054             nan     0.1000    0.0068\n",
      "    20        0.7055             nan     0.1000    0.0021\n",
      "    40        0.6277             nan     0.1000   -0.0001\n",
      "    60        0.5779             nan     0.1000    0.0001\n",
      "    80        0.5367             nan     0.1000   -0.0014\n",
      "   100        0.5031             nan     0.1000   -0.0006\n",
      "   120        0.4750             nan     0.1000   -0.0007\n",
      "   140        0.4491             nan     0.1000   -0.0007\n",
      "   160        0.4208             nan     0.1000   -0.0004\n",
      "   180        0.3973             nan     0.1000   -0.0007\n",
      "   200        0.3765             nan     0.1000   -0.0004\n",
      "   220        0.3584             nan     0.1000   -0.0009\n",
      "   240        0.3404             nan     0.1000   -0.0008\n",
      "   250        0.3313             nan     0.1000   -0.0007\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold07.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0493             nan     0.1000    0.0288\n",
      "     2        0.9928             nan     0.1000    0.0238\n",
      "     3        0.9469             nan     0.1000    0.0220\n",
      "     4        0.9132             nan     0.1000    0.0148\n",
      "     5        0.8803             nan     0.1000    0.0131\n",
      "     6        0.8533             nan     0.1000    0.0108\n",
      "     7        0.8272             nan     0.1000    0.0105\n",
      "     8        0.8057             nan     0.1000    0.0085\n",
      "     9        0.7848             nan     0.1000    0.0071\n",
      "    10        0.7706             nan     0.1000    0.0041\n",
      "    20        0.6582             nan     0.1000    0.0022\n",
      "    40        0.5535             nan     0.1000   -0.0001\n",
      "    60        0.4867             nan     0.1000   -0.0003\n",
      "    80        0.4341             nan     0.1000   -0.0006\n",
      "   100        0.3851             nan     0.1000   -0.0006\n",
      "   120        0.3461             nan     0.1000   -0.0011\n",
      "   140        0.3108             nan     0.1000   -0.0006\n",
      "   160        0.2807             nan     0.1000   -0.0004\n",
      "   180        0.2547             nan     0.1000   -0.0011\n",
      "   200        0.2316             nan     0.1000   -0.0004\n",
      "   220        0.2116             nan     0.1000   -0.0003\n",
      "   240        0.1904             nan     0.1000   -0.0003\n",
      "   250        0.1816             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold07.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1104             nan     0.0100    0.0019\n",
      "     2        1.1065             nan     0.0100    0.0019\n",
      "     3        1.1028             nan     0.0100    0.0018\n",
      "     4        1.0993             nan     0.0100    0.0018\n",
      "     5        1.0961             nan     0.0100    0.0017\n",
      "     6        1.0927             nan     0.0100    0.0017\n",
      "     7        1.0896             nan     0.0100    0.0017\n",
      "     8        1.0861             nan     0.0100    0.0016\n",
      "     9        1.0825             nan     0.0100    0.0016\n",
      "    10        1.0794             nan     0.0100    0.0016\n",
      "    20        1.0508             nan     0.0100    0.0013\n",
      "    40        1.0055             nan     0.0100    0.0010\n",
      "    60        0.9702             nan     0.0100    0.0008\n",
      "    80        0.9410             nan     0.0100    0.0004\n",
      "   100        0.9156             nan     0.0100    0.0006\n",
      "   120        0.8952             nan     0.0100    0.0003\n",
      "   140        0.8774             nan     0.0100    0.0004\n",
      "   160        0.8613             nan     0.0100    0.0004\n",
      "   180        0.8467             nan     0.0100    0.0004\n",
      "   200        0.8337             nan     0.0100    0.0003\n",
      "   220        0.8216             nan     0.0100    0.0003\n",
      "   240        0.8112             nan     0.0100    0.0002\n",
      "   250        0.8062             nan     0.0100    0.0003\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1079             nan     0.0100    0.0030\n",
      "     2        1.1008             nan     0.0100    0.0033\n",
      "     3        1.0942             nan     0.0100    0.0029\n",
      "     4        1.0881             nan     0.0100    0.0030\n",
      "     5        1.0820             nan     0.0100    0.0030\n",
      "     6        1.0760             nan     0.0100    0.0030\n",
      "     7        1.0706             nan     0.0100    0.0027\n",
      "     8        1.0649             nan     0.0100    0.0024\n",
      "     9        1.0590             nan     0.0100    0.0026\n",
      "    10        1.0537             nan     0.0100    0.0025\n",
      "    20        1.0045             nan     0.0100    0.0022\n",
      "    40        0.9290             nan     0.0100    0.0016\n",
      "    60        0.8735             nan     0.0100    0.0009\n",
      "    80        0.8305             nan     0.0100    0.0008\n",
      "   100        0.7974             nan     0.0100    0.0006\n",
      "   120        0.7700             nan     0.0100    0.0005\n",
      "   140        0.7471             nan     0.0100    0.0005\n",
      "   160        0.7281             nan     0.0100    0.0003\n",
      "   180        0.7111             nan     0.0100    0.0002\n",
      "   200        0.6961             nan     0.0100    0.0002\n",
      "   220        0.6840             nan     0.0100    0.0000\n",
      "   240        0.6723             nan     0.0100    0.0001\n",
      "   250        0.6670             nan     0.0100    0.0001\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1073             nan     0.0100    0.0032\n",
      "     2        1.1000             nan     0.0100    0.0033\n",
      "     3        1.0923             nan     0.0100    0.0035\n",
      "     4        1.0858             nan     0.0100    0.0027\n",
      "     5        1.0795             nan     0.0100    0.0030\n",
      "     6        1.0729             nan     0.0100    0.0029\n",
      "     7        1.0663             nan     0.0100    0.0031\n",
      "     8        1.0600             nan     0.0100    0.0028\n",
      "     9        1.0538             nan     0.0100    0.0028\n",
      "    10        1.0478             nan     0.0100    0.0026\n",
      "    20        0.9935             nan     0.0100    0.0019\n",
      "    40        0.9107             nan     0.0100    0.0014\n",
      "    60        0.8489             nan     0.0100    0.0009\n",
      "    80        0.8000             nan     0.0100    0.0008\n",
      "   100        0.7621             nan     0.0100    0.0006\n",
      "   120        0.7302             nan     0.0100    0.0004\n",
      "   140        0.7038             nan     0.0100    0.0004\n",
      "   160        0.6811             nan     0.0100    0.0002\n",
      "   180        0.6623             nan     0.0100    0.0003\n",
      "   200        0.6452             nan     0.0100    0.0002\n",
      "   220        0.6302             nan     0.0100    0.0001\n",
      "   240        0.6164             nan     0.0100    0.0001\n",
      "   250        0.6097             nan     0.0100   -0.0001\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0947             nan     0.0500    0.0093\n",
      "     2        1.0783             nan     0.0500    0.0084\n",
      "     3        1.0629             nan     0.0500    0.0075\n",
      "     4        1.0516             nan     0.0500    0.0050\n",
      "     5        1.0368             nan     0.0500    0.0067\n",
      "     6        1.0245             nan     0.0500    0.0061\n",
      "     7        1.0128             nan     0.0500    0.0055\n",
      "     8        1.0043             nan     0.0500    0.0033\n",
      "     9        0.9944             nan     0.0500    0.0049\n",
      "    10        0.9851             nan     0.0500    0.0044\n",
      "    20        0.9130             nan     0.0500    0.0029\n",
      "    40        0.8318             nan     0.0500    0.0014\n",
      "    60        0.7823             nan     0.0500    0.0007\n",
      "    80        0.7501             nan     0.0500    0.0005\n",
      "   100        0.7269             nan     0.0500    0.0007\n",
      "   120        0.7101             nan     0.0500    0.0001\n",
      "   140        0.6975             nan     0.0500   -0.0001\n",
      "   160        0.6866             nan     0.0500    0.0000\n",
      "   180        0.6774             nan     0.0500    0.0001\n",
      "   200        0.6691             nan     0.0500   -0.0002\n",
      "   220        0.6617             nan     0.0500   -0.0001\n",
      "   240        0.6563             nan     0.0500   -0.0004\n",
      "   250        0.6539             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0805             nan     0.0500    0.0155\n",
      "     2        1.0518             nan     0.0500    0.0137\n",
      "     3        1.0257             nan     0.0500    0.0124\n",
      "     4        1.0011             nan     0.0500    0.0103\n",
      "     5        0.9806             nan     0.0500    0.0096\n",
      "     6        0.9614             nan     0.0500    0.0087\n",
      "     7        0.9424             nan     0.0500    0.0095\n",
      "     8        0.9267             nan     0.0500    0.0071\n",
      "     9        0.9083             nan     0.0500    0.0080\n",
      "    10        0.8945             nan     0.0500    0.0063\n",
      "    20        0.7957             nan     0.0500    0.0030\n",
      "    40        0.6976             nan     0.0500    0.0008\n",
      "    60        0.6451             nan     0.0500    0.0001\n",
      "    80        0.6094             nan     0.0500    0.0001\n",
      "   100        0.5831             nan     0.0500   -0.0001\n",
      "   120        0.5592             nan     0.0500   -0.0000\n",
      "   140        0.5378             nan     0.0500   -0.0004\n",
      "   160        0.5192             nan     0.0500   -0.0003\n",
      "   180        0.5027             nan     0.0500   -0.0002\n",
      "   200        0.4835             nan     0.0500   -0.0002\n",
      "   220        0.4676             nan     0.0500   -0.0002\n",
      "   240        0.4517             nan     0.0500   -0.0003\n",
      "   250        0.4452             nan     0.0500   -0.0002\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0773             nan     0.0500    0.0153\n",
      "     2        1.0438             nan     0.0500    0.0152\n",
      "     3        1.0138             nan     0.0500    0.0136\n",
      "     4        0.9879             nan     0.0500    0.0114\n",
      "     5        0.9638             nan     0.0500    0.0104\n",
      "     6        0.9423             nan     0.0500    0.0095\n",
      "     7        0.9235             nan     0.0500    0.0078\n",
      "     8        0.9064             nan     0.0500    0.0071\n",
      "     9        0.8899             nan     0.0500    0.0076\n",
      "    10        0.8755             nan     0.0500    0.0061\n",
      "    20        0.7646             nan     0.0500    0.0036\n",
      "    40        0.6495             nan     0.0500    0.0003\n",
      "    60        0.5839             nan     0.0500    0.0001\n",
      "    80        0.5372             nan     0.0500   -0.0002\n",
      "   100        0.4992             nan     0.0500   -0.0008\n",
      "   120        0.4670             nan     0.0500   -0.0004\n",
      "   140        0.4377             nan     0.0500   -0.0004\n",
      "   160        0.4102             nan     0.0500   -0.0006\n",
      "   180        0.3864             nan     0.0500   -0.0007\n",
      "   200        0.3634             nan     0.0500   -0.0004\n",
      "   220        0.3414             nan     0.0500   -0.0000\n",
      "   240        0.3219             nan     0.0500   -0.0005\n",
      "   250        0.3136             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0749             nan     0.1000    0.0175\n",
      "     2        1.0447             nan     0.1000    0.0142\n",
      "     3        1.0205             nan     0.1000    0.0115\n",
      "     4        1.0006             nan     0.1000    0.0093\n",
      "     5        0.9819             nan     0.1000    0.0079\n",
      "     6        0.9652             nan     0.1000    0.0080\n",
      "     7        0.9506             nan     0.1000    0.0069\n",
      "     8        0.9376             nan     0.1000    0.0065\n",
      "     9        0.9277             nan     0.1000    0.0047\n",
      "    10        0.9141             nan     0.1000    0.0069\n",
      "    20        0.8315             nan     0.1000    0.0026\n",
      "    40        0.7505             nan     0.1000    0.0014\n",
      "    60        0.7119             nan     0.1000    0.0004\n",
      "    80        0.6875             nan     0.1000    0.0005\n",
      "   100        0.6722             nan     0.1000   -0.0002\n",
      "   120        0.6578             nan     0.1000    0.0002\n",
      "   140        0.6455             nan     0.1000   -0.0001\n",
      "   160        0.6387             nan     0.1000   -0.0001\n",
      "   180        0.6322             nan     0.1000   -0.0001\n",
      "   200        0.6266             nan     0.1000   -0.0004\n",
      "   220        0.6207             nan     0.1000   -0.0005\n",
      "   240        0.6165             nan     0.1000   -0.0005\n",
      "   250        0.6141             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0504             nan     0.1000    0.0311\n",
      "     2        0.9982             nan     0.1000    0.0252\n",
      "     3        0.9572             nan     0.1000    0.0190\n",
      "     4        0.9201             nan     0.1000    0.0185\n",
      "     5        0.8907             nan     0.1000    0.0128\n",
      "     6        0.8675             nan     0.1000    0.0097\n",
      "     7        0.8470             nan     0.1000    0.0085\n",
      "     8        0.8255             nan     0.1000    0.0102\n",
      "     9        0.8095             nan     0.1000    0.0066\n",
      "    10        0.7959             nan     0.1000    0.0048\n",
      "    20        0.6999             nan     0.1000    0.0029\n",
      "    40        0.6141             nan     0.1000    0.0002\n",
      "    60        0.5619             nan     0.1000   -0.0000\n",
      "    80        0.5203             nan     0.1000    0.0000\n",
      "   100        0.4884             nan     0.1000   -0.0002\n",
      "   120        0.4573             nan     0.1000   -0.0008\n",
      "   140        0.4293             nan     0.1000   -0.0008\n",
      "   160        0.4038             nan     0.1000   -0.0005\n",
      "   180        0.3836             nan     0.1000   -0.0010\n",
      "   200        0.3653             nan     0.1000   -0.0009\n",
      "   220        0.3484             nan     0.1000   -0.0009\n",
      "   240        0.3307             nan     0.1000   -0.0008\n",
      "   250        0.3210             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold08.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0401             nan     0.1000    0.0334\n",
      "     2        0.9841             nan     0.1000    0.0223\n",
      "     3        0.9390             nan     0.1000    0.0186\n",
      "     4        0.9028             nan     0.1000    0.0141\n",
      "     5        0.8717             nan     0.1000    0.0117\n",
      "     6        0.8448             nan     0.1000    0.0103\n",
      "     7        0.8173             nan     0.1000    0.0116\n",
      "     8        0.7961             nan     0.1000    0.0078\n",
      "     9        0.7756             nan     0.1000    0.0077\n",
      "    10        0.7584             nan     0.1000    0.0070\n",
      "    20        0.6538             nan     0.1000    0.0017\n",
      "    40        0.5384             nan     0.1000   -0.0008\n",
      "    60        0.4704             nan     0.1000   -0.0016\n",
      "    80        0.4140             nan     0.1000   -0.0019\n",
      "   100        0.3689             nan     0.1000   -0.0011\n",
      "   120        0.3309             nan     0.1000   -0.0009\n",
      "   140        0.3001             nan     0.1000   -0.0001\n",
      "   160        0.2685             nan     0.1000   -0.0005\n",
      "   180        0.2418             nan     0.1000   -0.0002\n",
      "   200        0.2209             nan     0.1000   -0.0002\n",
      "   220        0.2004             nan     0.1000   -0.0006\n",
      "   240        0.1813             nan     0.1000   -0.0003\n",
      "   250        0.1735             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold08.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1106             nan     0.0100    0.0018\n",
      "     2        1.1067             nan     0.0100    0.0018\n",
      "     3        1.1029             nan     0.0100    0.0017\n",
      "     4        1.0995             nan     0.0100    0.0017\n",
      "     5        1.0963             nan     0.0100    0.0017\n",
      "     6        1.0932             nan     0.0100    0.0016\n",
      "     7        1.0901             nan     0.0100    0.0016\n",
      "     8        1.0870             nan     0.0100    0.0016\n",
      "     9        1.0839             nan     0.0100    0.0016\n",
      "    10        1.0810             nan     0.0100    0.0015\n",
      "    20        1.0537             nan     0.0100    0.0013\n",
      "    40        1.0103             nan     0.0100    0.0010\n",
      "    60        0.9759             nan     0.0100    0.0008\n",
      "    80        0.9477             nan     0.0100    0.0007\n",
      "   100        0.9238             nan     0.0100    0.0005\n",
      "   120        0.9037             nan     0.0100    0.0004\n",
      "   140        0.8869             nan     0.0100    0.0003\n",
      "   160        0.8714             nan     0.0100    0.0004\n",
      "   180        0.8568             nan     0.0100    0.0003\n",
      "   200        0.8440             nan     0.0100    0.0003\n",
      "   220        0.8334             nan     0.0100    0.0002\n",
      "   240        0.8228             nan     0.0100    0.0002\n",
      "   250        0.8181             nan     0.0100    0.0002\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1076             nan     0.0100    0.0032\n",
      "     2        1.1015             nan     0.0100    0.0030\n",
      "     3        1.0950             nan     0.0100    0.0030\n",
      "     4        1.0889             nan     0.0100    0.0031\n",
      "     5        1.0827             nan     0.0100    0.0029\n",
      "     6        1.0767             nan     0.0100    0.0027\n",
      "     7        1.0711             nan     0.0100    0.0026\n",
      "     8        1.0659             nan     0.0100    0.0027\n",
      "     9        1.0606             nan     0.0100    0.0025\n",
      "    10        1.0554             nan     0.0100    0.0024\n",
      "    20        1.0081             nan     0.0100    0.0020\n",
      "    40        0.9357             nan     0.0100    0.0015\n",
      "    60        0.8843             nan     0.0100    0.0013\n",
      "    80        0.8433             nan     0.0100    0.0005\n",
      "   100        0.8106             nan     0.0100    0.0006\n",
      "   120        0.7838             nan     0.0100    0.0006\n",
      "   140        0.7624             nan     0.0100    0.0002\n",
      "   160        0.7437             nan     0.0100    0.0003\n",
      "   180        0.7279             nan     0.0100    0.0002\n",
      "   200        0.7138             nan     0.0100    0.0002\n",
      "   220        0.7009             nan     0.0100    0.0002\n",
      "   240        0.6904             nan     0.0100    0.0001\n",
      "   250        0.6855             nan     0.0100    0.0001\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1073             nan     0.0100    0.0031\n",
      "     2        1.1005             nan     0.0100    0.0034\n",
      "     3        1.0934             nan     0.0100    0.0031\n",
      "     4        1.0866             nan     0.0100    0.0032\n",
      "     5        1.0799             nan     0.0100    0.0031\n",
      "     6        1.0732             nan     0.0100    0.0030\n",
      "     7        1.0669             nan     0.0100    0.0028\n",
      "     8        1.0610             nan     0.0100    0.0029\n",
      "     9        1.0553             nan     0.0100    0.0026\n",
      "    10        1.0495             nan     0.0100    0.0027\n",
      "    20        0.9971             nan     0.0100    0.0021\n",
      "    40        0.9169             nan     0.0100    0.0015\n",
      "    60        0.8573             nan     0.0100    0.0011\n",
      "    80        0.8109             nan     0.0100    0.0008\n",
      "   100        0.7738             nan     0.0100    0.0004\n",
      "   120        0.7418             nan     0.0100    0.0004\n",
      "   140        0.7172             nan     0.0100    0.0002\n",
      "   160        0.6948             nan     0.0100    0.0002\n",
      "   180        0.6761             nan     0.0100    0.0001\n",
      "   200        0.6589             nan     0.0100    0.0001\n",
      "   220        0.6437             nan     0.0100    0.0001\n",
      "   240        0.6305             nan     0.0100    0.0000\n",
      "   250        0.6238             nan     0.0100    0.0000\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0970             nan     0.0500    0.0090\n",
      "     2        1.0801             nan     0.0500    0.0081\n",
      "     3        1.0654             nan     0.0500    0.0073\n",
      "     4        1.0540             nan     0.0500    0.0050\n",
      "     5        1.0398             nan     0.0500    0.0064\n",
      "     6        1.0279             nan     0.0500    0.0059\n",
      "     7        1.0190             nan     0.0500    0.0038\n",
      "     8        1.0081             nan     0.0500    0.0052\n",
      "     9        0.9984             nan     0.0500    0.0048\n",
      "    10        0.9916             nan     0.0500    0.0025\n",
      "    20        0.9234             nan     0.0500    0.0031\n",
      "    40        0.8429             nan     0.0500    0.0009\n",
      "    60        0.7976             nan     0.0500    0.0006\n",
      "    80        0.7667             nan     0.0500    0.0003\n",
      "   100        0.7454             nan     0.0500    0.0002\n",
      "   120        0.7289             nan     0.0500    0.0005\n",
      "   140        0.7162             nan     0.0500    0.0003\n",
      "   160        0.7049             nan     0.0500   -0.0001\n",
      "   180        0.6963             nan     0.0500   -0.0000\n",
      "   200        0.6891             nan     0.0500   -0.0003\n",
      "   220        0.6821             nan     0.0500    0.0001\n",
      "   240        0.6763             nan     0.0500   -0.0000\n",
      "   250        0.6735             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0823             nan     0.0500    0.0138\n",
      "     2        1.0585             nan     0.0500    0.0125\n",
      "     3        1.0329             nan     0.0500    0.0124\n",
      "     4        1.0086             nan     0.0500    0.0108\n",
      "     5        0.9888             nan     0.0500    0.0099\n",
      "     6        0.9697             nan     0.0500    0.0085\n",
      "     7        0.9527             nan     0.0500    0.0079\n",
      "     8        0.9358             nan     0.0500    0.0086\n",
      "     9        0.9192             nan     0.0500    0.0074\n",
      "    10        0.9068             nan     0.0500    0.0055\n",
      "    20        0.8080             nan     0.0500    0.0036\n",
      "    40        0.7153             nan     0.0500    0.0009\n",
      "    60        0.6616             nan     0.0500    0.0003\n",
      "    80        0.6267             nan     0.0500   -0.0003\n",
      "   100        0.6024             nan     0.0500   -0.0004\n",
      "   120        0.5784             nan     0.0500   -0.0006\n",
      "   140        0.5592             nan     0.0500   -0.0005\n",
      "   160        0.5376             nan     0.0500   -0.0005\n",
      "   180        0.5209             nan     0.0500   -0.0005\n",
      "   200        0.5033             nan     0.0500   -0.0005\n",
      "   220        0.4870             nan     0.0500   -0.0005\n",
      "   240        0.4715             nan     0.0500   -0.0003\n",
      "   250        0.4641             nan     0.0500   -0.0005\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0827             nan     0.0500    0.0148\n",
      "     2        1.0524             nan     0.0500    0.0138\n",
      "     3        1.0243             nan     0.0500    0.0109\n",
      "     4        0.9994             nan     0.0500    0.0096\n",
      "     5        0.9773             nan     0.0500    0.0091\n",
      "     6        0.9566             nan     0.0500    0.0100\n",
      "     7        0.9351             nan     0.0500    0.0091\n",
      "     8        0.9171             nan     0.0500    0.0082\n",
      "     9        0.8996             nan     0.0500    0.0077\n",
      "    10        0.8829             nan     0.0500    0.0065\n",
      "    20        0.7710             nan     0.0500    0.0028\n",
      "    40        0.6578             nan     0.0500    0.0005\n",
      "    60        0.5977             nan     0.0500    0.0004\n",
      "    80        0.5491             nan     0.0500   -0.0003\n",
      "   100        0.5106             nan     0.0500   -0.0002\n",
      "   120        0.4761             nan     0.0500   -0.0003\n",
      "   140        0.4460             nan     0.0500   -0.0004\n",
      "   160        0.4186             nan     0.0500   -0.0004\n",
      "   180        0.3956             nan     0.0500   -0.0003\n",
      "   200        0.3719             nan     0.0500   -0.0004\n",
      "   220        0.3503             nan     0.0500   -0.0004\n",
      "   240        0.3310             nan     0.0500   -0.0003\n",
      "   250        0.3218             nan     0.0500   -0.0001\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0784             nan     0.1000    0.0174\n",
      "     2        1.0508             nan     0.1000    0.0141\n",
      "     3        1.0302             nan     0.1000    0.0096\n",
      "     4        1.0069             nan     0.1000    0.0111\n",
      "     5        0.9893             nan     0.1000    0.0079\n",
      "     6        0.9716             nan     0.1000    0.0090\n",
      "     7        0.9580             nan     0.1000    0.0057\n",
      "     8        0.9426             nan     0.1000    0.0073\n",
      "     9        0.9328             nan     0.1000    0.0043\n",
      "    10        0.9205             nan     0.1000    0.0052\n",
      "    20        0.8408             nan     0.1000    0.0026\n",
      "    40        0.7650             nan     0.1000    0.0005\n",
      "    60        0.7251             nan     0.1000    0.0004\n",
      "    80        0.7037             nan     0.1000    0.0001\n",
      "   100        0.6891             nan     0.1000   -0.0003\n",
      "   120        0.6772             nan     0.1000   -0.0000\n",
      "   140        0.6655             nan     0.1000   -0.0001\n",
      "   160        0.6576             nan     0.1000   -0.0002\n",
      "   180        0.6491             nan     0.1000   -0.0001\n",
      "   200        0.6424             nan     0.1000   -0.0003\n",
      "   220        0.6361             nan     0.1000   -0.0005\n",
      "   240        0.6320             nan     0.1000   -0.0003\n",
      "   250        0.6288             nan     0.1000   -0.0003\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0511             nan     0.1000    0.0279\n",
      "     2        1.0001             nan     0.1000    0.0220\n",
      "     3        0.9652             nan     0.1000    0.0146\n",
      "     4        0.9342             nan     0.1000    0.0128\n",
      "     5        0.9033             nan     0.1000    0.0128\n",
      "     6        0.8782             nan     0.1000    0.0106\n",
      "     7        0.8531             nan     0.1000    0.0100\n",
      "     8        0.8350             nan     0.1000    0.0066\n",
      "     9        0.8168             nan     0.1000    0.0068\n",
      "    10        0.8010             nan     0.1000    0.0074\n",
      "    20        0.7089             nan     0.1000    0.0021\n",
      "    40        0.6230             nan     0.1000   -0.0008\n",
      "    60        0.5747             nan     0.1000   -0.0016\n",
      "    80        0.5372             nan     0.1000   -0.0011\n",
      "   100        0.5019             nan     0.1000   -0.0001\n",
      "   120        0.4683             nan     0.1000   -0.0005\n",
      "   140        0.4417             nan     0.1000   -0.0008\n",
      "   160        0.4157             nan     0.1000   -0.0009\n",
      "   180        0.3931             nan     0.1000   -0.0002\n",
      "   200        0.3704             nan     0.1000   -0.0005\n",
      "   220        0.3493             nan     0.1000   -0.0005\n",
      "   240        0.3305             nan     0.1000   -0.0003\n",
      "   250        0.3233             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold09.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0482             nan     0.1000    0.0283\n",
      "     2        0.9958             nan     0.1000    0.0236\n",
      "     3        0.9528             nan     0.1000    0.0175\n",
      "     4        0.9184             nan     0.1000    0.0139\n",
      "     5        0.8868             nan     0.1000    0.0120\n",
      "     6        0.8579             nan     0.1000    0.0111\n",
      "     7        0.8319             nan     0.1000    0.0102\n",
      "     8        0.8106             nan     0.1000    0.0078\n",
      "     9        0.7898             nan     0.1000    0.0081\n",
      "    10        0.7705             nan     0.1000    0.0075\n",
      "    20        0.6623             nan     0.1000    0.0014\n",
      "    40        0.5545             nan     0.1000   -0.0012\n",
      "    60        0.4873             nan     0.1000   -0.0002\n",
      "    80        0.4308             nan     0.1000   -0.0013\n",
      "   100        0.3829             nan     0.1000   -0.0008\n",
      "   120        0.3422             nan     0.1000   -0.0001\n",
      "   140        0.3063             nan     0.1000   -0.0013\n",
      "   160        0.2761             nan     0.1000   -0.0006\n",
      "   180        0.2513             nan     0.1000   -0.0010\n",
      "   200        0.2273             nan     0.1000   -0.0003\n",
      "   220        0.2063             nan     0.1000   -0.0004\n",
      "   240        0.1872             nan     0.1000   -0.0005\n",
      "   250        0.1775             nan     0.1000   -0.0001\n",
      "\n",
      "- Fold09.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1103             nan     0.0100    0.0018\n",
      "     2        1.1068             nan     0.0100    0.0018\n",
      "     3        1.1031             nan     0.0100    0.0017\n",
      "     4        1.0999             nan     0.0100    0.0017\n",
      "     5        1.0965             nan     0.0100    0.0017\n",
      "     6        1.0932             nan     0.0100    0.0016\n",
      "     7        1.0903             nan     0.0100    0.0016\n",
      "     8        1.0872             nan     0.0100    0.0016\n",
      "     9        1.0842             nan     0.0100    0.0015\n",
      "    10        1.0815             nan     0.0100    0.0015\n",
      "    20        1.0549             nan     0.0100    0.0013\n",
      "    40        1.0116             nan     0.0100    0.0010\n",
      "    60        0.9776             nan     0.0100    0.0007\n",
      "    80        0.9502             nan     0.0100    0.0007\n",
      "   100        0.9255             nan     0.0100    0.0006\n",
      "   120        0.9046             nan     0.0100    0.0005\n",
      "   140        0.8867             nan     0.0100    0.0003\n",
      "   160        0.8718             nan     0.0100    0.0004\n",
      "   180        0.8576             nan     0.0100    0.0003\n",
      "   200        0.8452             nan     0.0100    0.0002\n",
      "   220        0.8341             nan     0.0100    0.0001\n",
      "   240        0.8230             nan     0.0100    0.0002\n",
      "   250        0.8181             nan     0.0100    0.0002\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.01, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1081             nan     0.0100    0.0029\n",
      "     2        1.1015             nan     0.0100    0.0032\n",
      "     3        1.0954             nan     0.0100    0.0026\n",
      "     4        1.0891             nan     0.0100    0.0029\n",
      "     5        1.0836             nan     0.0100    0.0025\n",
      "     6        1.0777             nan     0.0100    0.0028\n",
      "     7        1.0718             nan     0.0100    0.0029\n",
      "     8        1.0661             nan     0.0100    0.0027\n",
      "     9        1.0605             nan     0.0100    0.0026\n",
      "    10        1.0548             nan     0.0100    0.0027\n",
      "    20        1.0067             nan     0.0100    0.0022\n",
      "    40        0.9321             nan     0.0100    0.0014\n",
      "    60        0.8785             nan     0.0100    0.0010\n",
      "    80        0.8373             nan     0.0100    0.0008\n",
      "   100        0.8040             nan     0.0100    0.0006\n",
      "   120        0.7775             nan     0.0100    0.0006\n",
      "   140        0.7562             nan     0.0100    0.0004\n",
      "   160        0.7379             nan     0.0100    0.0003\n",
      "   180        0.7212             nan     0.0100    0.0003\n",
      "   200        0.7076             nan     0.0100    0.0001\n",
      "   220        0.6951             nan     0.0100    0.0002\n",
      "   240        0.6839             nan     0.0100    0.0001\n",
      "   250        0.6790             nan     0.0100    0.0001\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.01, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.1065             nan     0.0100    0.0033\n",
      "     2        1.0987             nan     0.0100    0.0035\n",
      "     3        1.0917             nan     0.0100    0.0034\n",
      "     4        1.0848             nan     0.0100    0.0029\n",
      "     5        1.0783             nan     0.0100    0.0030\n",
      "     6        1.0717             nan     0.0100    0.0030\n",
      "     7        1.0659             nan     0.0100    0.0027\n",
      "     8        1.0596             nan     0.0100    0.0030\n",
      "     9        1.0538             nan     0.0100    0.0027\n",
      "    10        1.0478             nan     0.0100    0.0027\n",
      "    20        0.9938             nan     0.0100    0.0022\n",
      "    40        0.9127             nan     0.0100    0.0014\n",
      "    60        0.8527             nan     0.0100    0.0012\n",
      "    80        0.8055             nan     0.0100    0.0007\n",
      "   100        0.7686             nan     0.0100    0.0005\n",
      "   120        0.7389             nan     0.0100    0.0002\n",
      "   140        0.7135             nan     0.0100    0.0002\n",
      "   160        0.6919             nan     0.0100    0.0001\n",
      "   180        0.6722             nan     0.0100    0.0001\n",
      "   200        0.6553             nan     0.0100    0.0001\n",
      "   220        0.6398             nan     0.0100    0.0001\n",
      "   240        0.6261             nan     0.0100    0.0001\n",
      "   250        0.6197             nan     0.0100    0.0000\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.01, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0969             nan     0.0500    0.0088\n",
      "     2        1.0814             nan     0.0500    0.0080\n",
      "     3        1.0669             nan     0.0500    0.0072\n",
      "     4        1.0538             nan     0.0500    0.0065\n",
      "     5        1.0437             nan     0.0500    0.0043\n",
      "     6        1.0313             nan     0.0500    0.0058\n",
      "     7        1.0222             nan     0.0500    0.0042\n",
      "     8        1.0112             nan     0.0500    0.0052\n",
      "     9        1.0039             nan     0.0500    0.0038\n",
      "    10        0.9972             nan     0.0500    0.0028\n",
      "    20        0.9255             nan     0.0500    0.0019\n",
      "    40        0.8477             nan     0.0500    0.0011\n",
      "    60        0.7973             nan     0.0500    0.0010\n",
      "    80        0.7647             nan     0.0500    0.0005\n",
      "   100        0.7438             nan     0.0500    0.0002\n",
      "   120        0.7267             nan     0.0500    0.0003\n",
      "   140        0.7120             nan     0.0500   -0.0000\n",
      "   160        0.7014             nan     0.0500    0.0002\n",
      "   180        0.6918             nan     0.0500    0.0001\n",
      "   200        0.6838             nan     0.0500    0.0000\n",
      "   220        0.6775             nan     0.0500    0.0000\n",
      "   240        0.6713             nan     0.0500   -0.0001\n",
      "   250        0.6687             nan     0.0500    0.0001\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.05, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0816             nan     0.0500    0.0148\n",
      "     2        1.0538             nan     0.0500    0.0131\n",
      "     3        1.0278             nan     0.0500    0.0126\n",
      "     4        1.0071             nan     0.0500    0.0102\n",
      "     5        0.9873             nan     0.0500    0.0097\n",
      "     6        0.9679             nan     0.0500    0.0096\n",
      "     7        0.9500             nan     0.0500    0.0076\n",
      "     8        0.9339             nan     0.0500    0.0074\n",
      "     9        0.9192             nan     0.0500    0.0067\n",
      "    10        0.9051             nan     0.0500    0.0057\n",
      "    20        0.8009             nan     0.0500    0.0032\n",
      "    40        0.7066             nan     0.0500    0.0010\n",
      "    60        0.6568             nan     0.0500    0.0003\n",
      "    80        0.6226             nan     0.0500    0.0001\n",
      "   100        0.5956             nan     0.0500   -0.0000\n",
      "   120        0.5734             nan     0.0500    0.0002\n",
      "   140        0.5532             nan     0.0500   -0.0003\n",
      "   160        0.5338             nan     0.0500    0.0001\n",
      "   180        0.5165             nan     0.0500   -0.0002\n",
      "   200        0.4969             nan     0.0500   -0.0003\n",
      "   220        0.4827             nan     0.0500   -0.0003\n",
      "   240        0.4675             nan     0.0500   -0.0003\n",
      "   250        0.4594             nan     0.0500   -0.0003\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.05, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0790             nan     0.0500    0.0145\n",
      "     2        1.0454             nan     0.0500    0.0155\n",
      "     3        1.0174             nan     0.0500    0.0118\n",
      "     4        0.9912             nan     0.0500    0.0116\n",
      "     5        0.9677             nan     0.0500    0.0095\n",
      "     6        0.9489             nan     0.0500    0.0081\n",
      "     7        0.9288             nan     0.0500    0.0087\n",
      "     8        0.9111             nan     0.0500    0.0075\n",
      "     9        0.8941             nan     0.0500    0.0072\n",
      "    10        0.8795             nan     0.0500    0.0055\n",
      "    20        0.7683             nan     0.0500    0.0031\n",
      "    40        0.6578             nan     0.0500    0.0009\n",
      "    60        0.5955             nan     0.0500   -0.0002\n",
      "    80        0.5459             nan     0.0500   -0.0001\n",
      "   100        0.5089             nan     0.0500   -0.0000\n",
      "   120        0.4761             nan     0.0500   -0.0003\n",
      "   140        0.4483             nan     0.0500   -0.0004\n",
      "   160        0.4195             nan     0.0500    0.0003\n",
      "   180        0.3953             nan     0.0500   -0.0002\n",
      "   200        0.3734             nan     0.0500   -0.0007\n",
      "   220        0.3531             nan     0.0500   -0.0007\n",
      "   240        0.3343             nan     0.0500   -0.0004\n",
      "   250        0.3254             nan     0.0500   -0.0004\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.05, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0810             nan     0.1000    0.0171\n",
      "     2        1.0519             nan     0.1000    0.0140\n",
      "     3        1.0307             nan     0.1000    0.0069\n",
      "     4        1.0113             nan     0.1000    0.0082\n",
      "     5        0.9900             nan     0.1000    0.0109\n",
      "     6        0.9708             nan     0.1000    0.0090\n",
      "     7        0.9537             nan     0.1000    0.0071\n",
      "     8        0.9413             nan     0.1000    0.0055\n",
      "     9        0.9315             nan     0.1000    0.0048\n",
      "    10        0.9186             nan     0.1000    0.0057\n",
      "    20        0.8410             nan     0.1000    0.0021\n",
      "    40        0.7607             nan     0.1000    0.0009\n",
      "    60        0.7244             nan     0.1000    0.0004\n",
      "    80        0.7009             nan     0.1000   -0.0001\n",
      "   100        0.6838             nan     0.1000    0.0001\n",
      "   120        0.6713             nan     0.1000   -0.0004\n",
      "   140        0.6607             nan     0.1000   -0.0004\n",
      "   160        0.6532             nan     0.1000   -0.0002\n",
      "   180        0.6463             nan     0.1000   -0.0001\n",
      "   200        0.6396             nan     0.1000   -0.0001\n",
      "   220        0.6331             nan     0.1000   -0.0005\n",
      "   240        0.6291             nan     0.1000   -0.0004\n",
      "   250        0.6269             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.10, interaction.depth=1, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0491             nan     0.1000    0.0302\n",
      "     2        1.0041             nan     0.1000    0.0225\n",
      "     3        0.9651             nan     0.1000    0.0179\n",
      "     4        0.9307             nan     0.1000    0.0170\n",
      "     5        0.9015             nan     0.1000    0.0122\n",
      "     6        0.8747             nan     0.1000    0.0121\n",
      "     7        0.8541             nan     0.1000    0.0098\n",
      "     8        0.8314             nan     0.1000    0.0094\n",
      "     9        0.8156             nan     0.1000    0.0066\n",
      "    10        0.8011             nan     0.1000    0.0062\n",
      "    20        0.7060             nan     0.1000    0.0005\n",
      "    40        0.6211             nan     0.1000    0.0005\n",
      "    60        0.5738             nan     0.1000   -0.0001\n",
      "    80        0.5301             nan     0.1000   -0.0009\n",
      "   100        0.4952             nan     0.1000   -0.0008\n",
      "   120        0.4608             nan     0.1000   -0.0009\n",
      "   140        0.4356             nan     0.1000   -0.0006\n",
      "   160        0.4107             nan     0.1000   -0.0008\n",
      "   180        0.3877             nan     0.1000   -0.0000\n",
      "   200        0.3660             nan     0.1000   -0.0007\n",
      "   220        0.3455             nan     0.1000   -0.0005\n",
      "   240        0.3265             nan     0.1000   -0.0004\n",
      "   250        0.3185             nan     0.1000   -0.0006\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.10, interaction.depth=5, n.minobsinnode=10, n.trees=250 \n",
      "+ Fold10.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0469             nan     0.1000    0.0317\n",
      "     2        0.9930             nan     0.1000    0.0223\n",
      "     3        0.9473             nan     0.1000    0.0198\n",
      "     4        0.9098             nan     0.1000    0.0165\n",
      "     5        0.8768             nan     0.1000    0.0140\n",
      "     6        0.8518             nan     0.1000    0.0104\n",
      "     7        0.8295             nan     0.1000    0.0095\n",
      "     8        0.8050             nan     0.1000    0.0085\n",
      "     9        0.7845             nan     0.1000    0.0077\n",
      "    10        0.7668             nan     0.1000    0.0063\n",
      "    20        0.6610             nan     0.1000    0.0006\n",
      "    40        0.5542             nan     0.1000    0.0002\n",
      "    60        0.4848             nan     0.1000    0.0002\n",
      "    80        0.4270             nan     0.1000   -0.0007\n",
      "   100        0.3808             nan     0.1000   -0.0006\n",
      "   120        0.3384             nan     0.1000   -0.0010\n",
      "   140        0.3066             nan     0.1000   -0.0002\n",
      "   160        0.2753             nan     0.1000   -0.0004\n",
      "   180        0.2504             nan     0.1000   -0.0004\n",
      "   200        0.2262             nan     0.1000   -0.0008\n",
      "   220        0.2057             nan     0.1000   -0.0004\n",
      "   240        0.1882             nan     0.1000   -0.0004\n",
      "   250        0.1801             nan     0.1000   -0.0004\n",
      "\n",
      "- Fold10.Rep5: shrinkage=0.10, interaction.depth=9, n.minobsinnode=10, n.trees=250 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting n.trees = 250, interaction.depth = 1, shrinkage = 0.05, n.minobsinnode = 10 on full training set\n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1        1.0966             nan     0.0500    0.0089\n",
      "     2        1.0812             nan     0.0500    0.0079\n",
      "     3        1.0668             nan     0.0500    0.0072\n",
      "     4        1.0546             nan     0.0500    0.0066\n",
      "     5        1.0423             nan     0.0500    0.0061\n",
      "     6        1.0301             nan     0.0500    0.0054\n",
      "     7        1.0185             nan     0.0500    0.0046\n",
      "     8        1.0083             nan     0.0500    0.0047\n",
      "     9        0.9989             nan     0.0500    0.0046\n",
      "    10        0.9900             nan     0.0500    0.0048\n",
      "    20        0.9228             nan     0.0500    0.0023\n",
      "    40        0.8425             nan     0.0500    0.0012\n",
      "    60        0.7948             nan     0.0500    0.0009\n",
      "    80        0.7627             nan     0.0500    0.0004\n",
      "   100        0.7386             nan     0.0500    0.0003\n",
      "   120        0.7223             nan     0.0500   -0.0000\n",
      "   140        0.7101             nan     0.0500    0.0003\n",
      "   160        0.7006             nan     0.0500   -0.0000\n",
      "   180        0.6918             nan     0.0500    0.0000\n",
      "   200        0.6847             nan     0.0500    0.0000\n",
      "   220        0.6779             nan     0.0500   -0.0001\n",
      "   240        0.6715             nan     0.0500   -0.0001\n",
      "   250        0.6687             nan     0.0500    0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tunegrid <-  expand.grid(interaction.depth = c(1, 5, 9),n.trees = (1:5)*50, shrinkage = c(0.05,0.01, 0.1),n.minobsinnode = 10)\n",
    "SGB <- train(y~., data=traindata, method=\"gbm\", trControl = fitControl, tuneGrid = tunegrid)\n",
    "predSGB <- predict(SGB, submitdata, type='prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. Model Tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sgb_grid = expand.grid(n.trees = c(50,150,250), shrinkage = c(0,0.05,0.1), interaction.depth = c(10, 20), n.minobsinnode = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt_grid = expand.grid(cp = seq(0.01,0.1,0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data1_dt = train(y ~ ., data = traindata, method = \"rpart\", trControl = fitControl)\n",
    "data1_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set.seed(1000)\n",
    "DTree <- rpart(y~., data = traindata, method = \"class\", minbucket = 10, cp = 0.01)\n",
    "DTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predDtree1 <- predict(DTree,testdata[,-59],type = 'prob')  #testdata \n",
    "predDtree1 = as.data.table(predDtree1)\n",
    "predDtree1$b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf.roc<-roc(testdata$y,predDtree1$b)\n",
    "auc(rf.roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rpart.plot(DTree)\n",
    "predDTree <- predict(DTree, testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set.seed(300)\n",
    "RF6 <- train(y~., data=alldata, method=\"ranger\", trControl = fitControl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RF6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set.seed(750)\n",
    "predtest3rf <- predict(RF6,submitdata,type='prob')  #testdata \n",
    "predtest3rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tunegrid7 <- expand.grid(.mtry=c(30), .splitrule = \"gini\", .min.node.size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set.seed(750)\n",
    "RF7 <- train(y~., data=alldata, method=\"ranger\", trControl = fitControl, tuneGrid = tunegrid7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "RF7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predtest4rf <- predict(RF7,testdata,type='prob')  #testdata \n",
    "predtest4rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5. Stochastic Gradient Boosting II - 0.8450068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sgb_grid = expand.grid(n.trees = seq(10,100,10), shrinkage = c(0.01,0.05,0.1), \n",
    "                       interaction.depth = c(5,10, 20), n.minobsinnode = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data1_sgb = train(y ~ ., data = traindata, method = \"gbm\", trControl = fitControl, tuneGrid = sgb_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data1_sgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data1_sgb_predict <- predict(data1_sgb,submitdata,type='prob')\n",
    "data1_sgb_predict$b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions <- data1_sgb_predict$b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "send_submission(predictions, token, url=subm_url, submit_now= submit_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>x1</th><th scope=col>x2</th><th scope=col>x3</th><th scope=col>x4</th><th scope=col>x5</th><th scope=col>x6</th><th scope=col>x7</th><th scope=col>x8</th><th scope=col>x9</th><th scope=col>x10</th><th scope=col>...</th><th scope=col>x52</th><th scope=col>x53</th><th scope=col>x54</th><th scope=col>x55</th><th scope=col>x56</th><th scope=col>x57</th><th scope=col>x58</th><th scope=col>x59</th><th scope=col>x60</th><th scope=col>Class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>27   </td><td>1    </td><td>1    </td><td>1    </td><td>18   </td><td> 3   </td><td> 1   </td><td>28   </td><td>119.9</td><td>154.0</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>29   </td><td>0    </td><td>1    </td><td>1    </td><td>14   </td><td> 9   </td><td> 3   </td><td>29   </td><td>  8.8</td><td>126.8</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>27   </td><td>1    </td><td>1    </td><td>1    </td><td>13   </td><td> 4   </td><td>17   </td><td>34   </td><td>  4.9</td><td>172.3</td><td>...  </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>27   </td><td>0    </td><td>1    </td><td>0    </td><td> 8   </td><td>18   </td><td>18   </td><td>26   </td><td> 69.1</td><td>171.2</td><td>...  </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>29   </td><td>1    </td><td>0    </td><td>0    </td><td> 3   </td><td>14   </td><td> 1   </td><td>24   </td><td> 53.2</td><td>150.3</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>27   </td><td>0    </td><td>0    </td><td>1    </td><td>11   </td><td>17   </td><td> 4   </td><td>23   </td><td>167.3</td><td> 71.8</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       " x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x52 & x53 & x54 & x55 & x56 & x57 & x58 & x59 & x60 & Class\\\\\n",
       "\\hline\n",
       "\t 27    & 1     & 1     & 1     & 18    &  3    &  1    & 28    & 119.9 & 154.0 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t 29    & 0     & 1     & 1     & 14    &  9    &  3    & 29    &   8.8 & 126.8 & ...   & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t 27    & 1     & 1     & 1     & 13    &  4    & 17    & 34    &   4.9 & 172.3 & ...   & 0     & 0     & 1     & 0     & 0     & 0     & 1     & 0     & 0     & a    \\\\\n",
       "\t 27    & 0     & 1     & 0     &  8    & 18    & 18    & 26    &  69.1 & 171.2 & ...   & 0     & 0     & 1     & 0     & 0     & 0     & 1     & 0     & 0     & a    \\\\\n",
       "\t 29    & 1     & 0     & 0     &  3    & 14    &  1    & 24    &  53.2 & 150.3 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t 27    & 0     & 0     & 1     & 11    & 17    &  4    & 23    & 167.3 &  71.8 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 | x9 | x10 | ... | x52 | x53 | x54 | x55 | x56 | x57 | x58 | x59 | x60 | Class |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 27    | 1     | 1     | 1     | 18    |  3    |  1    | 28    | 119.9 | 154.0 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "| 29    | 0     | 1     | 1     | 14    |  9    |  3    | 29    |   8.8 | 126.8 | ...   | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | a     |\n",
       "| 27    | 1     | 1     | 1     | 13    |  4    | 17    | 34    |   4.9 | 172.3 | ...   | 0     | 0     | 1     | 0     | 0     | 0     | 1     | 0     | 0     | a     |\n",
       "| 27    | 0     | 1     | 0     |  8    | 18    | 18    | 26    |  69.1 | 171.2 | ...   | 0     | 0     | 1     | 0     | 0     | 0     | 1     | 0     | 0     | a     |\n",
       "| 29    | 1     | 0     | 0     |  3    | 14    |  1    | 24    |  53.2 | 150.3 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "| 27    | 0     | 0     | 1     | 11    | 17    |  4    | 23    | 167.3 |  71.8 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "\n"
      ],
      "text/plain": [
       "  x1 x2 x3 x4 x5 x6 x7 x8 x9    x10   ... x52 x53 x54 x55 x56 x57 x58 x59 x60\n",
       "1 27 1  1  1  18  3  1 28 119.9 154.0 ... 0   0   0   0   1   0   0   0   0  \n",
       "2 29 0  1  1  14  9  3 29   8.8 126.8 ... 0   0   0   0   0   0   0   0   0  \n",
       "3 27 1  1  1  13  4 17 34   4.9 172.3 ... 0   0   1   0   0   0   1   0   0  \n",
       "4 27 0  1  0   8 18 18 26  69.1 171.2 ... 0   0   1   0   0   0   1   0   0  \n",
       "5 29 1  0  0   3 14  1 24  53.2 150.3 ... 0   0   0   0   1   0   0   0   0  \n",
       "6 27 0  0  1  11 17  4 23 167.3  71.8 ... 0   0   0   0   1   0   0   0   0  \n",
       "  Class\n",
       "1 a    \n",
       "2 a    \n",
       "3 a    \n",
       "4 a    \n",
       "5 a    \n",
       "6 a    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "up_train <- upSample(x = traindata[, -ncol(traindata)],\n",
    "                     y = as.factor(traindata$y))\n",
    "head(up_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Variable Importance Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, p): 'up_train' nesnesi bulunamadı\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, p): 'up_train' nesnesi bulunamadı\nTraceback:\n",
      "1. train(Class ~ ., data = up_train, method = \"ranger\", trControl = fitControl, \n .     tuneGrid = tunegrid2, importance = \"permutation\")",
      "2. train.formula(Class ~ ., data = up_train, method = \"ranger\", \n .     trControl = fitControl, tuneGrid = tunegrid2, importance = \"permutation\")",
      "3. eval.parent(m$data)",
      "4. eval(expr, p)",
      "5. eval(expr, p)"
     ]
    }
   ],
   "source": [
    "fitControl <- trainControl(method = \"repeatedcv\", number = 10, verboseIter = TRUE, classProbs = TRUE)\n",
    "tunegrid2 <- expand.grid(.mtry=c(3,5,10,15), .splitrule = \"gini\", .min.node.size = 5)\n",
    "set.seed(500)\n",
    "RF11 <- train(Class~., data=up_train, method=\"ranger\",\n",
    "              trControl = fitControl, tuneGrid = tunegrid2, importance = 'permutation')\n",
    "RF11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_accuracy <- function(predicted, actual){\n",
    "  confusion_table = table(predicted,actual)\n",
    "  TP = confusion_table[2,2]\n",
    "  TN = confusion_table[1,1]\n",
    "  FN = confusion_table[1,2]\n",
    "  FP = confusion_table[2,1]\n",
    "  accuracy = ((FP/(sum(TN+FP))+FN/(sum(FN+TP)))*0.5)\n",
    "  return(accuracy)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in predict(RF11, traindata3): 'RF11' nesnesi bulunamadı\n",
     "output_type": "error",
     "traceback": [
      "Error in predict(RF11, traindata3): 'RF11' nesnesi bulunamadı\nTraceback:\n",
      "1. predict(RF11, traindata3)"
     ]
    }
   ],
   "source": [
    "predRF11 <- predict(RF11, testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in table(predicted, actual): 'predRF11' nesnesi bulunamadı\n",
     "output_type": "error",
     "traceback": [
      "Error in table(predicted, actual): 'predRF11' nesnesi bulunamadı\nTraceback:\n",
      "1. get_accuracy(predRF11, traindata3$y)",
      "2. table(predicted, actual)   # at line 2 of file <text>"
     ]
    }
   ],
   "source": [
    "1 - get_accuracy(predRF11, testdata$y )\n",
    "curve <- roc(as.numeric(predRF11), as.numeric(testdata$y) )\n",
    "auc(curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>x1</th><th scope=col>x2</th><th scope=col>x3</th><th scope=col>x4</th><th scope=col>x5</th><th scope=col>x6</th><th scope=col>x7</th><th scope=col>x8</th><th scope=col>x9</th><th scope=col>x10</th><th scope=col>...</th><th scope=col>x52</th><th scope=col>x53</th><th scope=col>x54</th><th scope=col>x55</th><th scope=col>x56</th><th scope=col>x57</th><th scope=col>x58</th><th scope=col>x59</th><th scope=col>x60</th><th scope=col>Class</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>27   </td><td>1    </td><td>1    </td><td>1    </td><td>18   </td><td> 3   </td><td> 1   </td><td>28   </td><td>119.9</td><td>154.0</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>30   </td><td>0    </td><td>1    </td><td>1    </td><td>18   </td><td>13   </td><td> 3   </td><td>19   </td><td> 86.7</td><td>132.9</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>29   </td><td>0    </td><td>1    </td><td>1    </td><td>14   </td><td> 9   </td><td> 3   </td><td>29   </td><td>  8.8</td><td>126.8</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>33   </td><td>1    </td><td>1    </td><td>0    </td><td> 2   </td><td>15   </td><td>12   </td><td>39   </td><td> 55.0</td><td>187.6</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>27   </td><td>1    </td><td>1    </td><td>1    </td><td>13   </td><td> 4   </td><td>17   </td><td>34   </td><td>  4.9</td><td>172.3</td><td>...  </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>0    </td><td>1    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "\t<tr><td>28   </td><td>1    </td><td>1    </td><td>1    </td><td> 0   </td><td> 0   </td><td> 2   </td><td>40   </td><td>121.3</td><td> 90.0</td><td>...  </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>0    </td><td>a    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       " x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x52 & x53 & x54 & x55 & x56 & x57 & x58 & x59 & x60 & Class\\\\\n",
       "\\hline\n",
       "\t 27    & 1     & 1     & 1     & 18    &  3    &  1    & 28    & 119.9 & 154.0 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t 30    & 0     & 1     & 1     & 18    & 13    &  3    & 19    &  86.7 & 132.9 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t 29    & 0     & 1     & 1     & 14    &  9    &  3    & 29    &   8.8 & 126.8 & ...   & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t 33    & 1     & 1     & 0     &  2    & 15    & 12    & 39    &  55.0 & 187.6 & ...   & 0     & 0     & 0     & 0     & 1     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\t 27    & 1     & 1     & 1     & 13    &  4    & 17    & 34    &   4.9 & 172.3 & ...   & 0     & 0     & 1     & 0     & 0     & 0     & 1     & 0     & 0     & a    \\\\\n",
       "\t 28    & 1     & 1     & 1     &  0    &  0    &  2    & 40    & 121.3 &  90.0 & ...   & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & 0     & a    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 | x9 | x10 | ... | x52 | x53 | x54 | x55 | x56 | x57 | x58 | x59 | x60 | Class |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 27    | 1     | 1     | 1     | 18    |  3    |  1    | 28    | 119.9 | 154.0 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "| 30    | 0     | 1     | 1     | 18    | 13    |  3    | 19    |  86.7 | 132.9 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "| 29    | 0     | 1     | 1     | 14    |  9    |  3    | 29    |   8.8 | 126.8 | ...   | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | a     |\n",
       "| 33    | 1     | 1     | 0     |  2    | 15    | 12    | 39    |  55.0 | 187.6 | ...   | 0     | 0     | 0     | 0     | 1     | 0     | 0     | 0     | 0     | a     |\n",
       "| 27    | 1     | 1     | 1     | 13    |  4    | 17    | 34    |   4.9 | 172.3 | ...   | 0     | 0     | 1     | 0     | 0     | 0     | 1     | 0     | 0     | a     |\n",
       "| 28    | 1     | 1     | 1     |  0    |  0    |  2    | 40    | 121.3 |  90.0 | ...   | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | 0     | a     |\n",
       "\n"
      ],
      "text/plain": [
       "  x1 x2 x3 x4 x5 x6 x7 x8 x9    x10   ... x52 x53 x54 x55 x56 x57 x58 x59 x60\n",
       "1 27 1  1  1  18  3  1 28 119.9 154.0 ... 0   0   0   0   1   0   0   0   0  \n",
       "2 30 0  1  1  18 13  3 19  86.7 132.9 ... 0   0   0   0   1   0   0   0   0  \n",
       "3 29 0  1  1  14  9  3 29   8.8 126.8 ... 0   0   0   0   0   0   0   0   0  \n",
       "4 33 1  1  0   2 15 12 39  55.0 187.6 ... 0   0   0   0   1   0   0   0   0  \n",
       "5 27 1  1  1  13  4 17 34   4.9 172.3 ... 0   0   1   0   0   0   1   0   0  \n",
       "6 28 1  1  1   0  0  2 40 121.3  90.0 ... 0   0   0   0   0   0   0   0   0  \n",
       "  Class\n",
       "1 a    \n",
       "2 a    \n",
       "3 a    \n",
       "4 a    \n",
       "5 a    \n",
       "6 a    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "up_trainall <- upSample(x = alldata[, -ncol(alldata)],\n",
    "                     y = as.factor(alldata$y))\n",
    "head(up_trainall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n",
      "Selecting tuning parameters\n",
      "Fitting mtry = 15, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "3130 samples\n",
       "  60 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 1 times) \n",
       "Summary of sample sizes: 2818, 2818, 2818, 2816, 2818, 2816, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   3    0.8897844  0.7795688\n",
       "   5    0.9246162  0.8492324\n",
       "  10    0.9309999  0.8619998\n",
       "  15    0.9335518  0.8671035\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 15, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RF11all <- train(Class~., data=up_trainall, method=\"ranger\",\n",
    "              trControl = fitControl, tuneGrid = tunegrid2, importance = 'permutation')\n",
    "RF11all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imptable = data.table(varImp(RF11all)$importance)\n",
    "imptable[,variable := paste(\"x\",1:dim(imptable)[1],sep=\"\") ]\n",
    "imptable <- imptable[order(Overall ,decreasing = TRUE),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Overall</th><th scope=col>variable</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1.000000e+02</td><td>x30         </td></tr>\n",
       "\t<tr><td>8.897589e+01</td><td>x23         </td></tr>\n",
       "\t<tr><td>8.123963e+01</td><td>x32         </td></tr>\n",
       "\t<tr><td>4.668737e+01</td><td>x56         </td></tr>\n",
       "\t<tr><td>4.514023e+01</td><td>x14         </td></tr>\n",
       "\t<tr><td>4.092372e+01</td><td>x42         </td></tr>\n",
       "\t<tr><td>2.953508e+01</td><td>x10         </td></tr>\n",
       "\t<tr><td>2.858001e+01</td><td>x27         </td></tr>\n",
       "\t<tr><td>2.650386e+01</td><td>x11         </td></tr>\n",
       "\t<tr><td>2.624266e+01</td><td>x9          </td></tr>\n",
       "\t<tr><td>2.400010e+01</td><td>x8          </td></tr>\n",
       "\t<tr><td>2.267413e+01</td><td>x54         </td></tr>\n",
       "\t<tr><td>2.216936e+01</td><td>x6          </td></tr>\n",
       "\t<tr><td>2.207805e+01</td><td>x5          </td></tr>\n",
       "\t<tr><td>1.969881e+01</td><td>x1          </td></tr>\n",
       "\t<tr><td>1.865524e+01</td><td>x7          </td></tr>\n",
       "\t<tr><td>1.829672e+01</td><td>x48         </td></tr>\n",
       "\t<tr><td>1.079595e+01</td><td>x38         </td></tr>\n",
       "\t<tr><td>9.326370e+00</td><td>x20         </td></tr>\n",
       "\t<tr><td>8.991935e+00</td><td>x17         </td></tr>\n",
       "\t<tr><td>7.694171e+00</td><td>x44         </td></tr>\n",
       "\t<tr><td>7.526384e+00</td><td>x36         </td></tr>\n",
       "\t<tr><td>7.397357e+00</td><td>x24         </td></tr>\n",
       "\t<tr><td>6.704623e+00</td><td>x40         </td></tr>\n",
       "\t<tr><td>5.901456e+00</td><td>x12         </td></tr>\n",
       "\t<tr><td>5.063896e+00</td><td>x3          </td></tr>\n",
       "\t<tr><td>4.487784e+00</td><td>x58         </td></tr>\n",
       "\t<tr><td>4.204273e+00</td><td>x53         </td></tr>\n",
       "\t<tr><td>4.196590e+00</td><td>x34         </td></tr>\n",
       "\t<tr><td>3.851315e+00</td><td>x2          </td></tr>\n",
       "\t<tr><td>3.829107e+00</td><td>x39         </td></tr>\n",
       "\t<tr><td>3.647534e+00</td><td>x4          </td></tr>\n",
       "\t<tr><td>3.281849e+00</td><td>x41         </td></tr>\n",
       "\t<tr><td>3.063102e+00</td><td>x45         </td></tr>\n",
       "\t<tr><td>2.978008e+00</td><td>x16         </td></tr>\n",
       "\t<tr><td>2.956893e+00</td><td>x51         </td></tr>\n",
       "\t<tr><td>2.891491e+00</td><td>x47         </td></tr>\n",
       "\t<tr><td>2.407541e+00</td><td>x15         </td></tr>\n",
       "\t<tr><td>1.941412e+00</td><td>x28         </td></tr>\n",
       "\t<tr><td>1.889586e+00</td><td>x25         </td></tr>\n",
       "\t<tr><td>1.819977e+00</td><td>x21         </td></tr>\n",
       "\t<tr><td>1.696796e+00</td><td>x31         </td></tr>\n",
       "\t<tr><td>1.685484e+00</td><td>x22         </td></tr>\n",
       "\t<tr><td>1.640731e+00</td><td>x19         </td></tr>\n",
       "\t<tr><td>1.454815e+00</td><td>x13         </td></tr>\n",
       "\t<tr><td>1.171878e+00</td><td>x35         </td></tr>\n",
       "\t<tr><td>1.113002e+00</td><td>x18         </td></tr>\n",
       "\t<tr><td>1.112545e+00</td><td>x33         </td></tr>\n",
       "\t<tr><td>9.351964e-01</td><td>x55         </td></tr>\n",
       "\t<tr><td>4.618445e-01</td><td>x60         </td></tr>\n",
       "\t<tr><td>3.782448e-01</td><td>x29         </td></tr>\n",
       "\t<tr><td>2.535659e-01</td><td>x43         </td></tr>\n",
       "\t<tr><td>7.643001e-02</td><td>x46         </td></tr>\n",
       "\t<tr><td>4.241071e-02</td><td>x26         </td></tr>\n",
       "\t<tr><td>2.641365e-02</td><td>x59         </td></tr>\n",
       "\t<tr><td>1.182119e-02</td><td>x57         </td></tr>\n",
       "\t<tr><td>4.228615e-03</td><td>x49         </td></tr>\n",
       "\t<tr><td>0.000000e+00</td><td>x37         </td></tr>\n",
       "\t<tr><td>0.000000e+00</td><td>x50         </td></tr>\n",
       "\t<tr><td>0.000000e+00</td><td>x52         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " Overall & variable\\\\\n",
       "\\hline\n",
       "\t 1.000000e+02 & x30         \\\\\n",
       "\t 8.897589e+01 & x23         \\\\\n",
       "\t 8.123963e+01 & x32         \\\\\n",
       "\t 4.668737e+01 & x56         \\\\\n",
       "\t 4.514023e+01 & x14         \\\\\n",
       "\t 4.092372e+01 & x42         \\\\\n",
       "\t 2.953508e+01 & x10         \\\\\n",
       "\t 2.858001e+01 & x27         \\\\\n",
       "\t 2.650386e+01 & x11         \\\\\n",
       "\t 2.624266e+01 & x9          \\\\\n",
       "\t 2.400010e+01 & x8          \\\\\n",
       "\t 2.267413e+01 & x54         \\\\\n",
       "\t 2.216936e+01 & x6          \\\\\n",
       "\t 2.207805e+01 & x5          \\\\\n",
       "\t 1.969881e+01 & x1          \\\\\n",
       "\t 1.865524e+01 & x7          \\\\\n",
       "\t 1.829672e+01 & x48         \\\\\n",
       "\t 1.079595e+01 & x38         \\\\\n",
       "\t 9.326370e+00 & x20         \\\\\n",
       "\t 8.991935e+00 & x17         \\\\\n",
       "\t 7.694171e+00 & x44         \\\\\n",
       "\t 7.526384e+00 & x36         \\\\\n",
       "\t 7.397357e+00 & x24         \\\\\n",
       "\t 6.704623e+00 & x40         \\\\\n",
       "\t 5.901456e+00 & x12         \\\\\n",
       "\t 5.063896e+00 & x3          \\\\\n",
       "\t 4.487784e+00 & x58         \\\\\n",
       "\t 4.204273e+00 & x53         \\\\\n",
       "\t 4.196590e+00 & x34         \\\\\n",
       "\t 3.851315e+00 & x2          \\\\\n",
       "\t 3.829107e+00 & x39         \\\\\n",
       "\t 3.647534e+00 & x4          \\\\\n",
       "\t 3.281849e+00 & x41         \\\\\n",
       "\t 3.063102e+00 & x45         \\\\\n",
       "\t 2.978008e+00 & x16         \\\\\n",
       "\t 2.956893e+00 & x51         \\\\\n",
       "\t 2.891491e+00 & x47         \\\\\n",
       "\t 2.407541e+00 & x15         \\\\\n",
       "\t 1.941412e+00 & x28         \\\\\n",
       "\t 1.889586e+00 & x25         \\\\\n",
       "\t 1.819977e+00 & x21         \\\\\n",
       "\t 1.696796e+00 & x31         \\\\\n",
       "\t 1.685484e+00 & x22         \\\\\n",
       "\t 1.640731e+00 & x19         \\\\\n",
       "\t 1.454815e+00 & x13         \\\\\n",
       "\t 1.171878e+00 & x35         \\\\\n",
       "\t 1.113002e+00 & x18         \\\\\n",
       "\t 1.112545e+00 & x33         \\\\\n",
       "\t 9.351964e-01 & x55         \\\\\n",
       "\t 4.618445e-01 & x60         \\\\\n",
       "\t 3.782448e-01 & x29         \\\\\n",
       "\t 2.535659e-01 & x43         \\\\\n",
       "\t 7.643001e-02 & x46         \\\\\n",
       "\t 4.241071e-02 & x26         \\\\\n",
       "\t 2.641365e-02 & x59         \\\\\n",
       "\t 1.182119e-02 & x57         \\\\\n",
       "\t 4.228615e-03 & x49         \\\\\n",
       "\t 0.000000e+00 & x37         \\\\\n",
       "\t 0.000000e+00 & x50         \\\\\n",
       "\t 0.000000e+00 & x52         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Overall | variable |\n",
       "|---|---|\n",
       "| 1.000000e+02 | x30          |\n",
       "| 8.897589e+01 | x23          |\n",
       "| 8.123963e+01 | x32          |\n",
       "| 4.668737e+01 | x56          |\n",
       "| 4.514023e+01 | x14          |\n",
       "| 4.092372e+01 | x42          |\n",
       "| 2.953508e+01 | x10          |\n",
       "| 2.858001e+01 | x27          |\n",
       "| 2.650386e+01 | x11          |\n",
       "| 2.624266e+01 | x9           |\n",
       "| 2.400010e+01 | x8           |\n",
       "| 2.267413e+01 | x54          |\n",
       "| 2.216936e+01 | x6           |\n",
       "| 2.207805e+01 | x5           |\n",
       "| 1.969881e+01 | x1           |\n",
       "| 1.865524e+01 | x7           |\n",
       "| 1.829672e+01 | x48          |\n",
       "| 1.079595e+01 | x38          |\n",
       "| 9.326370e+00 | x20          |\n",
       "| 8.991935e+00 | x17          |\n",
       "| 7.694171e+00 | x44          |\n",
       "| 7.526384e+00 | x36          |\n",
       "| 7.397357e+00 | x24          |\n",
       "| 6.704623e+00 | x40          |\n",
       "| 5.901456e+00 | x12          |\n",
       "| 5.063896e+00 | x3           |\n",
       "| 4.487784e+00 | x58          |\n",
       "| 4.204273e+00 | x53          |\n",
       "| 4.196590e+00 | x34          |\n",
       "| 3.851315e+00 | x2           |\n",
       "| 3.829107e+00 | x39          |\n",
       "| 3.647534e+00 | x4           |\n",
       "| 3.281849e+00 | x41          |\n",
       "| 3.063102e+00 | x45          |\n",
       "| 2.978008e+00 | x16          |\n",
       "| 2.956893e+00 | x51          |\n",
       "| 2.891491e+00 | x47          |\n",
       "| 2.407541e+00 | x15          |\n",
       "| 1.941412e+00 | x28          |\n",
       "| 1.889586e+00 | x25          |\n",
       "| 1.819977e+00 | x21          |\n",
       "| 1.696796e+00 | x31          |\n",
       "| 1.685484e+00 | x22          |\n",
       "| 1.640731e+00 | x19          |\n",
       "| 1.454815e+00 | x13          |\n",
       "| 1.171878e+00 | x35          |\n",
       "| 1.113002e+00 | x18          |\n",
       "| 1.112545e+00 | x33          |\n",
       "| 9.351964e-01 | x55          |\n",
       "| 4.618445e-01 | x60          |\n",
       "| 3.782448e-01 | x29          |\n",
       "| 2.535659e-01 | x43          |\n",
       "| 7.643001e-02 | x46          |\n",
       "| 4.241071e-02 | x26          |\n",
       "| 2.641365e-02 | x59          |\n",
       "| 1.182119e-02 | x57          |\n",
       "| 4.228615e-03 | x49          |\n",
       "| 0.000000e+00 | x37          |\n",
       "| 0.000000e+00 | x50          |\n",
       "| 0.000000e+00 | x52          |\n",
       "\n"
      ],
      "text/plain": [
       "   Overall      variable\n",
       "1  1.000000e+02 x30     \n",
       "2  8.897589e+01 x23     \n",
       "3  8.123963e+01 x32     \n",
       "4  4.668737e+01 x56     \n",
       "5  4.514023e+01 x14     \n",
       "6  4.092372e+01 x42     \n",
       "7  2.953508e+01 x10     \n",
       "8  2.858001e+01 x27     \n",
       "9  2.650386e+01 x11     \n",
       "10 2.624266e+01 x9      \n",
       "11 2.400010e+01 x8      \n",
       "12 2.267413e+01 x54     \n",
       "13 2.216936e+01 x6      \n",
       "14 2.207805e+01 x5      \n",
       "15 1.969881e+01 x1      \n",
       "16 1.865524e+01 x7      \n",
       "17 1.829672e+01 x48     \n",
       "18 1.079595e+01 x38     \n",
       "19 9.326370e+00 x20     \n",
       "20 8.991935e+00 x17     \n",
       "21 7.694171e+00 x44     \n",
       "22 7.526384e+00 x36     \n",
       "23 7.397357e+00 x24     \n",
       "24 6.704623e+00 x40     \n",
       "25 5.901456e+00 x12     \n",
       "26 5.063896e+00 x3      \n",
       "27 4.487784e+00 x58     \n",
       "28 4.204273e+00 x53     \n",
       "29 4.196590e+00 x34     \n",
       "30 3.851315e+00 x2      \n",
       "31 3.829107e+00 x39     \n",
       "32 3.647534e+00 x4      \n",
       "33 3.281849e+00 x41     \n",
       "34 3.063102e+00 x45     \n",
       "35 2.978008e+00 x16     \n",
       "36 2.956893e+00 x51     \n",
       "37 2.891491e+00 x47     \n",
       "38 2.407541e+00 x15     \n",
       "39 1.941412e+00 x28     \n",
       "40 1.889586e+00 x25     \n",
       "41 1.819977e+00 x21     \n",
       "42 1.696796e+00 x31     \n",
       "43 1.685484e+00 x22     \n",
       "44 1.640731e+00 x19     \n",
       "45 1.454815e+00 x13     \n",
       "46 1.171878e+00 x35     \n",
       "47 1.113002e+00 x18     \n",
       "48 1.112545e+00 x33     \n",
       "49 9.351964e-01 x55     \n",
       "50 4.618445e-01 x60     \n",
       "51 3.782448e-01 x29     \n",
       "52 2.535659e-01 x43     \n",
       "53 7.643001e-02 x46     \n",
       "54 4.241071e-02 x26     \n",
       "55 2.641365e-02 x59     \n",
       "56 1.182119e-02 x57     \n",
       "57 4.228615e-03 x49     \n",
       "58 0.000000e+00 x37     \n",
       "59 0.000000e+00 x50     \n",
       "60 0.000000e+00 x52     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAAgP9NTU1oaGh8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\nIEgQIEgQIEgQIEgQIEgQIEgQ8P8iZJt3AU6jjQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(varImp(RF11all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 5, summaryFunction = twoClassSummary,\n",
    "                           verboseIter = TRUE, classProbs = TRUE, sampling = \"smote\")\n",
    "tunegrid2 <- expand.grid(.mtry=c(3,5,10,15), .splitrule = \"gini\", .min.node.size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in train.default(x, y, weights = w, ...):\n",
      "\"The metric \"Accuracy\" was not in the result set. ROC will be used instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 5, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "2074 samples\n",
       "  60 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1867, 1866, 1867, 1866, 1867, 1867, ... \n",
       "Addtional sampling using SMOTE\n",
       "\n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  ROC        Sens       Spec     \n",
       "   3    0.8715151  0.9395582  0.4884000\n",
       "   5    0.8730617  0.9247264  0.5500863\n",
       "  10    0.8726159  0.9083709  0.5987922\n",
       "  15    0.8710204  0.9026172  0.6089804\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 5, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(121)\n",
    "RF11 <- train(y~., data = traindata, method=\"ranger\",\n",
    "              trControl = fitControl, tuneGrid = tunegrid2, importance = 'impurity')\n",
    "RF11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predRF11 <- predict(RF11, traindata3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.982760071821344"
      ],
      "text/latex": [
       "0.982760071821344"
      ],
      "text/markdown": [
       "0.982760071821344"
      ],
      "text/plain": [
       "[1] 0.9827601"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 1, case = 2\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.966580802603037"
      ],
      "text/latex": [
       "0.966580802603037"
      ],
      "text/markdown": [
       "0.966580802603037"
      ],
      "text/plain": [
       "Area under the curve: 0.9666"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 - get_accuracy(predRF11, traindata3$y )\n",
    "curve <- roc(as.numeric(predRF11), as.numeric(traindata3$y) )\n",
    "auc(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imptable = data.table(varImp(RF11)$importance)\n",
    "imptable[,variable := 1:dim(imptable)[1] ]\n",
    "imptable <- imptable[order(Overall ,decreasing = TRUE),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Overall</th><th scope=col>variable</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>5.814948e+00</td><td>16          </td></tr>\n",
       "\t<tr><td>4.840195e+00</td><td>25          </td></tr>\n",
       "\t<tr><td>4.662019e+00</td><td>24          </td></tr>\n",
       "\t<tr><td>4.653285e+00</td><td>51          </td></tr>\n",
       "\t<tr><td>3.050259e+00</td><td>19          </td></tr>\n",
       "\t<tr><td>2.751523e+00</td><td>45          </td></tr>\n",
       "\t<tr><td>2.571536e+00</td><td>34          </td></tr>\n",
       "\t<tr><td>2.085157e+00</td><td>13          </td></tr>\n",
       "\t<tr><td>1.819498e+00</td><td>31          </td></tr>\n",
       "\t<tr><td>1.609746e+00</td><td>18          </td></tr>\n",
       "\t<tr><td>1.450827e+00</td><td>33          </td></tr>\n",
       "\t<tr><td>1.280153e+00</td><td>21          </td></tr>\n",
       "\t<tr><td>1.154408e+00</td><td>55          </td></tr>\n",
       "\t<tr><td>9.531924e-01</td><td>22          </td></tr>\n",
       "\t<tr><td>8.303465e-01</td><td>29          </td></tr>\n",
       "\t<tr><td>6.833137e-01</td><td>43          </td></tr>\n",
       "\t<tr><td>6.011811e-01</td><td>60          </td></tr>\n",
       "\t<tr><td>2.505738e-01</td><td>46          </td></tr>\n",
       "\t<tr><td>2.450402e-01</td><td>26          </td></tr>\n",
       "\t<tr><td>2.191659e-01</td><td>49          </td></tr>\n",
       "\t<tr><td>5.110015e-02</td><td>59          </td></tr>\n",
       "\t<tr><td>3.534913e-02</td><td>57          </td></tr>\n",
       "\t<tr><td>6.320365e-05</td><td>37          </td></tr>\n",
       "\t<tr><td>0.000000e+00</td><td>50          </td></tr>\n",
       "\t<tr><td>0.000000e+00</td><td>52          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " Overall & variable\\\\\n",
       "\\hline\n",
       "\t 5.814948e+00 & 16          \\\\\n",
       "\t 4.840195e+00 & 25          \\\\\n",
       "\t 4.662019e+00 & 24          \\\\\n",
       "\t 4.653285e+00 & 51          \\\\\n",
       "\t 3.050259e+00 & 19          \\\\\n",
       "\t 2.751523e+00 & 45          \\\\\n",
       "\t 2.571536e+00 & 34          \\\\\n",
       "\t 2.085157e+00 & 13          \\\\\n",
       "\t 1.819498e+00 & 31          \\\\\n",
       "\t 1.609746e+00 & 18          \\\\\n",
       "\t 1.450827e+00 & 33          \\\\\n",
       "\t 1.280153e+00 & 21          \\\\\n",
       "\t 1.154408e+00 & 55          \\\\\n",
       "\t 9.531924e-01 & 22          \\\\\n",
       "\t 8.303465e-01 & 29          \\\\\n",
       "\t 6.833137e-01 & 43          \\\\\n",
       "\t 6.011811e-01 & 60          \\\\\n",
       "\t 2.505738e-01 & 46          \\\\\n",
       "\t 2.450402e-01 & 26          \\\\\n",
       "\t 2.191659e-01 & 49          \\\\\n",
       "\t 5.110015e-02 & 59          \\\\\n",
       "\t 3.534913e-02 & 57          \\\\\n",
       "\t 6.320365e-05 & 37          \\\\\n",
       "\t 0.000000e+00 & 50          \\\\\n",
       "\t 0.000000e+00 & 52          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Overall | variable |\n",
       "|---|---|\n",
       "| 5.814948e+00 | 16           |\n",
       "| 4.840195e+00 | 25           |\n",
       "| 4.662019e+00 | 24           |\n",
       "| 4.653285e+00 | 51           |\n",
       "| 3.050259e+00 | 19           |\n",
       "| 2.751523e+00 | 45           |\n",
       "| 2.571536e+00 | 34           |\n",
       "| 2.085157e+00 | 13           |\n",
       "| 1.819498e+00 | 31           |\n",
       "| 1.609746e+00 | 18           |\n",
       "| 1.450827e+00 | 33           |\n",
       "| 1.280153e+00 | 21           |\n",
       "| 1.154408e+00 | 55           |\n",
       "| 9.531924e-01 | 22           |\n",
       "| 8.303465e-01 | 29           |\n",
       "| 6.833137e-01 | 43           |\n",
       "| 6.011811e-01 | 60           |\n",
       "| 2.505738e-01 | 46           |\n",
       "| 2.450402e-01 | 26           |\n",
       "| 2.191659e-01 | 49           |\n",
       "| 5.110015e-02 | 59           |\n",
       "| 3.534913e-02 | 57           |\n",
       "| 6.320365e-05 | 37           |\n",
       "| 0.000000e+00 | 50           |\n",
       "| 0.000000e+00 | 52           |\n",
       "\n"
      ],
      "text/plain": [
       "   Overall      variable\n",
       "1  5.814948e+00 16      \n",
       "2  4.840195e+00 25      \n",
       "3  4.662019e+00 24      \n",
       "4  4.653285e+00 51      \n",
       "5  3.050259e+00 19      \n",
       "6  2.751523e+00 45      \n",
       "7  2.571536e+00 34      \n",
       "8  2.085157e+00 13      \n",
       "9  1.819498e+00 31      \n",
       "10 1.609746e+00 18      \n",
       "11 1.450827e+00 33      \n",
       "12 1.280153e+00 21      \n",
       "13 1.154408e+00 55      \n",
       "14 9.531924e-01 22      \n",
       "15 8.303465e-01 29      \n",
       "16 6.833137e-01 43      \n",
       "17 6.011811e-01 60      \n",
       "18 2.505738e-01 46      \n",
       "19 2.450402e-01 26      \n",
       "20 2.191659e-01 49      \n",
       "21 5.110015e-02 59      \n",
       "22 3.534913e-02 57      \n",
       "23 6.320365e-05 37      \n",
       "24 0.000000e+00 50      \n",
       "25 0.000000e+00 52      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tail(imptable,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAAgP9NTU1oaGh8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\nRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(varImp(RF11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction 1: 0.869453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>37</li>\n",
       "\t<li>50</li>\n",
       "\t<li>52</li>\n",
       "\t<li>57</li>\n",
       "\t<li>40</li>\n",
       "\t<li>15</li>\n",
       "\t<li>46</li>\n",
       "\t<li>55</li>\n",
       "\t<li>49</li>\n",
       "\t<li>31</li>\n",
       "\t<li>19</li>\n",
       "\t<li>51</li>\n",
       "\t<li>5</li>\n",
       "\t<li>33</li>\n",
       "\t<li>3</li>\n",
       "\t<li>2</li>\n",
       "\t<li>6</li>\n",
       "\t<li>12</li>\n",
       "\t<li>1</li>\n",
       "\t<li>7</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 37\n",
       "\\item 50\n",
       "\\item 52\n",
       "\\item 57\n",
       "\\item 40\n",
       "\\item 15\n",
       "\\item 46\n",
       "\\item 55\n",
       "\\item 49\n",
       "\\item 31\n",
       "\\item 19\n",
       "\\item 51\n",
       "\\item 5\n",
       "\\item 33\n",
       "\\item 3\n",
       "\\item 2\n",
       "\\item 6\n",
       "\\item 12\n",
       "\\item 1\n",
       "\\item 7\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 37\n",
       "2. 50\n",
       "3. 52\n",
       "4. 57\n",
       "5. 40\n",
       "6. 15\n",
       "7. 46\n",
       "8. 55\n",
       "9. 49\n",
       "10. 31\n",
       "11. 19\n",
       "12. 51\n",
       "13. 5\n",
       "14. 33\n",
       "15. 3\n",
       "16. 2\n",
       "17. 6\n",
       "18. 12\n",
       "19. 1\n",
       "20. 7\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 37 50 52 57 40 15 46 55 49 31 19 51  5 33  3  2  6 12  1  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "removecols <- imptable[(nrow(imptable)-19):nrow(imptable),variable ];removecols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceddata <- traindata[,-removecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "41"
      ],
      "text/latex": [
       "41"
      ],
      "text/markdown": [
       "41"
      ],
      "text/plain": [
       "[1] 41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncol(reduceddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(153)\n",
    "trainIndex = createDataPartition(reduceddata$y, p = 0.7, list = FALSE)\n",
    "reducedtrain = reduceddata[trainIndex, ]\n",
    "reducedtest = reduceddata[-trainIndex, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: x59\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: x59\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in preProcess.default(thresh = 0.95, k = 5, freqCut = 19, uniqueCut = 10, :\n",
      "\"These variables have zero variances: x59\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 3, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "1453 samples\n",
       "  40 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "Pre-processing: centered (40), scaled (40) \n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1308, 1307, 1308, 1307, 1308, 1308, ... \n",
       "Addtional sampling using down-sampling prior to pre-processing\n",
       "\n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  ROC        Sens       Spec     \n",
       "   3    0.8691979  0.7107973  0.8459206\n",
       "   5    0.8683102  0.7376214  0.8313492\n",
       "  10    0.8619392  0.7350792  0.8145714\n",
       "  15    0.8603091  0.7374495  0.8224762\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 3, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(821)\n",
    "RF11 <- train(y~., data = reducedtrain, method=\"ranger\",  num.trees = 350, metric = \"ROC\", preProc=c(\"center\", \"scale\"),\n",
    "              trControl = fitControl, tuneGrid = tunegrid2, importance = 'permutation')\n",
    "RF11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRF11 <- predict(RF11, reducedtest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.773047357198968"
      ],
      "text/latex": [
       "0.773047357198968"
      ],
      "text/markdown": [
       "0.773047357198968"
      ],
      "text/plain": [
       "[1] 0.7730474"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 1, case = 2\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.704571728849186"
      ],
      "text/latex": [
       "0.704571728849186"
      ],
      "text/markdown": [
       "0.704571728849186"
      ],
      "text/plain": [
       "Area under the curve: 0.7046"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 - get_accuracy(predRF11, reducedtest$y )\n",
    "curve <- roc(as.numeric(predRF11), as.numeric(reducedtest$y) )\n",
    "auc(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions <- predict(RF11, submitdata,type = \"prob\" )$b; predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Format OK\"\n",
      "$submission\n",
      "[0.0523,0.2588,0.4434,0.0562,0.6388,0.2108,0.5244,0.4603,0.1225,0.0023,0.2775,0.2808,0.039,0.8481,0.2843,0.0911,0.0135,0.2204,0.2994,0.0167,0.1243,0.4191,0.8808,0.0052,0.5705,0.1121,0.3643,0.3883,0.4374,0.324,0.4455,0.7116,0.0075,0.3681,0.6879,0.3699,0.7351,0.6243,0.9156,0.0163,0.1895,0.2016,0.1141,0.7187,0.2286,0.7592,0.2369,0.0533,0.254,0.1777,0.0343,0.0321,0.2266,0.3699,0.7994,0.2784,0.7149,0.404,0.1441,0.4706,0.8303,0.0073,0.1176,0.1559,0.0924,0.0806,0.004,0.0147,0.5023,0.142,0.5761,0.0412,0.0101,0.6691,0.062,0.2365,0.7351,0.0612,0.5356,0.1304,0.8571,0.1258,0.0147,0.6158,0.3664,0.3847,0.011,0.1022,0.803,0.2431,0.0329,0.0489,0.1039,0.6701,0.1478,0.4918,0.132,0.0971,0.5141,0.4477,0.5476,0.0303,0.6018,0.4275,0.4475,0.4431,0.0452,0.2084,0.2137,0.0578,0.0611,0.6624,0.1553,0.6379,0.2286,0.012,0.0194,0.386,0.0888,0.0315,0.0184,0.0213,0.0459,0.084,0.1936,0.0451,0.0305,0.2342,0.7028,0.0423,0.5971,0.4211,0.3769,0.295,0.5388,0.6047,0.0852,0.591,0.0976,0.6008,0.3868,0.1751,0.4224,0.4271,0.4041,0.3408,0.6408,0.2993,0.3137,0.1266,0.0665,0.3887,0.2243,0.2064,0.0118,0.1487,0.0471,0.4652,0.3725,0.8604,0.1496,0.1408,0.03,0.1497,0.0264,0.5672,0.0866,0.5359,0.0497,0.1686,0.6732,0.3562,0.4491,0.0685,0.4098,0.0295,0.2271,0.412,0.617,0.8338,0.8046,0.6409,0.4827,0.0055,0.776,0.0474,0.0019,0.1541,0.1988,0.1512,0.0157,0.518,0.6468,0.2596,0.0039,0.0034,0.0626,0.0532,0.0233,0.7085,0.5646,0.1901,0.1881,0.6384,0.4356,0.6602,0.027,0.8772,0.1112,0.7501,0.0973,0.0097,0.0973,0.5179,0.1632,0.668,0.2621,0.0695,0.0275,0.0687,0.128,0.0234,0.3999,0.4988,0.1076,0.1524,0.1842,0.7178,0.2605,0.0059,0.8323,0.0849,0.5678,0.0464,0.0193,0.0518,0.5354,0.1707,0.5679,0.6051,0.0798,0.0525,0.0385,0.102,0.0962,0.0539,0.417,0.5364,0.458,0.0143,0.054,0.6936,0.6938,0.401,0.0965,0.4608,0.771,0.2919,0.0641,0.1971,0.5316,0.1807,0.0831,0.3765,0.5293,0.3386,0.0652,0.184,0.1341,0.168,0.3886,0.2763,0.2061,0.2765,0.0628,0.2514,0.6008,0.0554,0.3541,0.1634,0.2841,0.073,0.3741,0.1185,0.6731,0.417,0.117,0.4785,0.5955,0.4881,0.4881,0.0698,0.4825,0.4814,0.3715,0.8214,0.5949,0.1486,0.0429,0.031,0.7627,0.0357,0.0505,0.0057,0.0607,0.0866,0.1638,0.2509,0.0424,0.0064,0.1192,0.5043,0.6133,0.0628,0.0584,0.5666,0.2516,0.1856,0.5856,0.608,0.0255,0.2038,0.0445,0.2945,0.2227,0.6413,0.6547,0.4167,0.0637,0.4283,0.5146,0.2381,0.5038,0.5861,0.744,0.2714,0.4702,0.6558,0.0495,0.2548,0.0898,0.0744,0.0715,0.3014,0.4648,0.0524,0.0989,0.3648,0.0608,0.1319,0.0786,0.0499,0.231,0.6533,0.1659,0.3325,0.613,0.121,0.1353,0.2296,0.0923,0.0397,0.2045,0.1754,0.4396,0.293,0.6872,0.0232,0.0675,0.0961,0.7091,0.1566,0.0064,0.0885,0.1158,0.7376,0.124,0.0788,0.0076,0.3104,0.2395,0.4697,0.0715,0.7333,0.1813,0.0764,0.0113,0.4086,0.1055,0.36,0.0154,0.1088,0.661,0.1826,0.0577,0.3723,0.0494,0.0708,0.0227,0.0426,0.0776,0.7821,0.1131,0.8659,0.0844,0.5017,0.0925,0.4698,0.6942,0.1129,0.3171,0.0331,0.4498,0.0155,0.3777,0.4762,0.3218,0.7133,0.5001,0.7208,0.3841,0.7273,0.0096,0.0536,0.2081,0.5727,0.2311,0.5444,0.0966,0.4442,0.4609,0.1364,0.364,0.3094,0.4424,0.2817,0.024,0.2895,0.5326,0.3355,0.1181,0.1545,0.143,0.7977,0.2282,0.4005,0.0024,0.0122,0.6905,0.023,0.4582,0.4238,0.0692,0.6509,0.2272,0.0013,0.4209,0.4161,0.7026,0.6485,0.1201,0.1356,0.2388,0.7791,0.1083,0.4168,0.2336,0.0391,0.0591,0.5665,0.4169,0.6306,0.1264,0.1072,0.4802,0.039,0.3209,0.6763,0.2637,0.0314,0.0171,0.1649,0.72,0.1352,0.0128,0.2135,0.6418,0.0599,0.4401,0.0608,0.5752,0.3978,0.6256,0.6452,0.1128,0.0378,0.7713,0.0136,0.7569,0.1079,0.1339,0.1152,0.024,0.0139,0.1766,0.0093,0.4018,0.5458,0.1366,0.0437,0.5244,0.3765,0.0526,0.0466,0.0536,0.3264,0.0454,0.1336,0.5836,0.4371,0.0261,0.048,0.51,0.0196,0.3536,0.3411,0.0558,0.7205,0.2431,0.0079,0.2951,0.0052,0.5662,0.6971,0.4388,0.0091,0.9119,0.7294,0.4945,0.2441,0.175,0.8525,0.0377,0.1994,0.1847,0.0773,0.6512,0.7907,0.0159,0.4663,0,0.0589,0.0994,0.7941,0.4466,0.4694,0.1302,0.0585,0.0764,0.4234,0.0475,0.0676,0.0817,0.2284,0.2474,0.0323,0.3191,0.1166,0.5625,0.3946,0.023,0.2053,0.3638,0.3444,0.2681,0.5103,0.4727,0.2376,0.1147,0.0256,0.0154,0.0222,0.2982,0.4229,0.3004,0.4191,0.01,0.1249,0.2751,0.1036,0.4751,0.1933,0.0718,0.0086,0.0206,0.5893,0.0193,0.0382,0.0596,0.5444,0.1238,0.3938,0.5155,0.0766,0.1094,0.4231,0.847,0.2071,0.0953,0.1311,0.2541,0.4086,0.6127,0.1943,0.1146,0.6208,0.0385,0.3557,0.2162,0.8953,0.24,0.5937,0.8485,0.4966,0.3824,0.3259,0.1416,0.0221,0.304,0.095,0.281,0.1798,0.4528,0.3106,0.3446,0.0147,0.4121,0.2805,0.0413,0.2501,0.2581,0.1451,0.017,0.1481,0.141,0.0249,0.1214,0.0856,0.5745,0.333,0.1508,0.1028,0.429,0.3978,0.2772,0.2897,0.5959,0.5934,0.5496,0.3219,0.2451,0.0235,0.1104,0.6096,0.2696,0.1428,0.6949,0.0507,0.0013,0.588,0.4673,0.0973,0.0031,0.6721,0.4342,0.525,0.0178,0.6095,0.3787,0.387,0.0651,0.8121,0.1054,0.1884,0.1148,0.5953,0.2935,0.483,0.1371,0.2284,0.0627,0.1399,0.507,0.437,0.0481,0.4129,0.0705,0.0973,0.0487,0.3977,0.0709,0.3088,0.6582,0.2305,0.5761,0.6105,0.2778,0.0706,0.3369,0.0678,0.5305,0.8897,0.0584,0.0328,0.3866,0.4777,0.5127,0.4762,0.7315,0.0721,0.1592,0.4517,0.5926,0.1698,0.1094,0.0628,0.3984,0.5199,0.4842,0.0902,0.4889,0.5934,0.2045,0.1211,0.019,0.1107,0.0446,0.5657,0.0653,0.019,0.0041,0.159,0.672,0.1322,0.7597,0.0896,0.4812,0.6703,0.0327,0.2345,0.0029,0.055,0.2439,0.2542,0.0988,0.7335,0.193,0.0873,0.0846,0.0314,0.2002,0.5974,0.1298,0.3766,0.8475,0.0975,0.0036,0.0498,0.5371,0.3793,0.0289,0.0488,0.5155,0.8486,0.5312,0.1291,0.8327,0.3222,0.0507,0.4903,0.1296,0.0121,0.6527,0.0099,0.0947,0.4451,0.4144,0.3216,0.0971,0.553,0.001,0.225,0.6321,0.0727,0.2851,0.0296,0.0124,0.0535,0.0577,0.4541,0.2932,0.0136,0.3503,0.2415,0.3723,0.0763,0.4592,0.0847,0.1177,0.5733,0.422,0.1381,0.0064,0.0956,0.4245,0.5431,0.2403,0.2753,0.4743,0.0859,0.218,0.0733,0.5219,0.5361,0.0689,0.0489,0.6422,0.0075,0.3544,0.5526,0.105,0.003,0.669,0.6222,0.7738,0.5916,0.174,0.4716,0.073,0.6977,0.3152,0.6614,0.0321,0.6829,0.6437,0.5872,0.0583,0.1345,0.0756,0.4092,0.0906,0.3558,0.6952,0.0234,0.7664,0.327,0.0411,0.0057,0.0181,0.5965,0.0091,0.1036,0.1174,0.3812,0.5903,0.107,0.1203,0.4988,0.3874,0.0941,0.0998,0.0223,0.0095,0.2678,0.3044,0.4545,0.182,0.4159,0.4753,0.7879,0.9394,0.5182,0.5597,0.2409,0.1146,0.0974,0.0543,0.831,0.4047,0.5941,0.6027,0.6602,0.1475,0.2838,0.3724,0.1946,0.2425,0.3351,0.1514,0.1504,0.1793,0.613,0.0142,0.415,0.6135,0.1344,0.1934,0.044,0.4246,0.5351,0.0644,0.8021,0.7057,0.4985,0.0651,0.0157,0.5416,0.4742,0.5915,0.1109,0.0408,0.7484,0.4452,0.8386,0.0984,0.0051,0.4038,0.2742,0.2001,0.4938,0.9368,0.078,0.1376,0.6925,0.2884,0.1299,0.4251,0.0573,0.1171,0.2571,0.4293,0.4302,0.5099,0.013,0.2603,0.0763,0.0589,0.9364,0.8032,0.7609,0.3302,0.3537,0.1288,0.7908,0.2285,0.3889,0.0257,0.0227,0.7236,0.0775,0.0659,0.2248,0.0124,0.0717,0.0724,0.4601,0.2378,0.1278,0.4845,0.7155,0.0867,0.3077,0.5845,0.4249,0.3338,0.0975,0.8836,0.3664,0.3897,0.6744,0.8643,0.6099,0.505,0.5729,0.6736,0.4311,0.2767,0.2794,0.4049,0.0034,0.078,0.0609,0.1164,0.354,0.1338,0.7906,0.5364,0.0229,0.0086,0.6053,0.0698,0.408,0.4696,0.382,0.3282,0.7705,0.6107,0.0442,0.0024,0.0776,0.3802,0.2382,0.0906,0.5881,0.1389,0.6654,0.3205,0.3647,0.2484,0.7017,0.2093,0.0152,0.0231,0.0108,0.0043,0.168,0.2776,0.4517,0.3077,0.0141,0.1352,0.4209,0.7828,0.4703,0.4721,0.0308,0.0802,0.5413,0.069,0.1425,0.8412,0.5846,0.7968,0.0799,0.0552,0.0585,0.2058,0.0517,0.8416,0.2163,0.4152,0.109,0.3837,0.0554,0.0438,0.3746,0.0043,0.0194,0.4185,0.0725,0.0148,0.1607,0.3459,0.2626,0.4409,0.3299,0.4216,0.0043,0.4929,0.8378,0.2838,0.6286,0.6233,0.2553,0.141,0.131,0.3719,0.0702,0.4455,0.6538,0.7608,0.0652,0.057,0.5419,0.6022,0.0451,0.4106,0.3896,0.9371,0.3521,0.5655,0.7316,0.7868,0.0613,0.3052,0.2197,0.386,0.684,0.0325,0.8753,0.2308,0.0216,0.1376,0.4754,0.2264,0.2442,0.5162,0.2143,0.1002,0.6646,0.0036,0.131,0.2203,0.3802,0.5407,0.4706,0.6061,0.0782,0.6747,0.0555,0.3661,0.2535,0.1262,0.7428,0.6197,0.2776,0.0134,0.0208,0.0518,0.1067,0.8308,0.4707,0.223,0.1185,0.0191,0.3151,0.2766,0.0877,0.4858,0.1295,0.631,0.004,0.6667,0.0255,0.0064,0.5769,0.7447,0.2533,0.1205,0.1189,0.0606,0.4721,0.0571,0.3719,0.1281,0.0141,0.1353,0.5678,0.1666,0.6529,0.7761,0.3609,0.6158,0.337,0.5885,0.6406,0.1169,0.6395,0.3719,0.2842,0.2305,0.5721,0.5827,0.1086,0.2952,0.0048,0.3626,0.3436,0.1181,0.4771,0.063,0.464,0.0394,0.5796,0.1012,0.3256,0.1552,0.0645,0.0116,0.1133,0.2423,0.5483,0.3653,0.0696,0.0415,0.0978,0.4465,0.0934,0.1409,0.1914,0.3779,0.2145,0.303,0.0856,0.1599,0.0137,0.3878,0.0701,0.1094,0.1171,0.0625,0.6201,0.0645,0.0219,0.8153,0.641,0.7702,0.1038,0.2076,0.1783,0.5547,0.0185,0.0341,0.342,0.0276,0.072,0.4603,0.5561,0.3642,0.0414,0.0796,0.627,0.5803,0.0371,0.1814,0.2656,0.2844,0.7992,0.646,0.5019,0.24,0.0112,0.0243,0.691,0.3644,0.048,0.6027,0.0438,0.2087,0.391,0.4925,0.921,0.4108,0.3201,0.1998,0.8645,0.0567,0.0843,0.4087,0.0437,0.3473,0.5739,0.0403,0.0431,0.5741,0.0899,0.1846,0.541,0.0689,0.5102,0.0649,0.0507,0.3631,0.1255,0.421,0.7075,0.3824,0.4741,0.028,0.0744,0.0387,0.5292,0.0332,0.0541,0.0623,0.0274,0.4348,0.0652,0.6336,0.1809,0.2854,0.8144,0.4993,0.7077,0.0768,0.6708,0.801,0.2962,0.0503,0.3041,0.5886,0.0163,0.4208,0.5248,0.3002,0.6657,0.0501,0.0738,0.6251,0,0.2474,0.1263,0.1667,0.0126,0.4965,0.5816,0.0077,0.178,0.5072,0.6868,0.5915,0.2464,0.3961,0.3085,0.0221,0.5159,0.2121,0.0053,0.2681,0.1018,0.0243,0.2679,0.3913,0.4061,0.1792,0.7014,0.4018,0.0598,0.0187,0.1974,0.105,0.4368,0.0167,0.1673,0.5893,0.3212,0.0006,0.4281,0.0487,0.1568,0.2909,0.055,0.0364,0.4017,0.0984,0.7739,0.5732,0.3125,0.3723,0.3258,0.0071,0.5545,0.5642,0.2982,0.4372,0.4195,0.0057,0.3613,0.3079,0.153,0.4616,0.2914,0.552,0.2321,0.0901,0.0846,0.2919,0.0037,0.8403,0.2333,0.5529,0.2754,0.3899,0.5851,0.1689,0.3926,0.5071,0.3509,0.4967,0.4585,0.2464,0.0326,0.0262,0.0792,0.2013,0.4052,0.09,0.2566,0.0791,0.0984,0.0791,0.2545,0.5944,0.0287,0.5328,0.846,0.0989,0.4328,0.2565,0.1013,0.183,0.6502,0.0105,0.7869,0.2298,0.1167,0.6164,0.6545,0.0843,0.0087,0.4674,0.4196,0.6704,0.3446,0.3764,0.4996,0.4832,0.2902,0.0226,0.7529,0.01,0.4032,0.0669,0.7398,0.0954,0.2689,0.2042,0.0176,0.306,0.4675,0.1772,0.0154,0.6449,0.1777,0.2542,0.6686,0.0571,0.2601,0.5472,0.0795,0.0527,0.2704,0.4706,0.1551,0.603,0.4505,0.4241,0.0153,0.4794,0.3944,0.0029,0.5489,0.4965,0.7263,0.0561,0.7838,0.1082,0.0552,0.0014,0.7626,0.5087,0.0057,0.0563,0.0063,0.0611,0.1583,0.4438,0.4194,0.2018,0.889,0.0621,0.3205,0.8478,0.0164,0.1816,0.4397,0.3466,0.4114,0.3564,0.5714,0.7784,0.7808,0.466,0.2992,0.2795,0.178,0.007,0.0256,0.1002,0.7074,0.3158,0.2633,0.1459,0.5339,0.0897,0.1044,0.7347,0.6273,0.5608,0.4218,0.4229,0.0097,0.1028,0.1165,0.8363,0.0017,0.1735,0.4081,0.4341,0.0827,0,0.2345,0.52,0.6787,0.4441,0.0614,0.2763,0.3799,0.8333,0.0385,0.2901,0.6291,0.4154,0.1333,0.7182,0.0369,0.2133,0.7375,0.2541,0.0181,0.4012,0.1493,0.0432,0.6566,0.1258,0.3323,0.0145,0.3302,0.0051,0.0432,0.1123,0.161,0.1087,0.0373,0.1276,0.08,0.075,0.001,0.067,0.0763,0.0362,0.0029,0.4182,0.8422,0.2714,0.0341,0.0177,0.5736,0.2815,0.2814,0.5864,0.7042,0.1552,0.5368,0.0673,0.0233,0.7123,0.3173,0.5383,0.3509,0.2977,0.2875,0.0259,0.526,0.1178,0.0508,0.3558,0.4284,0.5252,0.1919,0.7345,0.3714,0.1747,0.1761,0.1419,0.4937,0.1481,0.0204,0.5899,0.4091,0.3289,0.0087,0.0734,0.4602,0.3324,0.7116,0.0624,0.4574,0.1142,0.7969,0.0048,0.0424,0.2818,0.583,0.7295,0.0109,0.0537,0.0234,0.1696,0.4166,0.0164,0.9089,0.7035,0.644,0.3504,0.6826,0.1795,0.6114,0.2966,0.1636,0.2421,0.5103,0.3761,0.0607,0.4164,0.391,0,0.0899,0.007,0.5088,0.5917,0.1635,0.51,0.1002,0.2284,0.4239,0.0368,0.4339,0.1074,0.0096,0.5776,0.1428,0.0833,0.1054,0.3269,0.3249,0.0106,0.3588,0.5553,0.0209,0.5063,0.1155,0.0649,0.3389,0.3351,0.2204,0.016,0.2569,0.1209,0.3779,0.1896,0.0214,0.0769,0.3821,0.4562,0.8768,0.0267,0.2268,0.0512,0.5304,0.2574,0.1901,0.094,0.4294,0.187,0.0483,0.0149,0.3723,0.2258,0.0566,0.0604,0.516,0.5952,0.184,0.0416,0.0029,0.4109,0.1271,0.2936,0.1364,0.6094,0.1245,0.3439,0.1118,0.4338,0.1617,0.1028,0.6667,0.7905,0.1504,0.5086,0.2327,0.0179,0.0402,0.2315,0.3553,0.5578,0.3437,0.5089,0.0286,0.3143,0.7229,0.8233,0.0979,0.1159,0.0166,0.3821,0.0062,0.0498,0.6653,0.1242,0.1631,0.4404,0.7396,0.075,0.0812,0.2545,0.0221,0.603,0.5474,0.773,0.2814,0.2068,0.034,0.0978,0.0029,0.6587,0.4141,0.0771,0.3807,0.0793,0.7157,0.4779,0.8198,0.0347,0.7348,0.4845,0.1578,0.0572,0.3536,0.1096,0.1768,0.624,0.2544,0.0029,0.2513,0.3054,0.0809,0.0369,0.5882,0.3373,0.1789,0.011,0.286,0.2747,0.6187,0.4474,0.1296,0.4677,0.3377,0.6156,0.0319,0.2178,0.1,0.0344,0.0836,0.2037,0.2288,0.6681,0.0984,0.1953,0.0485,0.8322,0.0441,0.5629,0.0561,0.1026,0.1168,0.7827,0.581,0.4577,0.7483,0.0402,0.4561,0.558,0.1211,0.2161,0.7399,0.0036,0.3017,0.3861,0.0596,0.2879,0.1363,0.4491,0.6983,0.0194,0.6088,0.0839,0.6707,0.2217,0.5181,0.7176,0.0296,0.0191,0.0263,0.7937,0.6998,0.0412,0.0006,0.0413,0.0373,0.0654,0.0694,0.0837,0.663,0.1046,0.0021,0.7479,0.6569,0.213,0.0399,0.7124,0.031,0.2819,0.0302,0.226,0.0101,0.3866,0.405,0.1674,0.3998,0.0183,0.608,0.2598,0.0026,0.5164,0.0772,0.5742,0.5449,0.2599,0.1789,0.7087,0.1399,0.0995,0.4283,0.0792,0.4452,0.596,0.2156,0.0475,0.38,0.0761,0.2893,0.0072,0.5856,0.6032,0.6662,0.0768,0.0328,0.0721,0.8055,0.0372,0.438,0.0914,0.0266,0.2488,0.014,0.4109,0.0884,0.6136,0.6075,0.5381,0.0396,0.0231,0.3521,0.009,0.5795,0.017,0.8149,0.7261,0.6307,0.1166,0.265,0.3193,0.2544,0.5291,0.0215,0.0093,0.883,0.1319,0.6493,0.5646,0.0351,0.4515,0.1022,0.3112,0.3928,0.3278,0.2175,0.4048,0.3632,0.0412,0.0814,0.5945,0.5055,0.407,0.1571,0.2539,0.5034,0.2988,0.757,0.43,0.3766,0.5159,0.1314,0.03,0.7159,0.0086,0.0256,0.5728,0.7472,0.0161,0.0464,0.297,0.0831,0.9005,0.0595,0.4536,0.0306,0.2216,0.2641,0.8199,0.6559,0.5805,0.0889,0.6075,0.0789,0.2273,0.2952,0.3332,0.7289,0.4784,0.0178,0.2131,0.1509,0.2032,0.0419,0.6773,0.8256,0.4567,0.4626,0.4057,0.092,0.358,0.1168,0.1937,0.3073,0.283,0.2002,0.1011,0.8208,0.3551,0.0718,0.0875,0.7549,0.308,0.1955,0.1647,0.3192,0.0781,0.5577,0.7611,0.5502,0.3352,0.0638,0.5602,0.1481,0.109,0.5536,0.1915,0.391,0.0029,0.1219,0.5454,0.2706,0.1997,0.343,0.2673,0.0669,0.4076,0.4169,0.3304,0.4877,0.2431,0.3787,0.0006,0.7451,0.375,0.3593,0.1129,0.4422,0.2777,0.5354,0.3182,0.3359,0.3449,0.722,0.6265,0.2771,0.019,0.1725,0.6439,0.2677,0.0057,0.0253,0.3741,0.0229,0.4866,0.45,0.0208,0.0346,0.0024,0.0928,0.0914,0.1331,0.3256,0.1089,0.3872,0.0624,0.0375,0.3024,0.0391,0.3142,0.6469,0.351,0.6792,0.2439,0.0736,0.0687,0.1972,0.1207,0.0219,0.4187,0.02,0.6654,0.0342,0.2429,0.3722,0.0393,0.3736,0.0455,0.2275] \n",
      "\n",
      "[1] \"Successfully submitted. Below you can see the details of your submission\"\n",
      "$url\n",
      "[1] \"http://46.101.121.83/submission/472/\"\n",
      "\n",
      "$submission\n",
      "[1] \"[0.0523, 0.2588, 0.4434, 0.0562, 0.6388, 0.2108, 0.5244, 0.4603, 0.1225, 0.0023, 0.2775, 0.2808, 0.039, 0.8481, 0.2843, 0.0911, 0.0135, 0.2204, 0.2994, 0.0167, 0.1243, 0.4191, 0.8808, 0.0052, 0.5705, 0.1121, 0.3643, 0.3883, 0.4374, 0.324, 0.4455, 0.7116, 0.0075, 0.3681, 0.6879, 0.3699, 0.7351, 0.6243, 0.9156, 0.0163, 0.1895, 0.2016, 0.1141, 0.7187, 0.2286, 0.7592, 0.2369, 0.0533, 0.254, 0.1777, 0.0343, 0.0321, 0.2266, 0.3699, 0.7994, 0.2784, 0.7149, 0.404, 0.1441, 0.4706, 0.8303, 0.0073, 0.1176, 0.1559, 0.0924, 0.0806, 0.004, 0.0147, 0.5023, 0.142, 0.5761, 0.0412, 0.0101, 0.6691, 0.062, 0.2365, 0.7351, 0.0612, 0.5356, 0.1304, 0.8571, 0.1258, 0.0147, 0.6158, 0.3664, 0.3847, 0.011, 0.1022, 0.803, 0.2431, 0.0329, 0.0489, 0.1039, 0.6701, 0.1478, 0.4918, 0.132, 0.0971, 0.5141, 0.4477, 0.5476, 0.0303, 0.6018, 0.4275, 0.4475, 0.4431, 0.0452, 0.2084, 0.2137, 0.0578, 0.0611, 0.6624, 0.1553, 0.6379, 0.2286, 0.012, 0.0194, 0.386, 0.0888, 0.0315, 0.0184, 0.0213, 0.0459, 0.084, 0.1936, 0.0451, 0.0305, 0.2342, 0.7028, 0.0423, 0.5971, 0.4211, 0.3769, 0.295, 0.5388, 0.6047, 0.0852, 0.591, 0.0976, 0.6008, 0.3868, 0.1751, 0.4224, 0.4271, 0.4041, 0.3408, 0.6408, 0.2993, 0.3137, 0.1266, 0.0665, 0.3887, 0.2243, 0.2064, 0.0118, 0.1487, 0.0471, 0.4652, 0.3725, 0.8604, 0.1496, 0.1408, 0.03, 0.1497, 0.0264, 0.5672, 0.0866, 0.5359, 0.0497, 0.1686, 0.6732, 0.3562, 0.4491, 0.0685, 0.4098, 0.0295, 0.2271, 0.412, 0.617, 0.8338, 0.8046, 0.6409, 0.4827, 0.0055, 0.776, 0.0474, 0.0019, 0.1541, 0.1988, 0.1512, 0.0157, 0.518, 0.6468, 0.2596, 0.0039, 0.0034, 0.0626, 0.0532, 0.0233, 0.7085, 0.5646, 0.1901, 0.1881, 0.6384, 0.4356, 0.6602, 0.027, 0.8772, 0.1112, 0.7501, 0.0973, 0.0097, 0.0973, 0.5179, 0.1632, 0.668, 0.2621, 0.0695, 0.0275, 0.0687, 0.128, 0.0234, 0.3999, 0.4988, 0.1076, 0.1524, 0.1842, 0.7178, 0.2605, 0.0059, 0.8323, 0.0849, 0.5678, 0.0464, 0.0193, 0.0518, 0.5354, 0.1707, 0.5679, 0.6051, 0.0798, 0.0525, 0.0385, 0.102, 0.0962, 0.0539, 0.417, 0.5364, 0.458, 0.0143, 0.054, 0.6936, 0.6938, 0.401, 0.0965, 0.4608, 0.771, 0.2919, 0.0641, 0.1971, 0.5316, 0.1807, 0.0831, 0.3765, 0.5293, 0.3386, 0.0652, 0.184, 0.1341, 0.168, 0.3886, 0.2763, 0.2061, 0.2765, 0.0628, 0.2514, 0.6008, 0.0554, 0.3541, 0.1634, 0.2841, 0.073, 0.3741, 0.1185, 0.6731, 0.417, 0.117, 0.4785, 0.5955, 0.4881, 0.4881, 0.0698, 0.4825, 0.4814, 0.3715, 0.8214, 0.5949, 0.1486, 0.0429, 0.031, 0.7627, 0.0357, 0.0505, 0.0057, 0.0607, 0.0866, 0.1638, 0.2509, 0.0424, 0.0064, 0.1192, 0.5043, 0.6133, 0.0628, 0.0584, 0.5666, 0.2516, 0.1856, 0.5856, 0.608, 0.0255, 0.2038, 0.0445, 0.2945, 0.2227, 0.6413, 0.6547, 0.4167, 0.0637, 0.4283, 0.5146, 0.2381, 0.5038, 0.5861, 0.744, 0.2714, 0.4702, 0.6558, 0.0495, 0.2548, 0.0898, 0.0744, 0.0715, 0.3014, 0.4648, 0.0524, 0.0989, 0.3648, 0.0608, 0.1319, 0.0786, 0.0499, 0.231, 0.6533, 0.1659, 0.3325, 0.613, 0.121, 0.1353, 0.2296, 0.0923, 0.0397, 0.2045, 0.1754, 0.4396, 0.293, 0.6872, 0.0232, 0.0675, 0.0961, 0.7091, 0.1566, 0.0064, 0.0885, 0.1158, 0.7376, 0.124, 0.0788, 0.0076, 0.3104, 0.2395, 0.4697, 0.0715, 0.7333, 0.1813, 0.0764, 0.0113, 0.4086, 0.1055, 0.36, 0.0154, 0.1088, 0.661, 0.1826, 0.0577, 0.3723, 0.0494, 0.0708, 0.0227, 0.0426, 0.0776, 0.7821, 0.1131, 0.8659, 0.0844, 0.5017, 0.0925, 0.4698, 0.6942, 0.1129, 0.3171, 0.0331, 0.4498, 0.0155, 0.3777, 0.4762, 0.3218, 0.7133, 0.5001, 0.7208, 0.3841, 0.7273, 0.0096, 0.0536, 0.2081, 0.5727, 0.2311, 0.5444, 0.0966, 0.4442, 0.4609, 0.1364, 0.364, 0.3094, 0.4424, 0.2817, 0.024, 0.2895, 0.5326, 0.3355, 0.1181, 0.1545, 0.143, 0.7977, 0.2282, 0.4005, 0.0024, 0.0122, 0.6905, 0.023, 0.4582, 0.4238, 0.0692, 0.6509, 0.2272, 0.0013, 0.4209, 0.4161, 0.7026, 0.6485, 0.1201, 0.1356, 0.2388, 0.7791, 0.1083, 0.4168, 0.2336, 0.0391, 0.0591, 0.5665, 0.4169, 0.6306, 0.1264, 0.1072, 0.4802, 0.039, 0.3209, 0.6763, 0.2637, 0.0314, 0.0171, 0.1649, 0.72, 0.1352, 0.0128, 0.2135, 0.6418, 0.0599, 0.4401, 0.0608, 0.5752, 0.3978, 0.6256, 0.6452, 0.1128, 0.0378, 0.7713, 0.0136, 0.7569, 0.1079, 0.1339, 0.1152, 0.024, 0.0139, 0.1766, 0.0093, 0.4018, 0.5458, 0.1366, 0.0437, 0.5244, 0.3765, 0.0526, 0.0466, 0.0536, 0.3264, 0.0454, 0.1336, 0.5836, 0.4371, 0.0261, 0.048, 0.51, 0.0196, 0.3536, 0.3411, 0.0558, 0.7205, 0.2431, 0.0079, 0.2951, 0.0052, 0.5662, 0.6971, 0.4388, 0.0091, 0.9119, 0.7294, 0.4945, 0.2441, 0.175, 0.8525, 0.0377, 0.1994, 0.1847, 0.0773, 0.6512, 0.7907, 0.0159, 0.4663, 0, 0.0589, 0.0994, 0.7941, 0.4466, 0.4694, 0.1302, 0.0585, 0.0764, 0.4234, 0.0475, 0.0676, 0.0817, 0.2284, 0.2474, 0.0323, 0.3191, 0.1166, 0.5625, 0.3946, 0.023, 0.2053, 0.3638, 0.3444, 0.2681, 0.5103, 0.4727, 0.2376, 0.1147, 0.0256, 0.0154, 0.0222, 0.2982, 0.4229, 0.3004, 0.4191, 0.01, 0.1249, 0.2751, 0.1036, 0.4751, 0.1933, 0.0718, 0.0086, 0.0206, 0.5893, 0.0193, 0.0382, 0.0596, 0.5444, 0.1238, 0.3938, 0.5155, 0.0766, 0.1094, 0.4231, 0.847, 0.2071, 0.0953, 0.1311, 0.2541, 0.4086, 0.6127, 0.1943, 0.1146, 0.6208, 0.0385, 0.3557, 0.2162, 0.8953, 0.24, 0.5937, 0.8485, 0.4966, 0.3824, 0.3259, 0.1416, 0.0221, 0.304, 0.095, 0.281, 0.1798, 0.4528, 0.3106, 0.3446, 0.0147, 0.4121, 0.2805, 0.0413, 0.2501, 0.2581, 0.1451, 0.017, 0.1481, 0.141, 0.0249, 0.1214, 0.0856, 0.5745, 0.333, 0.1508, 0.1028, 0.429, 0.3978, 0.2772, 0.2897, 0.5959, 0.5934, 0.5496, 0.3219, 0.2451, 0.0235, 0.1104, 0.6096, 0.2696, 0.1428, 0.6949, 0.0507, 0.0013, 0.588, 0.4673, 0.0973, 0.0031, 0.6721, 0.4342, 0.525, 0.0178, 0.6095, 0.3787, 0.387, 0.0651, 0.8121, 0.1054, 0.1884, 0.1148, 0.5953, 0.2935, 0.483, 0.1371, 0.2284, 0.0627, 0.1399, 0.507, 0.437, 0.0481, 0.4129, 0.0705, 0.0973, 0.0487, 0.3977, 0.0709, 0.3088, 0.6582, 0.2305, 0.5761, 0.6105, 0.2778, 0.0706, 0.3369, 0.0678, 0.5305, 0.8897, 0.0584, 0.0328, 0.3866, 0.4777, 0.5127, 0.4762, 0.7315, 0.0721, 0.1592, 0.4517, 0.5926, 0.1698, 0.1094, 0.0628, 0.3984, 0.5199, 0.4842, 0.0902, 0.4889, 0.5934, 0.2045, 0.1211, 0.019, 0.1107, 0.0446, 0.5657, 0.0653, 0.019, 0.0041, 0.159, 0.672, 0.1322, 0.7597, 0.0896, 0.4812, 0.6703, 0.0327, 0.2345, 0.0029, 0.055, 0.2439, 0.2542, 0.0988, 0.7335, 0.193, 0.0873, 0.0846, 0.0314, 0.2002, 0.5974, 0.1298, 0.3766, 0.8475, 0.0975, 0.0036, 0.0498, 0.5371, 0.3793, 0.0289, 0.0488, 0.5155, 0.8486, 0.5312, 0.1291, 0.8327, 0.3222, 0.0507, 0.4903, 0.1296, 0.0121, 0.6527, 0.0099, 0.0947, 0.4451, 0.4144, 0.3216, 0.0971, 0.553, 0.001, 0.225, 0.6321, 0.0727, 0.2851, 0.0296, 0.0124, 0.0535, 0.0577, 0.4541, 0.2932, 0.0136, 0.3503, 0.2415, 0.3723, 0.0763, 0.4592, 0.0847, 0.1177, 0.5733, 0.422, 0.1381, 0.0064, 0.0956, 0.4245, 0.5431, 0.2403, 0.2753, 0.4743, 0.0859, 0.218, 0.0733, 0.5219, 0.5361, 0.0689, 0.0489, 0.6422, 0.0075, 0.3544, 0.5526, 0.105, 0.003, 0.669, 0.6222, 0.7738, 0.5916, 0.174, 0.4716, 0.073, 0.6977, 0.3152, 0.6614, 0.0321, 0.6829, 0.6437, 0.5872, 0.0583, 0.1345, 0.0756, 0.4092, 0.0906, 0.3558, 0.6952, 0.0234, 0.7664, 0.327, 0.0411, 0.0057, 0.0181, 0.5965, 0.0091, 0.1036, 0.1174, 0.3812, 0.5903, 0.107, 0.1203, 0.4988, 0.3874, 0.0941, 0.0998, 0.0223, 0.0095, 0.2678, 0.3044, 0.4545, 0.182, 0.4159, 0.4753, 0.7879, 0.9394, 0.5182, 0.5597, 0.2409, 0.1146, 0.0974, 0.0543, 0.831, 0.4047, 0.5941, 0.6027, 0.6602, 0.1475, 0.2838, 0.3724, 0.1946, 0.2425, 0.3351, 0.1514, 0.1504, 0.1793, 0.613, 0.0142, 0.415, 0.6135, 0.1344, 0.1934, 0.044, 0.4246, 0.5351, 0.0644, 0.8021, 0.7057, 0.4985, 0.0651, 0.0157, 0.5416, 0.4742, 0.5915, 0.1109, 0.0408, 0.7484, 0.4452, 0.8386, 0.0984, 0.0051, 0.4038, 0.2742, 0.2001, 0.4938, 0.9368, 0.078, 0.1376, 0.6925, 0.2884, 0.1299, 0.4251, 0.0573, 0.1171, 0.2571, 0.4293, 0.4302, 0.5099, 0.013, 0.2603, 0.0763, 0.0589, 0.9364, 0.8032, 0.7609, 0.3302, 0.3537, 0.1288, 0.7908, 0.2285, 0.3889, 0.0257, 0.0227, 0.7236, 0.0775, 0.0659, 0.2248, 0.0124, 0.0717, 0.0724, 0.4601, 0.2378, 0.1278, 0.4845, 0.7155, 0.0867, 0.3077, 0.5845, 0.4249, 0.3338, 0.0975, 0.8836, 0.3664, 0.3897, 0.6744, 0.8643, 0.6099, 0.505, 0.5729, 0.6736, 0.4311, 0.2767, 0.2794, 0.4049, 0.0034, 0.078, 0.0609, 0.1164, 0.354, 0.1338, 0.7906, 0.5364, 0.0229, 0.0086, 0.6053, 0.0698, 0.408, 0.4696, 0.382, 0.3282, 0.7705, 0.6107, 0.0442, 0.0024, 0.0776, 0.3802, 0.2382, 0.0906, 0.5881, 0.1389, 0.6654, 0.3205, 0.3647, 0.2484, 0.7017, 0.2093, 0.0152, 0.0231, 0.0108, 0.0043, 0.168, 0.2776, 0.4517, 0.3077, 0.0141, 0.1352, 0.4209, 0.7828, 0.4703, 0.4721, 0.0308, 0.0802, 0.5413, 0.069, 0.1425, 0.8412, 0.5846, 0.7968, 0.0799, 0.0552, 0.0585, 0.2058, 0.0517, 0.8416, 0.2163, 0.4152, 0.109, 0.3837, 0.0554, 0.0438, 0.3746, 0.0043, 0.0194, 0.4185, 0.0725, 0.0148, 0.1607, 0.3459, 0.2626, 0.4409, 0.3299, 0.4216, 0.0043, 0.4929, 0.8378, 0.2838, 0.6286, 0.6233, 0.2553, 0.141, 0.131, 0.3719, 0.0702, 0.4455, 0.6538, 0.7608, 0.0652, 0.057, 0.5419, 0.6022, 0.0451, 0.4106, 0.3896, 0.9371, 0.3521, 0.5655, 0.7316, 0.7868, 0.0613, 0.3052, 0.2197, 0.386, 0.684, 0.0325, 0.8753, 0.2308, 0.0216, 0.1376, 0.4754, 0.2264, 0.2442, 0.5162, 0.2143, 0.1002, 0.6646, 0.0036, 0.131, 0.2203, 0.3802, 0.5407, 0.4706, 0.6061, 0.0782, 0.6747, 0.0555, 0.3661, 0.2535, 0.1262, 0.7428, 0.6197, 0.2776, 0.0134, 0.0208, 0.0518, 0.1067, 0.8308, 0.4707, 0.223, 0.1185, 0.0191, 0.3151, 0.2766, 0.0877, 0.4858, 0.1295, 0.631, 0.004, 0.6667, 0.0255, 0.0064, 0.5769, 0.7447, 0.2533, 0.1205, 0.1189, 0.0606, 0.4721, 0.0571, 0.3719, 0.1281, 0.0141, 0.1353, 0.5678, 0.1666, 0.6529, 0.7761, 0.3609, 0.6158, 0.337, 0.5885, 0.6406, 0.1169, 0.6395, 0.3719, 0.2842, 0.2305, 0.5721, 0.5827, 0.1086, 0.2952, 0.0048, 0.3626, 0.3436, 0.1181, 0.4771, 0.063, 0.464, 0.0394, 0.5796, 0.1012, 0.3256, 0.1552, 0.0645, 0.0116, 0.1133, 0.2423, 0.5483, 0.3653, 0.0696, 0.0415, 0.0978, 0.4465, 0.0934, 0.1409, 0.1914, 0.3779, 0.2145, 0.303, 0.0856, 0.1599, 0.0137, 0.3878, 0.0701, 0.1094, 0.1171, 0.0625, 0.6201, 0.0645, 0.0219, 0.8153, 0.641, 0.7702, 0.1038, 0.2076, 0.1783, 0.5547, 0.0185, 0.0341, 0.342, 0.0276, 0.072, 0.4603, 0.5561, 0.3642, 0.0414, 0.0796, 0.627, 0.5803, 0.0371, 0.1814, 0.2656, 0.2844, 0.7992, 0.646, 0.5019, 0.24, 0.0112, 0.0243, 0.691, 0.3644, 0.048, 0.6027, 0.0438, 0.2087, 0.391, 0.4925, 0.921, 0.4108, 0.3201, 0.1998, 0.8645, 0.0567, 0.0843, 0.4087, 0.0437, 0.3473, 0.5739, 0.0403, 0.0431, 0.5741, 0.0899, 0.1846, 0.541, 0.0689, 0.5102, 0.0649, 0.0507, 0.3631, 0.1255, 0.421, 0.7075, 0.3824, 0.4741, 0.028, 0.0744, 0.0387, 0.5292, 0.0332, 0.0541, 0.0623, 0.0274, 0.4348, 0.0652, 0.6336, 0.1809, 0.2854, 0.8144, 0.4993, 0.7077, 0.0768, 0.6708, 0.801, 0.2962, 0.0503, 0.3041, 0.5886, 0.0163, 0.4208, 0.5248, 0.3002, 0.6657, 0.0501, 0.0738, 0.6251, 0, 0.2474, 0.1263, 0.1667, 0.0126, 0.4965, 0.5816, 0.0077, 0.178, 0.5072, 0.6868, 0.5915, 0.2464, 0.3961, 0.3085, 0.0221, 0.5159, 0.2121, 0.0053, 0.2681, 0.1018, 0.0243, 0.2679, 0.3913, 0.4061, 0.1792, 0.7014, 0.4018, 0.0598, 0.0187, 0.1974, 0.105, 0.4368, 0.0167, 0.1673, 0.5893, 0.3212, 0.0006, 0.4281, 0.0487, 0.1568, 0.2909, 0.055, 0.0364, 0.4017, 0.0984, 0.7739, 0.5732, 0.3125, 0.3723, 0.3258, 0.0071, 0.5545, 0.5642, 0.2982, 0.4372, 0.4195, 0.0057, 0.3613, 0.3079, 0.153, 0.4616, 0.2914, 0.552, 0.2321, 0.0901, 0.0846, 0.2919, 0.0037, 0.8403, 0.2333, 0.5529, 0.2754, 0.3899, 0.5851, 0.1689, 0.3926, 0.5071, 0.3509, 0.4967, 0.4585, 0.2464, 0.0326, 0.0262, 0.0792, 0.2013, 0.4052, 0.09, 0.2566, 0.0791, 0.0984, 0.0791, 0.2545, 0.5944, 0.0287, 0.5328, 0.846, 0.0989, 0.4328, 0.2565, 0.1013, 0.183, 0.6502, 0.0105, 0.7869, 0.2298, 0.1167, 0.6164, 0.6545, 0.0843, 0.0087, 0.4674, 0.4196, 0.6704, 0.3446, 0.3764, 0.4996, 0.4832, 0.2902, 0.0226, 0.7529, 0.01, 0.4032, 0.0669, 0.7398, 0.0954, 0.2689, 0.2042, 0.0176, 0.306, 0.4675, 0.1772, 0.0154, 0.6449, 0.1777, 0.2542, 0.6686, 0.0571, 0.2601, 0.5472, 0.0795, 0.0527, 0.2704, 0.4706, 0.1551, 0.603, 0.4505, 0.4241, 0.0153, 0.4794, 0.3944, 0.0029, 0.5489, 0.4965, 0.7263, 0.0561, 0.7838, 0.1082, 0.0552, 0.0014, 0.7626, 0.5087, 0.0057, 0.0563, 0.0063, 0.0611, 0.1583, 0.4438, 0.4194, 0.2018, 0.889, 0.0621, 0.3205, 0.8478, 0.0164, 0.1816, 0.4397, 0.3466, 0.4114, 0.3564, 0.5714, 0.7784, 0.7808, 0.466, 0.2992, 0.2795, 0.178, 0.007, 0.0256, 0.1002, 0.7074, 0.3158, 0.2633, 0.1459, 0.5339, 0.0897, 0.1044, 0.7347, 0.6273, 0.5608, 0.4218, 0.4229, 0.0097, 0.1028, 0.1165, 0.8363, 0.0017, 0.1735, 0.4081, 0.4341, 0.0827, 0, 0.2345, 0.52, 0.6787, 0.4441, 0.0614, 0.2763, 0.3799, 0.8333, 0.0385, 0.2901, 0.6291, 0.4154, 0.1333, 0.7182, 0.0369, 0.2133, 0.7375, 0.2541, 0.0181, 0.4012, 0.1493, 0.0432, 0.6566, 0.1258, 0.3323, 0.0145, 0.3302, 0.0051, 0.0432, 0.1123, 0.161, 0.1087, 0.0373, 0.1276, 0.08, 0.075, 0.001, 0.067, 0.0763, 0.0362, 0.0029, 0.4182, 0.8422, 0.2714, 0.0341, 0.0177, 0.5736, 0.2815, 0.2814, 0.5864, 0.7042, 0.1552, 0.5368, 0.0673, 0.0233, 0.7123, 0.3173, 0.5383, 0.3509, 0.2977, 0.2875, 0.0259, 0.526, 0.1178, 0.0508, 0.3558, 0.4284, 0.5252, 0.1919, 0.7345, 0.3714, 0.1747, 0.1761, 0.1419, 0.4937, 0.1481, 0.0204, 0.5899, 0.4091, 0.3289, 0.0087, 0.0734, 0.4602, 0.3324, 0.7116, 0.0624, 0.4574, 0.1142, 0.7969, 0.0048, 0.0424, 0.2818, 0.583, 0.7295, 0.0109, 0.0537, 0.0234, 0.1696, 0.4166, 0.0164, 0.9089, 0.7035, 0.644, 0.3504, 0.6826, 0.1795, 0.6114, 0.2966, 0.1636, 0.2421, 0.5103, 0.3761, 0.0607, 0.4164, 0.391, 0, 0.0899, 0.007, 0.5088, 0.5917, 0.1635, 0.51, 0.1002, 0.2284, 0.4239, 0.0368, 0.4339, 0.1074, 0.0096, 0.5776, 0.1428, 0.0833, 0.1054, 0.3269, 0.3249, 0.0106, 0.3588, 0.5553, 0.0209, 0.5063, 0.1155, 0.0649, 0.3389, 0.3351, 0.2204, 0.016, 0.2569, 0.1209, 0.3779, 0.1896, 0.0214, 0.0769, 0.3821, 0.4562, 0.8768, 0.0267, 0.2268, 0.0512, 0.5304, 0.2574, 0.1901, 0.094, 0.4294, 0.187, 0.0483, 0.0149, 0.3723, 0.2258, 0.0566, 0.0604, 0.516, 0.5952, 0.184, 0.0416, 0.0029, 0.4109, 0.1271, 0.2936, 0.1364, 0.6094, 0.1245, 0.3439, 0.1118, 0.4338, 0.1617, 0.1028, 0.6667, 0.7905, 0.1504, 0.5086, 0.2327, 0.0179, 0.0402, 0.2315, 0.3553, 0.5578, 0.3437, 0.5089, 0.0286, 0.3143, 0.7229, 0.8233, 0.0979, 0.1159, 0.0166, 0.3821, 0.0062, 0.0498, 0.6653, 0.1242, 0.1631, 0.4404, 0.7396, 0.075, 0.0812, 0.2545, 0.0221, 0.603, 0.5474, 0.773, 0.2814, 0.2068, 0.034, 0.0978, 0.0029, 0.6587, 0.4141, 0.0771, 0.3807, 0.0793, 0.7157, 0.4779, 0.8198, 0.0347, 0.7348, 0.4845, 0.1578, 0.0572, 0.3536, 0.1096, 0.1768, 0.624, 0.2544, 0.0029, 0.2513, 0.3054, 0.0809, 0.0369, 0.5882, 0.3373, 0.1789, 0.011, 0.286, 0.2747, 0.6187, 0.4474, 0.1296, 0.4677, 0.3377, 0.6156, 0.0319, 0.2178, 0.1, 0.0344, 0.0836, 0.2037, 0.2288, 0.6681, 0.0984, 0.1953, 0.0485, 0.8322, 0.0441, 0.5629, 0.0561, 0.1026, 0.1168, 0.7827, 0.581, 0.4577, 0.7483, 0.0402, 0.4561, 0.558, 0.1211, 0.2161, 0.7399, 0.0036, 0.3017, 0.3861, 0.0596, 0.2879, 0.1363, 0.4491, 0.6983, 0.0194, 0.6088, 0.0839, 0.6707, 0.2217, 0.5181, 0.7176, 0.0296, 0.0191, 0.0263, 0.7937, 0.6998, 0.0412, 0.0006, 0.0413, 0.0373, 0.0654, 0.0694, 0.0837, 0.663, 0.1046, 0.0021, 0.7479, 0.6569, 0.213, 0.0399, 0.7124, 0.031, 0.2819, 0.0302, 0.226, 0.0101, 0.3866, 0.405, 0.1674, 0.3998, 0.0183, 0.608, 0.2598, 0.0026, 0.5164, 0.0772, 0.5742, 0.5449, 0.2599, 0.1789, 0.7087, 0.1399, 0.0995, 0.4283, 0.0792, 0.4452, 0.596, 0.2156, 0.0475, 0.38, 0.0761, 0.2893, 0.0072, 0.5856, 0.6032, 0.6662, 0.0768, 0.0328, 0.0721, 0.8055, 0.0372, 0.438, 0.0914, 0.0266, 0.2488, 0.014, 0.4109, 0.0884, 0.6136, 0.6075, 0.5381, 0.0396, 0.0231, 0.3521, 0.009, 0.5795, 0.017, 0.8149, 0.7261, 0.6307, 0.1166, 0.265, 0.3193, 0.2544, 0.5291, 0.0215, 0.0093, 0.883, 0.1319, 0.6493, 0.5646, 0.0351, 0.4515, 0.1022, 0.3112, 0.3928, 0.3278, 0.2175, 0.4048, 0.3632, 0.0412, 0.0814, 0.5945, 0.5055, 0.407, 0.1571, 0.2539, 0.5034, 0.2988, 0.757, 0.43, 0.3766, 0.5159, 0.1314, 0.03, 0.7159, 0.0086, 0.0256, 0.5728, 0.7472, 0.0161, 0.0464, 0.297, 0.0831, 0.9005, 0.0595, 0.4536, 0.0306, 0.2216, 0.2641, 0.8199, 0.6559, 0.5805, 0.0889, 0.6075, 0.0789, 0.2273, 0.2952, 0.3332, 0.7289, 0.4784, 0.0178, 0.2131, 0.1509, 0.2032, 0.0419, 0.6773, 0.8256, 0.4567, 0.4626, 0.4057, 0.092, 0.358, 0.1168, 0.1937, 0.3073, 0.283, 0.2002, 0.1011, 0.8208, 0.3551, 0.0718, 0.0875, 0.7549, 0.308, 0.1955, 0.1647, 0.3192, 0.0781, 0.5577, 0.7611, 0.5502, 0.3352, 0.0638, 0.5602, 0.1481, 0.109, 0.5536, 0.1915, 0.391, 0.0029, 0.1219, 0.5454, 0.2706, 0.1997, 0.343, 0.2673, 0.0669, 0.4076, 0.4169, 0.3304, 0.4877, 0.2431, 0.3787, 0.0006, 0.7451, 0.375, 0.3593, 0.1129, 0.4422, 0.2777, 0.5354, 0.3182, 0.3359, 0.3449, 0.722, 0.6265, 0.2771, 0.019, 0.1725, 0.6439, 0.2677, 0.0057, 0.0253, 0.3741, 0.0229, 0.4866, 0.45, 0.0208, 0.0346, 0.0024, 0.0928, 0.0914, 0.1331, 0.3256, 0.1089, 0.3872, 0.0624, 0.0375, 0.3024, 0.0391, 0.3142, 0.6469, 0.351, 0.6792, 0.2439, 0.0736, 0.0687, 0.1972, 0.1207, 0.0219, 0.4187, 0.02, 0.6654, 0.0342, 0.2429, 0.3722, 0.0393, 0.3736, 0.0455, 0.2275]\"\n",
      "\n",
      "$user\n",
      "$user$url\n",
      "[1] \"http://46.101.121.83/group/11/\"\n",
      "\n",
      "$user$username\n",
      "[1] \"Los Galacticos\"\n",
      "\n",
      "$user$best_score\n",
      "[1] 0.8775\n",
      "\n",
      "$user$students\n",
      "[1] \"2019702165;2019702174;2020702024\"\n",
      "\n",
      "\n",
      "$competition\n",
      "[1] \"IE582-Test Data\"\n",
      "\n",
      "$auc\n",
      "[1] 0.9182757\n",
      "\n",
      "$ber\n",
      "[1] 0.8206304\n",
      "\n",
      "$score\n",
      "[1] 0.869453\n",
      "\n",
      "$date\n",
      "[1] \"2021-02-12T04:04:29.731055+03:00\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "send_submission(predictions, token, url=subm_url, submit_now= TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 5, summaryFunction = twoClassSummary,\n",
    "                           verboseIter = TRUE, classProbs = TRUE, sampling = \"up\")\n",
    "tunegrid2 <- expand.grid(.mtry=c(3,5,10,15), .splitrule = \"gini\", .min.node.size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>31</li>\n",
       "\t<li>55</li>\n",
       "\t<li>18</li>\n",
       "\t<li>60</li>\n",
       "\t<li>43</li>\n",
       "\t<li>29</li>\n",
       "\t<li>46</li>\n",
       "\t<li>49</li>\n",
       "\t<li>26</li>\n",
       "\t<li>59</li>\n",
       "\t<li>57</li>\n",
       "\t<li>37</li>\n",
       "\t<li>50</li>\n",
       "\t<li>52</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 31\n",
       "\\item 55\n",
       "\\item 18\n",
       "\\item 60\n",
       "\\item 43\n",
       "\\item 29\n",
       "\\item 46\n",
       "\\item 49\n",
       "\\item 26\n",
       "\\item 59\n",
       "\\item 57\n",
       "\\item 37\n",
       "\\item 50\n",
       "\\item 52\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 31\n",
       "2. 55\n",
       "3. 18\n",
       "4. 60\n",
       "5. 43\n",
       "6. 29\n",
       "7. 46\n",
       "8. 49\n",
       "9. 26\n",
       "10. 59\n",
       "11. 57\n",
       "12. 37\n",
       "13. 50\n",
       "14. 52\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 31 55 18 60 43 29 46 49 26 59 57 37 50 52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "removecols <- imptable[(nrow(imptable)-13):nrow(imptable),variable ];removecols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceddata <- traindata[,-removecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "47"
      ],
      "text/latex": [
       "47"
      ],
      "text/markdown": [
       "47"
      ],
      "text/plain": [
       "[1] 47"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ncol(reduceddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(153)\n",
    "trainIndex = createDataPartition(reduceddata$y, p = 0.7, list = FALSE)\n",
    "reducedtrain = reduceddata[trainIndex, ]\n",
    "reducedtest = reduceddata[-trainIndex, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 10, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "2074 samples\n",
       "  46 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (46) \n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1867, 1867, 1867, 1866, 1868, 1866, ... \n",
       "Addtional sampling using up-sampling prior to pre-processing\n",
       "\n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  ROC        Sens       Spec     \n",
       "   3    0.8794327  0.8718137  0.6731216\n",
       "   5    0.8818904  0.8949453  0.6302510\n",
       "  10    0.8825451  0.8994178  0.6353725\n",
       "  15    0.8796398  0.8958362  0.6318353\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 10, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(821)\n",
    "RF12 <- train(y~., data = reduceddata, method=\"ranger\",  num.trees = 350, metric = \"ROC\", preProc=c(\"range\"),\n",
    "              trControl = fitControl, tuneGrid = tunegrid2, importance = 'permutation')\n",
    "RF12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRF12 <- predict(RF12, reducedtest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "[1] 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting levels: control = 1, case = 2\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "1"
      ],
      "text/latex": [
       "1"
      ],
      "text/markdown": [
       "1"
      ],
      "text/plain": [
       "Area under the curve: 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "1 - get_accuracy(predRF12, reducedtest$y )\n",
    "curve <- roc(as.numeric(predRF12), as.numeric(reducedtest$y) )\n",
    "auc(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.044</li>\n",
       "\t<li>0.191904761904762</li>\n",
       "\t<li>0.380809523809524</li>\n",
       "\t<li>0.0375714285714286</li>\n",
       "\t<li>0.626761904761905</li>\n",
       "\t<li>0.181142857142857</li>\n",
       "\t<li>0.402333333333333</li>\n",
       "\t<li>0.496238095238095</li>\n",
       "\t<li>0.0908571428571429</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.246047619047619</li>\n",
       "\t<li>0.377326530612245</li>\n",
       "\t<li>0.0195238095238095</li>\n",
       "\t<li>0.912809523809524</li>\n",
       "\t<li>0.163047619047619</li>\n",
       "\t<li>0.0121428571428571</li>\n",
       "\t<li>0.0132380952380952</li>\n",
       "\t<li>0.148333333333333</li>\n",
       "\t<li>0.148714285714286</li>\n",
       "\t<li>0.00952380952380952</li>\n",
       "\t<li>0.0748571428571428</li>\n",
       "\t<li>0.377</li>\n",
       "\t<li>0.941809523809524</li>\n",
       "\t<li>0.00476190476190476</li>\n",
       "\t<li>0.593095238095238</li>\n",
       "\t<li>0.0771904761904762</li>\n",
       "\t<li>0.487238095238095</li>\n",
       "\t<li>0.461952380952381</li>\n",
       "\t<li>0.433333333333333</li>\n",
       "\t<li>0.263476190476191</li>\n",
       "\t<li>0.435</li>\n",
       "\t<li>0.711333333333333</li>\n",
       "\t<li>0.0171428571428571</li>\n",
       "\t<li>0.422285714285714</li>\n",
       "\t<li>0.663190476190476</li>\n",
       "\t<li>0.235333333333333</li>\n",
       "\t<li>0.697380952380952</li>\n",
       "\t<li>0.708619047619048</li>\n",
       "\t<li>0.920619047619048</li>\n",
       "\t<li>0.0225714285714286</li>\n",
       "\t<li>0.176761904761905</li>\n",
       "\t<li>0.137952380952381</li>\n",
       "\t<li>0.132761904761905</li>\n",
       "\t<li>0.813238095238095</li>\n",
       "\t<li>0.221714285714286</li>\n",
       "\t<li>0.854714285714286</li>\n",
       "\t<li>0.191904761904762</li>\n",
       "\t<li>0.0438095238095238</li>\n",
       "\t<li>0.291656084656085</li>\n",
       "\t<li>0.137428571428571</li>\n",
       "\t<li>0.0139047619047619</li>\n",
       "\t<li>0.0128571428571429</li>\n",
       "\t<li>0.144714285714286</li>\n",
       "\t<li>0.410421768707483</li>\n",
       "\t<li>0.850380952380953</li>\n",
       "\t<li>0.28752380952381</li>\n",
       "\t<li>0.756809523809524</li>\n",
       "\t<li>0.56747619047619</li>\n",
       "\t<li>0.094</li>\n",
       "\t<li>0.514380952380952</li>\n",
       "\t<li>0.869619047619048</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.0714285714285714</li>\n",
       "\t<li>0.181238095238095</li>\n",
       "\t<li>0.0534761904761905</li>\n",
       "\t<li>0.0583809523809524</li>\n",
       "\t<li>0.000571428571428571</li>\n",
       "\t<li>0.0171428571428571</li>\n",
       "\t<li>0.625952380952381</li>\n",
       "\t<li>0.179809523809524</li>\n",
       "\t<li>0.574666666666666</li>\n",
       "\t<li>0.0245238095238095</li>\n",
       "\t<li>0.0406666666666667</li>\n",
       "\t<li>0.690202380952381</li>\n",
       "\t<li>0.0747142857142857</li>\n",
       "\t<li>0.249190476190476</li>\n",
       "\t<li>0.709</li>\n",
       "\t<li>0.0418571428571428</li>\n",
       "\t<li>0.485047619047619</li>\n",
       "\t<li>0.0794285714285714</li>\n",
       "\t<li>0.933190476190476</li>\n",
       "\t<li>0.08</li>\n",
       "\t<li>0.0020952380952381</li>\n",
       "\t<li>0.528095238095238</li>\n",
       "\t<li>0.235380952380952</li>\n",
       "\t<li>0.162666666666667</li>\n",
       "\t<li>0.000952380952380952</li>\n",
       "\t<li>0.0670952380952381</li>\n",
       "\t<li>0.844714285714286</li>\n",
       "\t<li>0.257571428571429</li>\n",
       "\t<li>0.0162857142857143</li>\n",
       "\t<li>0.0182857142857143</li>\n",
       "\t<li>0.0859523809523809</li>\n",
       "\t<li>0.713047619047619</li>\n",
       "\t<li>0.102761904761905</li>\n",
       "\t<li>0.409952380952381</li>\n",
       "\t<li>0.156809523809524</li>\n",
       "\t<li>0.075</li>\n",
       "\t<li>0.347322751322751</li>\n",
       "\t<li>0.464523809523809</li>\n",
       "\t<li>0.567952380952381</li>\n",
       "\t<li>0.032952380952381</li>\n",
       "\t<li>0.598190476190476</li>\n",
       "\t<li>0.361619047619048</li>\n",
       "\t<li>0.343238095238095</li>\n",
       "\t<li>0.375714285714286</li>\n",
       "\t<li>0.0126190476190476</li>\n",
       "\t<li>0.252857142857143</li>\n",
       "\t<li>0.10352380952381</li>\n",
       "\t<li>0.0261428571428571</li>\n",
       "\t<li>0.0245238095238095</li>\n",
       "\t<li>0.875619047619048</li>\n",
       "\t<li>0.151</li>\n",
       "\t<li>0.765857142857143</li>\n",
       "\t<li>0.247904761904762</li>\n",
       "\t<li>0.00895238095238095</li>\n",
       "\t<li>0.0138095238095238</li>\n",
       "\t<li>0.347714285714286</li>\n",
       "\t<li>0.0343809523809524</li>\n",
       "\t<li>0.0234285714285714</li>\n",
       "\t<li>0.0102857142857143</li>\n",
       "\t<li>0.0591428571428571</li>\n",
       "\t<li>0.009</li>\n",
       "\t<li>0.12</li>\n",
       "\t<li>0.173666666666667</li>\n",
       "\t<li>0.0331904761904762</li>\n",
       "\t<li>0.0326190476190476</li>\n",
       "\t<li>0.28647619047619</li>\n",
       "\t<li>0.659285714285714</li>\n",
       "\t<li>0.0126190476190476</li>\n",
       "\t<li>0.585</li>\n",
       "\t<li>0.382571428571429</li>\n",
       "\t<li>0.312142857142857</li>\n",
       "\t<li>0.424380952380952</li>\n",
       "\t<li>0.41452380952381</li>\n",
       "\t<li>0.588190476190476</li>\n",
       "\t<li>0.0721428571428571</li>\n",
       "\t<li>0.70647619047619</li>\n",
       "\t<li>0.11152380952381</li>\n",
       "\t<li>0.734238095238095</li>\n",
       "\t<li>0.384904761904762</li>\n",
       "\t<li>0.220619047619048</li>\n",
       "\t<li>0.288952380952381</li>\n",
       "\t<li>0.324238095238095</li>\n",
       "\t<li>0.323904761904762</li>\n",
       "\t<li>0.18747619047619</li>\n",
       "\t<li>0.719523809523809</li>\n",
       "\t<li>0.138571428571429</li>\n",
       "\t<li>0.269666666666667</li>\n",
       "\t<li>0.0961904761904762</li>\n",
       "\t<li>0.037</li>\n",
       "\t<li>0.389761904761905</li>\n",
       "\t<li>0.242666666666667</li>\n",
       "\t<li>0.14952380952381</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.0926666666666667</li>\n",
       "\t<li>0.0282380952380952</li>\n",
       "\t<li>0.562380952380952</li>\n",
       "\t<li>0.371238095238095</li>\n",
       "\t<li>0.916047619047619</li>\n",
       "\t<li>0.172571428571429</li>\n",
       "\t<li>0.197857142857143</li>\n",
       "\t<li>0.0318571428571429</li>\n",
       "\t<li>0.030047619047619</li>\n",
       "\t<li>0.0128571428571429</li>\n",
       "\t<li>0.624142857142857</li>\n",
       "\t<li>0.0662380952380952</li>\n",
       "\t<li>0.493714285714286</li>\n",
       "\t<li>0.0181428571428571</li>\n",
       "\t<li>0.118809523809524</li>\n",
       "\t<li>0.735619047619048</li>\n",
       "\t<li>0.324857142857143</li>\n",
       "\t<li>0.349142857142857</li>\n",
       "\t<li>0.0395238095238095</li>\n",
       "\t<li>0.412714285714286</li>\n",
       "\t<li>0.0106666666666667</li>\n",
       "\t<li>0.219809523809524</li>\n",
       "\t<li>0.300095238095238</li>\n",
       "\t<li>0.571571428571428</li>\n",
       "\t<li>0.890857142857143</li>\n",
       "\t<li>0.769523809523809</li>\n",
       "\t<li>0.708285714285714</li>\n",
       "\t<li>0.427238095238095</li>\n",
       "\t<li>0.0128571428571429</li>\n",
       "\t<li>0.861952380952381</li>\n",
       "\t<li>0.023952380952381</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.241</li>\n",
       "\t<li>0.197428571428571</li>\n",
       "\t<li>0.202619047619048</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.447333333333333</li>\n",
       "\t<li>0.630285714285714</li>\n",
       "\t<li>0.118571428571429</li>\n",
       "\t<li>0.00666666666666667</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0477142857142857</li>\n",
       "\t<li>0.0114285714285714</li>\n",
       "\t<li>0.031</li>\n",
       "\t<li>0.72352380952381</li>\n",
       "\t<li>0.374047619047619</li>\n",
       "\t<li>0.157619047619048</li>\n",
       "\t<li>0.206095238095238</li>\n",
       "\t<li>0.519190476190476</li>\n",
       "\t<li>0.261333333333333</li>\n",
       "\t<li>0.626401360544217</li>\n",
       "\t<li>0.0287142857142857</li>\n",
       "\t<li>0.958857142857143</li>\n",
       "\t<li>0.0915238095238095</li>\n",
       "\t<li>0.715666666666667</li>\n",
       "\t<li>0.0171428571428571</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0732857142857143</li>\n",
       "\t<li>0.518238095238095</li>\n",
       "\t<li>0.142333333333333</li>\n",
       "\t<li>0.745619047619048</li>\n",
       "\t<li>0.15447619047619</li>\n",
       "\t<li>0.0339047619047619</li>\n",
       "\t<li>0.00914285714285714</li>\n",
       "\t<li>0.0650952380952381</li>\n",
       "\t<li>0.0540952380952381</li>\n",
       "\t<li>0.00914285714285714</li>\n",
       "\t<li>0.348714285714286</li>\n",
       "\t<li>0.658</li>\n",
       "\t<li>0.0193809523809524</li>\n",
       "\t<li>0.184142857142857</li>\n",
       "\t<li>0.112285714285714</li>\n",
       "\t<li>0.809857142857143</li>\n",
       "\t<li>0.212904761904762</li>\n",
       "\t<li>0.00838095238095238</li>\n",
       "\t<li>0.872571428571429</li>\n",
       "\t<li>0.031</li>\n",
       "\t<li>0.377904761904762</li>\n",
       "\t<li>0.0561428571428571</li>\n",
       "\t<li>0.005</li>\n",
       "\t<li>0.0305238095238095</li>\n",
       "\t<li>0.321190476190476</li>\n",
       "\t<li>0.119904761904762</li>\n",
       "\t<li>0.600454081632653</li>\n",
       "\t<li>0.699809523809524</li>\n",
       "\t<li>0.0618571428571429</li>\n",
       "\t<li>0.00952380952380952</li>\n",
       "\t<li>0.0105714285714286</li>\n",
       "\t<li>0.0430952380952381</li>\n",
       "\t<li>0.0620340136054422</li>\n",
       "\t<li>0.0388571428571429</li>\n",
       "\t<li>0.507142857142857</li>\n",
       "\t<li>0.623714285714286</li>\n",
       "\t<li>0.51</li>\n",
       "\t<li>0.0019047619047619</li>\n",
       "\t<li>0.0462380952380952</li>\n",
       "\t<li>0.79147619047619</li>\n",
       "\t<li>0.779095238095238</li>\n",
       "\t<li>0.438095238095238</li>\n",
       "\t<li>0.0565238095238095</li>\n",
       "\t<li>0.548238095238095</li>\n",
       "\t<li>0.828333333333333</li>\n",
       "\t<li>0.194571428571429</li>\n",
       "\t<li>0.205857142857143</li>\n",
       "\t<li>0.152333333333333</li>\n",
       "\t<li>0.515714285714286</li>\n",
       "\t<li>0.241571428571429</li>\n",
       "\t<li>0.0705238095238095</li>\n",
       "\t<li>0.284115646258503</li>\n",
       "\t<li>0.653857142857143</li>\n",
       "\t<li>0.33747619047619</li>\n",
       "\t<li>0.0201904761904762</li>\n",
       "\t<li>0.158619047619048</li>\n",
       "\t<li>0.110904761904762</li>\n",
       "\t<li>0.0979523809523809</li>\n",
       "\t<li>0.321428571428571</li>\n",
       "\t<li>0.212285714285714</li>\n",
       "\t<li>0.264571428571429</li>\n",
       "\t<li>0.170608465608466</li>\n",
       "\t<li>0.0511904761904762</li>\n",
       "\t<li>0.266238095238095</li>\n",
       "\t<li>0.647428571428571</li>\n",
       "\t<li>0.0587142857142857</li>\n",
       "\t<li>0.301190476190476</li>\n",
       "\t<li>0.105238095238095</li>\n",
       "\t<li>0.371707482993197</li>\n",
       "\t<li>0.0566190476190476</li>\n",
       "\t<li>0.246190476190476</li>\n",
       "\t<li>0.119904761904762</li>\n",
       "\t<li>0.678</li>\n",
       "\t<li>0.348761904761905</li>\n",
       "\t<li>0.0734285714285714</li>\n",
       "\t<li>0.385166666666667</li>\n",
       "\t<li>0.641428571428571</li>\n",
       "\t<li>0.562952380952381</li>\n",
       "\t<li>0.385285714285714</li>\n",
       "\t<li>0.0872857142857143</li>\n",
       "\t<li>0.32652380952381</li>\n",
       "\t<li>0.554</li>\n",
       "\t<li>0.285380952380952</li>\n",
       "\t<li>0.885666666666667</li>\n",
       "\t<li>0.548642857142857</li>\n",
       "\t<li>0.209</li>\n",
       "\t<li>0.0412380952380952</li>\n",
       "\t<li>0.0812380952380952</li>\n",
       "\t<li>0.856285714285714</li>\n",
       "\t<li>0.0225714285714286</li>\n",
       "\t<li>0.0439047619047619</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.049047619047619</li>\n",
       "\t<li>0.0372857142857143</li>\n",
       "\t<li>0.0788571428571428</li>\n",
       "\t<li>0.164809523809524</li>\n",
       "\t<li>0.0354761904761905</li>\n",
       "\t<li>0.00642857142857143</li>\n",
       "\t<li>0.0958095238095238</li>\n",
       "\t<li>0.514587301587302</li>\n",
       "\t<li>0.699154761904762</li>\n",
       "\t<li>0.0326190476190476</li>\n",
       "\t<li>0.0218095238095238</li>\n",
       "\t<li>0.710857142857143</li>\n",
       "\t<li>0.244142857142857</li>\n",
       "\t<li>0.121714285714286</li>\n",
       "\t<li>0.510624338624339</li>\n",
       "\t<li>0.538952380952381</li>\n",
       "\t<li>0.0133333333333333</li>\n",
       "\t<li>0.191142857142857</li>\n",
       "\t<li>0.0172380952380952</li>\n",
       "\t<li>0.331465608465609</li>\n",
       "\t<li>0.215095238095238</li>\n",
       "\t<li>0.543952380952381</li>\n",
       "\t<li>0.631</li>\n",
       "\t<li>0.549238095238095</li>\n",
       "\t<li>0.0415238095238095</li>\n",
       "\t<li>0.319285714285714</li>\n",
       "\t<li>0.472809523809524</li>\n",
       "\t<li>0.23647619047619</li>\n",
       "\t<li>0.529238095238095</li>\n",
       "\t<li>0.647904761904762</li>\n",
       "\t<li>0.845767195767196</li>\n",
       "\t<li>0.244142857142857</li>\n",
       "\t<li>0.496476190476191</li>\n",
       "\t<li>0.727952380952381</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.147857142857143</li>\n",
       "\t<li>0.115428571428571</li>\n",
       "\t<li>0.109571428571429</li>\n",
       "\t<li>0.054047619047619</li>\n",
       "\t<li>0.275428571428571</li>\n",
       "\t<li>0.355333333333333</li>\n",
       "\t<li>0.0136190476190476</li>\n",
       "\t<li>0.147857142857143</li>\n",
       "\t<li>0.196</li>\n",
       "\t<li>0.0373809523809524</li>\n",
       "\t<li>0.0676190476190476</li>\n",
       "\t<li>0.0344761904761905</li>\n",
       "\t<li>0.0174285714285714</li>\n",
       "\t<li>0.161095238095238</li>\n",
       "\t<li>0.812333333333333</li>\n",
       "\t<li>0.143142857142857</li>\n",
       "\t<li>0.381238095238095</li>\n",
       "\t<li>0.792</li>\n",
       "\t<li>0.185333333333333</li>\n",
       "\t<li>0.105095238095238</li>\n",
       "\t<li>0.305625850340136</li>\n",
       "\t<li>0.0691428571428571</li>\n",
       "\t<li>0.0235238095238095</li>\n",
       "\t<li>0.0740476190476191</li>\n",
       "\t<li>0.161190476190476</li>\n",
       "\t<li>0.458</li>\n",
       "\t<li>0.377238095238095</li>\n",
       "\t<li>0.64584126984127</li>\n",
       "\t<li>0.0218571428571429</li>\n",
       "\t<li>0.0412380952380952</li>\n",
       "\t<li>0.0695714285714286</li>\n",
       "\t<li>0.728380952380952</li>\n",
       "\t<li>0.064047619047619</li>\n",
       "\t<li>0.00642857142857143</li>\n",
       "\t<li>0.0625238095238095</li>\n",
       "\t<li>0.12752380952381</li>\n",
       "\t<li>0.709047619047619</li>\n",
       "\t<li>0.0662857142857143</li>\n",
       "\t<li>0.0432857142857143</li>\n",
       "\t<li>0.00214285714285714</li>\n",
       "\t<li>0.238571428571429</li>\n",
       "\t<li>0.222809523809524</li>\n",
       "\t<li>0.547333333333333</li>\n",
       "\t<li>0.0241428571428571</li>\n",
       "\t<li>0.81747619047619</li>\n",
       "\t<li>0.16047619047619</li>\n",
       "\t<li>0.040952380952381</li>\n",
       "\t<li>0.00857142857142857</li>\n",
       "\t<li>0.362666666666667</li>\n",
       "\t<li>0.0647619047619048</li>\n",
       "\t<li>0.438380952380952</li>\n",
       "\t<li>0.0203333333333333</li>\n",
       "\t<li>0.0693809523809524</li>\n",
       "\t<li>0.813333333333333</li>\n",
       "\t<li>0.120952380952381</li>\n",
       "\t<li>0.0430952380952381</li>\n",
       "\t<li>0.33547619047619</li>\n",
       "\t<li>0.105714285714286</li>\n",
       "\t<li>0.053</li>\n",
       "\t<li>0.00661904761904762</li>\n",
       "\t<li>0.0196190476190476</li>\n",
       "\t<li>0.127666666666667</li>\n",
       "\t<li>0.801857142857143</li>\n",
       "\t<li>0.17847619047619</li>\n",
       "\t<li>0.894238095238095</li>\n",
       "\t<li>0.0836666666666667</li>\n",
       "\t<li>0.438904761904762</li>\n",
       "\t<li>0.110047619047619</li>\n",
       "\t<li>0.269857142857143</li>\n",
       "\t<li>0.684952380952381</li>\n",
       "\t<li>0.05</li>\n",
       "\t<li>0.213873015873016</li>\n",
       "\t<li>0.0328095238095238</li>\n",
       "\t<li>0.541333333333333</li>\n",
       "\t<li>0.0242380952380952</li>\n",
       "\t<li>0.557142857142857</li>\n",
       "\t<li>0.438238095238095</li>\n",
       "\t<li>0.202857142857143</li>\n",
       "\t<li>0.807619047619048</li>\n",
       "\t<li>0.350095238095238</li>\n",
       "\t<li>0.679714285714286</li>\n",
       "\t<li>0.371285714285714</li>\n",
       "\t<li>0.671809523809524</li>\n",
       "\t<li>0.0216666666666667</li>\n",
       "\t<li>0.0465714285714286</li>\n",
       "\t<li>0.175714285714286</li>\n",
       "\t<li>0.397619047619048</li>\n",
       "\t<li>0.138659863945578</li>\n",
       "\t<li>0.526047619047619</li>\n",
       "\t<li>0.155761904761905</li>\n",
       "\t<li>0.457</li>\n",
       "\t<li>0.461428571428571</li>\n",
       "\t<li>0.0723809523809524</li>\n",
       "\t<li>0.32747619047619</li>\n",
       "\t<li>0.305095238095238</li>\n",
       "\t<li>0.436857142857143</li>\n",
       "\t<li>0.230142857142857</li>\n",
       "\t<li>0.00228571428571429</li>\n",
       "\t<li>0.22447619047619</li>\n",
       "\t<li>0.693904761904762</li>\n",
       "\t<li>0.344428571428571</li>\n",
       "\t<li>0.0567142857142857</li>\n",
       "\t<li>0.139047619047619</li>\n",
       "\t<li>0.0854761904761905</li>\n",
       "\t<li>0.880142857142857</li>\n",
       "\t<li>0.197380952380952</li>\n",
       "\t<li>0.563952380952381</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.717142857142857</li>\n",
       "\t<li>0.0463809523809524</li>\n",
       "\t<li>0.537428571428571</li>\n",
       "\t<li>0.261</li>\n",
       "\t<li>0.0388571428571429</li>\n",
       "\t<li>0.792333333333333</li>\n",
       "\t<li>0.185285714285714</li>\n",
       "\t<li>0.00742857142857143</li>\n",
       "\t<li>0.38647619047619</li>\n",
       "\t<li>0.411095238095238</li>\n",
       "\t<li>0.845047619047619</li>\n",
       "\t<li>0.571047619047619</li>\n",
       "\t<li>0.0521904761904762</li>\n",
       "\t<li>0.085047619047619</li>\n",
       "\t<li>0.153666666666667</li>\n",
       "\t<li>0.81047619047619</li>\n",
       "\t<li>0.0672380952380952</li>\n",
       "\t<li>0.333777777777778</li>\n",
       "\t<li>0.105571428571429</li>\n",
       "\t<li>0.0135238095238095</li>\n",
       "\t<li>0.0434285714285714</li>\n",
       "\t<li>0.580857142857143</li>\n",
       "\t<li>0.341047619047619</li>\n",
       "\t<li>0.713857142857143</li>\n",
       "\t<li>0.103095238095238</li>\n",
       "\t<li>0.0604285714285714</li>\n",
       "\t<li>0.503238095238095</li>\n",
       "\t<li>0.014</li>\n",
       "\t<li>0.162714285714286</li>\n",
       "\t<li>0.740761904761905</li>\n",
       "\t<li>0.332095238095238</li>\n",
       "\t<li>0.0171428571428571</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.10347619047619</li>\n",
       "\t<li>0.820761904761905</li>\n",
       "\t<li>0.134190476190476</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.156952380952381</li>\n",
       "\t<li>0.540428571428571</li>\n",
       "\t<li>0.0170952380952381</li>\n",
       "\t<li>0.34308843537415</li>\n",
       "\t<li>0.0818095238095238</li>\n",
       "\t<li>0.595428571428571</li>\n",
       "\t<li>0.360285714285714</li>\n",
       "\t<li>0.666095238095238</li>\n",
       "\t<li>0.699809523809524</li>\n",
       "\t<li>0.100238095238095</li>\n",
       "\t<li>0.0211428571428571</li>\n",
       "\t<li>0.75647619047619</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.812714285714286</li>\n",
       "\t<li>0.114</li>\n",
       "\t<li>0.165952380952381</li>\n",
       "\t<li>0.13647619047619</li>\n",
       "\t<li>0.00380952380952381</li>\n",
       "\t<li>0.0181904761904762</li>\n",
       "\t<li>0.116</li>\n",
       "\t<li>0.00857142857142857</li>\n",
       "\t<li>0.326523809523809</li>\n",
       "\t<li>0.625059523809524</li>\n",
       "\t<li>0.119857142857143</li>\n",
       "\t<li>0.021</li>\n",
       "\t<li>0.427857142857143</li>\n",
       "\t<li>0.414333333333333</li>\n",
       "\t<li>0.0223809523809524</li>\n",
       "\t<li>0.0226666666666667</li>\n",
       "\t<li>0.0305714285714286</li>\n",
       "\t<li>0.30737037037037</li>\n",
       "\t<li>0.0508571428571429</li>\n",
       "\t<li>0.0877142857142857</li>\n",
       "\t<li>0.524952380952381</li>\n",
       "\t<li>0.459047619047619</li>\n",
       "\t<li>0.00476190476190476</li>\n",
       "\t<li>0.0303333333333333</li>\n",
       "\t<li>0.403619047619048</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.261952380952381</li>\n",
       "\t<li>0.372904761904762</li>\n",
       "\t<li>0.0593809523809524</li>\n",
       "\t<li>0.801190476190476</li>\n",
       "\t<li>0.176619047619048</li>\n",
       "\t<li>0.00857142857142857</li>\n",
       "\t<li>0.251904761904762</li>\n",
       "\t<li>0.00742857142857143</li>\n",
       "\t<li>0.510603174603175</li>\n",
       "\t<li>0.765428571428571</li>\n",
       "\t<li>0.297666666666667</li>\n",
       "\t<li>0.00880952380952381</li>\n",
       "\t<li>0.95047619047619</li>\n",
       "\t<li>0.672714285714286</li>\n",
       "\t<li>0.27347619047619</li>\n",
       "\t<li>0.224285714285714</li>\n",
       "\t<li>0.185571428571429</li>\n",
       "\t<li>0.916380952380952</li>\n",
       "\t<li>0.0245238095238095</li>\n",
       "\t<li>0.1</li>\n",
       "\t<li>0.0958095238095238</li>\n",
       "\t<li>0.0422857142857143</li>\n",
       "\t<li>0.626190476190476</li>\n",
       "\t<li>0.762</li>\n",
       "\t<li>0.0266666666666667</li>\n",
       "\t<li>0.508619047619048</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.0341904761904762</li>\n",
       "\t<li>0.0885714285714286</li>\n",
       "\t<li>0.817190476190476</li>\n",
       "\t<li>0.516571428571429</li>\n",
       "\t<li>0.609761904761905</li>\n",
       "\t<li>0.0881558441558442</li>\n",
       "\t<li>0.0408571428571429</li>\n",
       "\t<li>0.0534285714285714</li>\n",
       "\t<li>0.382809523809524</li>\n",
       "\t<li>0.0567142857142857</li>\n",
       "\t<li>0.072</li>\n",
       "\t<li>0.0540952380952381</li>\n",
       "\t<li>0.140333333333333</li>\n",
       "\t<li>0.27191156462585</li>\n",
       "\t<li>0.0124285714285714</li>\n",
       "\t<li>0.272380952380952</li>\n",
       "\t<li>0.0554761904761905</li>\n",
       "\t<li>0.511619047619048</li>\n",
       "\t<li>0.334428571428571</li>\n",
       "\t<li>0.013</li>\n",
       "\t<li>0.222190476190476</li>\n",
       "\t<li>0.208857142857143</li>\n",
       "\t<li>0.328619047619048</li>\n",
       "\t<li>0.208857142857143</li>\n",
       "\t<li>0.453374149659864</li>\n",
       "\t<li>0.534809523809524</li>\n",
       "\t<li>0.195952380952381</li>\n",
       "\t<li>0.0801428571428571</li>\n",
       "\t<li>0.0931428571428572</li>\n",
       "\t<li>0.00666666666666667</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.403619047619048</li>\n",
       "\t<li>0.520619047619048</li>\n",
       "\t<li>0.269380952380952</li>\n",
       "\t<li>0.478095238095238</li>\n",
       "\t<li>0.0175844155844156</li>\n",
       "\t<li>0.123857142857143</li>\n",
       "\t<li>0.236</li>\n",
       "\t<li>0.0586666666666667</li>\n",
       "\t<li>0.571</li>\n",
       "\t<li>0.215380952380952</li>\n",
       "\t<li>0.0777619047619048</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0466666666666667</li>\n",
       "\t<li>0.671666666666667</li>\n",
       "\t<li>0.0275238095238095</li>\n",
       "\t<li>0.029952380952381</li>\n",
       "\t<li>0.0368571428571429</li>\n",
       "\t<li>0.542666666666667</li>\n",
       "\t<li>0.0767619047619048</li>\n",
       "\t<li>0.310619047619048</li>\n",
       "\t<li>0.612190476190476</li>\n",
       "\t<li>0.0524285714285714</li>\n",
       "\t<li>0.0798571428571428</li>\n",
       "\t<li>0.292476190476191</li>\n",
       "\t<li>0.869047619047619</li>\n",
       "\t<li>0.117142857142857</li>\n",
       "\t<li>0.0777619047619048</li>\n",
       "\t<li>0.123333333333333</li>\n",
       "\t<li>0.246238095238095</li>\n",
       "\t<li>0.467190476190476</li>\n",
       "\t<li>0.653809523809524</li>\n",
       "\t<li>0.161</li>\n",
       "\t<li>0.0935714285714286</li>\n",
       "\t<li>0.575095238095238</li>\n",
       "\t<li>0.036</li>\n",
       "\t<li>0.308714285714286</li>\n",
       "\t<li>0.177428571428571</li>\n",
       "\t<li>0.966148148148148</li>\n",
       "\t<li>0.134666666666667</li>\n",
       "\t<li>0.535095238095238</li>\n",
       "\t<li>0.91652380952381</li>\n",
       "\t<li>0.59647619047619</li>\n",
       "\t<li>0.275952380952381</li>\n",
       "\t<li>0.294761904761905</li>\n",
       "\t<li>0.0554761904761905</li>\n",
       "\t<li>0.0339047619047619</li>\n",
       "\t<li>0.335904761904762</li>\n",
       "\t<li>0.0308571428571429</li>\n",
       "\t<li>0.344619047619048</li>\n",
       "\t<li>0.11347619047619</li>\n",
       "\t<li>0.630523809523809</li>\n",
       "\t<li>0.238</li>\n",
       "\t<li>0.278428571428571</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.370417989417989</li>\n",
       "\t<li>0.164857142857143</li>\n",
       "\t<li>0.0195714285714286</li>\n",
       "\t<li>0.256761904761905</li>\n",
       "\t<li>0.320244897959184</li>\n",
       "\t<li>0.0954761904761905</li>\n",
       "\t<li>0.00557142857142857</li>\n",
       "\t<li>0.236857142857143</li>\n",
       "\t<li>0.127809523809524</li>\n",
       "\t<li>0.0164285714285714</li>\n",
       "\t<li>0.0946190476190476</li>\n",
       "\t<li>0.0703809523809524</li>\n",
       "\t<li>0.645809523809524</li>\n",
       "\t<li>0.434619047619048</li>\n",
       "\t<li>0.175380952380952</li>\n",
       "\t<li>0.0544761904761905</li>\n",
       "\t<li>0.472</li>\n",
       "\t<li>0.437571428571428</li>\n",
       "\t<li>0.376380952380952</li>\n",
       "\t<li>0.15347619047619</li>\n",
       "\t<li>0.685904761904762</li>\n",
       "\t<li>0.636857142857143</li>\n",
       "\t<li>0.575761904761905</li>\n",
       "\t<li>0.289809523809524</li>\n",
       "\t<li>0.267952380952381</li>\n",
       "\t<li>0.0232380952380952</li>\n",
       "\t<li>0.0395238095238095</li>\n",
       "\t<li>0.600095238095238</li>\n",
       "\t<li>0.212047619047619</li>\n",
       "\t<li>0.139619047619048</li>\n",
       "\t<li>0.613666666666667</li>\n",
       "\t<li>0.0416666666666667</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.735714285714286</li>\n",
       "\t<li>0.351380952380952</li>\n",
       "\t<li>0.114333333333333</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.754238095238095</li>\n",
       "\t<li>0.426047619047619</li>\n",
       "\t<li>0.452809523809524</li>\n",
       "\t<li>0.011047619047619</li>\n",
       "\t<li>0.642190476190476</li>\n",
       "\t<li>0.493904761904762</li>\n",
       "\t<li>0.397857142857143</li>\n",
       "\t<li>0.0324285714285714</li>\n",
       "\t<li>0.928285714285714</li>\n",
       "\t<li>0.0872380952380952</li>\n",
       "\t<li>0.277513227513228</li>\n",
       "\t<li>0.0963333333333333</li>\n",
       "\t<li>0.661047619047619</li>\n",
       "\t<li>0.40325</li>\n",
       "\t<li>0.483047619047619</li>\n",
       "\t<li>0.151428571428571</li>\n",
       "\t<li>0.190761904761905</li>\n",
       "\t<li>0.0289047619047619</li>\n",
       "\t<li>0.112047619047619</li>\n",
       "\t<li>0.505823129251701</li>\n",
       "\t<li>0.38452380952381</li>\n",
       "\t<li>0.0366796536796537</li>\n",
       "\t<li>0.462639455782313</li>\n",
       "\t<li>0.0467142857142857</li>\n",
       "\t<li>0.0438095238095238</li>\n",
       "\t<li>0.0238095238095238</li>\n",
       "\t<li>0.192857142857143</li>\n",
       "\t<li>0.0483809523809524</li>\n",
       "\t<li>0.27978231292517</li>\n",
       "\t<li>0.739285714285714</li>\n",
       "\t<li>0.18437037037037</li>\n",
       "\t<li>0.66152380952381</li>\n",
       "\t<li>0.634523809523809</li>\n",
       "\t<li>0.369619047619048</li>\n",
       "\t<li>0.0339523809523809</li>\n",
       "\t<li>0.342571428571429</li>\n",
       "\t<li>0.0417142857142857</li>\n",
       "\t<li>0.415659863945578</li>\n",
       "\t<li>0.953428571428572</li>\n",
       "\t<li>0.0456666666666667</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.157843537414966</li>\n",
       "\t<li>0.469190476190476</li>\n",
       "\t<li>0.521142857142857</li>\n",
       "\t<li>0.632476190476191</li>\n",
       "\t<li>0.913952380952381</li>\n",
       "\t<li>0.0314285714285714</li>\n",
       "\t<li>0.181904761904762</li>\n",
       "\t<li>0.441666666666667</li>\n",
       "\t<li>0.702285714285714</li>\n",
       "\t<li>0.0378095238095238</li>\n",
       "\t<li>0.042</li>\n",
       "\t<li>0.02</li>\n",
       "\t<li>0.387952380952381</li>\n",
       "\t<li>0.475428571428571</li>\n",
       "\t<li>0.602380952380952</li>\n",
       "\t<li>0.0551428571428571</li>\n",
       "\t<li>0.467761904761905</li>\n",
       "\t<li>0.652333333333333</li>\n",
       "\t<li>0.208428571428571</li>\n",
       "\t<li>0.0815714285714286</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.0672380952380952</li>\n",
       "\t<li>0.0209047619047619</li>\n",
       "\t<li>0.519761904761905</li>\n",
       "\t<li>0.0913809523809524</li>\n",
       "\t<li>0.0233809523809524</li>\n",
       "\t<li>0.00357142857142857</li>\n",
       "\t<li>0.098</li>\n",
       "\t<li>0.703190476190476</li>\n",
       "\t<li>0.0992857142857143</li>\n",
       "\t<li>0.903</li>\n",
       "\t<li>0.157619047619048</li>\n",
       "\t<li>0.624714285714286</li>\n",
       "\t<li>0.719</li>\n",
       "\t<li>0.00928571428571429</li>\n",
       "\t<li>0.203666666666667</li>\n",
       "\t<li>0.0121904761904762</li>\n",
       "\t<li>0.0327619047619048</li>\n",
       "\t<li>0.114</li>\n",
       "\t<li>0.269755102040816</li>\n",
       "\t<li>0.0568571428571429</li>\n",
       "\t<li>0.739714285714286</li>\n",
       "\t<li>0.163095238095238</li>\n",
       "\t<li>0.0393809523809524</li>\n",
       "\t<li>0.0123333333333333</li>\n",
       "\t<li>0.0182857142857143</li>\n",
       "\t<li>0.269619047619048</li>\n",
       "\t<li>0.547761904761905</li>\n",
       "\t<li>0.0841428571428571</li>\n",
       "\t<li>0.239380952380952</li>\n",
       "\t<li>0.908809523809524</li>\n",
       "\t<li>0.0987142857142857</li>\n",
       "\t<li>0.00342857142857143</li>\n",
       "\t<li>0.0187619047619048</li>\n",
       "\t<li>0.545761904761905</li>\n",
       "\t<li>0.286285714285714</li>\n",
       "\t<li>0.0157142857142857</li>\n",
       "\t<li>0.0345714285714286</li>\n",
       "\t<li>0.579619047619048</li>\n",
       "\t<li>0.885380952380952</li>\n",
       "\t<li>0.584666666666667</li>\n",
       "\t<li>0.116761904761905</li>\n",
       "\t<li>0.903034013605442</li>\n",
       "\t<li>0.347285714285714</li>\n",
       "\t<li>0.0279047619047619</li>\n",
       "\t<li>0.433142857142857</li>\n",
       "\t<li>0.130095238095238</li>\n",
       "\t<li>0.00342857142857143</li>\n",
       "\t<li>0.527571428571428</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0204761904761905</li>\n",
       "\t<li>0.41637037037037</li>\n",
       "\t<li>0.512285714285714</li>\n",
       "\t<li>0.340380952380952</li>\n",
       "\t<li>0.140428571428571</li>\n",
       "\t<li>0.471666666666667</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.277</li>\n",
       "\t<li>0.69952380952381</li>\n",
       "\t<li>0.0248571428571429</li>\n",
       "\t<li>0.156619047619048</li>\n",
       "\t<li>0.00842857142857143</li>\n",
       "\t<li>0.00342857142857143</li>\n",
       "\t<li>0.0114285714285714</li>\n",
       "\t<li>0.0345714285714286</li>\n",
       "\t<li>0.333428571428571</li>\n",
       "\t<li>0.213666666666667</li>\n",
       "\t<li>0.00857142857142857</li>\n",
       "\t<li>0.203428571428571</li>\n",
       "\t<li>0.178428571428571</li>\n",
       "\t<li>0.248476190476191</li>\n",
       "\t<li>0.0899047619047619</li>\n",
       "\t<li>0.55452380952381</li>\n",
       "\t<li>0.0520476190476191</li>\n",
       "\t<li>0.103571428571429</li>\n",
       "\t<li>0.667619047619048</li>\n",
       "\t<li>0.405809523809524</li>\n",
       "\t<li>0.0800952380952381</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0839523809523809</li>\n",
       "\t<li>0.366744520030234</li>\n",
       "\t<li>0.57652380952381</li>\n",
       "\t<li>0.167238095238095</li>\n",
       "\t<li>0.21952380952381</li>\n",
       "\t<li>0.47752380952381</li>\n",
       "\t<li>0.0203809523809524</li>\n",
       "\t<li>0.235047619047619</li>\n",
       "\t<li>0.0269047619047619</li>\n",
       "\t<li>0.634809523809524</li>\n",
       "\t<li>0.527</li>\n",
       "\t<li>0.0519047619047619</li>\n",
       "\t<li>0.0302857142857143</li>\n",
       "\t<li>0.75352380952381</li>\n",
       "\t<li>0.005</li>\n",
       "\t<li>0.213809523809524</li>\n",
       "\t<li>0.423857142857143</li>\n",
       "\t<li>0.0351428571428571</li>\n",
       "\t<li>0.0294285714285714</li>\n",
       "\t<li>0.739809523809524</li>\n",
       "\t<li>0.537095238095238</li>\n",
       "\t<li>0.892571428571428</li>\n",
       "\t<li>0.520095238095238</li>\n",
       "\t<li>0.158238095238095</li>\n",
       "\t<li>0.418857142857143</li>\n",
       "\t<li>0.0576190476190476</li>\n",
       "\t<li>0.661285714285714</li>\n",
       "\t<li>0.218619047619048</li>\n",
       "\t<li>0.794571428571429</li>\n",
       "\t<li>0.0224761904761905</li>\n",
       "\t<li>0.815904761904762</li>\n",
       "\t<li>0.685095238095238</li>\n",
       "\t<li>0.681238095238095</li>\n",
       "\t<li>0.0286796536796537</li>\n",
       "\t<li>0.162904761904762</li>\n",
       "\t<li>0.0384285714285714</li>\n",
       "\t<li>0.356333333333333</li>\n",
       "\t<li>0.0546666666666667</li>\n",
       "\t<li>0.580619047619048</li>\n",
       "\t<li>0.639380952380952</li>\n",
       "\t<li>0.0163333333333333</li>\n",
       "\t<li>0.857380952380952</li>\n",
       "\t<li>0.242809523809524</li>\n",
       "\t<li>0.0376190476190476</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.622190476190476</li>\n",
       "\t<li>0.00885714285714286</li>\n",
       "\t<li>0.0962380952380952</li>\n",
       "\t<li>0.137142857142857</li>\n",
       "\t<li>0.330571428571429</li>\n",
       "\t<li>0.581857142857143</li>\n",
       "\t<li>0.049</li>\n",
       "\t<li>0.114809523809524</li>\n",
       "\t<li>0.505047619047619</li>\n",
       "\t<li>0.484095238095238</li>\n",
       "\t<li>0.0908095238095238</li>\n",
       "\t<li>0.069952380952381</li>\n",
       "\t<li>0.0179047619047619</li>\n",
       "\t<li>0.00128571428571429</li>\n",
       "\t<li>0.239666666666667</li>\n",
       "\t<li>0.428095238095238</li>\n",
       "\t<li>0.435884353741497</li>\n",
       "\t<li>0.135761904761905</li>\n",
       "\t<li>0.389428571428571</li>\n",
       "\t<li>0.623809523809524</li>\n",
       "\t<li>0.816428571428571</li>\n",
       "\t<li>0.979428571428571</li>\n",
       "\t<li>0.40352380952381</li>\n",
       "\t<li>0.669571428571429</li>\n",
       "\t<li>0.210809523809524</li>\n",
       "\t<li>0.0727142857142857</li>\n",
       "\t<li>0.089952380952381</li>\n",
       "\t<li>0.0253333333333333</li>\n",
       "\t<li>0.844238095238095</li>\n",
       "\t<li>0.277476190476191</li>\n",
       "\t<li>0.525809523809524</li>\n",
       "\t<li>0.677761904761905</li>\n",
       "\t<li>0.681190476190476</li>\n",
       "\t<li>0.202952380952381</li>\n",
       "\t<li>0.233428571428571</li>\n",
       "\t<li>0.244047619047619</li>\n",
       "\t<li>0.122857142857143</li>\n",
       "\t<li>0.185190476190476</li>\n",
       "\t<li>0.291904761904762</li>\n",
       "\t<li>0.196</li>\n",
       "\t<li>0.0922721088435374</li>\n",
       "\t<li>0.147857142857143</li>\n",
       "\t<li>0.702476190476191</li>\n",
       "\t<li>0.0173809523809524</li>\n",
       "\t<li>0.427714285714286</li>\n",
       "\t<li>0.669047619047619</li>\n",
       "\t<li>0.07</li>\n",
       "\t<li>0.166190476190476</li>\n",
       "\t<li>0.0134285714285714</li>\n",
       "\t<li>0.486231292517007</li>\n",
       "\t<li>0.472761904761905</li>\n",
       "\t<li>0.0347619047619048</li>\n",
       "\t<li>0.818428571428572</li>\n",
       "\t<li>0.774047619047619</li>\n",
       "\t<li>0.450142857142857</li>\n",
       "\t<li>0.0462380952380952</li>\n",
       "\t<li>0.0157619047619048</li>\n",
       "\t<li>0.531428571428571</li>\n",
       "\t<li>0.518380952380952</li>\n",
       "\t<li>0.597714285714286</li>\n",
       "\t<li>0.109714285714286</li>\n",
       "\t<li>0.00828571428571429</li>\n",
       "\t<li>0.776183673469388</li>\n",
       "\t<li>0.388285714285714</li>\n",
       "\t<li>0.824857142857143</li>\n",
       "\t<li>0.0982857142857143</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.307285714285714</li>\n",
       "\t<li>0.182904761904762</li>\n",
       "\t<li>0.268904761904762</li>\n",
       "\t<li>0.621428571428572</li>\n",
       "\t<li>0.962666666666667</li>\n",
       "\t<li>0.0693809523809524</li>\n",
       "\t<li>0.122428571428571</li>\n",
       "\t<li>0.81</li>\n",
       "\t<li>0.265333333333333</li>\n",
       "\t<li>0.0974285714285714</li>\n",
       "\t<li>0.521857142857143</li>\n",
       "\t<li>0.0827619047619048</li>\n",
       "\t<li>0.0747142857142857</li>\n",
       "\t<li>0.239666666666667</li>\n",
       "\t<li>0.395238095238095</li>\n",
       "\t<li>0.588190476190476</li>\n",
       "\t<li>0.480666666666667</li>\n",
       "\t<li>0.0242857142857143</li>\n",
       "\t<li>0.227714285714286</li>\n",
       "\t<li>0.0513809523809524</li>\n",
       "\t<li>0.0182857142857143</li>\n",
       "\t<li>0.974523809523809</li>\n",
       "\t<li>0.868095238095238</li>\n",
       "\t<li>0.732380952380952</li>\n",
       "\t<li>0.289761904761905</li>\n",
       "\t<li>0.304231292517007</li>\n",
       "\t<li>0.0672857142857143</li>\n",
       "\t<li>0.87252380952381</li>\n",
       "\t<li>0.177857142857143</li>\n",
       "\t<li>0.388190476190476</li>\n",
       "\t<li>0.0262857142857143</li>\n",
       "\t<li>0.00380952380952381</li>\n",
       "\t<li>0.82247619047619</li>\n",
       "\t<li>0.0725238095238095</li>\n",
       "\t<li>0.0441904761904762</li>\n",
       "\t<li>0.248571428571428</li>\n",
       "\t<li>0.00476190476190476</li>\n",
       "\t<li>0.0646666666666667</li>\n",
       "\t<li>0.0400952380952381</li>\n",
       "\t<li>0.460523809523809</li>\n",
       "\t<li>0.158428571428571</li>\n",
       "\t<li>0.0683333333333333</li>\n",
       "\t<li>0.38230612244898</li>\n",
       "\t<li>0.609619047619048</li>\n",
       "\t<li>0.0534285714285714</li>\n",
       "\t<li>0.266809523809524</li>\n",
       "\t<li>0.752095238095238</li>\n",
       "\t<li>0.506714285714286</li>\n",
       "\t<li>0.231666666666667</li>\n",
       "\t<li>0.0482857142857143</li>\n",
       "\t<li>0.943428571428572</li>\n",
       "\t<li>0.341285714285714</li>\n",
       "\t<li>0.300904761904762</li>\n",
       "\t<li>0.748190476190476</li>\n",
       "\t<li>0.912952380952381</li>\n",
       "\t<li>0.672183673469388</li>\n",
       "\t<li>0.495619047619048</li>\n",
       "\t<li>0.539714285714286</li>\n",
       "\t<li>0.666020408163265</li>\n",
       "\t<li>0.527095238095238</li>\n",
       "\t<li>0.317666666666667</li>\n",
       "\t<li>0.469244897959184</li>\n",
       "\t<li>0.472333333333333</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.0795714285714286</li>\n",
       "\t<li>0.0309523809523809</li>\n",
       "\t<li>0.055</li>\n",
       "\t<li>0.26547619047619</li>\n",
       "\t<li>0.0849047619047619</li>\n",
       "\t<li>0.883047619047619</li>\n",
       "\t<li>0.429904761904762</li>\n",
       "\t<li>0.0134285714285714</li>\n",
       "\t<li>0.00171428571428571</li>\n",
       "\t<li>0.670238095238095</li>\n",
       "\t<li>0.0296190476190476</li>\n",
       "\t<li>0.243619047619048</li>\n",
       "\t<li>0.364111111111111</li>\n",
       "\t<li>0.365047619047619</li>\n",
       "\t<li>0.28308843537415</li>\n",
       "\t<li>0.859095238095238</li>\n",
       "\t<li>0.676142857142857</li>\n",
       "\t<li>0.0112380952380952</li>\n",
       "\t<li>0.0019047619047619</li>\n",
       "\t<li>0.0779047619047619</li>\n",
       "\t<li>0.418380952380952</li>\n",
       "\t<li>0.212809523809524</li>\n",
       "\t<li>0.0934761904761905</li>\n",
       "\t<li>0.588945578231293</li>\n",
       "\t<li>0.0851904761904762</li>\n",
       "\t<li>0.73147619047619</li>\n",
       "\t<li>0.288714285714286</li>\n",
       "\t<li>0.318132275132275</li>\n",
       "\t<li>0.19852380952381</li>\n",
       "\t<li>0.667904761904762</li>\n",
       "\t<li>0.184285714285714</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.0257142857142857</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.00928571428571429</li>\n",
       "\t<li>0.178619047619048</li>\n",
       "\t<li>0.317285714285714</li>\n",
       "\t<li>0.389374149659864</li>\n",
       "\t<li>0.352095238095238</li>\n",
       "\t<li>0.00628571428571429</li>\n",
       "\t<li>0.122142857142857</li>\n",
       "\t<li>0.503598639455782</li>\n",
       "\t<li>0.864190476190476</li>\n",
       "\t<li>0.443904761904762</li>\n",
       "\t<li>0.461904761904762</li>\n",
       "\t<li>0.028</li>\n",
       "\t<li>0.0925714285714286</li>\n",
       "\t<li>0.507714285714286</li>\n",
       "\t<li>0.0493333333333333</li>\n",
       "\t<li>0.111571428571429</li>\n",
       "\t<li>0.873857142857143</li>\n",
       "\t<li>0.582</li>\n",
       "\t<li>0.864714285714286</li>\n",
       "\t<li>0.0378095238095238</li>\n",
       "\t<li>0.0157142857142857</li>\n",
       "\t<li>0.0524285714285714</li>\n",
       "\t<li>0.197809523809524</li>\n",
       "\t<li>0.136428571428571</li>\n",
       "\t<li>0.81552380952381</li>\n",
       "\t<li>0.494761904761905</li>\n",
       "\t<li>0.423047619047619</li>\n",
       "\t<li>0.0154285714285714</li>\n",
       "\t<li>0.407047619047619</li>\n",
       "\t<li>0.0135714285714286</li>\n",
       "\t<li>0.0905714285714286</li>\n",
       "\t<li>0.409142857142857</li>\n",
       "\t<li>0.0121904761904762</li>\n",
       "\t<li>0.0262857142857143</li>\n",
       "\t<li>0.282190476190476</li>\n",
       "\t<li>0.0290952380952381</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0370952380952381</li>\n",
       "\t<li>0.399</li>\n",
       "\t<li>0.213414965986395</li>\n",
       "\t<li>0.331238095238095</li>\n",
       "\t<li>0.222904761904762</li>\n",
       "\t<li>0.295253968253968</li>\n",
       "\t<li>0.00514285714285714</li>\n",
       "\t<li>0.647380952380952</li>\n",
       "\t<li>0.806761904761905</li>\n",
       "\t<li>0.283714285714286</li>\n",
       "\t<li>0.628190476190476</li>\n",
       "\t<li>0.604666666666667</li>\n",
       "\t<li>0.221619047619048</li>\n",
       "\t<li>0.11952380952381</li>\n",
       "\t<li>0.0772380952380952</li>\n",
       "\t<li>0.329659863945578</li>\n",
       "\t<li>0.0132857142857143</li>\n",
       "\t<li>0.421136054421769</li>\n",
       "\t<li>0.668619047619047</li>\n",
       "\t<li>0.645</li>\n",
       "\t<li>0.0661904761904762</li>\n",
       "\t<li>0.0291428571428571</li>\n",
       "\t<li>0.603380952380952</li>\n",
       "\t<li>0.620714285714286</li>\n",
       "\t<li>0.01</li>\n",
       "\t<li>0.329761904761905</li>\n",
       "\t<li>0.336380952380952</li>\n",
       "\t<li>0.979428571428571</li>\n",
       "\t<li>0.281285714285714</li>\n",
       "\t<li>0.600380952380952</li>\n",
       "\t<li>0.772285714285714</li>\n",
       "\t<li>0.797142857142857</li>\n",
       "\t<li>0.067952380952381</li>\n",
       "\t<li>0.218904761904762</li>\n",
       "\t<li>0.200333333333333</li>\n",
       "\t<li>0.344238095238095</li>\n",
       "\t<li>0.614428571428572</li>\n",
       "\t<li>0.0587619047619048</li>\n",
       "\t<li>0.931428571428572</li>\n",
       "\t<li>0.205095238095238</li>\n",
       "\t<li>0.0135714285714286</li>\n",
       "\t<li>0.144714285714286</li>\n",
       "\t<li>0.575285714285714</li>\n",
       "\t<li>0.166857142857143</li>\n",
       "\t<li>0.209748299319728</li>\n",
       "\t<li>0.523037037037037</li>\n",
       "\t<li>0.117619047619048</li>\n",
       "\t<li>0.0724761904761905</li>\n",
       "\t<li>0.627206349206349</li>\n",
       "\t<li>0.0019047619047619</li>\n",
       "\t<li>0.173571428571429</li>\n",
       "\t<li>0.25047619047619</li>\n",
       "\t<li>0.301047619047619</li>\n",
       "\t<li>0.462952380952381</li>\n",
       "\t<li>0.540263605442177</li>\n",
       "\t<li>0.651238095238095</li>\n",
       "\t<li>0.0721428571428571</li>\n",
       "\t<li>0.677666666666667</li>\n",
       "\t<li>0.0378571428571429</li>\n",
       "\t<li>0.312285714285714</li>\n",
       "\t<li>0.209190476190476</li>\n",
       "\t<li>0.0603809523809524</li>\n",
       "\t<li>0.773380952380953</li>\n",
       "\t<li>0.475047619047619</li>\n",
       "\t<li>0.18268253968254</li>\n",
       "\t<li>0.0162857142857143</li>\n",
       "\t<li>0.0504285714285714</li>\n",
       "\t<li>0.0315714285714286</li>\n",
       "\t<li>0.0703809523809524</li>\n",
       "\t<li>0.911285714285714</li>\n",
       "\t<li>0.319904761904762</li>\n",
       "\t<li>0.14847619047619</li>\n",
       "\t<li>0.127047619047619</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.164428571428571</li>\n",
       "\t<li>0.176571428571429</li>\n",
       "\t<li>0.0527619047619048</li>\n",
       "\t<li>0.442231292517007</li>\n",
       "\t<li>0.0968571428571428</li>\n",
       "\t<li>0.562095238095238</li>\n",
       "\t<li>0.00628571428571429</li>\n",
       "\t<li>0.718904761904762</li>\n",
       "\t<li>0.120333333333333</li>\n",
       "\t<li>0.00857142857142857</li>\n",
       "\t<li>0.674714285714286</li>\n",
       "\t<li>0.752619047619048</li>\n",
       "\t<li>0.21952380952381</li>\n",
       "\t<li>0.158142857142857</li>\n",
       "\t<li>0.0441904761904762</li>\n",
       "\t<li>0.0465238095238095</li>\n",
       "\t<li>0.519238095238095</li>\n",
       "\t<li>0.0471428571428571</li>\n",
       "\t<li>0.352142857142857</li>\n",
       "\t<li>0.0792857142857143</li>\n",
       "\t<li>0.0146666666666667</li>\n",
       "\t<li>0.112666666666667</li>\n",
       "\t<li>0.690549319727891</li>\n",
       "\t<li>0.101</li>\n",
       "\t<li>0.828047619047619</li>\n",
       "\t<li>0.797</li>\n",
       "\t<li>0.188047619047619</li>\n",
       "\t<li>0.717619047619048</li>\n",
       "\t<li>0.320904761904762</li>\n",
       "\t<li>0.663</li>\n",
       "\t<li>0.636148148148148</li>\n",
       "\t<li>0.105619047619048</li>\n",
       "\t<li>0.624714285714286</li>\n",
       "\t<li>0.280571428571429</li>\n",
       "\t<li>0.147238095238095</li>\n",
       "\t<li>0.189857142857143</li>\n",
       "\t<li>0.564380952380952</li>\n",
       "\t<li>0.406666666666667</li>\n",
       "\t<li>0.208190476190476</li>\n",
       "\t<li>0.457666666666667</li>\n",
       "\t<li>0.0147619047619048</li>\n",
       "\t<li>0.272904761904762</li>\n",
       "\t<li>0.291238095238095</li>\n",
       "\t<li>0.0905238095238095</li>\n",
       "\t<li>0.337238095238095</li>\n",
       "\t<li>0.084</li>\n",
       "\t<li>0.512392857142857</li>\n",
       "\t<li>0.00952380952380952</li>\n",
       "\t<li>0.622802721088435</li>\n",
       "\t<li>0.0122380952380952</li>\n",
       "\t<li>0.325047619047619</li>\n",
       "\t<li>0.136285714285714</li>\n",
       "\t<li>0.0472857142857143</li>\n",
       "\t<li>0.00476190476190476</li>\n",
       "\t<li>0.101619047619048</li>\n",
       "\t<li>0.313333333333333</li>\n",
       "\t<li>0.605380952380952</li>\n",
       "\t<li>0.241952380952381</li>\n",
       "\t<li>0.0332857142857143</li>\n",
       "\t<li>0.0122857142857143</li>\n",
       "\t<li>0.109428571428571</li>\n",
       "\t<li>0.450380952380952</li>\n",
       "\t<li>0.0738571428571429</li>\n",
       "\t<li>0.119095238095238</li>\n",
       "\t<li>0.272142857142857</li>\n",
       "\t<li>0.457333333333333</li>\n",
       "\t<li>0.0888503401360544</li>\n",
       "\t<li>0.302190476190476</li>\n",
       "\t<li>0.0646666666666667</li>\n",
       "\t<li>0.130380952380952</li>\n",
       "\t<li>0.026047619047619</li>\n",
       "\t<li>0.331</li>\n",
       "\t<li>0.0284285714285714</li>\n",
       "\t<li>0.0868095238095238</li>\n",
       "\t<li>0.0691904761904762</li>\n",
       "\t<li>0.045952380952381</li>\n",
       "\t<li>0.579285714285714</li>\n",
       "\t<li>0.0391428571428571</li>\n",
       "\t<li>0.0271904761904762</li>\n",
       "\t<li>0.943857142857143</li>\n",
       "\t<li>0.692204081632653</li>\n",
       "\t<li>0.779285714285714</li>\n",
       "\t<li>0.12147619047619</li>\n",
       "\t<li>0.131761904761905</li>\n",
       "\t<li>0.151571428571429</li>\n",
       "\t<li>0.506068027210884</li>\n",
       "\t<li>0.013047619047619</li>\n",
       "\t<li>0.0355238095238095</li>\n",
       "\t<li>0.52852380952381</li>\n",
       "\t<li>0.025952380952381</li>\n",
       "\t<li>0.0804285714285714</li>\n",
       "\t<li>0.561142857142857</li>\n",
       "\t<li>0.498190476190476</li>\n",
       "\t<li>0.255666666666667</li>\n",
       "\t<li>0.0215238095238095</li>\n",
       "\t<li>0.0504761904761905</li>\n",
       "\t<li>0.678190476190476</li>\n",
       "\t<li>0.681904761904762</li>\n",
       "\t<li>0.0394761904761905</li>\n",
       "\t<li>0.0945238095238095</li>\n",
       "\t<li>0.153857142857143</li>\n",
       "\t<li>0.224142857142857</li>\n",
       "\t<li>0.861761904761905</li>\n",
       "\t<li>0.698523809523809</li>\n",
       "\t<li>0.368142857142857</li>\n",
       "\t<li>0.226047619047619</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0131428571428571</li>\n",
       "\t<li>0.698904761904762</li>\n",
       "\t<li>0.426666666666667</li>\n",
       "\t<li>0.0287619047619048</li>\n",
       "\t<li>0.581278911564626</li>\n",
       "\t<li>0.0291428571428571</li>\n",
       "\t<li>0.220047619047619</li>\n",
       "\t<li>0.286095238095238</li>\n",
       "\t<li>0.688142857142857</li>\n",
       "\t<li>0.952619047619047</li>\n",
       "\t<li>0.309619047619048</li>\n",
       "\t<li>0.265142857142857</li>\n",
       "\t<li>0.165</li>\n",
       "\t<li>0.908666666666667</li>\n",
       "\t<li>0.0225238095238095</li>\n",
       "\t<li>0.0347142857142857</li>\n",
       "\t<li>0.371761904761905</li>\n",
       "\t<li>0.0166666666666667</li>\n",
       "\t<li>0.288714285714286</li>\n",
       "\t<li>0.500761904761905</li>\n",
       "\t<li>0.0203333333333333</li>\n",
       "\t<li>0.0380952380952381</li>\n",
       "\t<li>0.488719576719577</li>\n",
       "\t<li>0.0426190476190476</li>\n",
       "\t<li>0.181285714285714</li>\n",
       "\t<li>0.445380952380952</li>\n",
       "\t<li>0.0730952380952381</li>\n",
       "\t<li>0.448380952380952</li>\n",
       "\t<li>0.0331904761904762</li>\n",
       "\t<li>0.0204285714285714</li>\n",
       "\t<li>0.490380952380952</li>\n",
       "\t<li>0.0957619047619048</li>\n",
       "\t<li>0.384142857142857</li>\n",
       "\t<li>0.801428571428571</li>\n",
       "\t<li>0.293904761904762</li>\n",
       "\t<li>0.493285714285714</li>\n",
       "\t<li>0.0222857142857143</li>\n",
       "\t<li>0.0555714285714286</li>\n",
       "\t<li>0.0192857142857143</li>\n",
       "\t<li>0.594952380952381</li>\n",
       "\t<li>0.013</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.075</li>\n",
       "\t<li>0.0172857142857143</li>\n",
       "\t<li>0.494190476190476</li>\n",
       "\t<li>0.0483333333333333</li>\n",
       "\t<li>0.777809523809524</li>\n",
       "\t<li>0.118857142857143</li>\n",
       "\t<li>0.203571428571429</li>\n",
       "\t<li>0.854</li>\n",
       "\t<li>0.489761904761905</li>\n",
       "\t<li>0.67852380952381</li>\n",
       "\t<li>0.0278571428571429</li>\n",
       "\t<li>0.823714285714286</li>\n",
       "\t<li>0.768333333333333</li>\n",
       "\t<li>0.200809523809524</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.386224489795918</li>\n",
       "\t<li>0.565380952380952</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.275095238095238</li>\n",
       "\t<li>0.39347619047619</li>\n",
       "\t<li>0.305952380952381</li>\n",
       "\t<li>0.743904761904762</li>\n",
       "\t<li>0.0519047619047619</li>\n",
       "\t<li>0.0356666666666667</li>\n",
       "\t<li>0.721142857142857</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.127904761904762</li>\n",
       "\t<li>0.108952380952381</li>\n",
       "\t<li>0.149380952380952</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.386666666666667</li>\n",
       "\t<li>0.47212925170068</li>\n",
       "\t<li>0.00476190476190476</li>\n",
       "\t<li>0.166857142857143</li>\n",
       "\t<li>0.537952380952381</li>\n",
       "\t<li>0.672619047619048</li>\n",
       "\t<li>0.803714285714286</li>\n",
       "\t<li>0.192428571428571</li>\n",
       "\t<li>0.430428571428572</li>\n",
       "\t<li>0.294619047619048</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.501619047619048</li>\n",
       "\t<li>0.170142857142857</li>\n",
       "\t<li>0.0108571428571429</li>\n",
       "\t<li>0.284333333333333</li>\n",
       "\t<li>0.07</li>\n",
       "\t<li>0.0257142857142857</li>\n",
       "\t<li>0.177904761904762</li>\n",
       "\t<li>0.608571428571429</li>\n",
       "\t<li>0.370380952380952</li>\n",
       "\t<li>0.171904761904762</li>\n",
       "\t<li>0.796095238095238</li>\n",
       "\t<li>0.413238095238095</li>\n",
       "\t<li>0.0391428571428571</li>\n",
       "\t<li>0.00457142857142857</li>\n",
       "\t<li>0.14952380952381</li>\n",
       "\t<li>0.131</li>\n",
       "\t<li>0.316619047619048</li>\n",
       "\t<li>0.005</li>\n",
       "\t<li>0.202333333333333</li>\n",
       "\t<li>0.50852380952381</li>\n",
       "\t<li>0.228714285714286</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.370656084656085</li>\n",
       "\t<li>0.0304761904761905</li>\n",
       "\t<li>0.172285714285714</li>\n",
       "\t<li>0.204714285714286</li>\n",
       "\t<li>0.0191428571428571</li>\n",
       "\t<li>0.0638571428571429</li>\n",
       "\t<li>0.314952380952381</li>\n",
       "\t<li>0.0394285714285714</li>\n",
       "\t<li>0.770891156462585</li>\n",
       "\t<li>0.539095238095238</li>\n",
       "\t<li>0.419571428571429</li>\n",
       "\t<li>0.231714285714286</li>\n",
       "\t<li>0.380619047619048</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.576</li>\n",
       "\t<li>0.417047619047619</li>\n",
       "\t<li>0.396619047619048</li>\n",
       "\t<li>0.413476190476191</li>\n",
       "\t<li>0.281707482993197</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.345666666666667</li>\n",
       "\t<li>0.229333333333333</li>\n",
       "\t<li>0.104238095238095</li>\n",
       "\t<li>0.306952380952381</li>\n",
       "\t<li>0.232142857142857</li>\n",
       "\t<li>0.669068027210884</li>\n",
       "\t<li>0.233761904761905</li>\n",
       "\t<li>0.0374761904761905</li>\n",
       "\t<li>0.0327142857142857</li>\n",
       "\t<li>0.243904761904762</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.932190476190476</li>\n",
       "\t<li>0.235857142857143</li>\n",
       "\t<li>0.564190476190476</li>\n",
       "\t<li>0.319714285714286</li>\n",
       "\t<li>0.444571428571429</li>\n",
       "\t<li>0.518619047619048</li>\n",
       "\t<li>0.13847619047619</li>\n",
       "\t<li>0.356238095238095</li>\n",
       "\t<li>0.363634920634921</li>\n",
       "\t<li>0.342571428571429</li>\n",
       "\t<li>0.588571428571429</li>\n",
       "\t<li>0.388380952380952</li>\n",
       "\t<li>0.181</li>\n",
       "\t<li>0.0239047619047619</li>\n",
       "\t<li>0.0149047619047619</li>\n",
       "\t<li>0.0545238095238095</li>\n",
       "\t<li>0.179666666666667</li>\n",
       "\t<li>0.427571428571429</li>\n",
       "\t<li>0.0766666666666667</li>\n",
       "\t<li>0.381238095238095</li>\n",
       "\t<li>0.0219047619047619</li>\n",
       "\t<li>0.111285714285714</li>\n",
       "\t<li>0.0412857142857143</li>\n",
       "\t<li>0.174809523809524</li>\n",
       "\t<li>0.574238095238095</li>\n",
       "\t<li>0.0255238095238095</li>\n",
       "\t<li>0.545285714285714</li>\n",
       "\t<li>0.912380952380953</li>\n",
       "\t<li>0.0385714285714286</li>\n",
       "\t<li>0.467285714285714</li>\n",
       "\t<li>0.16412925170068</li>\n",
       "\t<li>0.054</li>\n",
       "\t<li>0.158904761904762</li>\n",
       "\t<li>0.694857142857143</li>\n",
       "\t<li>0.00142857142857143</li>\n",
       "\t<li>0.781952380952381</li>\n",
       "\t<li>0.26347619047619</li>\n",
       "\t<li>0.030952380952381</li>\n",
       "\t<li>0.601761904761905</li>\n",
       "\t<li>0.748666666666667</li>\n",
       "\t<li>0.0636190476190476</li>\n",
       "\t<li>0.00357142857142857</li>\n",
       "\t<li>0.416696900982615</li>\n",
       "\t<li>0.308619047619048</li>\n",
       "\t<li>0.709952380952381</li>\n",
       "\t<li>0.36352380952381</li>\n",
       "\t<li>0.367285714285714</li>\n",
       "\t<li>0.496</li>\n",
       "\t<li>0.557619047619048</li>\n",
       "\t<li>0.381809523809524</li>\n",
       "\t<li>0.0135714285714286</li>\n",
       "\t<li>0.715809523809524</li>\n",
       "\t<li>0.0130952380952381</li>\n",
       "\t<li>0.460380952380952</li>\n",
       "\t<li>0.0426666666666667</li>\n",
       "\t<li>0.733952380952381</li>\n",
       "\t<li>0.0657142857142857</li>\n",
       "\t<li>0.197666666666667</li>\n",
       "\t<li>0.170952380952381</li>\n",
       "\t<li>0.00380952380952381</li>\n",
       "\t<li>0.220224489795918</li>\n",
       "\t<li>0.428142857142857</li>\n",
       "\t<li>0.0698095238095238</li>\n",
       "\t<li>0.0149047619047619</li>\n",
       "\t<li>0.731047619047619</li>\n",
       "\t<li>0.147619047619048</li>\n",
       "\t<li>0.245047619047619</li>\n",
       "\t<li>0.610761904761905</li>\n",
       "\t<li>0.0408095238095238</li>\n",
       "\t<li>0.343292517006803</li>\n",
       "\t<li>0.546238095238095</li>\n",
       "\t<li>0.0597619047619048</li>\n",
       "\t<li>0.0470952380952381</li>\n",
       "\t<li>0.300333333333333</li>\n",
       "\t<li>0.364619047619048</li>\n",
       "\t<li>0.240095238095238</li>\n",
       "\t<li>0.638904761904762</li>\n",
       "\t<li>0.375666666666667</li>\n",
       "\t<li>0.256047619047619</li>\n",
       "\t<li>0.00457142857142857</li>\n",
       "\t<li>0.400333333333333</li>\n",
       "\t<li>0.374904761904762</li>\n",
       "\t<li>0.00666666666666667</li>\n",
       "\t<li>0.609809523809524</li>\n",
       "\t<li>0.405952380952381</li>\n",
       "\t<li>0.772190476190476</li>\n",
       "\t<li>0.167666666666667</li>\n",
       "\t<li>0.870047619047619</li>\n",
       "\t<li>0.0993333333333334</li>\n",
       "\t<li>0.00714285714285714</li>\n",
       "\t<li>0.00342857142857143</li>\n",
       "\t<li>0.809904761904762</li>\n",
       "\t<li>0.468904761904762</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0380952380952381</li>\n",
       "\t<li>0.00914285714285714</li>\n",
       "\t<li>0.0375238095238095</li>\n",
       "\t<li>0.171238095238095</li>\n",
       "\t<li>0.452333333333333</li>\n",
       "\t<li>0.294571428571429</li>\n",
       "\t<li>0.112857142857143</li>\n",
       "\t<li>0.905142857142857</li>\n",
       "\t<li>0.0421428571428571</li>\n",
       "\t<li>0.216047619047619</li>\n",
       "\t<li>0.929809523809524</li>\n",
       "\t<li>0.00171428571428571</li>\n",
       "\t<li>0.325047619047619</li>\n",
       "\t<li>0.491</li>\n",
       "\t<li>0.284401360544218</li>\n",
       "\t<li>0.281333333333333</li>\n",
       "\t<li>0.27665306122449</li>\n",
       "\t<li>0.433523809523809</li>\n",
       "\t<li>0.718095238095238</li>\n",
       "\t<li>0.726761904761905</li>\n",
       "\t<li>0.452275132275132</li>\n",
       "\t<li>0.31047619047619</li>\n",
       "\t<li>0.18447619047619</li>\n",
       "\t<li>0.129571428571429</li>\n",
       "\t<li>0.0019047619047619</li>\n",
       "\t<li>0.00838095238095238</li>\n",
       "\t<li>0.0488571428571429</li>\n",
       "\t<li>0.701571428571429</li>\n",
       "\t<li>0.262285714285714</li>\n",
       "\t<li>0.24247619047619</li>\n",
       "\t<li>0.0877142857142857</li>\n",
       "\t<li>0.478142857142857</li>\n",
       "\t<li>0.0736190476190476</li>\n",
       "\t<li>0.0558095238095238</li>\n",
       "\t<li>0.752333333333333</li>\n",
       "\t<li>0.727666666666667</li>\n",
       "\t<li>0.721</li>\n",
       "\t<li>0.473904761904762</li>\n",
       "\t<li>0.371095238095238</li>\n",
       "\t<li>0.000571428571428571</li>\n",
       "\t<li>0.174285714285714</li>\n",
       "\t<li>0.11847619047619</li>\n",
       "\t<li>0.86052380952381</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0988095238095238</li>\n",
       "\t<li>0.332031746031746</li>\n",
       "\t<li>0.526476190476191</li>\n",
       "\t<li>0.0235714285714286</li>\n",
       "\t<li>0.00357142857142857</li>\n",
       "\t<li>0.271761904761905</li>\n",
       "\t<li>0.407619047619048</li>\n",
       "\t<li>0.829523809523809</li>\n",
       "\t<li>0.408428571428572</li>\n",
       "\t<li>0.0323809523809524</li>\n",
       "\t<li>0.270265306122449</li>\n",
       "\t<li>0.366</li>\n",
       "\t<li>0.907571428571428</li>\n",
       "\t<li>0.0439047619047619</li>\n",
       "\t<li>0.311428571428571</li>\n",
       "\t<li>0.709904761904762</li>\n",
       "\t<li>0.359190476190476</li>\n",
       "\t<li>0.169904761904762</li>\n",
       "\t<li>0.762523809523809</li>\n",
       "\t<li>0.0147619047619048</li>\n",
       "\t<li>0.188095238095238</li>\n",
       "\t<li>0.852333333333333</li>\n",
       "\t<li>0.132190476190476</li>\n",
       "\t<li>0.0175714285714286</li>\n",
       "\t<li>0.415639455782313</li>\n",
       "\t<li>0.155142857142857</li>\n",
       "\t<li>0.0414761904761905</li>\n",
       "\t<li>0.624904761904762</li>\n",
       "\t<li>0.162761904761905</li>\n",
       "\t<li>0.268619047619048</li>\n",
       "\t<li>0.0107142857142857</li>\n",
       "\t<li>0.299761904761905</li>\n",
       "\t<li>0.0019047619047619</li>\n",
       "\t<li>0.0381904761904762</li>\n",
       "\t<li>0.0530952380952381</li>\n",
       "\t<li>0.153428571428571</li>\n",
       "\t<li>0.037952380952381</li>\n",
       "\t<li>0.0318571428571429</li>\n",
       "\t<li>0.0722380952380952</li>\n",
       "\t<li>0.0796666666666667</li>\n",
       "\t<li>0.0401904761904762</li>\n",
       "\t<li>0.0103809523809524</li>\n",
       "\t<li>0.0798095238095238</li>\n",
       "\t<li>0.0388095238095238</li>\n",
       "\t<li>0.0101904761904762</li>\n",
       "\t<li>0.000571428571428571</li>\n",
       "\t<li>0.367428571428571</li>\n",
       "\t<li>0.926095238095238</li>\n",
       "\t<li>0.288571428571429</li>\n",
       "\t<li>0.0554761904761905</li>\n",
       "\t<li>0.0161428571428571</li>\n",
       "\t<li>0.504952380952381</li>\n",
       "\t<li>0.286047619047619</li>\n",
       "\t<li>0.291761904761905</li>\n",
       "\t<li>0.615333333333333</li>\n",
       "\t<li>0.725309523809524</li>\n",
       "\t<li>0.0867142857142857</li>\n",
       "\t<li>0.414142857142857</li>\n",
       "\t<li>0.0497142857142857</li>\n",
       "\t<li>0.0105714285714286</li>\n",
       "\t<li>0.687428571428571</li>\n",
       "\t<li>0.27352380952381</li>\n",
       "\t<li>0.468523809523809</li>\n",
       "\t<li>0.238142857142857</li>\n",
       "\t<li>0.230734693877551</li>\n",
       "\t<li>0.210333333333333</li>\n",
       "\t<li>0.0282857142857143</li>\n",
       "\t<li>0.495857142857143</li>\n",
       "\t<li>0.0521428571428571</li>\n",
       "\t<li>0.0628095238095238</li>\n",
       "\t<li>0.254444444444444</li>\n",
       "\t<li>0.42256462585034</li>\n",
       "\t<li>0.523333333333333</li>\n",
       "\t<li>0.167857142857143</li>\n",
       "\t<li>0.795285714285714</li>\n",
       "\t<li>0.402619047619048</li>\n",
       "\t<li>0.166619047619048</li>\n",
       "\t<li>0.182619047619048</li>\n",
       "\t<li>0.072952380952381</li>\n",
       "\t<li>0.399142857142857</li>\n",
       "\t<li>0.11347619047619</li>\n",
       "\t<li>0.00657142857142857</li>\n",
       "\t<li>0.444571428571429</li>\n",
       "\t<li>0.340380952380952</li>\n",
       "\t<li>0.220904761904762</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.0562380952380952</li>\n",
       "\t<li>0.502857142857143</li>\n",
       "\t<li>0.189714285714286</li>\n",
       "\t<li>0.803333333333334</li>\n",
       "\t<li>0.0363333333333333</li>\n",
       "\t<li>0.66452380952381</li>\n",
       "\t<li>0.0931428571428572</li>\n",
       "\t<li>0.909068027210884</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.0395714285714286</li>\n",
       "\t<li>0.336571428571428</li>\n",
       "\t<li>0.831857142857143</li>\n",
       "\t<li>0.842238095238095</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.028047619047619</li>\n",
       "\t<li>0.007</li>\n",
       "\t<li>0.228714285714286</li>\n",
       "\t<li>0.411714285714286</li>\n",
       "\t<li>0.00142857142857143</li>\n",
       "\t<li>0.968666666666667</li>\n",
       "\t<li>0.741380952380952</li>\n",
       "\t<li>0.757904761904762</li>\n",
       "\t<li>0.410285714285714</li>\n",
       "\t<li>0.673095238095238</li>\n",
       "\t<li>0.15552380952381</li>\n",
       "\t<li>0.710047619047619</li>\n",
       "\t<li>0.296571428571429</li>\n",
       "\t<li>0.0721428571428571</li>\n",
       "\t<li>0.217666666666667</li>\n",
       "\t<li>0.470333333333333</li>\n",
       "\t<li>0.308571428571429</li>\n",
       "\t<li>0.195142857142857</li>\n",
       "\t<li>0.43856462585034</li>\n",
       "\t<li>0.405142857142857</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.0936666666666667</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.531476190476191</li>\n",
       "\t<li>0.573061224489796</li>\n",
       "\t<li>0.132809523809524</li>\n",
       "\t<li>0.356714285714286</li>\n",
       "\t<li>0.0667142857142857</li>\n",
       "\t<li>0.199333333333333</li>\n",
       "\t<li>0.351666666666667</li>\n",
       "\t<li>0.0710476190476191</li>\n",
       "\t<li>0.371993197278912</li>\n",
       "\t<li>0.0696190476190476</li>\n",
       "\t<li>0.00733333333333333</li>\n",
       "\t<li>0.632374149659864</li>\n",
       "\t<li>0.0752857142857143</li>\n",
       "\t<li>0.0213809523809524</li>\n",
       "\t<li>0.0893333333333333</li>\n",
       "\t<li>0.485816326530612</li>\n",
       "\t<li>0.208687074829932</li>\n",
       "\t<li>0.0103809523809524</li>\n",
       "\t<li>0.318619047619048</li>\n",
       "\t<li>0.661857142857143</li>\n",
       "\t<li>0.016</li>\n",
       "\t<li>0.472857142857143</li>\n",
       "\t<li>0.127190476190476</li>\n",
       "\t<li>0.0238095238095238</li>\n",
       "\t<li>0.321190476190476</li>\n",
       "\t<li>0.252190476190476</li>\n",
       "\t<li>0.153666666666667</li>\n",
       "\t<li>0.0108571428571429</li>\n",
       "\t<li>0.390142857142857</li>\n",
       "\t<li>0.0610952380952381</li>\n",
       "\t<li>0.293666666666667</li>\n",
       "\t<li>0.24047619047619</li>\n",
       "\t<li>0.00857142857142857</li>\n",
       "\t<li>0.0652380952380952</li>\n",
       "\t<li>0.328489795918367</li>\n",
       "\t<li>0.386571428571429</li>\n",
       "\t<li>0.913386243386244</li>\n",
       "\t<li>0.0273333333333333</li>\n",
       "\t<li>0.200714285714286</li>\n",
       "\t<li>0.00857142857142857</li>\n",
       "\t<li>0.522476190476191</li>\n",
       "\t<li>0.193904761904762</li>\n",
       "\t<li>0.331333333333333</li>\n",
       "\t<li>0.0877619047619048</li>\n",
       "\t<li>0.489619047619048</li>\n",
       "\t<li>0.268904761904762</li>\n",
       "\t<li>0.0321904761904762</li>\n",
       "\t<li>0.0424761904761905</li>\n",
       "\t<li>0.237190476190476</li>\n",
       "\t<li>0.162142857142857</li>\n",
       "\t<li>0.0171428571428571</li>\n",
       "\t<li>0.044952380952381</li>\n",
       "\t<li>0.427095238095238</li>\n",
       "\t<li>0.549595238095238</li>\n",
       "\t<li>0.165</li>\n",
       "\t<li>0.0114761904761905</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.481619047619048</li>\n",
       "\t<li>0.0963809523809524</li>\n",
       "\t<li>0.366380952380952</li>\n",
       "\t<li>0.184380952380952</li>\n",
       "\t<li>0.532285714285714</li>\n",
       "\t<li>0.132714285714286</li>\n",
       "\t<li>0.252020408163265</li>\n",
       "\t<li>0.110380952380952</li>\n",
       "\t<li>0.590836734693878</li>\n",
       "\t<li>0.102333333333333</li>\n",
       "\t<li>0.0573809523809524</li>\n",
       "\t<li>0.695047619047619</li>\n",
       "\t<li>0.843857142857143</li>\n",
       "\t<li>0.140761904761905</li>\n",
       "\t<li>0.426666666666667</li>\n",
       "\t<li>0.153238095238095</li>\n",
       "\t<li>0.0205714285714286</li>\n",
       "\t<li>0.022952380952381</li>\n",
       "\t<li>0.288238095238095</li>\n",
       "\t<li>0.20947619047619</li>\n",
       "\t<li>0.468993197278912</li>\n",
       "\t<li>0.395</li>\n",
       "\t<li>0.502571428571429</li>\n",
       "\t<li>0.010952380952381</li>\n",
       "\t<li>0.233857142857143</li>\n",
       "\t<li>0.722428571428572</li>\n",
       "\t<li>0.906238095238095</li>\n",
       "\t<li>0.0976190476190476</li>\n",
       "\t<li>0.0846666666666667</li>\n",
       "\t<li>0.00171428571428571</li>\n",
       "\t<li>0.459857142857143</li>\n",
       "\t<li>0.0380952380952381</li>\n",
       "\t<li>0.0248571428571429</li>\n",
       "\t<li>0.719857142857143</li>\n",
       "\t<li>0.172285714285714</li>\n",
       "\t<li>0.270904761904762</li>\n",
       "\t<li>0.410952380952381</li>\n",
       "\t<li>0.867285714285714</li>\n",
       "\t<li>0.0474285714285714</li>\n",
       "\t<li>0.0428571428571429</li>\n",
       "\t<li>0.25247619047619</li>\n",
       "\t<li>0.00876190476190476</li>\n",
       "\t<li>0.436666666666667</li>\n",
       "\t<li>0.627095238095238</li>\n",
       "\t<li>0.802</li>\n",
       "\t<li>0.397380952380952</li>\n",
       "\t<li>0.1705</li>\n",
       "\t<li>0.00957142857142857</li>\n",
       "\t<li>0.0893095238095238</li>\n",
       "\t<li>0.0101904761904762</li>\n",
       "\t<li>0.559380952380952</li>\n",
       "\t<li>0.442285714285714</li>\n",
       "\t<li>0.0645714285714286</li>\n",
       "\t<li>0.434380952380952</li>\n",
       "\t<li>0.0474285714285714</li>\n",
       "\t<li>0.75452380952381</li>\n",
       "\t<li>0.282190476190476</li>\n",
       "\t<li>0.883666666666667</li>\n",
       "\t<li>0.00857142857142857</li>\n",
       "\t<li>0.694142857142857</li>\n",
       "\t<li>0.425952380952381</li>\n",
       "\t<li>0.202571428571429</li>\n",
       "\t<li>0.0233809523809524</li>\n",
       "\t<li>0.282666666666667</li>\n",
       "\t<li>0.105761904761905</li>\n",
       "\t<li>0.0820952380952381</li>\n",
       "\t<li>0.686809523809524</li>\n",
       "\t<li>0.171714285714286</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.214571428571429</li>\n",
       "\t<li>0.280714285714286</li>\n",
       "\t<li>0.0681428571428571</li>\n",
       "\t<li>0.032952380952381</li>\n",
       "\t<li>0.696469387755102</li>\n",
       "\t<li>0.270857142857143</li>\n",
       "\t<li>0.203285714285714</li>\n",
       "\t<li>0.0121428571428571</li>\n",
       "\t<li>0.350666666666667</li>\n",
       "\t<li>0.187619047619048</li>\n",
       "\t<li>0.568761904761905</li>\n",
       "\t<li>0.396</li>\n",
       "\t<li>0.0926190476190476</li>\n",
       "\t<li>0.361013605442177</li>\n",
       "\t<li>0.314190476190476</li>\n",
       "\t<li>0.645338624338624</li>\n",
       "\t<li>0.0178571428571429</li>\n",
       "\t<li>0.173285714285714</li>\n",
       "\t<li>0.107380952380952</li>\n",
       "\t<li>0.00666666666666667</li>\n",
       "\t<li>0.0511428571428571</li>\n",
       "\t<li>0.122095238095238</li>\n",
       "\t<li>0.193714285714286</li>\n",
       "\t<li>0.684714285714286</li>\n",
       "\t<li>0.0462380952380952</li>\n",
       "\t<li>0.0932380952380952</li>\n",
       "\t<li>0.0330952380952381</li>\n",
       "\t<li>0.815190476190476</li>\n",
       "\t<li>0.0311428571428571</li>\n",
       "\t<li>0.66152380952381</li>\n",
       "\t<li>0.0555238095238095</li>\n",
       "\t<li>0.0527142857142857</li>\n",
       "\t<li>0.14447619047619</li>\n",
       "\t<li>0.805857142857143</li>\n",
       "\t<li>0.508333333333333</li>\n",
       "\t<li>0.511333333333333</li>\n",
       "\t<li>0.851761904761905</li>\n",
       "\t<li>0.00142857142857143</li>\n",
       "\t<li>0.398666666666667</li>\n",
       "\t<li>0.59852380952381</li>\n",
       "\t<li>0.127428571428571</li>\n",
       "\t<li>0.188571428571429</li>\n",
       "\t<li>0.786428571428571</li>\n",
       "\t<li>0.00476190476190476</li>\n",
       "\t<li>0.204380952380952</li>\n",
       "\t<li>0.451714285714286</li>\n",
       "\t<li>0.04</li>\n",
       "\t<li>0.317312925170068</li>\n",
       "\t<li>0.0978571428571429</li>\n",
       "\t<li>0.395190476190476</li>\n",
       "\t<li>0.640063492063492</li>\n",
       "\t<li>0.0128571428571429</li>\n",
       "\t<li>0.706190476190476</li>\n",
       "\t<li>0.0417142857142857</li>\n",
       "\t<li>0.713380952380952</li>\n",
       "\t<li>0.263238095238095</li>\n",
       "\t<li>0.480571428571429</li>\n",
       "\t<li>0.663619047619048</li>\n",
       "\t<li>0.00723809523809524</li>\n",
       "\t<li>0.0201428571428571</li>\n",
       "\t<li>0.0141428571428571</li>\n",
       "\t<li>0.828809523809524</li>\n",
       "\t<li>0.802809523809524</li>\n",
       "\t<li>0.0204761904761905</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.016</li>\n",
       "\t<li>0.0596666666666667</li>\n",
       "\t<li>0.0272380952380952</li>\n",
       "\t<li>0.0446666666666667</li>\n",
       "\t<li>0.0371428571428571</li>\n",
       "\t<li>0.731095238095238</li>\n",
       "\t<li>0.156333333333333</li>\n",
       "\t<li>0.00571428571428571</li>\n",
       "\t<li>0.777142857142857</li>\n",
       "\t<li>0.767142857142857</li>\n",
       "\t<li>0.218666666666667</li>\n",
       "\t<li>0.0467619047619048</li>\n",
       "\t<li>0.753857142857143</li>\n",
       "\t<li>0.0226190476190476</li>\n",
       "\t<li>0.227047619047619</li>\n",
       "\t<li>0.0409523809523809</li>\n",
       "\t<li>0.180333333333333</li>\n",
       "\t<li>0.00142857142857143</li>\n",
       "\t<li>0.338238095238095</li>\n",
       "\t<li>0.527666666666667</li>\n",
       "\t<li>0.23452380952381</li>\n",
       "\t<li>0.310761904761905</li>\n",
       "\t<li>0.006</li>\n",
       "\t<li>0.548619047619048</li>\n",
       "\t<li>0.152619047619048</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.606428571428571</li>\n",
       "\t<li>0.0563809523809524</li>\n",
       "\t<li>0.719809523809524</li>\n",
       "\t<li>0.605476190476191</li>\n",
       "\t<li>0.321</li>\n",
       "\t<li>0.165761904761905</li>\n",
       "\t<li>0.775428571428572</li>\n",
       "\t<li>0.143857142857143</li>\n",
       "\t<li>0.0890952380952381</li>\n",
       "\t<li>0.307190476190476</li>\n",
       "\t<li>0.0317619047619048</li>\n",
       "\t<li>0.379619047619048</li>\n",
       "\t<li>0.713666666666667</li>\n",
       "\t<li>0.196857142857143</li>\n",
       "\t<li>0.032</li>\n",
       "\t<li>0.435059523809524</li>\n",
       "\t<li>0.0265714285714286</li>\n",
       "\t<li>0.336428571428571</li>\n",
       "\t<li>0.00733333333333333</li>\n",
       "\t<li>0.403333333333333</li>\n",
       "\t<li>0.55030612244898</li>\n",
       "\t<li>0.525142857142857</li>\n",
       "\t<li>0.0394285714285714</li>\n",
       "\t<li>0.0137142857142857</li>\n",
       "\t<li>0.0454761904761905</li>\n",
       "\t<li>0.938095238095238</li>\n",
       "\t<li>0.0355714285714286</li>\n",
       "\t<li>0.427857142857143</li>\n",
       "\t<li>0.11752380952381</li>\n",
       "\t<li>0.0185714285714286</li>\n",
       "\t<li>0.173761904761905</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.364047619047619</li>\n",
       "\t<li>0.0668095238095238</li>\n",
       "\t<li>0.683571428571429</li>\n",
       "\t<li>0.456333333333333</li>\n",
       "\t<li>0.60052380952381</li>\n",
       "\t<li>0.0135714285714286</li>\n",
       "\t<li>0.00171428571428571</li>\n",
       "\t<li>0.351392857142857</li>\n",
       "\t<li>0.0131428571428571</li>\n",
       "\t<li>0.68825</li>\n",
       "\t<li>0.006</li>\n",
       "\t<li>0.924333333333333</li>\n",
       "\t<li>0.787333333333333</li>\n",
       "\t<li>0.735238095238095</li>\n",
       "\t<li>0.0461428571428571</li>\n",
       "\t<li>0.24</li>\n",
       "\t<li>0.326571428571429</li>\n",
       "\t<li>0.144619047619048</li>\n",
       "\t<li>0.619142857142857</li>\n",
       "\t<li>0.00785714285714286</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.927047619047619</li>\n",
       "\t<li>0.0876190476190476</li>\n",
       "\t<li>0.768904761904762</li>\n",
       "\t<li>0.531714285714286</li>\n",
       "\t<li>0.0140952380952381</li>\n",
       "\t<li>0.526714285714286</li>\n",
       "\t<li>0.0852857142857143</li>\n",
       "\t<li>0.361047619047619</li>\n",
       "\t<li>0.414476190476191</li>\n",
       "\t<li>0.301751322751323</li>\n",
       "\t<li>0.155</li>\n",
       "\t<li>0.338517006802721</li>\n",
       "\t<li>0.400142857142857</li>\n",
       "\t<li>0.0252857142857143</li>\n",
       "\t<li>0.0468095238095238</li>\n",
       "\t<li>0.604428571428572</li>\n",
       "\t<li>0.506714285714286</li>\n",
       "\t<li>0.511047619047619</li>\n",
       "\t<li>0.185380952380952</li>\n",
       "\t<li>0.161047619047619</li>\n",
       "\t<li>0.563619047619048</li>\n",
       "\t<li>0.362227513227513</li>\n",
       "\t<li>0.795761904761905</li>\n",
       "\t<li>0.39912925170068</li>\n",
       "\t<li>0.244952380952381</li>\n",
       "\t<li>0.424634920634921</li>\n",
       "\t<li>0.139904761904762</li>\n",
       "\t<li>0.0142857142857143</li>\n",
       "\t<li>0.768904761904762</li>\n",
       "\t<li>0.0069047619047619</li>\n",
       "\t<li>0.00942857142857143</li>\n",
       "\t<li>0.504190476190476</li>\n",
       "\t<li>0.596952380952381</li>\n",
       "\t<li>0.00404761904761905</li>\n",
       "\t<li>0.0146190476190476</li>\n",
       "\t<li>0.229428571428572</li>\n",
       "\t<li>0.0504285714285714</li>\n",
       "\t<li>0.928809523809524</li>\n",
       "\t<li>0.106238095238095</li>\n",
       "\t<li>0.433619047619048</li>\n",
       "\t<li>0.010952380952381</li>\n",
       "\t<li>0.214285714285714</li>\n",
       "\t<li>0.159809523809524</li>\n",
       "\t<li>0.916492063492063</li>\n",
       "\t<li>0.740639455782313</li>\n",
       "\t<li>0.679904761904762</li>\n",
       "\t<li>0.0261904761904762</li>\n",
       "\t<li>0.450333333333333</li>\n",
       "\t<li>0.0745714285714286</li>\n",
       "\t<li>0.162428571428571</li>\n",
       "\t<li>0.356333333333333</li>\n",
       "\t<li>0.25752380952381</li>\n",
       "\t<li>0.728904761904762</li>\n",
       "\t<li>0.361253968253968</li>\n",
       "\t<li>0.0189047619047619</li>\n",
       "\t<li>0.162761904761905</li>\n",
       "\t<li>0.0874761904761905</li>\n",
       "\t<li>0.204190476190476</li>\n",
       "\t<li>0.0388571428571429</li>\n",
       "\t<li>0.716857142857143</li>\n",
       "\t<li>0.922571428571428</li>\n",
       "\t<li>0.439380952380952</li>\n",
       "\t<li>0.461476190476191</li>\n",
       "\t<li>0.264301587301587</li>\n",
       "\t<li>0.120619047619048</li>\n",
       "\t<li>0.342945578231293</li>\n",
       "\t<li>0.0617142857142857</li>\n",
       "\t<li>0.288503401360544</li>\n",
       "\t<li>0.278761904761905</li>\n",
       "\t<li>0.262836734693877</li>\n",
       "\t<li>0.243904761904762</li>\n",
       "\t<li>0.08</li>\n",
       "\t<li>0.83352380952381</li>\n",
       "\t<li>0.217666666666667</li>\n",
       "\t<li>0.0510476190476191</li>\n",
       "\t<li>0.0807142857142857</li>\n",
       "\t<li>0.864714285714286</li>\n",
       "\t<li>0.296285714285714</li>\n",
       "\t<li>0.293809523809524</li>\n",
       "\t<li>0.168190476190476</li>\n",
       "\t<li>0.32002380952381</li>\n",
       "\t<li>0.0669047619047619</li>\n",
       "\t<li>0.537571428571429</li>\n",
       "\t<li>0.891285714285714</li>\n",
       "\t<li>0.489666666666667</li>\n",
       "\t<li>0.555095238095238</li>\n",
       "\t<li>0.0395238095238095</li>\n",
       "\t<li>0.618761904761905</li>\n",
       "\t<li>0.0948095238095238</li>\n",
       "\t<li>0.0710476190476191</li>\n",
       "\t<li>0.610619047619048</li>\n",
       "\t<li>0.149714285714286</li>\n",
       "\t<li>0.299666666666667</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.122428571428571</li>\n",
       "\t<li>0.640809523809524</li>\n",
       "\t<li>0.217333333333333</li>\n",
       "\t<li>0.164095238095238</li>\n",
       "\t<li>0.229380952380952</li>\n",
       "\t<li>0.164571428571429</li>\n",
       "\t<li>0.034</li>\n",
       "\t<li>0.342952380952381</li>\n",
       "\t<li>0.265571428571429</li>\n",
       "\t<li>0.269095238095238</li>\n",
       "\t<li>0.461761904761905</li>\n",
       "\t<li>0.181095238095238</li>\n",
       "\t<li>0.334857142857143</li>\n",
       "\t<li>0.0060952380952381</li>\n",
       "\t<li>0.756904761904762</li>\n",
       "\t<li>0.216047619047619</li>\n",
       "\t<li>0.310571428571429</li>\n",
       "\t<li>0.100285714285714</li>\n",
       "\t<li>0.476809523809524</li>\n",
       "\t<li>0.259761904761905</li>\n",
       "\t<li>0.613619047619048</li>\n",
       "\t<li>0.195238095238095</li>\n",
       "\t<li>0.251666666666667</li>\n",
       "\t<li>0.25752380952381</li>\n",
       "\t<li>0.668380952380952</li>\n",
       "\t<li>0.509333333333333</li>\n",
       "\t<li>0.335349206349206</li>\n",
       "\t<li>0.0157142857142857</li>\n",
       "\t<li>0.0666190476190476</li>\n",
       "\t<li>0.755714285714286</li>\n",
       "\t<li>0.167619047619048</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.00528571428571429</li>\n",
       "\t<li>0.262285714285714</li>\n",
       "\t<li>0.0264285714285714</li>\n",
       "\t<li>0.487047619047619</li>\n",
       "\t<li>0.510238095238095</li>\n",
       "\t<li>0.00952380952380952</li>\n",
       "\t<li>0.0213809523809524</li>\n",
       "\t<li>0.00285714285714286</li>\n",
       "\t<li>0.0383333333333333</li>\n",
       "\t<li>0.0834761904761905</li>\n",
       "\t<li>0.115571428571429</li>\n",
       "\t<li>0.238428571428571</li>\n",
       "\t<li>0.0884761904761905</li>\n",
       "\t<li>0.447047619047619</li>\n",
       "\t<li>0.0718095238095238</li>\n",
       "\t<li>0.0383333333333333</li>\n",
       "\t<li>0.329166666666667</li>\n",
       "\t<li>0.0306190476190476</li>\n",
       "\t<li>0.18452380952381</li>\n",
       "\t<li>0.737761904761905</li>\n",
       "\t<li>0.310761904761905</li>\n",
       "\t<li>0.782571428571429</li>\n",
       "\t<li>0.218571428571429</li>\n",
       "\t<li>0.0222857142857143</li>\n",
       "\t<li>0.0597142857142857</li>\n",
       "\t<li>0.164571428571429</li>\n",
       "\t<li>0.0807142857142857</li>\n",
       "\t<li>0.0187142857142857</li>\n",
       "\t<li>0.319714285714286</li>\n",
       "\t<li>0.0135714285714286</li>\n",
       "\t<li>0.771619047619047</li>\n",
       "\t<li>0.003</li>\n",
       "\t<li>0.0992380952380952</li>\n",
       "\t<li>0.29352380952381</li>\n",
       "\t<li>0.0524761904761905</li>\n",
       "\t<li>0.255095238095238</li>\n",
       "\t<li>0.0107142857142857</li>\n",
       "\t<li>0.214904761904762</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.044\n",
       "\\item 0.191904761904762\n",
       "\\item 0.380809523809524\n",
       "\\item 0.0375714285714286\n",
       "\\item 0.626761904761905\n",
       "\\item 0.181142857142857\n",
       "\\item 0.402333333333333\n",
       "\\item 0.496238095238095\n",
       "\\item 0.0908571428571429\n",
       "\\item 0\n",
       "\\item 0.246047619047619\n",
       "\\item 0.377326530612245\n",
       "\\item 0.0195238095238095\n",
       "\\item 0.912809523809524\n",
       "\\item 0.163047619047619\n",
       "\\item 0.0121428571428571\n",
       "\\item 0.0132380952380952\n",
       "\\item 0.148333333333333\n",
       "\\item 0.148714285714286\n",
       "\\item 0.00952380952380952\n",
       "\\item 0.0748571428571428\n",
       "\\item 0.377\n",
       "\\item 0.941809523809524\n",
       "\\item 0.00476190476190476\n",
       "\\item 0.593095238095238\n",
       "\\item 0.0771904761904762\n",
       "\\item 0.487238095238095\n",
       "\\item 0.461952380952381\n",
       "\\item 0.433333333333333\n",
       "\\item 0.263476190476191\n",
       "\\item 0.435\n",
       "\\item 0.711333333333333\n",
       "\\item 0.0171428571428571\n",
       "\\item 0.422285714285714\n",
       "\\item 0.663190476190476\n",
       "\\item 0.235333333333333\n",
       "\\item 0.697380952380952\n",
       "\\item 0.708619047619048\n",
       "\\item 0.920619047619048\n",
       "\\item 0.0225714285714286\n",
       "\\item 0.176761904761905\n",
       "\\item 0.137952380952381\n",
       "\\item 0.132761904761905\n",
       "\\item 0.813238095238095\n",
       "\\item 0.221714285714286\n",
       "\\item 0.854714285714286\n",
       "\\item 0.191904761904762\n",
       "\\item 0.0438095238095238\n",
       "\\item 0.291656084656085\n",
       "\\item 0.137428571428571\n",
       "\\item 0.0139047619047619\n",
       "\\item 0.0128571428571429\n",
       "\\item 0.144714285714286\n",
       "\\item 0.410421768707483\n",
       "\\item 0.850380952380953\n",
       "\\item 0.28752380952381\n",
       "\\item 0.756809523809524\n",
       "\\item 0.56747619047619\n",
       "\\item 0.094\n",
       "\\item 0.514380952380952\n",
       "\\item 0.869619047619048\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.0714285714285714\n",
       "\\item 0.181238095238095\n",
       "\\item 0.0534761904761905\n",
       "\\item 0.0583809523809524\n",
       "\\item 0.000571428571428571\n",
       "\\item 0.0171428571428571\n",
       "\\item 0.625952380952381\n",
       "\\item 0.179809523809524\n",
       "\\item 0.574666666666666\n",
       "\\item 0.0245238095238095\n",
       "\\item 0.0406666666666667\n",
       "\\item 0.690202380952381\n",
       "\\item 0.0747142857142857\n",
       "\\item 0.249190476190476\n",
       "\\item 0.709\n",
       "\\item 0.0418571428571428\n",
       "\\item 0.485047619047619\n",
       "\\item 0.0794285714285714\n",
       "\\item 0.933190476190476\n",
       "\\item 0.08\n",
       "\\item 0.0020952380952381\n",
       "\\item 0.528095238095238\n",
       "\\item 0.235380952380952\n",
       "\\item 0.162666666666667\n",
       "\\item 0.000952380952380952\n",
       "\\item 0.0670952380952381\n",
       "\\item 0.844714285714286\n",
       "\\item 0.257571428571429\n",
       "\\item 0.0162857142857143\n",
       "\\item 0.0182857142857143\n",
       "\\item 0.0859523809523809\n",
       "\\item 0.713047619047619\n",
       "\\item 0.102761904761905\n",
       "\\item 0.409952380952381\n",
       "\\item 0.156809523809524\n",
       "\\item 0.075\n",
       "\\item 0.347322751322751\n",
       "\\item 0.464523809523809\n",
       "\\item 0.567952380952381\n",
       "\\item 0.032952380952381\n",
       "\\item 0.598190476190476\n",
       "\\item 0.361619047619048\n",
       "\\item 0.343238095238095\n",
       "\\item 0.375714285714286\n",
       "\\item 0.0126190476190476\n",
       "\\item 0.252857142857143\n",
       "\\item 0.10352380952381\n",
       "\\item 0.0261428571428571\n",
       "\\item 0.0245238095238095\n",
       "\\item 0.875619047619048\n",
       "\\item 0.151\n",
       "\\item 0.765857142857143\n",
       "\\item 0.247904761904762\n",
       "\\item 0.00895238095238095\n",
       "\\item 0.0138095238095238\n",
       "\\item 0.347714285714286\n",
       "\\item 0.0343809523809524\n",
       "\\item 0.0234285714285714\n",
       "\\item 0.0102857142857143\n",
       "\\item 0.0591428571428571\n",
       "\\item 0.009\n",
       "\\item 0.12\n",
       "\\item 0.173666666666667\n",
       "\\item 0.0331904761904762\n",
       "\\item 0.0326190476190476\n",
       "\\item 0.28647619047619\n",
       "\\item 0.659285714285714\n",
       "\\item 0.0126190476190476\n",
       "\\item 0.585\n",
       "\\item 0.382571428571429\n",
       "\\item 0.312142857142857\n",
       "\\item 0.424380952380952\n",
       "\\item 0.41452380952381\n",
       "\\item 0.588190476190476\n",
       "\\item 0.0721428571428571\n",
       "\\item 0.70647619047619\n",
       "\\item 0.11152380952381\n",
       "\\item 0.734238095238095\n",
       "\\item 0.384904761904762\n",
       "\\item 0.220619047619048\n",
       "\\item 0.288952380952381\n",
       "\\item 0.324238095238095\n",
       "\\item 0.323904761904762\n",
       "\\item 0.18747619047619\n",
       "\\item 0.719523809523809\n",
       "\\item 0.138571428571429\n",
       "\\item 0.269666666666667\n",
       "\\item 0.0961904761904762\n",
       "\\item 0.037\n",
       "\\item 0.389761904761905\n",
       "\\item 0.242666666666667\n",
       "\\item 0.14952380952381\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.0926666666666667\n",
       "\\item 0.0282380952380952\n",
       "\\item 0.562380952380952\n",
       "\\item 0.371238095238095\n",
       "\\item 0.916047619047619\n",
       "\\item 0.172571428571429\n",
       "\\item 0.197857142857143\n",
       "\\item 0.0318571428571429\n",
       "\\item 0.030047619047619\n",
       "\\item 0.0128571428571429\n",
       "\\item 0.624142857142857\n",
       "\\item 0.0662380952380952\n",
       "\\item 0.493714285714286\n",
       "\\item 0.0181428571428571\n",
       "\\item 0.118809523809524\n",
       "\\item 0.735619047619048\n",
       "\\item 0.324857142857143\n",
       "\\item 0.349142857142857\n",
       "\\item 0.0395238095238095\n",
       "\\item 0.412714285714286\n",
       "\\item 0.0106666666666667\n",
       "\\item 0.219809523809524\n",
       "\\item 0.300095238095238\n",
       "\\item 0.571571428571428\n",
       "\\item 0.890857142857143\n",
       "\\item 0.769523809523809\n",
       "\\item 0.708285714285714\n",
       "\\item 0.427238095238095\n",
       "\\item 0.0128571428571429\n",
       "\\item 0.861952380952381\n",
       "\\item 0.023952380952381\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.241\n",
       "\\item 0.197428571428571\n",
       "\\item 0.202619047619048\n",
       "\\item 0\n",
       "\\item 0.447333333333333\n",
       "\\item 0.630285714285714\n",
       "\\item 0.118571428571429\n",
       "\\item 0.00666666666666667\n",
       "\\item 0\n",
       "\\item 0.0477142857142857\n",
       "\\item 0.0114285714285714\n",
       "\\item 0.031\n",
       "\\item 0.72352380952381\n",
       "\\item 0.374047619047619\n",
       "\\item 0.157619047619048\n",
       "\\item 0.206095238095238\n",
       "\\item 0.519190476190476\n",
       "\\item 0.261333333333333\n",
       "\\item 0.626401360544217\n",
       "\\item 0.0287142857142857\n",
       "\\item 0.958857142857143\n",
       "\\item 0.0915238095238095\n",
       "\\item 0.715666666666667\n",
       "\\item 0.0171428571428571\n",
       "\\item 0\n",
       "\\item 0.0732857142857143\n",
       "\\item 0.518238095238095\n",
       "\\item 0.142333333333333\n",
       "\\item 0.745619047619048\n",
       "\\item 0.15447619047619\n",
       "\\item 0.0339047619047619\n",
       "\\item 0.00914285714285714\n",
       "\\item 0.0650952380952381\n",
       "\\item 0.0540952380952381\n",
       "\\item 0.00914285714285714\n",
       "\\item 0.348714285714286\n",
       "\\item 0.658\n",
       "\\item 0.0193809523809524\n",
       "\\item 0.184142857142857\n",
       "\\item 0.112285714285714\n",
       "\\item 0.809857142857143\n",
       "\\item 0.212904761904762\n",
       "\\item 0.00838095238095238\n",
       "\\item 0.872571428571429\n",
       "\\item 0.031\n",
       "\\item 0.377904761904762\n",
       "\\item 0.0561428571428571\n",
       "\\item 0.005\n",
       "\\item 0.0305238095238095\n",
       "\\item 0.321190476190476\n",
       "\\item 0.119904761904762\n",
       "\\item 0.600454081632653\n",
       "\\item 0.699809523809524\n",
       "\\item 0.0618571428571429\n",
       "\\item 0.00952380952380952\n",
       "\\item 0.0105714285714286\n",
       "\\item 0.0430952380952381\n",
       "\\item 0.0620340136054422\n",
       "\\item 0.0388571428571429\n",
       "\\item 0.507142857142857\n",
       "\\item 0.623714285714286\n",
       "\\item 0.51\n",
       "\\item 0.0019047619047619\n",
       "\\item 0.0462380952380952\n",
       "\\item 0.79147619047619\n",
       "\\item 0.779095238095238\n",
       "\\item 0.438095238095238\n",
       "\\item 0.0565238095238095\n",
       "\\item 0.548238095238095\n",
       "\\item 0.828333333333333\n",
       "\\item 0.194571428571429\n",
       "\\item 0.205857142857143\n",
       "\\item 0.152333333333333\n",
       "\\item 0.515714285714286\n",
       "\\item 0.241571428571429\n",
       "\\item 0.0705238095238095\n",
       "\\item 0.284115646258503\n",
       "\\item 0.653857142857143\n",
       "\\item 0.33747619047619\n",
       "\\item 0.0201904761904762\n",
       "\\item 0.158619047619048\n",
       "\\item 0.110904761904762\n",
       "\\item 0.0979523809523809\n",
       "\\item 0.321428571428571\n",
       "\\item 0.212285714285714\n",
       "\\item 0.264571428571429\n",
       "\\item 0.170608465608466\n",
       "\\item 0.0511904761904762\n",
       "\\item 0.266238095238095\n",
       "\\item 0.647428571428571\n",
       "\\item 0.0587142857142857\n",
       "\\item 0.301190476190476\n",
       "\\item 0.105238095238095\n",
       "\\item 0.371707482993197\n",
       "\\item 0.0566190476190476\n",
       "\\item 0.246190476190476\n",
       "\\item 0.119904761904762\n",
       "\\item 0.678\n",
       "\\item 0.348761904761905\n",
       "\\item 0.0734285714285714\n",
       "\\item 0.385166666666667\n",
       "\\item 0.641428571428571\n",
       "\\item 0.562952380952381\n",
       "\\item 0.385285714285714\n",
       "\\item 0.0872857142857143\n",
       "\\item 0.32652380952381\n",
       "\\item 0.554\n",
       "\\item 0.285380952380952\n",
       "\\item 0.885666666666667\n",
       "\\item 0.548642857142857\n",
       "\\item 0.209\n",
       "\\item 0.0412380952380952\n",
       "\\item 0.0812380952380952\n",
       "\\item 0.856285714285714\n",
       "\\item 0.0225714285714286\n",
       "\\item 0.0439047619047619\n",
       "\\item 0\n",
       "\\item 0.049047619047619\n",
       "\\item 0.0372857142857143\n",
       "\\item 0.0788571428571428\n",
       "\\item 0.164809523809524\n",
       "\\item 0.0354761904761905\n",
       "\\item 0.00642857142857143\n",
       "\\item 0.0958095238095238\n",
       "\\item 0.514587301587302\n",
       "\\item 0.699154761904762\n",
       "\\item 0.0326190476190476\n",
       "\\item 0.0218095238095238\n",
       "\\item 0.710857142857143\n",
       "\\item 0.244142857142857\n",
       "\\item 0.121714285714286\n",
       "\\item 0.510624338624339\n",
       "\\item 0.538952380952381\n",
       "\\item 0.0133333333333333\n",
       "\\item 0.191142857142857\n",
       "\\item 0.0172380952380952\n",
       "\\item 0.331465608465609\n",
       "\\item 0.215095238095238\n",
       "\\item 0.543952380952381\n",
       "\\item 0.631\n",
       "\\item 0.549238095238095\n",
       "\\item 0.0415238095238095\n",
       "\\item 0.319285714285714\n",
       "\\item 0.472809523809524\n",
       "\\item 0.23647619047619\n",
       "\\item 0.529238095238095\n",
       "\\item 0.647904761904762\n",
       "\\item 0.845767195767196\n",
       "\\item 0.244142857142857\n",
       "\\item 0.496476190476191\n",
       "\\item 0.727952380952381\n",
       "\\item 0.012\n",
       "\\item 0.147857142857143\n",
       "\\item 0.115428571428571\n",
       "\\item 0.109571428571429\n",
       "\\item 0.054047619047619\n",
       "\\item 0.275428571428571\n",
       "\\item 0.355333333333333\n",
       "\\item 0.0136190476190476\n",
       "\\item 0.147857142857143\n",
       "\\item 0.196\n",
       "\\item 0.0373809523809524\n",
       "\\item 0.0676190476190476\n",
       "\\item 0.0344761904761905\n",
       "\\item 0.0174285714285714\n",
       "\\item 0.161095238095238\n",
       "\\item 0.812333333333333\n",
       "\\item 0.143142857142857\n",
       "\\item 0.381238095238095\n",
       "\\item 0.792\n",
       "\\item 0.185333333333333\n",
       "\\item 0.105095238095238\n",
       "\\item 0.305625850340136\n",
       "\\item 0.0691428571428571\n",
       "\\item 0.0235238095238095\n",
       "\\item 0.0740476190476191\n",
       "\\item 0.161190476190476\n",
       "\\item 0.458\n",
       "\\item 0.377238095238095\n",
       "\\item 0.64584126984127\n",
       "\\item 0.0218571428571429\n",
       "\\item 0.0412380952380952\n",
       "\\item 0.0695714285714286\n",
       "\\item 0.728380952380952\n",
       "\\item 0.064047619047619\n",
       "\\item 0.00642857142857143\n",
       "\\item 0.0625238095238095\n",
       "\\item 0.12752380952381\n",
       "\\item 0.709047619047619\n",
       "\\item 0.0662857142857143\n",
       "\\item 0.0432857142857143\n",
       "\\item 0.00214285714285714\n",
       "\\item 0.238571428571429\n",
       "\\item 0.222809523809524\n",
       "\\item 0.547333333333333\n",
       "\\item 0.0241428571428571\n",
       "\\item 0.81747619047619\n",
       "\\item 0.16047619047619\n",
       "\\item 0.040952380952381\n",
       "\\item 0.00857142857142857\n",
       "\\item 0.362666666666667\n",
       "\\item 0.0647619047619048\n",
       "\\item 0.438380952380952\n",
       "\\item 0.0203333333333333\n",
       "\\item 0.0693809523809524\n",
       "\\item 0.813333333333333\n",
       "\\item 0.120952380952381\n",
       "\\item 0.0430952380952381\n",
       "\\item 0.33547619047619\n",
       "\\item 0.105714285714286\n",
       "\\item 0.053\n",
       "\\item 0.00661904761904762\n",
       "\\item 0.0196190476190476\n",
       "\\item 0.127666666666667\n",
       "\\item 0.801857142857143\n",
       "\\item 0.17847619047619\n",
       "\\item 0.894238095238095\n",
       "\\item 0.0836666666666667\n",
       "\\item 0.438904761904762\n",
       "\\item 0.110047619047619\n",
       "\\item 0.269857142857143\n",
       "\\item 0.684952380952381\n",
       "\\item 0.05\n",
       "\\item 0.213873015873016\n",
       "\\item 0.0328095238095238\n",
       "\\item 0.541333333333333\n",
       "\\item 0.0242380952380952\n",
       "\\item 0.557142857142857\n",
       "\\item 0.438238095238095\n",
       "\\item 0.202857142857143\n",
       "\\item 0.807619047619048\n",
       "\\item 0.350095238095238\n",
       "\\item 0.679714285714286\n",
       "\\item 0.371285714285714\n",
       "\\item 0.671809523809524\n",
       "\\item 0.0216666666666667\n",
       "\\item 0.0465714285714286\n",
       "\\item 0.175714285714286\n",
       "\\item 0.397619047619048\n",
       "\\item 0.138659863945578\n",
       "\\item 0.526047619047619\n",
       "\\item 0.155761904761905\n",
       "\\item 0.457\n",
       "\\item 0.461428571428571\n",
       "\\item 0.0723809523809524\n",
       "\\item 0.32747619047619\n",
       "\\item 0.305095238095238\n",
       "\\item 0.436857142857143\n",
       "\\item 0.230142857142857\n",
       "\\item 0.00228571428571429\n",
       "\\item 0.22447619047619\n",
       "\\item 0.693904761904762\n",
       "\\item 0.344428571428571\n",
       "\\item 0.0567142857142857\n",
       "\\item 0.139047619047619\n",
       "\\item 0.0854761904761905\n",
       "\\item 0.880142857142857\n",
       "\\item 0.197380952380952\n",
       "\\item 0.563952380952381\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.717142857142857\n",
       "\\item 0.0463809523809524\n",
       "\\item 0.537428571428571\n",
       "\\item 0.261\n",
       "\\item 0.0388571428571429\n",
       "\\item 0.792333333333333\n",
       "\\item 0.185285714285714\n",
       "\\item 0.00742857142857143\n",
       "\\item 0.38647619047619\n",
       "\\item 0.411095238095238\n",
       "\\item 0.845047619047619\n",
       "\\item 0.571047619047619\n",
       "\\item 0.0521904761904762\n",
       "\\item 0.085047619047619\n",
       "\\item 0.153666666666667\n",
       "\\item 0.81047619047619\n",
       "\\item 0.0672380952380952\n",
       "\\item 0.333777777777778\n",
       "\\item 0.105571428571429\n",
       "\\item 0.0135238095238095\n",
       "\\item 0.0434285714285714\n",
       "\\item 0.580857142857143\n",
       "\\item 0.341047619047619\n",
       "\\item 0.713857142857143\n",
       "\\item 0.103095238095238\n",
       "\\item 0.0604285714285714\n",
       "\\item 0.503238095238095\n",
       "\\item 0.014\n",
       "\\item 0.162714285714286\n",
       "\\item 0.740761904761905\n",
       "\\item 0.332095238095238\n",
       "\\item 0.0171428571428571\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.10347619047619\n",
       "\\item 0.820761904761905\n",
       "\\item 0.134190476190476\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.156952380952381\n",
       "\\item 0.540428571428571\n",
       "\\item 0.0170952380952381\n",
       "\\item 0.34308843537415\n",
       "\\item 0.0818095238095238\n",
       "\\item 0.595428571428571\n",
       "\\item 0.360285714285714\n",
       "\\item 0.666095238095238\n",
       "\\item 0.699809523809524\n",
       "\\item 0.100238095238095\n",
       "\\item 0.0211428571428571\n",
       "\\item 0.75647619047619\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.812714285714286\n",
       "\\item 0.114\n",
       "\\item 0.165952380952381\n",
       "\\item 0.13647619047619\n",
       "\\item 0.00380952380952381\n",
       "\\item 0.0181904761904762\n",
       "\\item 0.116\n",
       "\\item 0.00857142857142857\n",
       "\\item 0.326523809523809\n",
       "\\item 0.625059523809524\n",
       "\\item 0.119857142857143\n",
       "\\item 0.021\n",
       "\\item 0.427857142857143\n",
       "\\item 0.414333333333333\n",
       "\\item 0.0223809523809524\n",
       "\\item 0.0226666666666667\n",
       "\\item 0.0305714285714286\n",
       "\\item 0.30737037037037\n",
       "\\item 0.0508571428571429\n",
       "\\item 0.0877142857142857\n",
       "\\item 0.524952380952381\n",
       "\\item 0.459047619047619\n",
       "\\item 0.00476190476190476\n",
       "\\item 0.0303333333333333\n",
       "\\item 0.403619047619048\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.261952380952381\n",
       "\\item 0.372904761904762\n",
       "\\item 0.0593809523809524\n",
       "\\item 0.801190476190476\n",
       "\\item 0.176619047619048\n",
       "\\item 0.00857142857142857\n",
       "\\item 0.251904761904762\n",
       "\\item 0.00742857142857143\n",
       "\\item 0.510603174603175\n",
       "\\item 0.765428571428571\n",
       "\\item 0.297666666666667\n",
       "\\item 0.00880952380952381\n",
       "\\item 0.95047619047619\n",
       "\\item 0.672714285714286\n",
       "\\item 0.27347619047619\n",
       "\\item 0.224285714285714\n",
       "\\item 0.185571428571429\n",
       "\\item 0.916380952380952\n",
       "\\item 0.0245238095238095\n",
       "\\item 0.1\n",
       "\\item 0.0958095238095238\n",
       "\\item 0.0422857142857143\n",
       "\\item 0.626190476190476\n",
       "\\item 0.762\n",
       "\\item 0.0266666666666667\n",
       "\\item 0.508619047619048\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.0341904761904762\n",
       "\\item 0.0885714285714286\n",
       "\\item 0.817190476190476\n",
       "\\item 0.516571428571429\n",
       "\\item 0.609761904761905\n",
       "\\item 0.0881558441558442\n",
       "\\item 0.0408571428571429\n",
       "\\item 0.0534285714285714\n",
       "\\item 0.382809523809524\n",
       "\\item 0.0567142857142857\n",
       "\\item 0.072\n",
       "\\item 0.0540952380952381\n",
       "\\item 0.140333333333333\n",
       "\\item 0.27191156462585\n",
       "\\item 0.0124285714285714\n",
       "\\item 0.272380952380952\n",
       "\\item 0.0554761904761905\n",
       "\\item 0.511619047619048\n",
       "\\item 0.334428571428571\n",
       "\\item 0.013\n",
       "\\item 0.222190476190476\n",
       "\\item 0.208857142857143\n",
       "\\item 0.328619047619048\n",
       "\\item 0.208857142857143\n",
       "\\item 0.453374149659864\n",
       "\\item 0.534809523809524\n",
       "\\item 0.195952380952381\n",
       "\\item 0.0801428571428571\n",
       "\\item 0.0931428571428572\n",
       "\\item 0.00666666666666667\n",
       "\\item 0.008\n",
       "\\item 0.403619047619048\n",
       "\\item 0.520619047619048\n",
       "\\item 0.269380952380952\n",
       "\\item 0.478095238095238\n",
       "\\item 0.0175844155844156\n",
       "\\item 0.123857142857143\n",
       "\\item 0.236\n",
       "\\item 0.0586666666666667\n",
       "\\item 0.571\n",
       "\\item 0.215380952380952\n",
       "\\item 0.0777619047619048\n",
       "\\item 0\n",
       "\\item 0.0466666666666667\n",
       "\\item 0.671666666666667\n",
       "\\item 0.0275238095238095\n",
       "\\item 0.029952380952381\n",
       "\\item 0.0368571428571429\n",
       "\\item 0.542666666666667\n",
       "\\item 0.0767619047619048\n",
       "\\item 0.310619047619048\n",
       "\\item 0.612190476190476\n",
       "\\item 0.0524285714285714\n",
       "\\item 0.0798571428571428\n",
       "\\item 0.292476190476191\n",
       "\\item 0.869047619047619\n",
       "\\item 0.117142857142857\n",
       "\\item 0.0777619047619048\n",
       "\\item 0.123333333333333\n",
       "\\item 0.246238095238095\n",
       "\\item 0.467190476190476\n",
       "\\item 0.653809523809524\n",
       "\\item 0.161\n",
       "\\item 0.0935714285714286\n",
       "\\item 0.575095238095238\n",
       "\\item 0.036\n",
       "\\item 0.308714285714286\n",
       "\\item 0.177428571428571\n",
       "\\item 0.966148148148148\n",
       "\\item 0.134666666666667\n",
       "\\item 0.535095238095238\n",
       "\\item 0.91652380952381\n",
       "\\item 0.59647619047619\n",
       "\\item 0.275952380952381\n",
       "\\item 0.294761904761905\n",
       "\\item 0.0554761904761905\n",
       "\\item 0.0339047619047619\n",
       "\\item 0.335904761904762\n",
       "\\item 0.0308571428571429\n",
       "\\item 0.344619047619048\n",
       "\\item 0.11347619047619\n",
       "\\item 0.630523809523809\n",
       "\\item 0.238\n",
       "\\item 0.278428571428571\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.370417989417989\n",
       "\\item 0.164857142857143\n",
       "\\item 0.0195714285714286\n",
       "\\item 0.256761904761905\n",
       "\\item 0.320244897959184\n",
       "\\item 0.0954761904761905\n",
       "\\item 0.00557142857142857\n",
       "\\item 0.236857142857143\n",
       "\\item 0.127809523809524\n",
       "\\item 0.0164285714285714\n",
       "\\item 0.0946190476190476\n",
       "\\item 0.0703809523809524\n",
       "\\item 0.645809523809524\n",
       "\\item 0.434619047619048\n",
       "\\item 0.175380952380952\n",
       "\\item 0.0544761904761905\n",
       "\\item 0.472\n",
       "\\item 0.437571428571428\n",
       "\\item 0.376380952380952\n",
       "\\item 0.15347619047619\n",
       "\\item 0.685904761904762\n",
       "\\item 0.636857142857143\n",
       "\\item 0.575761904761905\n",
       "\\item 0.289809523809524\n",
       "\\item 0.267952380952381\n",
       "\\item 0.0232380952380952\n",
       "\\item 0.0395238095238095\n",
       "\\item 0.600095238095238\n",
       "\\item 0.212047619047619\n",
       "\\item 0.139619047619048\n",
       "\\item 0.613666666666667\n",
       "\\item 0.0416666666666667\n",
       "\\item 0\n",
       "\\item 0.735714285714286\n",
       "\\item 0.351380952380952\n",
       "\\item 0.114333333333333\n",
       "\\item 0\n",
       "\\item 0.754238095238095\n",
       "\\item 0.426047619047619\n",
       "\\item 0.452809523809524\n",
       "\\item 0.011047619047619\n",
       "\\item 0.642190476190476\n",
       "\\item 0.493904761904762\n",
       "\\item 0.397857142857143\n",
       "\\item 0.0324285714285714\n",
       "\\item 0.928285714285714\n",
       "\\item 0.0872380952380952\n",
       "\\item 0.277513227513228\n",
       "\\item 0.0963333333333333\n",
       "\\item 0.661047619047619\n",
       "\\item 0.40325\n",
       "\\item 0.483047619047619\n",
       "\\item 0.151428571428571\n",
       "\\item 0.190761904761905\n",
       "\\item 0.0289047619047619\n",
       "\\item 0.112047619047619\n",
       "\\item 0.505823129251701\n",
       "\\item 0.38452380952381\n",
       "\\item 0.0366796536796537\n",
       "\\item 0.462639455782313\n",
       "\\item 0.0467142857142857\n",
       "\\item 0.0438095238095238\n",
       "\\item 0.0238095238095238\n",
       "\\item 0.192857142857143\n",
       "\\item 0.0483809523809524\n",
       "\\item 0.27978231292517\n",
       "\\item 0.739285714285714\n",
       "\\item 0.18437037037037\n",
       "\\item 0.66152380952381\n",
       "\\item 0.634523809523809\n",
       "\\item 0.369619047619048\n",
       "\\item 0.0339523809523809\n",
       "\\item 0.342571428571429\n",
       "\\item 0.0417142857142857\n",
       "\\item 0.415659863945578\n",
       "\\item 0.953428571428572\n",
       "\\item 0.0456666666666667\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.157843537414966\n",
       "\\item 0.469190476190476\n",
       "\\item 0.521142857142857\n",
       "\\item 0.632476190476191\n",
       "\\item 0.913952380952381\n",
       "\\item 0.0314285714285714\n",
       "\\item 0.181904761904762\n",
       "\\item 0.441666666666667\n",
       "\\item 0.702285714285714\n",
       "\\item 0.0378095238095238\n",
       "\\item 0.042\n",
       "\\item 0.02\n",
       "\\item 0.387952380952381\n",
       "\\item 0.475428571428571\n",
       "\\item 0.602380952380952\n",
       "\\item 0.0551428571428571\n",
       "\\item 0.467761904761905\n",
       "\\item 0.652333333333333\n",
       "\\item 0.208428571428571\n",
       "\\item 0.0815714285714286\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.0672380952380952\n",
       "\\item 0.0209047619047619\n",
       "\\item 0.519761904761905\n",
       "\\item 0.0913809523809524\n",
       "\\item 0.0233809523809524\n",
       "\\item 0.00357142857142857\n",
       "\\item 0.098\n",
       "\\item 0.703190476190476\n",
       "\\item 0.0992857142857143\n",
       "\\item 0.903\n",
       "\\item 0.157619047619048\n",
       "\\item 0.624714285714286\n",
       "\\item 0.719\n",
       "\\item 0.00928571428571429\n",
       "\\item 0.203666666666667\n",
       "\\item 0.0121904761904762\n",
       "\\item 0.0327619047619048\n",
       "\\item 0.114\n",
       "\\item 0.269755102040816\n",
       "\\item 0.0568571428571429\n",
       "\\item 0.739714285714286\n",
       "\\item 0.163095238095238\n",
       "\\item 0.0393809523809524\n",
       "\\item 0.0123333333333333\n",
       "\\item 0.0182857142857143\n",
       "\\item 0.269619047619048\n",
       "\\item 0.547761904761905\n",
       "\\item 0.0841428571428571\n",
       "\\item 0.239380952380952\n",
       "\\item 0.908809523809524\n",
       "\\item 0.0987142857142857\n",
       "\\item 0.00342857142857143\n",
       "\\item 0.0187619047619048\n",
       "\\item 0.545761904761905\n",
       "\\item 0.286285714285714\n",
       "\\item 0.0157142857142857\n",
       "\\item 0.0345714285714286\n",
       "\\item 0.579619047619048\n",
       "\\item 0.885380952380952\n",
       "\\item 0.584666666666667\n",
       "\\item 0.116761904761905\n",
       "\\item 0.903034013605442\n",
       "\\item 0.347285714285714\n",
       "\\item 0.0279047619047619\n",
       "\\item 0.433142857142857\n",
       "\\item 0.130095238095238\n",
       "\\item 0.00342857142857143\n",
       "\\item 0.527571428571428\n",
       "\\item 0\n",
       "\\item 0.0204761904761905\n",
       "\\item 0.41637037037037\n",
       "\\item 0.512285714285714\n",
       "\\item 0.340380952380952\n",
       "\\item 0.140428571428571\n",
       "\\item 0.471666666666667\n",
       "\\item 0\n",
       "\\item 0.277\n",
       "\\item 0.69952380952381\n",
       "\\item 0.0248571428571429\n",
       "\\item 0.156619047619048\n",
       "\\item 0.00842857142857143\n",
       "\\item 0.00342857142857143\n",
       "\\item 0.0114285714285714\n",
       "\\item 0.0345714285714286\n",
       "\\item 0.333428571428571\n",
       "\\item 0.213666666666667\n",
       "\\item 0.00857142857142857\n",
       "\\item 0.203428571428571\n",
       "\\item 0.178428571428571\n",
       "\\item 0.248476190476191\n",
       "\\item 0.0899047619047619\n",
       "\\item 0.55452380952381\n",
       "\\item 0.0520476190476191\n",
       "\\item 0.103571428571429\n",
       "\\item 0.667619047619048\n",
       "\\item 0.405809523809524\n",
       "\\item 0.0800952380952381\n",
       "\\item 0\n",
       "\\item 0.0839523809523809\n",
       "\\item 0.366744520030234\n",
       "\\item 0.57652380952381\n",
       "\\item 0.167238095238095\n",
       "\\item 0.21952380952381\n",
       "\\item 0.47752380952381\n",
       "\\item 0.0203809523809524\n",
       "\\item 0.235047619047619\n",
       "\\item 0.0269047619047619\n",
       "\\item 0.634809523809524\n",
       "\\item 0.527\n",
       "\\item 0.0519047619047619\n",
       "\\item 0.0302857142857143\n",
       "\\item 0.75352380952381\n",
       "\\item 0.005\n",
       "\\item 0.213809523809524\n",
       "\\item 0.423857142857143\n",
       "\\item 0.0351428571428571\n",
       "\\item 0.0294285714285714\n",
       "\\item 0.739809523809524\n",
       "\\item 0.537095238095238\n",
       "\\item 0.892571428571428\n",
       "\\item 0.520095238095238\n",
       "\\item 0.158238095238095\n",
       "\\item 0.418857142857143\n",
       "\\item 0.0576190476190476\n",
       "\\item 0.661285714285714\n",
       "\\item 0.218619047619048\n",
       "\\item 0.794571428571429\n",
       "\\item 0.0224761904761905\n",
       "\\item 0.815904761904762\n",
       "\\item 0.685095238095238\n",
       "\\item 0.681238095238095\n",
       "\\item 0.0286796536796537\n",
       "\\item 0.162904761904762\n",
       "\\item 0.0384285714285714\n",
       "\\item 0.356333333333333\n",
       "\\item 0.0546666666666667\n",
       "\\item 0.580619047619048\n",
       "\\item 0.639380952380952\n",
       "\\item 0.0163333333333333\n",
       "\\item 0.857380952380952\n",
       "\\item 0.242809523809524\n",
       "\\item 0.0376190476190476\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0.622190476190476\n",
       "\\item 0.00885714285714286\n",
       "\\item 0.0962380952380952\n",
       "\\item 0.137142857142857\n",
       "\\item 0.330571428571429\n",
       "\\item 0.581857142857143\n",
       "\\item 0.049\n",
       "\\item 0.114809523809524\n",
       "\\item 0.505047619047619\n",
       "\\item 0.484095238095238\n",
       "\\item 0.0908095238095238\n",
       "\\item 0.069952380952381\n",
       "\\item 0.0179047619047619\n",
       "\\item 0.00128571428571429\n",
       "\\item 0.239666666666667\n",
       "\\item 0.428095238095238\n",
       "\\item 0.435884353741497\n",
       "\\item 0.135761904761905\n",
       "\\item 0.389428571428571\n",
       "\\item 0.623809523809524\n",
       "\\item 0.816428571428571\n",
       "\\item 0.979428571428571\n",
       "\\item 0.40352380952381\n",
       "\\item 0.669571428571429\n",
       "\\item 0.210809523809524\n",
       "\\item 0.0727142857142857\n",
       "\\item 0.089952380952381\n",
       "\\item 0.0253333333333333\n",
       "\\item 0.844238095238095\n",
       "\\item 0.277476190476191\n",
       "\\item 0.525809523809524\n",
       "\\item 0.677761904761905\n",
       "\\item 0.681190476190476\n",
       "\\item 0.202952380952381\n",
       "\\item 0.233428571428571\n",
       "\\item 0.244047619047619\n",
       "\\item 0.122857142857143\n",
       "\\item 0.185190476190476\n",
       "\\item 0.291904761904762\n",
       "\\item 0.196\n",
       "\\item 0.0922721088435374\n",
       "\\item 0.147857142857143\n",
       "\\item 0.702476190476191\n",
       "\\item 0.0173809523809524\n",
       "\\item 0.427714285714286\n",
       "\\item 0.669047619047619\n",
       "\\item 0.07\n",
       "\\item 0.166190476190476\n",
       "\\item 0.0134285714285714\n",
       "\\item 0.486231292517007\n",
       "\\item 0.472761904761905\n",
       "\\item 0.0347619047619048\n",
       "\\item 0.818428571428572\n",
       "\\item 0.774047619047619\n",
       "\\item 0.450142857142857\n",
       "\\item 0.0462380952380952\n",
       "\\item 0.0157619047619048\n",
       "\\item 0.531428571428571\n",
       "\\item 0.518380952380952\n",
       "\\item 0.597714285714286\n",
       "\\item 0.109714285714286\n",
       "\\item 0.00828571428571429\n",
       "\\item 0.776183673469388\n",
       "\\item 0.388285714285714\n",
       "\\item 0.824857142857143\n",
       "\\item 0.0982857142857143\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.307285714285714\n",
       "\\item 0.182904761904762\n",
       "\\item 0.268904761904762\n",
       "\\item 0.621428571428572\n",
       "\\item 0.962666666666667\n",
       "\\item 0.0693809523809524\n",
       "\\item 0.122428571428571\n",
       "\\item 0.81\n",
       "\\item 0.265333333333333\n",
       "\\item 0.0974285714285714\n",
       "\\item 0.521857142857143\n",
       "\\item 0.0827619047619048\n",
       "\\item 0.0747142857142857\n",
       "\\item 0.239666666666667\n",
       "\\item 0.395238095238095\n",
       "\\item 0.588190476190476\n",
       "\\item 0.480666666666667\n",
       "\\item 0.0242857142857143\n",
       "\\item 0.227714285714286\n",
       "\\item 0.0513809523809524\n",
       "\\item 0.0182857142857143\n",
       "\\item 0.974523809523809\n",
       "\\item 0.868095238095238\n",
       "\\item 0.732380952380952\n",
       "\\item 0.289761904761905\n",
       "\\item 0.304231292517007\n",
       "\\item 0.0672857142857143\n",
       "\\item 0.87252380952381\n",
       "\\item 0.177857142857143\n",
       "\\item 0.388190476190476\n",
       "\\item 0.0262857142857143\n",
       "\\item 0.00380952380952381\n",
       "\\item 0.82247619047619\n",
       "\\item 0.0725238095238095\n",
       "\\item 0.0441904761904762\n",
       "\\item 0.248571428571428\n",
       "\\item 0.00476190476190476\n",
       "\\item 0.0646666666666667\n",
       "\\item 0.0400952380952381\n",
       "\\item 0.460523809523809\n",
       "\\item 0.158428571428571\n",
       "\\item 0.0683333333333333\n",
       "\\item 0.38230612244898\n",
       "\\item 0.609619047619048\n",
       "\\item 0.0534285714285714\n",
       "\\item 0.266809523809524\n",
       "\\item 0.752095238095238\n",
       "\\item 0.506714285714286\n",
       "\\item 0.231666666666667\n",
       "\\item 0.0482857142857143\n",
       "\\item 0.943428571428572\n",
       "\\item 0.341285714285714\n",
       "\\item 0.300904761904762\n",
       "\\item 0.748190476190476\n",
       "\\item 0.912952380952381\n",
       "\\item 0.672183673469388\n",
       "\\item 0.495619047619048\n",
       "\\item 0.539714285714286\n",
       "\\item 0.666020408163265\n",
       "\\item 0.527095238095238\n",
       "\\item 0.317666666666667\n",
       "\\item 0.469244897959184\n",
       "\\item 0.472333333333333\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.0795714285714286\n",
       "\\item 0.0309523809523809\n",
       "\\item 0.055\n",
       "\\item 0.26547619047619\n",
       "\\item 0.0849047619047619\n",
       "\\item 0.883047619047619\n",
       "\\item 0.429904761904762\n",
       "\\item 0.0134285714285714\n",
       "\\item 0.00171428571428571\n",
       "\\item 0.670238095238095\n",
       "\\item 0.0296190476190476\n",
       "\\item 0.243619047619048\n",
       "\\item 0.364111111111111\n",
       "\\item 0.365047619047619\n",
       "\\item 0.28308843537415\n",
       "\\item 0.859095238095238\n",
       "\\item 0.676142857142857\n",
       "\\item 0.0112380952380952\n",
       "\\item 0.0019047619047619\n",
       "\\item 0.0779047619047619\n",
       "\\item 0.418380952380952\n",
       "\\item 0.212809523809524\n",
       "\\item 0.0934761904761905\n",
       "\\item 0.588945578231293\n",
       "\\item 0.0851904761904762\n",
       "\\item 0.73147619047619\n",
       "\\item 0.288714285714286\n",
       "\\item 0.318132275132275\n",
       "\\item 0.19852380952381\n",
       "\\item 0.667904761904762\n",
       "\\item 0.184285714285714\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.0257142857142857\n",
       "\\item 0\n",
       "\\item 0.00928571428571429\n",
       "\\item 0.178619047619048\n",
       "\\item 0.317285714285714\n",
       "\\item 0.389374149659864\n",
       "\\item 0.352095238095238\n",
       "\\item 0.00628571428571429\n",
       "\\item 0.122142857142857\n",
       "\\item 0.503598639455782\n",
       "\\item 0.864190476190476\n",
       "\\item 0.443904761904762\n",
       "\\item 0.461904761904762\n",
       "\\item 0.028\n",
       "\\item 0.0925714285714286\n",
       "\\item 0.507714285714286\n",
       "\\item 0.0493333333333333\n",
       "\\item 0.111571428571429\n",
       "\\item 0.873857142857143\n",
       "\\item 0.582\n",
       "\\item 0.864714285714286\n",
       "\\item 0.0378095238095238\n",
       "\\item 0.0157142857142857\n",
       "\\item 0.0524285714285714\n",
       "\\item 0.197809523809524\n",
       "\\item 0.136428571428571\n",
       "\\item 0.81552380952381\n",
       "\\item 0.494761904761905\n",
       "\\item 0.423047619047619\n",
       "\\item 0.0154285714285714\n",
       "\\item 0.407047619047619\n",
       "\\item 0.0135714285714286\n",
       "\\item 0.0905714285714286\n",
       "\\item 0.409142857142857\n",
       "\\item 0.0121904761904762\n",
       "\\item 0.0262857142857143\n",
       "\\item 0.282190476190476\n",
       "\\item 0.0290952380952381\n",
       "\\item 0\n",
       "\\item 0.0370952380952381\n",
       "\\item 0.399\n",
       "\\item 0.213414965986395\n",
       "\\item 0.331238095238095\n",
       "\\item 0.222904761904762\n",
       "\\item 0.295253968253968\n",
       "\\item 0.00514285714285714\n",
       "\\item 0.647380952380952\n",
       "\\item 0.806761904761905\n",
       "\\item 0.283714285714286\n",
       "\\item 0.628190476190476\n",
       "\\item 0.604666666666667\n",
       "\\item 0.221619047619048\n",
       "\\item 0.11952380952381\n",
       "\\item 0.0772380952380952\n",
       "\\item 0.329659863945578\n",
       "\\item 0.0132857142857143\n",
       "\\item 0.421136054421769\n",
       "\\item 0.668619047619047\n",
       "\\item 0.645\n",
       "\\item 0.0661904761904762\n",
       "\\item 0.0291428571428571\n",
       "\\item 0.603380952380952\n",
       "\\item 0.620714285714286\n",
       "\\item 0.01\n",
       "\\item 0.329761904761905\n",
       "\\item 0.336380952380952\n",
       "\\item 0.979428571428571\n",
       "\\item 0.281285714285714\n",
       "\\item 0.600380952380952\n",
       "\\item 0.772285714285714\n",
       "\\item 0.797142857142857\n",
       "\\item 0.067952380952381\n",
       "\\item 0.218904761904762\n",
       "\\item 0.200333333333333\n",
       "\\item 0.344238095238095\n",
       "\\item 0.614428571428572\n",
       "\\item 0.0587619047619048\n",
       "\\item 0.931428571428572\n",
       "\\item 0.205095238095238\n",
       "\\item 0.0135714285714286\n",
       "\\item 0.144714285714286\n",
       "\\item 0.575285714285714\n",
       "\\item 0.166857142857143\n",
       "\\item 0.209748299319728\n",
       "\\item 0.523037037037037\n",
       "\\item 0.117619047619048\n",
       "\\item 0.0724761904761905\n",
       "\\item 0.627206349206349\n",
       "\\item 0.0019047619047619\n",
       "\\item 0.173571428571429\n",
       "\\item 0.25047619047619\n",
       "\\item 0.301047619047619\n",
       "\\item 0.462952380952381\n",
       "\\item 0.540263605442177\n",
       "\\item 0.651238095238095\n",
       "\\item 0.0721428571428571\n",
       "\\item 0.677666666666667\n",
       "\\item 0.0378571428571429\n",
       "\\item 0.312285714285714\n",
       "\\item 0.209190476190476\n",
       "\\item 0.0603809523809524\n",
       "\\item 0.773380952380953\n",
       "\\item 0.475047619047619\n",
       "\\item 0.18268253968254\n",
       "\\item 0.0162857142857143\n",
       "\\item 0.0504285714285714\n",
       "\\item 0.0315714285714286\n",
       "\\item 0.0703809523809524\n",
       "\\item 0.911285714285714\n",
       "\\item 0.319904761904762\n",
       "\\item 0.14847619047619\n",
       "\\item 0.127047619047619\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.164428571428571\n",
       "\\item 0.176571428571429\n",
       "\\item 0.0527619047619048\n",
       "\\item 0.442231292517007\n",
       "\\item 0.0968571428571428\n",
       "\\item 0.562095238095238\n",
       "\\item 0.00628571428571429\n",
       "\\item 0.718904761904762\n",
       "\\item 0.120333333333333\n",
       "\\item 0.00857142857142857\n",
       "\\item 0.674714285714286\n",
       "\\item 0.752619047619048\n",
       "\\item 0.21952380952381\n",
       "\\item 0.158142857142857\n",
       "\\item 0.0441904761904762\n",
       "\\item 0.0465238095238095\n",
       "\\item 0.519238095238095\n",
       "\\item 0.0471428571428571\n",
       "\\item 0.352142857142857\n",
       "\\item 0.0792857142857143\n",
       "\\item 0.0146666666666667\n",
       "\\item 0.112666666666667\n",
       "\\item 0.690549319727891\n",
       "\\item 0.101\n",
       "\\item 0.828047619047619\n",
       "\\item 0.797\n",
       "\\item 0.188047619047619\n",
       "\\item 0.717619047619048\n",
       "\\item 0.320904761904762\n",
       "\\item 0.663\n",
       "\\item 0.636148148148148\n",
       "\\item 0.105619047619048\n",
       "\\item 0.624714285714286\n",
       "\\item 0.280571428571429\n",
       "\\item 0.147238095238095\n",
       "\\item 0.189857142857143\n",
       "\\item 0.564380952380952\n",
       "\\item 0.406666666666667\n",
       "\\item 0.208190476190476\n",
       "\\item 0.457666666666667\n",
       "\\item 0.0147619047619048\n",
       "\\item 0.272904761904762\n",
       "\\item 0.291238095238095\n",
       "\\item 0.0905238095238095\n",
       "\\item 0.337238095238095\n",
       "\\item 0.084\n",
       "\\item 0.512392857142857\n",
       "\\item 0.00952380952380952\n",
       "\\item 0.622802721088435\n",
       "\\item 0.0122380952380952\n",
       "\\item 0.325047619047619\n",
       "\\item 0.136285714285714\n",
       "\\item 0.0472857142857143\n",
       "\\item 0.00476190476190476\n",
       "\\item 0.101619047619048\n",
       "\\item 0.313333333333333\n",
       "\\item 0.605380952380952\n",
       "\\item 0.241952380952381\n",
       "\\item 0.0332857142857143\n",
       "\\item 0.0122857142857143\n",
       "\\item 0.109428571428571\n",
       "\\item 0.450380952380952\n",
       "\\item 0.0738571428571429\n",
       "\\item 0.119095238095238\n",
       "\\item 0.272142857142857\n",
       "\\item 0.457333333333333\n",
       "\\item 0.0888503401360544\n",
       "\\item 0.302190476190476\n",
       "\\item 0.0646666666666667\n",
       "\\item 0.130380952380952\n",
       "\\item 0.026047619047619\n",
       "\\item 0.331\n",
       "\\item 0.0284285714285714\n",
       "\\item 0.0868095238095238\n",
       "\\item 0.0691904761904762\n",
       "\\item 0.045952380952381\n",
       "\\item 0.579285714285714\n",
       "\\item 0.0391428571428571\n",
       "\\item 0.0271904761904762\n",
       "\\item 0.943857142857143\n",
       "\\item 0.692204081632653\n",
       "\\item 0.779285714285714\n",
       "\\item 0.12147619047619\n",
       "\\item 0.131761904761905\n",
       "\\item 0.151571428571429\n",
       "\\item 0.506068027210884\n",
       "\\item 0.013047619047619\n",
       "\\item 0.0355238095238095\n",
       "\\item 0.52852380952381\n",
       "\\item 0.025952380952381\n",
       "\\item 0.0804285714285714\n",
       "\\item 0.561142857142857\n",
       "\\item 0.498190476190476\n",
       "\\item 0.255666666666667\n",
       "\\item 0.0215238095238095\n",
       "\\item 0.0504761904761905\n",
       "\\item 0.678190476190476\n",
       "\\item 0.681904761904762\n",
       "\\item 0.0394761904761905\n",
       "\\item 0.0945238095238095\n",
       "\\item 0.153857142857143\n",
       "\\item 0.224142857142857\n",
       "\\item 0.861761904761905\n",
       "\\item 0.698523809523809\n",
       "\\item 0.368142857142857\n",
       "\\item 0.226047619047619\n",
       "\\item 0\n",
       "\\item 0.0131428571428571\n",
       "\\item 0.698904761904762\n",
       "\\item 0.426666666666667\n",
       "\\item 0.0287619047619048\n",
       "\\item 0.581278911564626\n",
       "\\item 0.0291428571428571\n",
       "\\item 0.220047619047619\n",
       "\\item 0.286095238095238\n",
       "\\item 0.688142857142857\n",
       "\\item 0.952619047619047\n",
       "\\item 0.309619047619048\n",
       "\\item 0.265142857142857\n",
       "\\item 0.165\n",
       "\\item 0.908666666666667\n",
       "\\item 0.0225238095238095\n",
       "\\item 0.0347142857142857\n",
       "\\item 0.371761904761905\n",
       "\\item 0.0166666666666667\n",
       "\\item 0.288714285714286\n",
       "\\item 0.500761904761905\n",
       "\\item 0.0203333333333333\n",
       "\\item 0.0380952380952381\n",
       "\\item 0.488719576719577\n",
       "\\item 0.0426190476190476\n",
       "\\item 0.181285714285714\n",
       "\\item 0.445380952380952\n",
       "\\item 0.0730952380952381\n",
       "\\item 0.448380952380952\n",
       "\\item 0.0331904761904762\n",
       "\\item 0.0204285714285714\n",
       "\\item 0.490380952380952\n",
       "\\item 0.0957619047619048\n",
       "\\item 0.384142857142857\n",
       "\\item 0.801428571428571\n",
       "\\item 0.293904761904762\n",
       "\\item 0.493285714285714\n",
       "\\item 0.0222857142857143\n",
       "\\item 0.0555714285714286\n",
       "\\item 0.0192857142857143\n",
       "\\item 0.594952380952381\n",
       "\\item 0.013\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.075\n",
       "\\item 0.0172857142857143\n",
       "\\item 0.494190476190476\n",
       "\\item 0.0483333333333333\n",
       "\\item 0.777809523809524\n",
       "\\item 0.118857142857143\n",
       "\\item 0.203571428571429\n",
       "\\item 0.854\n",
       "\\item 0.489761904761905\n",
       "\\item 0.67852380952381\n",
       "\\item 0.0278571428571429\n",
       "\\item 0.823714285714286\n",
       "\\item 0.768333333333333\n",
       "\\item 0.200809523809524\n",
       "\\item 0.012\n",
       "\\item 0.386224489795918\n",
       "\\item 0.565380952380952\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.275095238095238\n",
       "\\item 0.39347619047619\n",
       "\\item 0.305952380952381\n",
       "\\item 0.743904761904762\n",
       "\\item 0.0519047619047619\n",
       "\\item 0.0356666666666667\n",
       "\\item 0.721142857142857\n",
       "\\item 0\n",
       "\\item 0.127904761904762\n",
       "\\item 0.108952380952381\n",
       "\\item 0.149380952380952\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.386666666666667\n",
       "\\item 0.47212925170068\n",
       "\\item 0.00476190476190476\n",
       "\\item 0.166857142857143\n",
       "\\item 0.537952380952381\n",
       "\\item 0.672619047619048\n",
       "\\item 0.803714285714286\n",
       "\\item 0.192428571428571\n",
       "\\item 0.430428571428572\n",
       "\\item 0.294619047619048\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.501619047619048\n",
       "\\item 0.170142857142857\n",
       "\\item 0.0108571428571429\n",
       "\\item 0.284333333333333\n",
       "\\item 0.07\n",
       "\\item 0.0257142857142857\n",
       "\\item 0.177904761904762\n",
       "\\item 0.608571428571429\n",
       "\\item 0.370380952380952\n",
       "\\item 0.171904761904762\n",
       "\\item 0.796095238095238\n",
       "\\item 0.413238095238095\n",
       "\\item 0.0391428571428571\n",
       "\\item 0.00457142857142857\n",
       "\\item 0.14952380952381\n",
       "\\item 0.131\n",
       "\\item 0.316619047619048\n",
       "\\item 0.005\n",
       "\\item 0.202333333333333\n",
       "\\item 0.50852380952381\n",
       "\\item 0.228714285714286\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.370656084656085\n",
       "\\item 0.0304761904761905\n",
       "\\item 0.172285714285714\n",
       "\\item 0.204714285714286\n",
       "\\item 0.0191428571428571\n",
       "\\item 0.0638571428571429\n",
       "\\item 0.314952380952381\n",
       "\\item 0.0394285714285714\n",
       "\\item 0.770891156462585\n",
       "\\item 0.539095238095238\n",
       "\\item 0.419571428571429\n",
       "\\item 0.231714285714286\n",
       "\\item 0.380619047619048\n",
       "\\item 0\n",
       "\\item 0.576\n",
       "\\item 0.417047619047619\n",
       "\\item 0.396619047619048\n",
       "\\item 0.413476190476191\n",
       "\\item 0.281707482993197\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.345666666666667\n",
       "\\item 0.229333333333333\n",
       "\\item 0.104238095238095\n",
       "\\item 0.306952380952381\n",
       "\\item 0.232142857142857\n",
       "\\item 0.669068027210884\n",
       "\\item 0.233761904761905\n",
       "\\item 0.0374761904761905\n",
       "\\item 0.0327142857142857\n",
       "\\item 0.243904761904762\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.932190476190476\n",
       "\\item 0.235857142857143\n",
       "\\item 0.564190476190476\n",
       "\\item 0.319714285714286\n",
       "\\item 0.444571428571429\n",
       "\\item 0.518619047619048\n",
       "\\item 0.13847619047619\n",
       "\\item 0.356238095238095\n",
       "\\item 0.363634920634921\n",
       "\\item 0.342571428571429\n",
       "\\item 0.588571428571429\n",
       "\\item 0.388380952380952\n",
       "\\item 0.181\n",
       "\\item 0.0239047619047619\n",
       "\\item 0.0149047619047619\n",
       "\\item 0.0545238095238095\n",
       "\\item 0.179666666666667\n",
       "\\item 0.427571428571429\n",
       "\\item 0.0766666666666667\n",
       "\\item 0.381238095238095\n",
       "\\item 0.0219047619047619\n",
       "\\item 0.111285714285714\n",
       "\\item 0.0412857142857143\n",
       "\\item 0.174809523809524\n",
       "\\item 0.574238095238095\n",
       "\\item 0.0255238095238095\n",
       "\\item 0.545285714285714\n",
       "\\item 0.912380952380953\n",
       "\\item 0.0385714285714286\n",
       "\\item 0.467285714285714\n",
       "\\item 0.16412925170068\n",
       "\\item 0.054\n",
       "\\item 0.158904761904762\n",
       "\\item 0.694857142857143\n",
       "\\item 0.00142857142857143\n",
       "\\item 0.781952380952381\n",
       "\\item 0.26347619047619\n",
       "\\item 0.030952380952381\n",
       "\\item 0.601761904761905\n",
       "\\item 0.748666666666667\n",
       "\\item 0.0636190476190476\n",
       "\\item 0.00357142857142857\n",
       "\\item 0.416696900982615\n",
       "\\item 0.308619047619048\n",
       "\\item 0.709952380952381\n",
       "\\item 0.36352380952381\n",
       "\\item 0.367285714285714\n",
       "\\item 0.496\n",
       "\\item 0.557619047619048\n",
       "\\item 0.381809523809524\n",
       "\\item 0.0135714285714286\n",
       "\\item 0.715809523809524\n",
       "\\item 0.0130952380952381\n",
       "\\item 0.460380952380952\n",
       "\\item 0.0426666666666667\n",
       "\\item 0.733952380952381\n",
       "\\item 0.0657142857142857\n",
       "\\item 0.197666666666667\n",
       "\\item 0.170952380952381\n",
       "\\item 0.00380952380952381\n",
       "\\item 0.220224489795918\n",
       "\\item 0.428142857142857\n",
       "\\item 0.0698095238095238\n",
       "\\item 0.0149047619047619\n",
       "\\item 0.731047619047619\n",
       "\\item 0.147619047619048\n",
       "\\item 0.245047619047619\n",
       "\\item 0.610761904761905\n",
       "\\item 0.0408095238095238\n",
       "\\item 0.343292517006803\n",
       "\\item 0.546238095238095\n",
       "\\item 0.0597619047619048\n",
       "\\item 0.0470952380952381\n",
       "\\item 0.300333333333333\n",
       "\\item 0.364619047619048\n",
       "\\item 0.240095238095238\n",
       "\\item 0.638904761904762\n",
       "\\item 0.375666666666667\n",
       "\\item 0.256047619047619\n",
       "\\item 0.00457142857142857\n",
       "\\item 0.400333333333333\n",
       "\\item 0.374904761904762\n",
       "\\item 0.00666666666666667\n",
       "\\item 0.609809523809524\n",
       "\\item 0.405952380952381\n",
       "\\item 0.772190476190476\n",
       "\\item 0.167666666666667\n",
       "\\item 0.870047619047619\n",
       "\\item 0.0993333333333334\n",
       "\\item 0.00714285714285714\n",
       "\\item 0.00342857142857143\n",
       "\\item 0.809904761904762\n",
       "\\item 0.468904761904762\n",
       "\\item 0\n",
       "\\item 0.0380952380952381\n",
       "\\item 0.00914285714285714\n",
       "\\item 0.0375238095238095\n",
       "\\item 0.171238095238095\n",
       "\\item 0.452333333333333\n",
       "\\item 0.294571428571429\n",
       "\\item 0.112857142857143\n",
       "\\item 0.905142857142857\n",
       "\\item 0.0421428571428571\n",
       "\\item 0.216047619047619\n",
       "\\item 0.929809523809524\n",
       "\\item 0.00171428571428571\n",
       "\\item 0.325047619047619\n",
       "\\item 0.491\n",
       "\\item 0.284401360544218\n",
       "\\item 0.281333333333333\n",
       "\\item 0.27665306122449\n",
       "\\item 0.433523809523809\n",
       "\\item 0.718095238095238\n",
       "\\item 0.726761904761905\n",
       "\\item 0.452275132275132\n",
       "\\item 0.31047619047619\n",
       "\\item 0.18447619047619\n",
       "\\item 0.129571428571429\n",
       "\\item 0.0019047619047619\n",
       "\\item 0.00838095238095238\n",
       "\\item 0.0488571428571429\n",
       "\\item 0.701571428571429\n",
       "\\item 0.262285714285714\n",
       "\\item 0.24247619047619\n",
       "\\item 0.0877142857142857\n",
       "\\item 0.478142857142857\n",
       "\\item 0.0736190476190476\n",
       "\\item 0.0558095238095238\n",
       "\\item 0.752333333333333\n",
       "\\item 0.727666666666667\n",
       "\\item 0.721\n",
       "\\item 0.473904761904762\n",
       "\\item 0.371095238095238\n",
       "\\item 0.000571428571428571\n",
       "\\item 0.174285714285714\n",
       "\\item 0.11847619047619\n",
       "\\item 0.86052380952381\n",
       "\\item 0\n",
       "\\item 0.0988095238095238\n",
       "\\item 0.332031746031746\n",
       "\\item 0.526476190476191\n",
       "\\item 0.0235714285714286\n",
       "\\item 0.00357142857142857\n",
       "\\item 0.271761904761905\n",
       "\\item 0.407619047619048\n",
       "\\item 0.829523809523809\n",
       "\\item 0.408428571428572\n",
       "\\item 0.0323809523809524\n",
       "\\item 0.270265306122449\n",
       "\\item 0.366\n",
       "\\item 0.907571428571428\n",
       "\\item 0.0439047619047619\n",
       "\\item 0.311428571428571\n",
       "\\item 0.709904761904762\n",
       "\\item 0.359190476190476\n",
       "\\item 0.169904761904762\n",
       "\\item 0.762523809523809\n",
       "\\item 0.0147619047619048\n",
       "\\item 0.188095238095238\n",
       "\\item 0.852333333333333\n",
       "\\item 0.132190476190476\n",
       "\\item 0.0175714285714286\n",
       "\\item 0.415639455782313\n",
       "\\item 0.155142857142857\n",
       "\\item 0.0414761904761905\n",
       "\\item 0.624904761904762\n",
       "\\item 0.162761904761905\n",
       "\\item 0.268619047619048\n",
       "\\item 0.0107142857142857\n",
       "\\item 0.299761904761905\n",
       "\\item 0.0019047619047619\n",
       "\\item 0.0381904761904762\n",
       "\\item 0.0530952380952381\n",
       "\\item 0.153428571428571\n",
       "\\item 0.037952380952381\n",
       "\\item 0.0318571428571429\n",
       "\\item 0.0722380952380952\n",
       "\\item 0.0796666666666667\n",
       "\\item 0.0401904761904762\n",
       "\\item 0.0103809523809524\n",
       "\\item 0.0798095238095238\n",
       "\\item 0.0388095238095238\n",
       "\\item 0.0101904761904762\n",
       "\\item 0.000571428571428571\n",
       "\\item 0.367428571428571\n",
       "\\item 0.926095238095238\n",
       "\\item 0.288571428571429\n",
       "\\item 0.0554761904761905\n",
       "\\item 0.0161428571428571\n",
       "\\item 0.504952380952381\n",
       "\\item 0.286047619047619\n",
       "\\item 0.291761904761905\n",
       "\\item 0.615333333333333\n",
       "\\item 0.725309523809524\n",
       "\\item 0.0867142857142857\n",
       "\\item 0.414142857142857\n",
       "\\item 0.0497142857142857\n",
       "\\item 0.0105714285714286\n",
       "\\item 0.687428571428571\n",
       "\\item 0.27352380952381\n",
       "\\item 0.468523809523809\n",
       "\\item 0.238142857142857\n",
       "\\item 0.230734693877551\n",
       "\\item 0.210333333333333\n",
       "\\item 0.0282857142857143\n",
       "\\item 0.495857142857143\n",
       "\\item 0.0521428571428571\n",
       "\\item 0.0628095238095238\n",
       "\\item 0.254444444444444\n",
       "\\item 0.42256462585034\n",
       "\\item 0.523333333333333\n",
       "\\item 0.167857142857143\n",
       "\\item 0.795285714285714\n",
       "\\item 0.402619047619048\n",
       "\\item 0.166619047619048\n",
       "\\item 0.182619047619048\n",
       "\\item 0.072952380952381\n",
       "\\item 0.399142857142857\n",
       "\\item 0.11347619047619\n",
       "\\item 0.00657142857142857\n",
       "\\item 0.444571428571429\n",
       "\\item 0.340380952380952\n",
       "\\item 0.220904761904762\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.0562380952380952\n",
       "\\item 0.502857142857143\n",
       "\\item 0.189714285714286\n",
       "\\item 0.803333333333334\n",
       "\\item 0.0363333333333333\n",
       "\\item 0.66452380952381\n",
       "\\item 0.0931428571428572\n",
       "\\item 0.909068027210884\n",
       "\\item 0.008\n",
       "\\item 0.0395714285714286\n",
       "\\item 0.336571428571428\n",
       "\\item 0.831857142857143\n",
       "\\item 0.842238095238095\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.028047619047619\n",
       "\\item 0.007\n",
       "\\item 0.228714285714286\n",
       "\\item 0.411714285714286\n",
       "\\item 0.00142857142857143\n",
       "\\item 0.968666666666667\n",
       "\\item 0.741380952380952\n",
       "\\item 0.757904761904762\n",
       "\\item 0.410285714285714\n",
       "\\item 0.673095238095238\n",
       "\\item 0.15552380952381\n",
       "\\item 0.710047619047619\n",
       "\\item 0.296571428571429\n",
       "\\item 0.0721428571428571\n",
       "\\item 0.217666666666667\n",
       "\\item 0.470333333333333\n",
       "\\item 0.308571428571429\n",
       "\\item 0.195142857142857\n",
       "\\item 0.43856462585034\n",
       "\\item 0.405142857142857\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.0936666666666667\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.531476190476191\n",
       "\\item 0.573061224489796\n",
       "\\item 0.132809523809524\n",
       "\\item 0.356714285714286\n",
       "\\item 0.0667142857142857\n",
       "\\item 0.199333333333333\n",
       "\\item 0.351666666666667\n",
       "\\item 0.0710476190476191\n",
       "\\item 0.371993197278912\n",
       "\\item 0.0696190476190476\n",
       "\\item 0.00733333333333333\n",
       "\\item 0.632374149659864\n",
       "\\item 0.0752857142857143\n",
       "\\item 0.0213809523809524\n",
       "\\item 0.0893333333333333\n",
       "\\item 0.485816326530612\n",
       "\\item 0.208687074829932\n",
       "\\item 0.0103809523809524\n",
       "\\item 0.318619047619048\n",
       "\\item 0.661857142857143\n",
       "\\item 0.016\n",
       "\\item 0.472857142857143\n",
       "\\item 0.127190476190476\n",
       "\\item 0.0238095238095238\n",
       "\\item 0.321190476190476\n",
       "\\item 0.252190476190476\n",
       "\\item 0.153666666666667\n",
       "\\item 0.0108571428571429\n",
       "\\item 0.390142857142857\n",
       "\\item 0.0610952380952381\n",
       "\\item 0.293666666666667\n",
       "\\item 0.24047619047619\n",
       "\\item 0.00857142857142857\n",
       "\\item 0.0652380952380952\n",
       "\\item 0.328489795918367\n",
       "\\item 0.386571428571429\n",
       "\\item 0.913386243386244\n",
       "\\item 0.0273333333333333\n",
       "\\item 0.200714285714286\n",
       "\\item 0.00857142857142857\n",
       "\\item 0.522476190476191\n",
       "\\item 0.193904761904762\n",
       "\\item 0.331333333333333\n",
       "\\item 0.0877619047619048\n",
       "\\item 0.489619047619048\n",
       "\\item 0.268904761904762\n",
       "\\item 0.0321904761904762\n",
       "\\item 0.0424761904761905\n",
       "\\item 0.237190476190476\n",
       "\\item 0.162142857142857\n",
       "\\item 0.0171428571428571\n",
       "\\item 0.044952380952381\n",
       "\\item 0.427095238095238\n",
       "\\item 0.549595238095238\n",
       "\\item 0.165\n",
       "\\item 0.0114761904761905\n",
       "\\item 0\n",
       "\\item 0.481619047619048\n",
       "\\item 0.0963809523809524\n",
       "\\item 0.366380952380952\n",
       "\\item 0.184380952380952\n",
       "\\item 0.532285714285714\n",
       "\\item 0.132714285714286\n",
       "\\item 0.252020408163265\n",
       "\\item 0.110380952380952\n",
       "\\item 0.590836734693878\n",
       "\\item 0.102333333333333\n",
       "\\item 0.0573809523809524\n",
       "\\item 0.695047619047619\n",
       "\\item 0.843857142857143\n",
       "\\item 0.140761904761905\n",
       "\\item 0.426666666666667\n",
       "\\item 0.153238095238095\n",
       "\\item 0.0205714285714286\n",
       "\\item 0.022952380952381\n",
       "\\item 0.288238095238095\n",
       "\\item 0.20947619047619\n",
       "\\item 0.468993197278912\n",
       "\\item 0.395\n",
       "\\item 0.502571428571429\n",
       "\\item 0.010952380952381\n",
       "\\item 0.233857142857143\n",
       "\\item 0.722428571428572\n",
       "\\item 0.906238095238095\n",
       "\\item 0.0976190476190476\n",
       "\\item 0.0846666666666667\n",
       "\\item 0.00171428571428571\n",
       "\\item 0.459857142857143\n",
       "\\item 0.0380952380952381\n",
       "\\item 0.0248571428571429\n",
       "\\item 0.719857142857143\n",
       "\\item 0.172285714285714\n",
       "\\item 0.270904761904762\n",
       "\\item 0.410952380952381\n",
       "\\item 0.867285714285714\n",
       "\\item 0.0474285714285714\n",
       "\\item 0.0428571428571429\n",
       "\\item 0.25247619047619\n",
       "\\item 0.00876190476190476\n",
       "\\item 0.436666666666667\n",
       "\\item 0.627095238095238\n",
       "\\item 0.802\n",
       "\\item 0.397380952380952\n",
       "\\item 0.1705\n",
       "\\item 0.00957142857142857\n",
       "\\item 0.0893095238095238\n",
       "\\item 0.0101904761904762\n",
       "\\item 0.559380952380952\n",
       "\\item 0.442285714285714\n",
       "\\item 0.0645714285714286\n",
       "\\item 0.434380952380952\n",
       "\\item 0.0474285714285714\n",
       "\\item 0.75452380952381\n",
       "\\item 0.282190476190476\n",
       "\\item 0.883666666666667\n",
       "\\item 0.00857142857142857\n",
       "\\item 0.694142857142857\n",
       "\\item 0.425952380952381\n",
       "\\item 0.202571428571429\n",
       "\\item 0.0233809523809524\n",
       "\\item 0.282666666666667\n",
       "\\item 0.105761904761905\n",
       "\\item 0.0820952380952381\n",
       "\\item 0.686809523809524\n",
       "\\item 0.171714285714286\n",
       "\\item 0\n",
       "\\item 0.214571428571429\n",
       "\\item 0.280714285714286\n",
       "\\item 0.0681428571428571\n",
       "\\item 0.032952380952381\n",
       "\\item 0.696469387755102\n",
       "\\item 0.270857142857143\n",
       "\\item 0.203285714285714\n",
       "\\item 0.0121428571428571\n",
       "\\item 0.350666666666667\n",
       "\\item 0.187619047619048\n",
       "\\item 0.568761904761905\n",
       "\\item 0.396\n",
       "\\item 0.0926190476190476\n",
       "\\item 0.361013605442177\n",
       "\\item 0.314190476190476\n",
       "\\item 0.645338624338624\n",
       "\\item 0.0178571428571429\n",
       "\\item 0.173285714285714\n",
       "\\item 0.107380952380952\n",
       "\\item 0.00666666666666667\n",
       "\\item 0.0511428571428571\n",
       "\\item 0.122095238095238\n",
       "\\item 0.193714285714286\n",
       "\\item 0.684714285714286\n",
       "\\item 0.0462380952380952\n",
       "\\item 0.0932380952380952\n",
       "\\item 0.0330952380952381\n",
       "\\item 0.815190476190476\n",
       "\\item 0.0311428571428571\n",
       "\\item 0.66152380952381\n",
       "\\item 0.0555238095238095\n",
       "\\item 0.0527142857142857\n",
       "\\item 0.14447619047619\n",
       "\\item 0.805857142857143\n",
       "\\item 0.508333333333333\n",
       "\\item 0.511333333333333\n",
       "\\item 0.851761904761905\n",
       "\\item 0.00142857142857143\n",
       "\\item 0.398666666666667\n",
       "\\item 0.59852380952381\n",
       "\\item 0.127428571428571\n",
       "\\item 0.188571428571429\n",
       "\\item 0.786428571428571\n",
       "\\item 0.00476190476190476\n",
       "\\item 0.204380952380952\n",
       "\\item 0.451714285714286\n",
       "\\item 0.04\n",
       "\\item 0.317312925170068\n",
       "\\item 0.0978571428571429\n",
       "\\item 0.395190476190476\n",
       "\\item 0.640063492063492\n",
       "\\item 0.0128571428571429\n",
       "\\item 0.706190476190476\n",
       "\\item 0.0417142857142857\n",
       "\\item 0.713380952380952\n",
       "\\item 0.263238095238095\n",
       "\\item 0.480571428571429\n",
       "\\item 0.663619047619048\n",
       "\\item 0.00723809523809524\n",
       "\\item 0.0201428571428571\n",
       "\\item 0.0141428571428571\n",
       "\\item 0.828809523809524\n",
       "\\item 0.802809523809524\n",
       "\\item 0.0204761904761905\n",
       "\\item 0\n",
       "\\item 0.016\n",
       "\\item 0.0596666666666667\n",
       "\\item 0.0272380952380952\n",
       "\\item 0.0446666666666667\n",
       "\\item 0.0371428571428571\n",
       "\\item 0.731095238095238\n",
       "\\item 0.156333333333333\n",
       "\\item 0.00571428571428571\n",
       "\\item 0.777142857142857\n",
       "\\item 0.767142857142857\n",
       "\\item 0.218666666666667\n",
       "\\item 0.0467619047619048\n",
       "\\item 0.753857142857143\n",
       "\\item 0.0226190476190476\n",
       "\\item 0.227047619047619\n",
       "\\item 0.0409523809523809\n",
       "\\item 0.180333333333333\n",
       "\\item 0.00142857142857143\n",
       "\\item 0.338238095238095\n",
       "\\item 0.527666666666667\n",
       "\\item 0.23452380952381\n",
       "\\item 0.310761904761905\n",
       "\\item 0.006\n",
       "\\item 0.548619047619048\n",
       "\\item 0.152619047619048\n",
       "\\item 0\n",
       "\\item 0.606428571428571\n",
       "\\item 0.0563809523809524\n",
       "\\item 0.719809523809524\n",
       "\\item 0.605476190476191\n",
       "\\item 0.321\n",
       "\\item 0.165761904761905\n",
       "\\item 0.775428571428572\n",
       "\\item 0.143857142857143\n",
       "\\item 0.0890952380952381\n",
       "\\item 0.307190476190476\n",
       "\\item 0.0317619047619048\n",
       "\\item 0.379619047619048\n",
       "\\item 0.713666666666667\n",
       "\\item 0.196857142857143\n",
       "\\item 0.032\n",
       "\\item 0.435059523809524\n",
       "\\item 0.0265714285714286\n",
       "\\item 0.336428571428571\n",
       "\\item 0.00733333333333333\n",
       "\\item 0.403333333333333\n",
       "\\item 0.55030612244898\n",
       "\\item 0.525142857142857\n",
       "\\item 0.0394285714285714\n",
       "\\item 0.0137142857142857\n",
       "\\item 0.0454761904761905\n",
       "\\item 0.938095238095238\n",
       "\\item 0.0355714285714286\n",
       "\\item 0.427857142857143\n",
       "\\item 0.11752380952381\n",
       "\\item 0.0185714285714286\n",
       "\\item 0.173761904761905\n",
       "\\item 0\n",
       "\\item 0.364047619047619\n",
       "\\item 0.0668095238095238\n",
       "\\item 0.683571428571429\n",
       "\\item 0.456333333333333\n",
       "\\item 0.60052380952381\n",
       "\\item 0.0135714285714286\n",
       "\\item 0.00171428571428571\n",
       "\\item 0.351392857142857\n",
       "\\item 0.0131428571428571\n",
       "\\item 0.68825\n",
       "\\item 0.006\n",
       "\\item 0.924333333333333\n",
       "\\item 0.787333333333333\n",
       "\\item 0.735238095238095\n",
       "\\item 0.0461428571428571\n",
       "\\item 0.24\n",
       "\\item 0.326571428571429\n",
       "\\item 0.144619047619048\n",
       "\\item 0.619142857142857\n",
       "\\item 0.00785714285714286\n",
       "\\item 0\n",
       "\\item 0.927047619047619\n",
       "\\item 0.0876190476190476\n",
       "\\item 0.768904761904762\n",
       "\\item 0.531714285714286\n",
       "\\item 0.0140952380952381\n",
       "\\item 0.526714285714286\n",
       "\\item 0.0852857142857143\n",
       "\\item 0.361047619047619\n",
       "\\item 0.414476190476191\n",
       "\\item 0.301751322751323\n",
       "\\item 0.155\n",
       "\\item 0.338517006802721\n",
       "\\item 0.400142857142857\n",
       "\\item 0.0252857142857143\n",
       "\\item 0.0468095238095238\n",
       "\\item 0.604428571428572\n",
       "\\item 0.506714285714286\n",
       "\\item 0.511047619047619\n",
       "\\item 0.185380952380952\n",
       "\\item 0.161047619047619\n",
       "\\item 0.563619047619048\n",
       "\\item 0.362227513227513\n",
       "\\item 0.795761904761905\n",
       "\\item 0.39912925170068\n",
       "\\item 0.244952380952381\n",
       "\\item 0.424634920634921\n",
       "\\item 0.139904761904762\n",
       "\\item 0.0142857142857143\n",
       "\\item 0.768904761904762\n",
       "\\item 0.0069047619047619\n",
       "\\item 0.00942857142857143\n",
       "\\item 0.504190476190476\n",
       "\\item 0.596952380952381\n",
       "\\item 0.00404761904761905\n",
       "\\item 0.0146190476190476\n",
       "\\item 0.229428571428572\n",
       "\\item 0.0504285714285714\n",
       "\\item 0.928809523809524\n",
       "\\item 0.106238095238095\n",
       "\\item 0.433619047619048\n",
       "\\item 0.010952380952381\n",
       "\\item 0.214285714285714\n",
       "\\item 0.159809523809524\n",
       "\\item 0.916492063492063\n",
       "\\item 0.740639455782313\n",
       "\\item 0.679904761904762\n",
       "\\item 0.0261904761904762\n",
       "\\item 0.450333333333333\n",
       "\\item 0.0745714285714286\n",
       "\\item 0.162428571428571\n",
       "\\item 0.356333333333333\n",
       "\\item 0.25752380952381\n",
       "\\item 0.728904761904762\n",
       "\\item 0.361253968253968\n",
       "\\item 0.0189047619047619\n",
       "\\item 0.162761904761905\n",
       "\\item 0.0874761904761905\n",
       "\\item 0.204190476190476\n",
       "\\item 0.0388571428571429\n",
       "\\item 0.716857142857143\n",
       "\\item 0.922571428571428\n",
       "\\item 0.439380952380952\n",
       "\\item 0.461476190476191\n",
       "\\item 0.264301587301587\n",
       "\\item 0.120619047619048\n",
       "\\item 0.342945578231293\n",
       "\\item 0.0617142857142857\n",
       "\\item 0.288503401360544\n",
       "\\item 0.278761904761905\n",
       "\\item 0.262836734693877\n",
       "\\item 0.243904761904762\n",
       "\\item 0.08\n",
       "\\item 0.83352380952381\n",
       "\\item 0.217666666666667\n",
       "\\item 0.0510476190476191\n",
       "\\item 0.0807142857142857\n",
       "\\item 0.864714285714286\n",
       "\\item 0.296285714285714\n",
       "\\item 0.293809523809524\n",
       "\\item 0.168190476190476\n",
       "\\item 0.32002380952381\n",
       "\\item 0.0669047619047619\n",
       "\\item 0.537571428571429\n",
       "\\item 0.891285714285714\n",
       "\\item 0.489666666666667\n",
       "\\item 0.555095238095238\n",
       "\\item 0.0395238095238095\n",
       "\\item 0.618761904761905\n",
       "\\item 0.0948095238095238\n",
       "\\item 0.0710476190476191\n",
       "\\item 0.610619047619048\n",
       "\\item 0.149714285714286\n",
       "\\item 0.299666666666667\n",
       "\\item 0.004\n",
       "\\item 0.122428571428571\n",
       "\\item 0.640809523809524\n",
       "\\item 0.217333333333333\n",
       "\\item 0.164095238095238\n",
       "\\item 0.229380952380952\n",
       "\\item 0.164571428571429\n",
       "\\item 0.034\n",
       "\\item 0.342952380952381\n",
       "\\item 0.265571428571429\n",
       "\\item 0.269095238095238\n",
       "\\item 0.461761904761905\n",
       "\\item 0.181095238095238\n",
       "\\item 0.334857142857143\n",
       "\\item 0.0060952380952381\n",
       "\\item 0.756904761904762\n",
       "\\item 0.216047619047619\n",
       "\\item 0.310571428571429\n",
       "\\item 0.100285714285714\n",
       "\\item 0.476809523809524\n",
       "\\item 0.259761904761905\n",
       "\\item 0.613619047619048\n",
       "\\item 0.195238095238095\n",
       "\\item 0.251666666666667\n",
       "\\item 0.25752380952381\n",
       "\\item 0.668380952380952\n",
       "\\item 0.509333333333333\n",
       "\\item 0.335349206349206\n",
       "\\item 0.0157142857142857\n",
       "\\item 0.0666190476190476\n",
       "\\item 0.755714285714286\n",
       "\\item 0.167619047619048\n",
       "\\item 0\n",
       "\\item 0.00528571428571429\n",
       "\\item 0.262285714285714\n",
       "\\item 0.0264285714285714\n",
       "\\item 0.487047619047619\n",
       "\\item 0.510238095238095\n",
       "\\item 0.00952380952380952\n",
       "\\item 0.0213809523809524\n",
       "\\item 0.00285714285714286\n",
       "\\item 0.0383333333333333\n",
       "\\item 0.0834761904761905\n",
       "\\item 0.115571428571429\n",
       "\\item 0.238428571428571\n",
       "\\item 0.0884761904761905\n",
       "\\item 0.447047619047619\n",
       "\\item 0.0718095238095238\n",
       "\\item 0.0383333333333333\n",
       "\\item 0.329166666666667\n",
       "\\item 0.0306190476190476\n",
       "\\item 0.18452380952381\n",
       "\\item 0.737761904761905\n",
       "\\item 0.310761904761905\n",
       "\\item 0.782571428571429\n",
       "\\item 0.218571428571429\n",
       "\\item 0.0222857142857143\n",
       "\\item 0.0597142857142857\n",
       "\\item 0.164571428571429\n",
       "\\item 0.0807142857142857\n",
       "\\item 0.0187142857142857\n",
       "\\item 0.319714285714286\n",
       "\\item 0.0135714285714286\n",
       "\\item 0.771619047619047\n",
       "\\item 0.003\n",
       "\\item 0.0992380952380952\n",
       "\\item 0.29352380952381\n",
       "\\item 0.0524761904761905\n",
       "\\item 0.255095238095238\n",
       "\\item 0.0107142857142857\n",
       "\\item 0.214904761904762\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.044\n",
       "2. 0.191904761904762\n",
       "3. 0.380809523809524\n",
       "4. 0.0375714285714286\n",
       "5. 0.626761904761905\n",
       "6. 0.181142857142857\n",
       "7. 0.402333333333333\n",
       "8. 0.496238095238095\n",
       "9. 0.0908571428571429\n",
       "10. 0\n",
       "11. 0.246047619047619\n",
       "12. 0.377326530612245\n",
       "13. 0.0195238095238095\n",
       "14. 0.912809523809524\n",
       "15. 0.163047619047619\n",
       "16. 0.0121428571428571\n",
       "17. 0.0132380952380952\n",
       "18. 0.148333333333333\n",
       "19. 0.148714285714286\n",
       "20. 0.00952380952380952\n",
       "21. 0.0748571428571428\n",
       "22. 0.377\n",
       "23. 0.941809523809524\n",
       "24. 0.00476190476190476\n",
       "25. 0.593095238095238\n",
       "26. 0.0771904761904762\n",
       "27. 0.487238095238095\n",
       "28. 0.461952380952381\n",
       "29. 0.433333333333333\n",
       "30. 0.263476190476191\n",
       "31. 0.435\n",
       "32. 0.711333333333333\n",
       "33. 0.0171428571428571\n",
       "34. 0.422285714285714\n",
       "35. 0.663190476190476\n",
       "36. 0.235333333333333\n",
       "37. 0.697380952380952\n",
       "38. 0.708619047619048\n",
       "39. 0.920619047619048\n",
       "40. 0.0225714285714286\n",
       "41. 0.176761904761905\n",
       "42. 0.137952380952381\n",
       "43. 0.132761904761905\n",
       "44. 0.813238095238095\n",
       "45. 0.221714285714286\n",
       "46. 0.854714285714286\n",
       "47. 0.191904761904762\n",
       "48. 0.0438095238095238\n",
       "49. 0.291656084656085\n",
       "50. 0.137428571428571\n",
       "51. 0.0139047619047619\n",
       "52. 0.0128571428571429\n",
       "53. 0.144714285714286\n",
       "54. 0.410421768707483\n",
       "55. 0.850380952380953\n",
       "56. 0.28752380952381\n",
       "57. 0.756809523809524\n",
       "58. 0.56747619047619\n",
       "59. 0.094\n",
       "60. 0.514380952380952\n",
       "61. 0.869619047619048\n",
       "62. 0.00571428571428571\n",
       "63. 0.0714285714285714\n",
       "64. 0.181238095238095\n",
       "65. 0.0534761904761905\n",
       "66. 0.0583809523809524\n",
       "67. 0.000571428571428571\n",
       "68. 0.0171428571428571\n",
       "69. 0.625952380952381\n",
       "70. 0.179809523809524\n",
       "71. 0.574666666666666\n",
       "72. 0.0245238095238095\n",
       "73. 0.0406666666666667\n",
       "74. 0.690202380952381\n",
       "75. 0.0747142857142857\n",
       "76. 0.249190476190476\n",
       "77. 0.709\n",
       "78. 0.0418571428571428\n",
       "79. 0.485047619047619\n",
       "80. 0.0794285714285714\n",
       "81. 0.933190476190476\n",
       "82. 0.08\n",
       "83. 0.0020952380952381\n",
       "84. 0.528095238095238\n",
       "85. 0.235380952380952\n",
       "86. 0.162666666666667\n",
       "87. 0.000952380952380952\n",
       "88. 0.0670952380952381\n",
       "89. 0.844714285714286\n",
       "90. 0.257571428571429\n",
       "91. 0.0162857142857143\n",
       "92. 0.0182857142857143\n",
       "93. 0.0859523809523809\n",
       "94. 0.713047619047619\n",
       "95. 0.102761904761905\n",
       "96. 0.409952380952381\n",
       "97. 0.156809523809524\n",
       "98. 0.075\n",
       "99. 0.347322751322751\n",
       "100. 0.464523809523809\n",
       "101. 0.567952380952381\n",
       "102. 0.032952380952381\n",
       "103. 0.598190476190476\n",
       "104. 0.361619047619048\n",
       "105. 0.343238095238095\n",
       "106. 0.375714285714286\n",
       "107. 0.0126190476190476\n",
       "108. 0.252857142857143\n",
       "109. 0.10352380952381\n",
       "110. 0.0261428571428571\n",
       "111. 0.0245238095238095\n",
       "112. 0.875619047619048\n",
       "113. 0.151\n",
       "114. 0.765857142857143\n",
       "115. 0.247904761904762\n",
       "116. 0.00895238095238095\n",
       "117. 0.0138095238095238\n",
       "118. 0.347714285714286\n",
       "119. 0.0343809523809524\n",
       "120. 0.0234285714285714\n",
       "121. 0.0102857142857143\n",
       "122. 0.0591428571428571\n",
       "123. 0.009\n",
       "124. 0.12\n",
       "125. 0.173666666666667\n",
       "126. 0.0331904761904762\n",
       "127. 0.0326190476190476\n",
       "128. 0.28647619047619\n",
       "129. 0.659285714285714\n",
       "130. 0.0126190476190476\n",
       "131. 0.585\n",
       "132. 0.382571428571429\n",
       "133. 0.312142857142857\n",
       "134. 0.424380952380952\n",
       "135. 0.41452380952381\n",
       "136. 0.588190476190476\n",
       "137. 0.0721428571428571\n",
       "138. 0.70647619047619\n",
       "139. 0.11152380952381\n",
       "140. 0.734238095238095\n",
       "141. 0.384904761904762\n",
       "142. 0.220619047619048\n",
       "143. 0.288952380952381\n",
       "144. 0.324238095238095\n",
       "145. 0.323904761904762\n",
       "146. 0.18747619047619\n",
       "147. 0.719523809523809\n",
       "148. 0.138571428571429\n",
       "149. 0.269666666666667\n",
       "150. 0.0961904761904762\n",
       "151. 0.037\n",
       "152. 0.389761904761905\n",
       "153. 0.242666666666667\n",
       "154. 0.14952380952381\n",
       "155. 0.00571428571428571\n",
       "156. 0.0926666666666667\n",
       "157. 0.0282380952380952\n",
       "158. 0.562380952380952\n",
       "159. 0.371238095238095\n",
       "160. 0.916047619047619\n",
       "161. 0.172571428571429\n",
       "162. 0.197857142857143\n",
       "163. 0.0318571428571429\n",
       "164. 0.030047619047619\n",
       "165. 0.0128571428571429\n",
       "166. 0.624142857142857\n",
       "167. 0.0662380952380952\n",
       "168. 0.493714285714286\n",
       "169. 0.0181428571428571\n",
       "170. 0.118809523809524\n",
       "171. 0.735619047619048\n",
       "172. 0.324857142857143\n",
       "173. 0.349142857142857\n",
       "174. 0.0395238095238095\n",
       "175. 0.412714285714286\n",
       "176. 0.0106666666666667\n",
       "177. 0.219809523809524\n",
       "178. 0.300095238095238\n",
       "179. 0.571571428571428\n",
       "180. 0.890857142857143\n",
       "181. 0.769523809523809\n",
       "182. 0.708285714285714\n",
       "183. 0.427238095238095\n",
       "184. 0.0128571428571429\n",
       "185. 0.861952380952381\n",
       "186. 0.023952380952381\n",
       "187. 0.00285714285714286\n",
       "188. 0.241\n",
       "189. 0.197428571428571\n",
       "190. 0.202619047619048\n",
       "191. 0\n",
       "192. 0.447333333333333\n",
       "193. 0.630285714285714\n",
       "194. 0.118571428571429\n",
       "195. 0.00666666666666667\n",
       "196. 0\n",
       "197. 0.0477142857142857\n",
       "198. 0.0114285714285714\n",
       "199. 0.031\n",
       "200. 0.72352380952381\n",
       "201. 0.374047619047619\n",
       "202. 0.157619047619048\n",
       "203. 0.206095238095238\n",
       "204. 0.519190476190476\n",
       "205. 0.261333333333333\n",
       "206. 0.626401360544217\n",
       "207. 0.0287142857142857\n",
       "208. 0.958857142857143\n",
       "209. 0.0915238095238095\n",
       "210. 0.715666666666667\n",
       "211. 0.0171428571428571\n",
       "212. 0\n",
       "213. 0.0732857142857143\n",
       "214. 0.518238095238095\n",
       "215. 0.142333333333333\n",
       "216. 0.745619047619048\n",
       "217. 0.15447619047619\n",
       "218. 0.0339047619047619\n",
       "219. 0.00914285714285714\n",
       "220. 0.0650952380952381\n",
       "221. 0.0540952380952381\n",
       "222. 0.00914285714285714\n",
       "223. 0.348714285714286\n",
       "224. 0.658\n",
       "225. 0.0193809523809524\n",
       "226. 0.184142857142857\n",
       "227. 0.112285714285714\n",
       "228. 0.809857142857143\n",
       "229. 0.212904761904762\n",
       "230. 0.00838095238095238\n",
       "231. 0.872571428571429\n",
       "232. 0.031\n",
       "233. 0.377904761904762\n",
       "234. 0.0561428571428571\n",
       "235. 0.005\n",
       "236. 0.0305238095238095\n",
       "237. 0.321190476190476\n",
       "238. 0.119904761904762\n",
       "239. 0.600454081632653\n",
       "240. 0.699809523809524\n",
       "241. 0.0618571428571429\n",
       "242. 0.00952380952380952\n",
       "243. 0.0105714285714286\n",
       "244. 0.0430952380952381\n",
       "245. 0.0620340136054422\n",
       "246. 0.0388571428571429\n",
       "247. 0.507142857142857\n",
       "248. 0.623714285714286\n",
       "249. 0.51\n",
       "250. 0.0019047619047619\n",
       "251. 0.0462380952380952\n",
       "252. 0.79147619047619\n",
       "253. 0.779095238095238\n",
       "254. 0.438095238095238\n",
       "255. 0.0565238095238095\n",
       "256. 0.548238095238095\n",
       "257. 0.828333333333333\n",
       "258. 0.194571428571429\n",
       "259. 0.205857142857143\n",
       "260. 0.152333333333333\n",
       "261. 0.515714285714286\n",
       "262. 0.241571428571429\n",
       "263. 0.0705238095238095\n",
       "264. 0.284115646258503\n",
       "265. 0.653857142857143\n",
       "266. 0.33747619047619\n",
       "267. 0.0201904761904762\n",
       "268. 0.158619047619048\n",
       "269. 0.110904761904762\n",
       "270. 0.0979523809523809\n",
       "271. 0.321428571428571\n",
       "272. 0.212285714285714\n",
       "273. 0.264571428571429\n",
       "274. 0.170608465608466\n",
       "275. 0.0511904761904762\n",
       "276. 0.266238095238095\n",
       "277. 0.647428571428571\n",
       "278. 0.0587142857142857\n",
       "279. 0.301190476190476\n",
       "280. 0.105238095238095\n",
       "281. 0.371707482993197\n",
       "282. 0.0566190476190476\n",
       "283. 0.246190476190476\n",
       "284. 0.119904761904762\n",
       "285. 0.678\n",
       "286. 0.348761904761905\n",
       "287. 0.0734285714285714\n",
       "288. 0.385166666666667\n",
       "289. 0.641428571428571\n",
       "290. 0.562952380952381\n",
       "291. 0.385285714285714\n",
       "292. 0.0872857142857143\n",
       "293. 0.32652380952381\n",
       "294. 0.554\n",
       "295. 0.285380952380952\n",
       "296. 0.885666666666667\n",
       "297. 0.548642857142857\n",
       "298. 0.209\n",
       "299. 0.0412380952380952\n",
       "300. 0.0812380952380952\n",
       "301. 0.856285714285714\n",
       "302. 0.0225714285714286\n",
       "303. 0.0439047619047619\n",
       "304. 0\n",
       "305. 0.049047619047619\n",
       "306. 0.0372857142857143\n",
       "307. 0.0788571428571428\n",
       "308. 0.164809523809524\n",
       "309. 0.0354761904761905\n",
       "310. 0.00642857142857143\n",
       "311. 0.0958095238095238\n",
       "312. 0.514587301587302\n",
       "313. 0.699154761904762\n",
       "314. 0.0326190476190476\n",
       "315. 0.0218095238095238\n",
       "316. 0.710857142857143\n",
       "317. 0.244142857142857\n",
       "318. 0.121714285714286\n",
       "319. 0.510624338624339\n",
       "320. 0.538952380952381\n",
       "321. 0.0133333333333333\n",
       "322. 0.191142857142857\n",
       "323. 0.0172380952380952\n",
       "324. 0.331465608465609\n",
       "325. 0.215095238095238\n",
       "326. 0.543952380952381\n",
       "327. 0.631\n",
       "328. 0.549238095238095\n",
       "329. 0.0415238095238095\n",
       "330. 0.319285714285714\n",
       "331. 0.472809523809524\n",
       "332. 0.23647619047619\n",
       "333. 0.529238095238095\n",
       "334. 0.647904761904762\n",
       "335. 0.845767195767196\n",
       "336. 0.244142857142857\n",
       "337. 0.496476190476191\n",
       "338. 0.727952380952381\n",
       "339. 0.012\n",
       "340. 0.147857142857143\n",
       "341. 0.115428571428571\n",
       "342. 0.109571428571429\n",
       "343. 0.054047619047619\n",
       "344. 0.275428571428571\n",
       "345. 0.355333333333333\n",
       "346. 0.0136190476190476\n",
       "347. 0.147857142857143\n",
       "348. 0.196\n",
       "349. 0.0373809523809524\n",
       "350. 0.0676190476190476\n",
       "351. 0.0344761904761905\n",
       "352. 0.0174285714285714\n",
       "353. 0.161095238095238\n",
       "354. 0.812333333333333\n",
       "355. 0.143142857142857\n",
       "356. 0.381238095238095\n",
       "357. 0.792\n",
       "358. 0.185333333333333\n",
       "359. 0.105095238095238\n",
       "360. 0.305625850340136\n",
       "361. 0.0691428571428571\n",
       "362. 0.0235238095238095\n",
       "363. 0.0740476190476191\n",
       "364. 0.161190476190476\n",
       "365. 0.458\n",
       "366. 0.377238095238095\n",
       "367. 0.64584126984127\n",
       "368. 0.0218571428571429\n",
       "369. 0.0412380952380952\n",
       "370. 0.0695714285714286\n",
       "371. 0.728380952380952\n",
       "372. 0.064047619047619\n",
       "373. 0.00642857142857143\n",
       "374. 0.0625238095238095\n",
       "375. 0.12752380952381\n",
       "376. 0.709047619047619\n",
       "377. 0.0662857142857143\n",
       "378. 0.0432857142857143\n",
       "379. 0.00214285714285714\n",
       "380. 0.238571428571429\n",
       "381. 0.222809523809524\n",
       "382. 0.547333333333333\n",
       "383. 0.0241428571428571\n",
       "384. 0.81747619047619\n",
       "385. 0.16047619047619\n",
       "386. 0.040952380952381\n",
       "387. 0.00857142857142857\n",
       "388. 0.362666666666667\n",
       "389. 0.0647619047619048\n",
       "390. 0.438380952380952\n",
       "391. 0.0203333333333333\n",
       "392. 0.0693809523809524\n",
       "393. 0.813333333333333\n",
       "394. 0.120952380952381\n",
       "395. 0.0430952380952381\n",
       "396. 0.33547619047619\n",
       "397. 0.105714285714286\n",
       "398. 0.053\n",
       "399. 0.00661904761904762\n",
       "400. 0.0196190476190476\n",
       "401. 0.127666666666667\n",
       "402. 0.801857142857143\n",
       "403. 0.17847619047619\n",
       "404. 0.894238095238095\n",
       "405. 0.0836666666666667\n",
       "406. 0.438904761904762\n",
       "407. 0.110047619047619\n",
       "408. 0.269857142857143\n",
       "409. 0.684952380952381\n",
       "410. 0.05\n",
       "411. 0.213873015873016\n",
       "412. 0.0328095238095238\n",
       "413. 0.541333333333333\n",
       "414. 0.0242380952380952\n",
       "415. 0.557142857142857\n",
       "416. 0.438238095238095\n",
       "417. 0.202857142857143\n",
       "418. 0.807619047619048\n",
       "419. 0.350095238095238\n",
       "420. 0.679714285714286\n",
       "421. 0.371285714285714\n",
       "422. 0.671809523809524\n",
       "423. 0.0216666666666667\n",
       "424. 0.0465714285714286\n",
       "425. 0.175714285714286\n",
       "426. 0.397619047619048\n",
       "427. 0.138659863945578\n",
       "428. 0.526047619047619\n",
       "429. 0.155761904761905\n",
       "430. 0.457\n",
       "431. 0.461428571428571\n",
       "432. 0.0723809523809524\n",
       "433. 0.32747619047619\n",
       "434. 0.305095238095238\n",
       "435. 0.436857142857143\n",
       "436. 0.230142857142857\n",
       "437. 0.00228571428571429\n",
       "438. 0.22447619047619\n",
       "439. 0.693904761904762\n",
       "440. 0.344428571428571\n",
       "441. 0.0567142857142857\n",
       "442. 0.139047619047619\n",
       "443. 0.0854761904761905\n",
       "444. 0.880142857142857\n",
       "445. 0.197380952380952\n",
       "446. 0.563952380952381\n",
       "447. 0.00285714285714286\n",
       "448. 0.00285714285714286\n",
       "449. 0.717142857142857\n",
       "450. 0.0463809523809524\n",
       "451. 0.537428571428571\n",
       "452. 0.261\n",
       "453. 0.0388571428571429\n",
       "454. 0.792333333333333\n",
       "455. 0.185285714285714\n",
       "456. 0.00742857142857143\n",
       "457. 0.38647619047619\n",
       "458. 0.411095238095238\n",
       "459. 0.845047619047619\n",
       "460. 0.571047619047619\n",
       "461. 0.0521904761904762\n",
       "462. 0.085047619047619\n",
       "463. 0.153666666666667\n",
       "464. 0.81047619047619\n",
       "465. 0.0672380952380952\n",
       "466. 0.333777777777778\n",
       "467. 0.105571428571429\n",
       "468. 0.0135238095238095\n",
       "469. 0.0434285714285714\n",
       "470. 0.580857142857143\n",
       "471. 0.341047619047619\n",
       "472. 0.713857142857143\n",
       "473. 0.103095238095238\n",
       "474. 0.0604285714285714\n",
       "475. 0.503238095238095\n",
       "476. 0.014\n",
       "477. 0.162714285714286\n",
       "478. 0.740761904761905\n",
       "479. 0.332095238095238\n",
       "480. 0.0171428571428571\n",
       "481. 0.00285714285714286\n",
       "482. 0.10347619047619\n",
       "483. 0.820761904761905\n",
       "484. 0.134190476190476\n",
       "485. 0.00285714285714286\n",
       "486. 0.156952380952381\n",
       "487. 0.540428571428571\n",
       "488. 0.0170952380952381\n",
       "489. 0.34308843537415\n",
       "490. 0.0818095238095238\n",
       "491. 0.595428571428571\n",
       "492. 0.360285714285714\n",
       "493. 0.666095238095238\n",
       "494. 0.699809523809524\n",
       "495. 0.100238095238095\n",
       "496. 0.0211428571428571\n",
       "497. 0.75647619047619\n",
       "498. 0.00285714285714286\n",
       "499. 0.812714285714286\n",
       "500. 0.114\n",
       "501. 0.165952380952381\n",
       "502. 0.13647619047619\n",
       "503. 0.00380952380952381\n",
       "504. 0.0181904761904762\n",
       "505. 0.116\n",
       "506. 0.00857142857142857\n",
       "507. 0.326523809523809\n",
       "508. 0.625059523809524\n",
       "509. 0.119857142857143\n",
       "510. 0.021\n",
       "511. 0.427857142857143\n",
       "512. 0.414333333333333\n",
       "513. 0.0223809523809524\n",
       "514. 0.0226666666666667\n",
       "515. 0.0305714285714286\n",
       "516. 0.30737037037037\n",
       "517. 0.0508571428571429\n",
       "518. 0.0877142857142857\n",
       "519. 0.524952380952381\n",
       "520. 0.459047619047619\n",
       "521. 0.00476190476190476\n",
       "522. 0.0303333333333333\n",
       "523. 0.403619047619048\n",
       "524. 0.00571428571428571\n",
       "525. 0.261952380952381\n",
       "526. 0.372904761904762\n",
       "527. 0.0593809523809524\n",
       "528. 0.801190476190476\n",
       "529. 0.176619047619048\n",
       "530. 0.00857142857142857\n",
       "531. 0.251904761904762\n",
       "532. 0.00742857142857143\n",
       "533. 0.510603174603175\n",
       "534. 0.765428571428571\n",
       "535. 0.297666666666667\n",
       "536. 0.00880952380952381\n",
       "537. 0.95047619047619\n",
       "538. 0.672714285714286\n",
       "539. 0.27347619047619\n",
       "540. 0.224285714285714\n",
       "541. 0.185571428571429\n",
       "542. 0.916380952380952\n",
       "543. 0.0245238095238095\n",
       "544. 0.1\n",
       "545. 0.0958095238095238\n",
       "546. 0.0422857142857143\n",
       "547. 0.626190476190476\n",
       "548. 0.762\n",
       "549. 0.0266666666666667\n",
       "550. 0.508619047619048\n",
       "551. 0.00571428571428571\n",
       "552. 0.0341904761904762\n",
       "553. 0.0885714285714286\n",
       "554. 0.817190476190476\n",
       "555. 0.516571428571429\n",
       "556. 0.609761904761905\n",
       "557. 0.0881558441558442\n",
       "558. 0.0408571428571429\n",
       "559. 0.0534285714285714\n",
       "560. 0.382809523809524\n",
       "561. 0.0567142857142857\n",
       "562. 0.072\n",
       "563. 0.0540952380952381\n",
       "564. 0.140333333333333\n",
       "565. 0.27191156462585\n",
       "566. 0.0124285714285714\n",
       "567. 0.272380952380952\n",
       "568. 0.0554761904761905\n",
       "569. 0.511619047619048\n",
       "570. 0.334428571428571\n",
       "571. 0.013\n",
       "572. 0.222190476190476\n",
       "573. 0.208857142857143\n",
       "574. 0.328619047619048\n",
       "575. 0.208857142857143\n",
       "576. 0.453374149659864\n",
       "577. 0.534809523809524\n",
       "578. 0.195952380952381\n",
       "579. 0.0801428571428571\n",
       "580. 0.0931428571428572\n",
       "581. 0.00666666666666667\n",
       "582. 0.008\n",
       "583. 0.403619047619048\n",
       "584. 0.520619047619048\n",
       "585. 0.269380952380952\n",
       "586. 0.478095238095238\n",
       "587. 0.0175844155844156\n",
       "588. 0.123857142857143\n",
       "589. 0.236\n",
       "590. 0.0586666666666667\n",
       "591. 0.571\n",
       "592. 0.215380952380952\n",
       "593. 0.0777619047619048\n",
       "594. 0\n",
       "595. 0.0466666666666667\n",
       "596. 0.671666666666667\n",
       "597. 0.0275238095238095\n",
       "598. 0.029952380952381\n",
       "599. 0.0368571428571429\n",
       "600. 0.542666666666667\n",
       "601. 0.0767619047619048\n",
       "602. 0.310619047619048\n",
       "603. 0.612190476190476\n",
       "604. 0.0524285714285714\n",
       "605. 0.0798571428571428\n",
       "606. 0.292476190476191\n",
       "607. 0.869047619047619\n",
       "608. 0.117142857142857\n",
       "609. 0.0777619047619048\n",
       "610. 0.123333333333333\n",
       "611. 0.246238095238095\n",
       "612. 0.467190476190476\n",
       "613. 0.653809523809524\n",
       "614. 0.161\n",
       "615. 0.0935714285714286\n",
       "616. 0.575095238095238\n",
       "617. 0.036\n",
       "618. 0.308714285714286\n",
       "619. 0.177428571428571\n",
       "620. 0.966148148148148\n",
       "621. 0.134666666666667\n",
       "622. 0.535095238095238\n",
       "623. 0.91652380952381\n",
       "624. 0.59647619047619\n",
       "625. 0.275952380952381\n",
       "626. 0.294761904761905\n",
       "627. 0.0554761904761905\n",
       "628. 0.0339047619047619\n",
       "629. 0.335904761904762\n",
       "630. 0.0308571428571429\n",
       "631. 0.344619047619048\n",
       "632. 0.11347619047619\n",
       "633. 0.630523809523809\n",
       "634. 0.238\n",
       "635. 0.278428571428571\n",
       "636. 0.00285714285714286\n",
       "637. 0.370417989417989\n",
       "638. 0.164857142857143\n",
       "639. 0.0195714285714286\n",
       "640. 0.256761904761905\n",
       "641. 0.320244897959184\n",
       "642. 0.0954761904761905\n",
       "643. 0.00557142857142857\n",
       "644. 0.236857142857143\n",
       "645. 0.127809523809524\n",
       "646. 0.0164285714285714\n",
       "647. 0.0946190476190476\n",
       "648. 0.0703809523809524\n",
       "649. 0.645809523809524\n",
       "650. 0.434619047619048\n",
       "651. 0.175380952380952\n",
       "652. 0.0544761904761905\n",
       "653. 0.472\n",
       "654. 0.437571428571428\n",
       "655. 0.376380952380952\n",
       "656. 0.15347619047619\n",
       "657. 0.685904761904762\n",
       "658. 0.636857142857143\n",
       "659. 0.575761904761905\n",
       "660. 0.289809523809524\n",
       "661. 0.267952380952381\n",
       "662. 0.0232380952380952\n",
       "663. 0.0395238095238095\n",
       "664. 0.600095238095238\n",
       "665. 0.212047619047619\n",
       "666. 0.139619047619048\n",
       "667. 0.613666666666667\n",
       "668. 0.0416666666666667\n",
       "669. 0\n",
       "670. 0.735714285714286\n",
       "671. 0.351380952380952\n",
       "672. 0.114333333333333\n",
       "673. 0\n",
       "674. 0.754238095238095\n",
       "675. 0.426047619047619\n",
       "676. 0.452809523809524\n",
       "677. 0.011047619047619\n",
       "678. 0.642190476190476\n",
       "679. 0.493904761904762\n",
       "680. 0.397857142857143\n",
       "681. 0.0324285714285714\n",
       "682. 0.928285714285714\n",
       "683. 0.0872380952380952\n",
       "684. 0.277513227513228\n",
       "685. 0.0963333333333333\n",
       "686. 0.661047619047619\n",
       "687. 0.40325\n",
       "688. 0.483047619047619\n",
       "689. 0.151428571428571\n",
       "690. 0.190761904761905\n",
       "691. 0.0289047619047619\n",
       "692. 0.112047619047619\n",
       "693. 0.505823129251701\n",
       "694. 0.38452380952381\n",
       "695. 0.0366796536796537\n",
       "696. 0.462639455782313\n",
       "697. 0.0467142857142857\n",
       "698. 0.0438095238095238\n",
       "699. 0.0238095238095238\n",
       "700. 0.192857142857143\n",
       "701. 0.0483809523809524\n",
       "702. 0.27978231292517\n",
       "703. 0.739285714285714\n",
       "704. 0.18437037037037\n",
       "705. 0.66152380952381\n",
       "706. 0.634523809523809\n",
       "707. 0.369619047619048\n",
       "708. 0.0339523809523809\n",
       "709. 0.342571428571429\n",
       "710. 0.0417142857142857\n",
       "711. 0.415659863945578\n",
       "712. 0.953428571428572\n",
       "713. 0.0456666666666667\n",
       "714. 0.00571428571428571\n",
       "715. 0.157843537414966\n",
       "716. 0.469190476190476\n",
       "717. 0.521142857142857\n",
       "718. 0.632476190476191\n",
       "719. 0.913952380952381\n",
       "720. 0.0314285714285714\n",
       "721. 0.181904761904762\n",
       "722. 0.441666666666667\n",
       "723. 0.702285714285714\n",
       "724. 0.0378095238095238\n",
       "725. 0.042\n",
       "726. 0.02\n",
       "727. 0.387952380952381\n",
       "728. 0.475428571428571\n",
       "729. 0.602380952380952\n",
       "730. 0.0551428571428571\n",
       "731. 0.467761904761905\n",
       "732. 0.652333333333333\n",
       "733. 0.208428571428571\n",
       "734. 0.0815714285714286\n",
       "735. 0.00285714285714286\n",
       "736. 0.0672380952380952\n",
       "737. 0.0209047619047619\n",
       "738. 0.519761904761905\n",
       "739. 0.0913809523809524\n",
       "740. 0.0233809523809524\n",
       "741. 0.00357142857142857\n",
       "742. 0.098\n",
       "743. 0.703190476190476\n",
       "744. 0.0992857142857143\n",
       "745. 0.903\n",
       "746. 0.157619047619048\n",
       "747. 0.624714285714286\n",
       "748. 0.719\n",
       "749. 0.00928571428571429\n",
       "750. 0.203666666666667\n",
       "751. 0.0121904761904762\n",
       "752. 0.0327619047619048\n",
       "753. 0.114\n",
       "754. 0.269755102040816\n",
       "755. 0.0568571428571429\n",
       "756. 0.739714285714286\n",
       "757. 0.163095238095238\n",
       "758. 0.0393809523809524\n",
       "759. 0.0123333333333333\n",
       "760. 0.0182857142857143\n",
       "761. 0.269619047619048\n",
       "762. 0.547761904761905\n",
       "763. 0.0841428571428571\n",
       "764. 0.239380952380952\n",
       "765. 0.908809523809524\n",
       "766. 0.0987142857142857\n",
       "767. 0.00342857142857143\n",
       "768. 0.0187619047619048\n",
       "769. 0.545761904761905\n",
       "770. 0.286285714285714\n",
       "771. 0.0157142857142857\n",
       "772. 0.0345714285714286\n",
       "773. 0.579619047619048\n",
       "774. 0.885380952380952\n",
       "775. 0.584666666666667\n",
       "776. 0.116761904761905\n",
       "777. 0.903034013605442\n",
       "778. 0.347285714285714\n",
       "779. 0.0279047619047619\n",
       "780. 0.433142857142857\n",
       "781. 0.130095238095238\n",
       "782. 0.00342857142857143\n",
       "783. 0.527571428571428\n",
       "784. 0\n",
       "785. 0.0204761904761905\n",
       "786. 0.41637037037037\n",
       "787. 0.512285714285714\n",
       "788. 0.340380952380952\n",
       "789. 0.140428571428571\n",
       "790. 0.471666666666667\n",
       "791. 0\n",
       "792. 0.277\n",
       "793. 0.69952380952381\n",
       "794. 0.0248571428571429\n",
       "795. 0.156619047619048\n",
       "796. 0.00842857142857143\n",
       "797. 0.00342857142857143\n",
       "798. 0.0114285714285714\n",
       "799. 0.0345714285714286\n",
       "800. 0.333428571428571\n",
       "801. 0.213666666666667\n",
       "802. 0.00857142857142857\n",
       "803. 0.203428571428571\n",
       "804. 0.178428571428571\n",
       "805. 0.248476190476191\n",
       "806. 0.0899047619047619\n",
       "807. 0.55452380952381\n",
       "808. 0.0520476190476191\n",
       "809. 0.103571428571429\n",
       "810. 0.667619047619048\n",
       "811. 0.405809523809524\n",
       "812. 0.0800952380952381\n",
       "813. 0\n",
       "814. 0.0839523809523809\n",
       "815. 0.366744520030234\n",
       "816. 0.57652380952381\n",
       "817. 0.167238095238095\n",
       "818. 0.21952380952381\n",
       "819. 0.47752380952381\n",
       "820. 0.0203809523809524\n",
       "821. 0.235047619047619\n",
       "822. 0.0269047619047619\n",
       "823. 0.634809523809524\n",
       "824. 0.527\n",
       "825. 0.0519047619047619\n",
       "826. 0.0302857142857143\n",
       "827. 0.75352380952381\n",
       "828. 0.005\n",
       "829. 0.213809523809524\n",
       "830. 0.423857142857143\n",
       "831. 0.0351428571428571\n",
       "832. 0.0294285714285714\n",
       "833. 0.739809523809524\n",
       "834. 0.537095238095238\n",
       "835. 0.892571428571428\n",
       "836. 0.520095238095238\n",
       "837. 0.158238095238095\n",
       "838. 0.418857142857143\n",
       "839. 0.0576190476190476\n",
       "840. 0.661285714285714\n",
       "841. 0.218619047619048\n",
       "842. 0.794571428571429\n",
       "843. 0.0224761904761905\n",
       "844. 0.815904761904762\n",
       "845. 0.685095238095238\n",
       "846. 0.681238095238095\n",
       "847. 0.0286796536796537\n",
       "848. 0.162904761904762\n",
       "849. 0.0384285714285714\n",
       "850. 0.356333333333333\n",
       "851. 0.0546666666666667\n",
       "852. 0.580619047619048\n",
       "853. 0.639380952380952\n",
       "854. 0.0163333333333333\n",
       "855. 0.857380952380952\n",
       "856. 0.242809523809524\n",
       "857. 0.0376190476190476\n",
       "858. 0\n",
       "859. 0\n",
       "860. 0.622190476190476\n",
       "861. 0.00885714285714286\n",
       "862. 0.0962380952380952\n",
       "863. 0.137142857142857\n",
       "864. 0.330571428571429\n",
       "865. 0.581857142857143\n",
       "866. 0.049\n",
       "867. 0.114809523809524\n",
       "868. 0.505047619047619\n",
       "869. 0.484095238095238\n",
       "870. 0.0908095238095238\n",
       "871. 0.069952380952381\n",
       "872. 0.0179047619047619\n",
       "873. 0.00128571428571429\n",
       "874. 0.239666666666667\n",
       "875. 0.428095238095238\n",
       "876. 0.435884353741497\n",
       "877. 0.135761904761905\n",
       "878. 0.389428571428571\n",
       "879. 0.623809523809524\n",
       "880. 0.816428571428571\n",
       "881. 0.979428571428571\n",
       "882. 0.40352380952381\n",
       "883. 0.669571428571429\n",
       "884. 0.210809523809524\n",
       "885. 0.0727142857142857\n",
       "886. 0.089952380952381\n",
       "887. 0.0253333333333333\n",
       "888. 0.844238095238095\n",
       "889. 0.277476190476191\n",
       "890. 0.525809523809524\n",
       "891. 0.677761904761905\n",
       "892. 0.681190476190476\n",
       "893. 0.202952380952381\n",
       "894. 0.233428571428571\n",
       "895. 0.244047619047619\n",
       "896. 0.122857142857143\n",
       "897. 0.185190476190476\n",
       "898. 0.291904761904762\n",
       "899. 0.196\n",
       "900. 0.0922721088435374\n",
       "901. 0.147857142857143\n",
       "902. 0.702476190476191\n",
       "903. 0.0173809523809524\n",
       "904. 0.427714285714286\n",
       "905. 0.669047619047619\n",
       "906. 0.07\n",
       "907. 0.166190476190476\n",
       "908. 0.0134285714285714\n",
       "909. 0.486231292517007\n",
       "910. 0.472761904761905\n",
       "911. 0.0347619047619048\n",
       "912. 0.818428571428572\n",
       "913. 0.774047619047619\n",
       "914. 0.450142857142857\n",
       "915. 0.0462380952380952\n",
       "916. 0.0157619047619048\n",
       "917. 0.531428571428571\n",
       "918. 0.518380952380952\n",
       "919. 0.597714285714286\n",
       "920. 0.109714285714286\n",
       "921. 0.00828571428571429\n",
       "922. 0.776183673469388\n",
       "923. 0.388285714285714\n",
       "924. 0.824857142857143\n",
       "925. 0.0982857142857143\n",
       "926. 0.00285714285714286\n",
       "927. 0.307285714285714\n",
       "928. 0.182904761904762\n",
       "929. 0.268904761904762\n",
       "930. 0.621428571428572\n",
       "931. 0.962666666666667\n",
       "932. 0.0693809523809524\n",
       "933. 0.122428571428571\n",
       "934. 0.81\n",
       "935. 0.265333333333333\n",
       "936. 0.0974285714285714\n",
       "937. 0.521857142857143\n",
       "938. 0.0827619047619048\n",
       "939. 0.0747142857142857\n",
       "940. 0.239666666666667\n",
       "941. 0.395238095238095\n",
       "942. 0.588190476190476\n",
       "943. 0.480666666666667\n",
       "944. 0.0242857142857143\n",
       "945. 0.227714285714286\n",
       "946. 0.0513809523809524\n",
       "947. 0.0182857142857143\n",
       "948. 0.974523809523809\n",
       "949. 0.868095238095238\n",
       "950. 0.732380952380952\n",
       "951. 0.289761904761905\n",
       "952. 0.304231292517007\n",
       "953. 0.0672857142857143\n",
       "954. 0.87252380952381\n",
       "955. 0.177857142857143\n",
       "956. 0.388190476190476\n",
       "957. 0.0262857142857143\n",
       "958. 0.00380952380952381\n",
       "959. 0.82247619047619\n",
       "960. 0.0725238095238095\n",
       "961. 0.0441904761904762\n",
       "962. 0.248571428571428\n",
       "963. 0.00476190476190476\n",
       "964. 0.0646666666666667\n",
       "965. 0.0400952380952381\n",
       "966. 0.460523809523809\n",
       "967. 0.158428571428571\n",
       "968. 0.0683333333333333\n",
       "969. 0.38230612244898\n",
       "970. 0.609619047619048\n",
       "971. 0.0534285714285714\n",
       "972. 0.266809523809524\n",
       "973. 0.752095238095238\n",
       "974. 0.506714285714286\n",
       "975. 0.231666666666667\n",
       "976. 0.0482857142857143\n",
       "977. 0.943428571428572\n",
       "978. 0.341285714285714\n",
       "979. 0.300904761904762\n",
       "980. 0.748190476190476\n",
       "981. 0.912952380952381\n",
       "982. 0.672183673469388\n",
       "983. 0.495619047619048\n",
       "984. 0.539714285714286\n",
       "985. 0.666020408163265\n",
       "986. 0.527095238095238\n",
       "987. 0.317666666666667\n",
       "988. 0.469244897959184\n",
       "989. 0.472333333333333\n",
       "990. 0.00285714285714286\n",
       "991. 0.0795714285714286\n",
       "992. 0.0309523809523809\n",
       "993. 0.055\n",
       "994. 0.26547619047619\n",
       "995. 0.0849047619047619\n",
       "996. 0.883047619047619\n",
       "997. 0.429904761904762\n",
       "998. 0.0134285714285714\n",
       "999. 0.00171428571428571\n",
       "1000. 0.670238095238095\n",
       "1001. 0.0296190476190476\n",
       "1002. 0.243619047619048\n",
       "1003. 0.364111111111111\n",
       "1004. 0.365047619047619\n",
       "1005. 0.28308843537415\n",
       "1006. 0.859095238095238\n",
       "1007. 0.676142857142857\n",
       "1008. 0.0112380952380952\n",
       "1009. 0.0019047619047619\n",
       "1010. 0.0779047619047619\n",
       "1011. 0.418380952380952\n",
       "1012. 0.212809523809524\n",
       "1013. 0.0934761904761905\n",
       "1014. 0.588945578231293\n",
       "1015. 0.0851904761904762\n",
       "1016. 0.73147619047619\n",
       "1017. 0.288714285714286\n",
       "1018. 0.318132275132275\n",
       "1019. 0.19852380952381\n",
       "1020. 0.667904761904762\n",
       "1021. 0.184285714285714\n",
       "1022. 0.00571428571428571\n",
       "1023. 0.0257142857142857\n",
       "1024. 0\n",
       "1025. 0.00928571428571429\n",
       "1026. 0.178619047619048\n",
       "1027. 0.317285714285714\n",
       "1028. 0.389374149659864\n",
       "1029. 0.352095238095238\n",
       "1030. 0.00628571428571429\n",
       "1031. 0.122142857142857\n",
       "1032. 0.503598639455782\n",
       "1033. 0.864190476190476\n",
       "1034. 0.443904761904762\n",
       "1035. 0.461904761904762\n",
       "1036. 0.028\n",
       "1037. 0.0925714285714286\n",
       "1038. 0.507714285714286\n",
       "1039. 0.0493333333333333\n",
       "1040. 0.111571428571429\n",
       "1041. 0.873857142857143\n",
       "1042. 0.582\n",
       "1043. 0.864714285714286\n",
       "1044. 0.0378095238095238\n",
       "1045. 0.0157142857142857\n",
       "1046. 0.0524285714285714\n",
       "1047. 0.197809523809524\n",
       "1048. 0.136428571428571\n",
       "1049. 0.81552380952381\n",
       "1050. 0.494761904761905\n",
       "1051. 0.423047619047619\n",
       "1052. 0.0154285714285714\n",
       "1053. 0.407047619047619\n",
       "1054. 0.0135714285714286\n",
       "1055. 0.0905714285714286\n",
       "1056. 0.409142857142857\n",
       "1057. 0.0121904761904762\n",
       "1058. 0.0262857142857143\n",
       "1059. 0.282190476190476\n",
       "1060. 0.0290952380952381\n",
       "1061. 0\n",
       "1062. 0.0370952380952381\n",
       "1063. 0.399\n",
       "1064. 0.213414965986395\n",
       "1065. 0.331238095238095\n",
       "1066. 0.222904761904762\n",
       "1067. 0.295253968253968\n",
       "1068. 0.00514285714285714\n",
       "1069. 0.647380952380952\n",
       "1070. 0.806761904761905\n",
       "1071. 0.283714285714286\n",
       "1072. 0.628190476190476\n",
       "1073. 0.604666666666667\n",
       "1074. 0.221619047619048\n",
       "1075. 0.11952380952381\n",
       "1076. 0.0772380952380952\n",
       "1077. 0.329659863945578\n",
       "1078. 0.0132857142857143\n",
       "1079. 0.421136054421769\n",
       "1080. 0.668619047619047\n",
       "1081. 0.645\n",
       "1082. 0.0661904761904762\n",
       "1083. 0.0291428571428571\n",
       "1084. 0.603380952380952\n",
       "1085. 0.620714285714286\n",
       "1086. 0.01\n",
       "1087. 0.329761904761905\n",
       "1088. 0.336380952380952\n",
       "1089. 0.979428571428571\n",
       "1090. 0.281285714285714\n",
       "1091. 0.600380952380952\n",
       "1092. 0.772285714285714\n",
       "1093. 0.797142857142857\n",
       "1094. 0.067952380952381\n",
       "1095. 0.218904761904762\n",
       "1096. 0.200333333333333\n",
       "1097. 0.344238095238095\n",
       "1098. 0.614428571428572\n",
       "1099. 0.0587619047619048\n",
       "1100. 0.931428571428572\n",
       "1101. 0.205095238095238\n",
       "1102. 0.0135714285714286\n",
       "1103. 0.144714285714286\n",
       "1104. 0.575285714285714\n",
       "1105. 0.166857142857143\n",
       "1106. 0.209748299319728\n",
       "1107. 0.523037037037037\n",
       "1108. 0.117619047619048\n",
       "1109. 0.0724761904761905\n",
       "1110. 0.627206349206349\n",
       "1111. 0.0019047619047619\n",
       "1112. 0.173571428571429\n",
       "1113. 0.25047619047619\n",
       "1114. 0.301047619047619\n",
       "1115. 0.462952380952381\n",
       "1116. 0.540263605442177\n",
       "1117. 0.651238095238095\n",
       "1118. 0.0721428571428571\n",
       "1119. 0.677666666666667\n",
       "1120. 0.0378571428571429\n",
       "1121. 0.312285714285714\n",
       "1122. 0.209190476190476\n",
       "1123. 0.0603809523809524\n",
       "1124. 0.773380952380953\n",
       "1125. 0.475047619047619\n",
       "1126. 0.18268253968254\n",
       "1127. 0.0162857142857143\n",
       "1128. 0.0504285714285714\n",
       "1129. 0.0315714285714286\n",
       "1130. 0.0703809523809524\n",
       "1131. 0.911285714285714\n",
       "1132. 0.319904761904762\n",
       "1133. 0.14847619047619\n",
       "1134. 0.127047619047619\n",
       "1135. 0.00285714285714286\n",
       "1136. 0.164428571428571\n",
       "1137. 0.176571428571429\n",
       "1138. 0.0527619047619048\n",
       "1139. 0.442231292517007\n",
       "1140. 0.0968571428571428\n",
       "1141. 0.562095238095238\n",
       "1142. 0.00628571428571429\n",
       "1143. 0.718904761904762\n",
       "1144. 0.120333333333333\n",
       "1145. 0.00857142857142857\n",
       "1146. 0.674714285714286\n",
       "1147. 0.752619047619048\n",
       "1148. 0.21952380952381\n",
       "1149. 0.158142857142857\n",
       "1150. 0.0441904761904762\n",
       "1151. 0.0465238095238095\n",
       "1152. 0.519238095238095\n",
       "1153. 0.0471428571428571\n",
       "1154. 0.352142857142857\n",
       "1155. 0.0792857142857143\n",
       "1156. 0.0146666666666667\n",
       "1157. 0.112666666666667\n",
       "1158. 0.690549319727891\n",
       "1159. 0.101\n",
       "1160. 0.828047619047619\n",
       "1161. 0.797\n",
       "1162. 0.188047619047619\n",
       "1163. 0.717619047619048\n",
       "1164. 0.320904761904762\n",
       "1165. 0.663\n",
       "1166. 0.636148148148148\n",
       "1167. 0.105619047619048\n",
       "1168. 0.624714285714286\n",
       "1169. 0.280571428571429\n",
       "1170. 0.147238095238095\n",
       "1171. 0.189857142857143\n",
       "1172. 0.564380952380952\n",
       "1173. 0.406666666666667\n",
       "1174. 0.208190476190476\n",
       "1175. 0.457666666666667\n",
       "1176. 0.0147619047619048\n",
       "1177. 0.272904761904762\n",
       "1178. 0.291238095238095\n",
       "1179. 0.0905238095238095\n",
       "1180. 0.337238095238095\n",
       "1181. 0.084\n",
       "1182. 0.512392857142857\n",
       "1183. 0.00952380952380952\n",
       "1184. 0.622802721088435\n",
       "1185. 0.0122380952380952\n",
       "1186. 0.325047619047619\n",
       "1187. 0.136285714285714\n",
       "1188. 0.0472857142857143\n",
       "1189. 0.00476190476190476\n",
       "1190. 0.101619047619048\n",
       "1191. 0.313333333333333\n",
       "1192. 0.605380952380952\n",
       "1193. 0.241952380952381\n",
       "1194. 0.0332857142857143\n",
       "1195. 0.0122857142857143\n",
       "1196. 0.109428571428571\n",
       "1197. 0.450380952380952\n",
       "1198. 0.0738571428571429\n",
       "1199. 0.119095238095238\n",
       "1200. 0.272142857142857\n",
       "1201. 0.457333333333333\n",
       "1202. 0.0888503401360544\n",
       "1203. 0.302190476190476\n",
       "1204. 0.0646666666666667\n",
       "1205. 0.130380952380952\n",
       "1206. 0.026047619047619\n",
       "1207. 0.331\n",
       "1208. 0.0284285714285714\n",
       "1209. 0.0868095238095238\n",
       "1210. 0.0691904761904762\n",
       "1211. 0.045952380952381\n",
       "1212. 0.579285714285714\n",
       "1213. 0.0391428571428571\n",
       "1214. 0.0271904761904762\n",
       "1215. 0.943857142857143\n",
       "1216. 0.692204081632653\n",
       "1217. 0.779285714285714\n",
       "1218. 0.12147619047619\n",
       "1219. 0.131761904761905\n",
       "1220. 0.151571428571429\n",
       "1221. 0.506068027210884\n",
       "1222. 0.013047619047619\n",
       "1223. 0.0355238095238095\n",
       "1224. 0.52852380952381\n",
       "1225. 0.025952380952381\n",
       "1226. 0.0804285714285714\n",
       "1227. 0.561142857142857\n",
       "1228. 0.498190476190476\n",
       "1229. 0.255666666666667\n",
       "1230. 0.0215238095238095\n",
       "1231. 0.0504761904761905\n",
       "1232. 0.678190476190476\n",
       "1233. 0.681904761904762\n",
       "1234. 0.0394761904761905\n",
       "1235. 0.0945238095238095\n",
       "1236. 0.153857142857143\n",
       "1237. 0.224142857142857\n",
       "1238. 0.861761904761905\n",
       "1239. 0.698523809523809\n",
       "1240. 0.368142857142857\n",
       "1241. 0.226047619047619\n",
       "1242. 0\n",
       "1243. 0.0131428571428571\n",
       "1244. 0.698904761904762\n",
       "1245. 0.426666666666667\n",
       "1246. 0.0287619047619048\n",
       "1247. 0.581278911564626\n",
       "1248. 0.0291428571428571\n",
       "1249. 0.220047619047619\n",
       "1250. 0.286095238095238\n",
       "1251. 0.688142857142857\n",
       "1252. 0.952619047619047\n",
       "1253. 0.309619047619048\n",
       "1254. 0.265142857142857\n",
       "1255. 0.165\n",
       "1256. 0.908666666666667\n",
       "1257. 0.0225238095238095\n",
       "1258. 0.0347142857142857\n",
       "1259. 0.371761904761905\n",
       "1260. 0.0166666666666667\n",
       "1261. 0.288714285714286\n",
       "1262. 0.500761904761905\n",
       "1263. 0.0203333333333333\n",
       "1264. 0.0380952380952381\n",
       "1265. 0.488719576719577\n",
       "1266. 0.0426190476190476\n",
       "1267. 0.181285714285714\n",
       "1268. 0.445380952380952\n",
       "1269. 0.0730952380952381\n",
       "1270. 0.448380952380952\n",
       "1271. 0.0331904761904762\n",
       "1272. 0.0204285714285714\n",
       "1273. 0.490380952380952\n",
       "1274. 0.0957619047619048\n",
       "1275. 0.384142857142857\n",
       "1276. 0.801428571428571\n",
       "1277. 0.293904761904762\n",
       "1278. 0.493285714285714\n",
       "1279. 0.0222857142857143\n",
       "1280. 0.0555714285714286\n",
       "1281. 0.0192857142857143\n",
       "1282. 0.594952380952381\n",
       "1283. 0.013\n",
       "1284. 0.00571428571428571\n",
       "1285. 0.075\n",
       "1286. 0.0172857142857143\n",
       "1287. 0.494190476190476\n",
       "1288. 0.0483333333333333\n",
       "1289. 0.777809523809524\n",
       "1290. 0.118857142857143\n",
       "1291. 0.203571428571429\n",
       "1292. 0.854\n",
       "1293. 0.489761904761905\n",
       "1294. 0.67852380952381\n",
       "1295. 0.0278571428571429\n",
       "1296. 0.823714285714286\n",
       "1297. 0.768333333333333\n",
       "1298. 0.200809523809524\n",
       "1299. 0.012\n",
       "1300. 0.386224489795918\n",
       "1301. 0.565380952380952\n",
       "1302. 0.00285714285714286\n",
       "1303. 0.275095238095238\n",
       "1304. 0.39347619047619\n",
       "1305. 0.305952380952381\n",
       "1306. 0.743904761904762\n",
       "1307. 0.0519047619047619\n",
       "1308. 0.0356666666666667\n",
       "1309. 0.721142857142857\n",
       "1310. 0\n",
       "1311. 0.127904761904762\n",
       "1312. 0.108952380952381\n",
       "1313. 0.149380952380952\n",
       "1314. 0.00571428571428571\n",
       "1315. 0.386666666666667\n",
       "1316. 0.47212925170068\n",
       "1317. 0.00476190476190476\n",
       "1318. 0.166857142857143\n",
       "1319. 0.537952380952381\n",
       "1320. 0.672619047619048\n",
       "1321. 0.803714285714286\n",
       "1322. 0.192428571428571\n",
       "1323. 0.430428571428572\n",
       "1324. 0.294619047619048\n",
       "1325. 0.00571428571428571\n",
       "1326. 0.501619047619048\n",
       "1327. 0.170142857142857\n",
       "1328. 0.0108571428571429\n",
       "1329. 0.284333333333333\n",
       "1330. 0.07\n",
       "1331. 0.0257142857142857\n",
       "1332. 0.177904761904762\n",
       "1333. 0.608571428571429\n",
       "1334. 0.370380952380952\n",
       "1335. 0.171904761904762\n",
       "1336. 0.796095238095238\n",
       "1337. 0.413238095238095\n",
       "1338. 0.0391428571428571\n",
       "1339. 0.00457142857142857\n",
       "1340. 0.14952380952381\n",
       "1341. 0.131\n",
       "1342. 0.316619047619048\n",
       "1343. 0.005\n",
       "1344. 0.202333333333333\n",
       "1345. 0.50852380952381\n",
       "1346. 0.228714285714286\n",
       "1347. 0.00285714285714286\n",
       "1348. 0.370656084656085\n",
       "1349. 0.0304761904761905\n",
       "1350. 0.172285714285714\n",
       "1351. 0.204714285714286\n",
       "1352. 0.0191428571428571\n",
       "1353. 0.0638571428571429\n",
       "1354. 0.314952380952381\n",
       "1355. 0.0394285714285714\n",
       "1356. 0.770891156462585\n",
       "1357. 0.539095238095238\n",
       "1358. 0.419571428571429\n",
       "1359. 0.231714285714286\n",
       "1360. 0.380619047619048\n",
       "1361. 0\n",
       "1362. 0.576\n",
       "1363. 0.417047619047619\n",
       "1364. 0.396619047619048\n",
       "1365. 0.413476190476191\n",
       "1366. 0.281707482993197\n",
       "1367. 0.00285714285714286\n",
       "1368. 0.345666666666667\n",
       "1369. 0.229333333333333\n",
       "1370. 0.104238095238095\n",
       "1371. 0.306952380952381\n",
       "1372. 0.232142857142857\n",
       "1373. 0.669068027210884\n",
       "1374. 0.233761904761905\n",
       "1375. 0.0374761904761905\n",
       "1376. 0.0327142857142857\n",
       "1377. 0.243904761904762\n",
       "1378. 0.00285714285714286\n",
       "1379. 0.932190476190476\n",
       "1380. 0.235857142857143\n",
       "1381. 0.564190476190476\n",
       "1382. 0.319714285714286\n",
       "1383. 0.444571428571429\n",
       "1384. 0.518619047619048\n",
       "1385. 0.13847619047619\n",
       "1386. 0.356238095238095\n",
       "1387. 0.363634920634921\n",
       "1388. 0.342571428571429\n",
       "1389. 0.588571428571429\n",
       "1390. 0.388380952380952\n",
       "1391. 0.181\n",
       "1392. 0.0239047619047619\n",
       "1393. 0.0149047619047619\n",
       "1394. 0.0545238095238095\n",
       "1395. 0.179666666666667\n",
       "1396. 0.427571428571429\n",
       "1397. 0.0766666666666667\n",
       "1398. 0.381238095238095\n",
       "1399. 0.0219047619047619\n",
       "1400. 0.111285714285714\n",
       "1401. 0.0412857142857143\n",
       "1402. 0.174809523809524\n",
       "1403. 0.574238095238095\n",
       "1404. 0.0255238095238095\n",
       "1405. 0.545285714285714\n",
       "1406. 0.912380952380953\n",
       "1407. 0.0385714285714286\n",
       "1408. 0.467285714285714\n",
       "1409. 0.16412925170068\n",
       "1410. 0.054\n",
       "1411. 0.158904761904762\n",
       "1412. 0.694857142857143\n",
       "1413. 0.00142857142857143\n",
       "1414. 0.781952380952381\n",
       "1415. 0.26347619047619\n",
       "1416. 0.030952380952381\n",
       "1417. 0.601761904761905\n",
       "1418. 0.748666666666667\n",
       "1419. 0.0636190476190476\n",
       "1420. 0.00357142857142857\n",
       "1421. 0.416696900982615\n",
       "1422. 0.308619047619048\n",
       "1423. 0.709952380952381\n",
       "1424. 0.36352380952381\n",
       "1425. 0.367285714285714\n",
       "1426. 0.496\n",
       "1427. 0.557619047619048\n",
       "1428. 0.381809523809524\n",
       "1429. 0.0135714285714286\n",
       "1430. 0.715809523809524\n",
       "1431. 0.0130952380952381\n",
       "1432. 0.460380952380952\n",
       "1433. 0.0426666666666667\n",
       "1434. 0.733952380952381\n",
       "1435. 0.0657142857142857\n",
       "1436. 0.197666666666667\n",
       "1437. 0.170952380952381\n",
       "1438. 0.00380952380952381\n",
       "1439. 0.220224489795918\n",
       "1440. 0.428142857142857\n",
       "1441. 0.0698095238095238\n",
       "1442. 0.0149047619047619\n",
       "1443. 0.731047619047619\n",
       "1444. 0.147619047619048\n",
       "1445. 0.245047619047619\n",
       "1446. 0.610761904761905\n",
       "1447. 0.0408095238095238\n",
       "1448. 0.343292517006803\n",
       "1449. 0.546238095238095\n",
       "1450. 0.0597619047619048\n",
       "1451. 0.0470952380952381\n",
       "1452. 0.300333333333333\n",
       "1453. 0.364619047619048\n",
       "1454. 0.240095238095238\n",
       "1455. 0.638904761904762\n",
       "1456. 0.375666666666667\n",
       "1457. 0.256047619047619\n",
       "1458. 0.00457142857142857\n",
       "1459. 0.400333333333333\n",
       "1460. 0.374904761904762\n",
       "1461. 0.00666666666666667\n",
       "1462. 0.609809523809524\n",
       "1463. 0.405952380952381\n",
       "1464. 0.772190476190476\n",
       "1465. 0.167666666666667\n",
       "1466. 0.870047619047619\n",
       "1467. 0.0993333333333334\n",
       "1468. 0.00714285714285714\n",
       "1469. 0.00342857142857143\n",
       "1470. 0.809904761904762\n",
       "1471. 0.468904761904762\n",
       "1472. 0\n",
       "1473. 0.0380952380952381\n",
       "1474. 0.00914285714285714\n",
       "1475. 0.0375238095238095\n",
       "1476. 0.171238095238095\n",
       "1477. 0.452333333333333\n",
       "1478. 0.294571428571429\n",
       "1479. 0.112857142857143\n",
       "1480. 0.905142857142857\n",
       "1481. 0.0421428571428571\n",
       "1482. 0.216047619047619\n",
       "1483. 0.929809523809524\n",
       "1484. 0.00171428571428571\n",
       "1485. 0.325047619047619\n",
       "1486. 0.491\n",
       "1487. 0.284401360544218\n",
       "1488. 0.281333333333333\n",
       "1489. 0.27665306122449\n",
       "1490. 0.433523809523809\n",
       "1491. 0.718095238095238\n",
       "1492. 0.726761904761905\n",
       "1493. 0.452275132275132\n",
       "1494. 0.31047619047619\n",
       "1495. 0.18447619047619\n",
       "1496. 0.129571428571429\n",
       "1497. 0.0019047619047619\n",
       "1498. 0.00838095238095238\n",
       "1499. 0.0488571428571429\n",
       "1500. 0.701571428571429\n",
       "1501. 0.262285714285714\n",
       "1502. 0.24247619047619\n",
       "1503. 0.0877142857142857\n",
       "1504. 0.478142857142857\n",
       "1505. 0.0736190476190476\n",
       "1506. 0.0558095238095238\n",
       "1507. 0.752333333333333\n",
       "1508. 0.727666666666667\n",
       "1509. 0.721\n",
       "1510. 0.473904761904762\n",
       "1511. 0.371095238095238\n",
       "1512. 0.000571428571428571\n",
       "1513. 0.174285714285714\n",
       "1514. 0.11847619047619\n",
       "1515. 0.86052380952381\n",
       "1516. 0\n",
       "1517. 0.0988095238095238\n",
       "1518. 0.332031746031746\n",
       "1519. 0.526476190476191\n",
       "1520. 0.0235714285714286\n",
       "1521. 0.00357142857142857\n",
       "1522. 0.271761904761905\n",
       "1523. 0.407619047619048\n",
       "1524. 0.829523809523809\n",
       "1525. 0.408428571428572\n",
       "1526. 0.0323809523809524\n",
       "1527. 0.270265306122449\n",
       "1528. 0.366\n",
       "1529. 0.907571428571428\n",
       "1530. 0.0439047619047619\n",
       "1531. 0.311428571428571\n",
       "1532. 0.709904761904762\n",
       "1533. 0.359190476190476\n",
       "1534. 0.169904761904762\n",
       "1535. 0.762523809523809\n",
       "1536. 0.0147619047619048\n",
       "1537. 0.188095238095238\n",
       "1538. 0.852333333333333\n",
       "1539. 0.132190476190476\n",
       "1540. 0.0175714285714286\n",
       "1541. 0.415639455782313\n",
       "1542. 0.155142857142857\n",
       "1543. 0.0414761904761905\n",
       "1544. 0.624904761904762\n",
       "1545. 0.162761904761905\n",
       "1546. 0.268619047619048\n",
       "1547. 0.0107142857142857\n",
       "1548. 0.299761904761905\n",
       "1549. 0.0019047619047619\n",
       "1550. 0.0381904761904762\n",
       "1551. 0.0530952380952381\n",
       "1552. 0.153428571428571\n",
       "1553. 0.037952380952381\n",
       "1554. 0.0318571428571429\n",
       "1555. 0.0722380952380952\n",
       "1556. 0.0796666666666667\n",
       "1557. 0.0401904761904762\n",
       "1558. 0.0103809523809524\n",
       "1559. 0.0798095238095238\n",
       "1560. 0.0388095238095238\n",
       "1561. 0.0101904761904762\n",
       "1562. 0.000571428571428571\n",
       "1563. 0.367428571428571\n",
       "1564. 0.926095238095238\n",
       "1565. 0.288571428571429\n",
       "1566. 0.0554761904761905\n",
       "1567. 0.0161428571428571\n",
       "1568. 0.504952380952381\n",
       "1569. 0.286047619047619\n",
       "1570. 0.291761904761905\n",
       "1571. 0.615333333333333\n",
       "1572. 0.725309523809524\n",
       "1573. 0.0867142857142857\n",
       "1574. 0.414142857142857\n",
       "1575. 0.0497142857142857\n",
       "1576. 0.0105714285714286\n",
       "1577. 0.687428571428571\n",
       "1578. 0.27352380952381\n",
       "1579. 0.468523809523809\n",
       "1580. 0.238142857142857\n",
       "1581. 0.230734693877551\n",
       "1582. 0.210333333333333\n",
       "1583. 0.0282857142857143\n",
       "1584. 0.495857142857143\n",
       "1585. 0.0521428571428571\n",
       "1586. 0.0628095238095238\n",
       "1587. 0.254444444444444\n",
       "1588. 0.42256462585034\n",
       "1589. 0.523333333333333\n",
       "1590. 0.167857142857143\n",
       "1591. 0.795285714285714\n",
       "1592. 0.402619047619048\n",
       "1593. 0.166619047619048\n",
       "1594. 0.182619047619048\n",
       "1595. 0.072952380952381\n",
       "1596. 0.399142857142857\n",
       "1597. 0.11347619047619\n",
       "1598. 0.00657142857142857\n",
       "1599. 0.444571428571429\n",
       "1600. 0.340380952380952\n",
       "1601. 0.220904761904762\n",
       "1602. 0.00285714285714286\n",
       "1603. 0.0562380952380952\n",
       "1604. 0.502857142857143\n",
       "1605. 0.189714285714286\n",
       "1606. 0.803333333333334\n",
       "1607. 0.0363333333333333\n",
       "1608. 0.66452380952381\n",
       "1609. 0.0931428571428572\n",
       "1610. 0.909068027210884\n",
       "1611. 0.008\n",
       "1612. 0.0395714285714286\n",
       "1613. 0.336571428571428\n",
       "1614. 0.831857142857143\n",
       "1615. 0.842238095238095\n",
       "1616. 0.00285714285714286\n",
       "1617. 0.028047619047619\n",
       "1618. 0.007\n",
       "1619. 0.228714285714286\n",
       "1620. 0.411714285714286\n",
       "1621. 0.00142857142857143\n",
       "1622. 0.968666666666667\n",
       "1623. 0.741380952380952\n",
       "1624. 0.757904761904762\n",
       "1625. 0.410285714285714\n",
       "1626. 0.673095238095238\n",
       "1627. 0.15552380952381\n",
       "1628. 0.710047619047619\n",
       "1629. 0.296571428571429\n",
       "1630. 0.0721428571428571\n",
       "1631. 0.217666666666667\n",
       "1632. 0.470333333333333\n",
       "1633. 0.308571428571429\n",
       "1634. 0.195142857142857\n",
       "1635. 0.43856462585034\n",
       "1636. 0.405142857142857\n",
       "1637. 0.00285714285714286\n",
       "1638. 0.0936666666666667\n",
       "1639. 0.00285714285714286\n",
       "1640. 0.531476190476191\n",
       "1641. 0.573061224489796\n",
       "1642. 0.132809523809524\n",
       "1643. 0.356714285714286\n",
       "1644. 0.0667142857142857\n",
       "1645. 0.199333333333333\n",
       "1646. 0.351666666666667\n",
       "1647. 0.0710476190476191\n",
       "1648. 0.371993197278912\n",
       "1649. 0.0696190476190476\n",
       "1650. 0.00733333333333333\n",
       "1651. 0.632374149659864\n",
       "1652. 0.0752857142857143\n",
       "1653. 0.0213809523809524\n",
       "1654. 0.0893333333333333\n",
       "1655. 0.485816326530612\n",
       "1656. 0.208687074829932\n",
       "1657. 0.0103809523809524\n",
       "1658. 0.318619047619048\n",
       "1659. 0.661857142857143\n",
       "1660. 0.016\n",
       "1661. 0.472857142857143\n",
       "1662. 0.127190476190476\n",
       "1663. 0.0238095238095238\n",
       "1664. 0.321190476190476\n",
       "1665. 0.252190476190476\n",
       "1666. 0.153666666666667\n",
       "1667. 0.0108571428571429\n",
       "1668. 0.390142857142857\n",
       "1669. 0.0610952380952381\n",
       "1670. 0.293666666666667\n",
       "1671. 0.24047619047619\n",
       "1672. 0.00857142857142857\n",
       "1673. 0.0652380952380952\n",
       "1674. 0.328489795918367\n",
       "1675. 0.386571428571429\n",
       "1676. 0.913386243386244\n",
       "1677. 0.0273333333333333\n",
       "1678. 0.200714285714286\n",
       "1679. 0.00857142857142857\n",
       "1680. 0.522476190476191\n",
       "1681. 0.193904761904762\n",
       "1682. 0.331333333333333\n",
       "1683. 0.0877619047619048\n",
       "1684. 0.489619047619048\n",
       "1685. 0.268904761904762\n",
       "1686. 0.0321904761904762\n",
       "1687. 0.0424761904761905\n",
       "1688. 0.237190476190476\n",
       "1689. 0.162142857142857\n",
       "1690. 0.0171428571428571\n",
       "1691. 0.044952380952381\n",
       "1692. 0.427095238095238\n",
       "1693. 0.549595238095238\n",
       "1694. 0.165\n",
       "1695. 0.0114761904761905\n",
       "1696. 0\n",
       "1697. 0.481619047619048\n",
       "1698. 0.0963809523809524\n",
       "1699. 0.366380952380952\n",
       "1700. 0.184380952380952\n",
       "1701. 0.532285714285714\n",
       "1702. 0.132714285714286\n",
       "1703. 0.252020408163265\n",
       "1704. 0.110380952380952\n",
       "1705. 0.590836734693878\n",
       "1706. 0.102333333333333\n",
       "1707. 0.0573809523809524\n",
       "1708. 0.695047619047619\n",
       "1709. 0.843857142857143\n",
       "1710. 0.140761904761905\n",
       "1711. 0.426666666666667\n",
       "1712. 0.153238095238095\n",
       "1713. 0.0205714285714286\n",
       "1714. 0.022952380952381\n",
       "1715. 0.288238095238095\n",
       "1716. 0.20947619047619\n",
       "1717. 0.468993197278912\n",
       "1718. 0.395\n",
       "1719. 0.502571428571429\n",
       "1720. 0.010952380952381\n",
       "1721. 0.233857142857143\n",
       "1722. 0.722428571428572\n",
       "1723. 0.906238095238095\n",
       "1724. 0.0976190476190476\n",
       "1725. 0.0846666666666667\n",
       "1726. 0.00171428571428571\n",
       "1727. 0.459857142857143\n",
       "1728. 0.0380952380952381\n",
       "1729. 0.0248571428571429\n",
       "1730. 0.719857142857143\n",
       "1731. 0.172285714285714\n",
       "1732. 0.270904761904762\n",
       "1733. 0.410952380952381\n",
       "1734. 0.867285714285714\n",
       "1735. 0.0474285714285714\n",
       "1736. 0.0428571428571429\n",
       "1737. 0.25247619047619\n",
       "1738. 0.00876190476190476\n",
       "1739. 0.436666666666667\n",
       "1740. 0.627095238095238\n",
       "1741. 0.802\n",
       "1742. 0.397380952380952\n",
       "1743. 0.1705\n",
       "1744. 0.00957142857142857\n",
       "1745. 0.0893095238095238\n",
       "1746. 0.0101904761904762\n",
       "1747. 0.559380952380952\n",
       "1748. 0.442285714285714\n",
       "1749. 0.0645714285714286\n",
       "1750. 0.434380952380952\n",
       "1751. 0.0474285714285714\n",
       "1752. 0.75452380952381\n",
       "1753. 0.282190476190476\n",
       "1754. 0.883666666666667\n",
       "1755. 0.00857142857142857\n",
       "1756. 0.694142857142857\n",
       "1757. 0.425952380952381\n",
       "1758. 0.202571428571429\n",
       "1759. 0.0233809523809524\n",
       "1760. 0.282666666666667\n",
       "1761. 0.105761904761905\n",
       "1762. 0.0820952380952381\n",
       "1763. 0.686809523809524\n",
       "1764. 0.171714285714286\n",
       "1765. 0\n",
       "1766. 0.214571428571429\n",
       "1767. 0.280714285714286\n",
       "1768. 0.0681428571428571\n",
       "1769. 0.032952380952381\n",
       "1770. 0.696469387755102\n",
       "1771. 0.270857142857143\n",
       "1772. 0.203285714285714\n",
       "1773. 0.0121428571428571\n",
       "1774. 0.350666666666667\n",
       "1775. 0.187619047619048\n",
       "1776. 0.568761904761905\n",
       "1777. 0.396\n",
       "1778. 0.0926190476190476\n",
       "1779. 0.361013605442177\n",
       "1780. 0.314190476190476\n",
       "1781. 0.645338624338624\n",
       "1782. 0.0178571428571429\n",
       "1783. 0.173285714285714\n",
       "1784. 0.107380952380952\n",
       "1785. 0.00666666666666667\n",
       "1786. 0.0511428571428571\n",
       "1787. 0.122095238095238\n",
       "1788. 0.193714285714286\n",
       "1789. 0.684714285714286\n",
       "1790. 0.0462380952380952\n",
       "1791. 0.0932380952380952\n",
       "1792. 0.0330952380952381\n",
       "1793. 0.815190476190476\n",
       "1794. 0.0311428571428571\n",
       "1795. 0.66152380952381\n",
       "1796. 0.0555238095238095\n",
       "1797. 0.0527142857142857\n",
       "1798. 0.14447619047619\n",
       "1799. 0.805857142857143\n",
       "1800. 0.508333333333333\n",
       "1801. 0.511333333333333\n",
       "1802. 0.851761904761905\n",
       "1803. 0.00142857142857143\n",
       "1804. 0.398666666666667\n",
       "1805. 0.59852380952381\n",
       "1806. 0.127428571428571\n",
       "1807. 0.188571428571429\n",
       "1808. 0.786428571428571\n",
       "1809. 0.00476190476190476\n",
       "1810. 0.204380952380952\n",
       "1811. 0.451714285714286\n",
       "1812. 0.04\n",
       "1813. 0.317312925170068\n",
       "1814. 0.0978571428571429\n",
       "1815. 0.395190476190476\n",
       "1816. 0.640063492063492\n",
       "1817. 0.0128571428571429\n",
       "1818. 0.706190476190476\n",
       "1819. 0.0417142857142857\n",
       "1820. 0.713380952380952\n",
       "1821. 0.263238095238095\n",
       "1822. 0.480571428571429\n",
       "1823. 0.663619047619048\n",
       "1824. 0.00723809523809524\n",
       "1825. 0.0201428571428571\n",
       "1826. 0.0141428571428571\n",
       "1827. 0.828809523809524\n",
       "1828. 0.802809523809524\n",
       "1829. 0.0204761904761905\n",
       "1830. 0\n",
       "1831. 0.016\n",
       "1832. 0.0596666666666667\n",
       "1833. 0.0272380952380952\n",
       "1834. 0.0446666666666667\n",
       "1835. 0.0371428571428571\n",
       "1836. 0.731095238095238\n",
       "1837. 0.156333333333333\n",
       "1838. 0.00571428571428571\n",
       "1839. 0.777142857142857\n",
       "1840. 0.767142857142857\n",
       "1841. 0.218666666666667\n",
       "1842. 0.0467619047619048\n",
       "1843. 0.753857142857143\n",
       "1844. 0.0226190476190476\n",
       "1845. 0.227047619047619\n",
       "1846. 0.0409523809523809\n",
       "1847. 0.180333333333333\n",
       "1848. 0.00142857142857143\n",
       "1849. 0.338238095238095\n",
       "1850. 0.527666666666667\n",
       "1851. 0.23452380952381\n",
       "1852. 0.310761904761905\n",
       "1853. 0.006\n",
       "1854. 0.548619047619048\n",
       "1855. 0.152619047619048\n",
       "1856. 0\n",
       "1857. 0.606428571428571\n",
       "1858. 0.0563809523809524\n",
       "1859. 0.719809523809524\n",
       "1860. 0.605476190476191\n",
       "1861. 0.321\n",
       "1862. 0.165761904761905\n",
       "1863. 0.775428571428572\n",
       "1864. 0.143857142857143\n",
       "1865. 0.0890952380952381\n",
       "1866. 0.307190476190476\n",
       "1867. 0.0317619047619048\n",
       "1868. 0.379619047619048\n",
       "1869. 0.713666666666667\n",
       "1870. 0.196857142857143\n",
       "1871. 0.032\n",
       "1872. 0.435059523809524\n",
       "1873. 0.0265714285714286\n",
       "1874. 0.336428571428571\n",
       "1875. 0.00733333333333333\n",
       "1876. 0.403333333333333\n",
       "1877. 0.55030612244898\n",
       "1878. 0.525142857142857\n",
       "1879. 0.0394285714285714\n",
       "1880. 0.0137142857142857\n",
       "1881. 0.0454761904761905\n",
       "1882. 0.938095238095238\n",
       "1883. 0.0355714285714286\n",
       "1884. 0.427857142857143\n",
       "1885. 0.11752380952381\n",
       "1886. 0.0185714285714286\n",
       "1887. 0.173761904761905\n",
       "1888. 0\n",
       "1889. 0.364047619047619\n",
       "1890. 0.0668095238095238\n",
       "1891. 0.683571428571429\n",
       "1892. 0.456333333333333\n",
       "1893. 0.60052380952381\n",
       "1894. 0.0135714285714286\n",
       "1895. 0.00171428571428571\n",
       "1896. 0.351392857142857\n",
       "1897. 0.0131428571428571\n",
       "1898. 0.68825\n",
       "1899. 0.006\n",
       "1900. 0.924333333333333\n",
       "1901. 0.787333333333333\n",
       "1902. 0.735238095238095\n",
       "1903. 0.0461428571428571\n",
       "1904. 0.24\n",
       "1905. 0.326571428571429\n",
       "1906. 0.144619047619048\n",
       "1907. 0.619142857142857\n",
       "1908. 0.00785714285714286\n",
       "1909. 0\n",
       "1910. 0.927047619047619\n",
       "1911. 0.0876190476190476\n",
       "1912. 0.768904761904762\n",
       "1913. 0.531714285714286\n",
       "1914. 0.0140952380952381\n",
       "1915. 0.526714285714286\n",
       "1916. 0.0852857142857143\n",
       "1917. 0.361047619047619\n",
       "1918. 0.414476190476191\n",
       "1919. 0.301751322751323\n",
       "1920. 0.155\n",
       "1921. 0.338517006802721\n",
       "1922. 0.400142857142857\n",
       "1923. 0.0252857142857143\n",
       "1924. 0.0468095238095238\n",
       "1925. 0.604428571428572\n",
       "1926. 0.506714285714286\n",
       "1927. 0.511047619047619\n",
       "1928. 0.185380952380952\n",
       "1929. 0.161047619047619\n",
       "1930. 0.563619047619048\n",
       "1931. 0.362227513227513\n",
       "1932. 0.795761904761905\n",
       "1933. 0.39912925170068\n",
       "1934. 0.244952380952381\n",
       "1935. 0.424634920634921\n",
       "1936. 0.139904761904762\n",
       "1937. 0.0142857142857143\n",
       "1938. 0.768904761904762\n",
       "1939. 0.0069047619047619\n",
       "1940. 0.00942857142857143\n",
       "1941. 0.504190476190476\n",
       "1942. 0.596952380952381\n",
       "1943. 0.00404761904761905\n",
       "1944. 0.0146190476190476\n",
       "1945. 0.229428571428572\n",
       "1946. 0.0504285714285714\n",
       "1947. 0.928809523809524\n",
       "1948. 0.106238095238095\n",
       "1949. 0.433619047619048\n",
       "1950. 0.010952380952381\n",
       "1951. 0.214285714285714\n",
       "1952. 0.159809523809524\n",
       "1953. 0.916492063492063\n",
       "1954. 0.740639455782313\n",
       "1955. 0.679904761904762\n",
       "1956. 0.0261904761904762\n",
       "1957. 0.450333333333333\n",
       "1958. 0.0745714285714286\n",
       "1959. 0.162428571428571\n",
       "1960. 0.356333333333333\n",
       "1961. 0.25752380952381\n",
       "1962. 0.728904761904762\n",
       "1963. 0.361253968253968\n",
       "1964. 0.0189047619047619\n",
       "1965. 0.162761904761905\n",
       "1966. 0.0874761904761905\n",
       "1967. 0.204190476190476\n",
       "1968. 0.0388571428571429\n",
       "1969. 0.716857142857143\n",
       "1970. 0.922571428571428\n",
       "1971. 0.439380952380952\n",
       "1972. 0.461476190476191\n",
       "1973. 0.264301587301587\n",
       "1974. 0.120619047619048\n",
       "1975. 0.342945578231293\n",
       "1976. 0.0617142857142857\n",
       "1977. 0.288503401360544\n",
       "1978. 0.278761904761905\n",
       "1979. 0.262836734693877\n",
       "1980. 0.243904761904762\n",
       "1981. 0.08\n",
       "1982. 0.83352380952381\n",
       "1983. 0.217666666666667\n",
       "1984. 0.0510476190476191\n",
       "1985. 0.0807142857142857\n",
       "1986. 0.864714285714286\n",
       "1987. 0.296285714285714\n",
       "1988. 0.293809523809524\n",
       "1989. 0.168190476190476\n",
       "1990. 0.32002380952381\n",
       "1991. 0.0669047619047619\n",
       "1992. 0.537571428571429\n",
       "1993. 0.891285714285714\n",
       "1994. 0.489666666666667\n",
       "1995. 0.555095238095238\n",
       "1996. 0.0395238095238095\n",
       "1997. 0.618761904761905\n",
       "1998. 0.0948095238095238\n",
       "1999. 0.0710476190476191\n",
       "2000. 0.610619047619048\n",
       "2001. 0.149714285714286\n",
       "2002. 0.299666666666667\n",
       "2003. 0.004\n",
       "2004. 0.122428571428571\n",
       "2005. 0.640809523809524\n",
       "2006. 0.217333333333333\n",
       "2007. 0.164095238095238\n",
       "2008. 0.229380952380952\n",
       "2009. 0.164571428571429\n",
       "2010. 0.034\n",
       "2011. 0.342952380952381\n",
       "2012. 0.265571428571429\n",
       "2013. 0.269095238095238\n",
       "2014. 0.461761904761905\n",
       "2015. 0.181095238095238\n",
       "2016. 0.334857142857143\n",
       "2017. 0.0060952380952381\n",
       "2018. 0.756904761904762\n",
       "2019. 0.216047619047619\n",
       "2020. 0.310571428571429\n",
       "2021. 0.100285714285714\n",
       "2022. 0.476809523809524\n",
       "2023. 0.259761904761905\n",
       "2024. 0.613619047619048\n",
       "2025. 0.195238095238095\n",
       "2026. 0.251666666666667\n",
       "2027. 0.25752380952381\n",
       "2028. 0.668380952380952\n",
       "2029. 0.509333333333333\n",
       "2030. 0.335349206349206\n",
       "2031. 0.0157142857142857\n",
       "2032. 0.0666190476190476\n",
       "2033. 0.755714285714286\n",
       "2034. 0.167619047619048\n",
       "2035. 0\n",
       "2036. 0.00528571428571429\n",
       "2037. 0.262285714285714\n",
       "2038. 0.0264285714285714\n",
       "2039. 0.487047619047619\n",
       "2040. 0.510238095238095\n",
       "2041. 0.00952380952380952\n",
       "2042. 0.0213809523809524\n",
       "2043. 0.00285714285714286\n",
       "2044. 0.0383333333333333\n",
       "2045. 0.0834761904761905\n",
       "2046. 0.115571428571429\n",
       "2047. 0.238428571428571\n",
       "2048. 0.0884761904761905\n",
       "2049. 0.447047619047619\n",
       "2050. 0.0718095238095238\n",
       "2051. 0.0383333333333333\n",
       "2052. 0.329166666666667\n",
       "2053. 0.0306190476190476\n",
       "2054. 0.18452380952381\n",
       "2055. 0.737761904761905\n",
       "2056. 0.310761904761905\n",
       "2057. 0.782571428571429\n",
       "2058. 0.218571428571429\n",
       "2059. 0.0222857142857143\n",
       "2060. 0.0597142857142857\n",
       "2061. 0.164571428571429\n",
       "2062. 0.0807142857142857\n",
       "2063. 0.0187142857142857\n",
       "2064. 0.319714285714286\n",
       "2065. 0.0135714285714286\n",
       "2066. 0.771619047619047\n",
       "2067. 0.003\n",
       "2068. 0.0992380952380952\n",
       "2069. 0.29352380952381\n",
       "2070. 0.0524761904761905\n",
       "2071. 0.255095238095238\n",
       "2072. 0.0107142857142857\n",
       "2073. 0.214904761904762\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   [1] 0.0440000000 0.1919047619 0.3808095238 0.0375714286 0.6267619048\n",
       "   [6] 0.1811428571 0.4023333333 0.4962380952 0.0908571429 0.0000000000\n",
       "  [11] 0.2460476190 0.3773265306 0.0195238095 0.9128095238 0.1630476190\n",
       "  [16] 0.0121428571 0.0132380952 0.1483333333 0.1487142857 0.0095238095\n",
       "  [21] 0.0748571429 0.3770000000 0.9418095238 0.0047619048 0.5930952381\n",
       "  [26] 0.0771904762 0.4872380952 0.4619523810 0.4333333333 0.2634761905\n",
       "  [31] 0.4350000000 0.7113333333 0.0171428571 0.4222857143 0.6631904762\n",
       "  [36] 0.2353333333 0.6973809524 0.7086190476 0.9206190476 0.0225714286\n",
       "  [41] 0.1767619048 0.1379523810 0.1327619048 0.8132380952 0.2217142857\n",
       "  [46] 0.8547142857 0.1919047619 0.0438095238 0.2916560847 0.1374285714\n",
       "  [51] 0.0139047619 0.0128571429 0.1447142857 0.4104217687 0.8503809524\n",
       "  [56] 0.2875238095 0.7568095238 0.5674761905 0.0940000000 0.5143809524\n",
       "  [61] 0.8696190476 0.0057142857 0.0714285714 0.1812380952 0.0534761905\n",
       "  [66] 0.0583809524 0.0005714286 0.0171428571 0.6259523810 0.1798095238\n",
       "  [71] 0.5746666667 0.0245238095 0.0406666667 0.6902023810 0.0747142857\n",
       "  [76] 0.2491904762 0.7090000000 0.0418571429 0.4850476190 0.0794285714\n",
       "  [81] 0.9331904762 0.0800000000 0.0020952381 0.5280952381 0.2353809524\n",
       "  [86] 0.1626666667 0.0009523810 0.0670952381 0.8447142857 0.2575714286\n",
       "  [91] 0.0162857143 0.0182857143 0.0859523810 0.7130476190 0.1027619048\n",
       "  [96] 0.4099523810 0.1568095238 0.0750000000 0.3473227513 0.4645238095\n",
       " [101] 0.5679523810 0.0329523810 0.5981904762 0.3616190476 0.3432380952\n",
       " [106] 0.3757142857 0.0126190476 0.2528571429 0.1035238095 0.0261428571\n",
       " [111] 0.0245238095 0.8756190476 0.1510000000 0.7658571429 0.2479047619\n",
       " [116] 0.0089523810 0.0138095238 0.3477142857 0.0343809524 0.0234285714\n",
       " [121] 0.0102857143 0.0591428571 0.0090000000 0.1200000000 0.1736666667\n",
       " [126] 0.0331904762 0.0326190476 0.2864761905 0.6592857143 0.0126190476\n",
       " [131] 0.5850000000 0.3825714286 0.3121428571 0.4243809524 0.4145238095\n",
       " [136] 0.5881904762 0.0721428571 0.7064761905 0.1115238095 0.7342380952\n",
       " [141] 0.3849047619 0.2206190476 0.2889523810 0.3242380952 0.3239047619\n",
       " [146] 0.1874761905 0.7195238095 0.1385714286 0.2696666667 0.0961904762\n",
       " [151] 0.0370000000 0.3897619048 0.2426666667 0.1495238095 0.0057142857\n",
       " [156] 0.0926666667 0.0282380952 0.5623809524 0.3712380952 0.9160476190\n",
       " [161] 0.1725714286 0.1978571429 0.0318571429 0.0300476190 0.0128571429\n",
       " [166] 0.6241428571 0.0662380952 0.4937142857 0.0181428571 0.1188095238\n",
       " [171] 0.7356190476 0.3248571429 0.3491428571 0.0395238095 0.4127142857\n",
       " [176] 0.0106666667 0.2198095238 0.3000952381 0.5715714286 0.8908571429\n",
       " [181] 0.7695238095 0.7082857143 0.4272380952 0.0128571429 0.8619523810\n",
       " [186] 0.0239523810 0.0028571429 0.2410000000 0.1974285714 0.2026190476\n",
       " [191] 0.0000000000 0.4473333333 0.6302857143 0.1185714286 0.0066666667\n",
       " [196] 0.0000000000 0.0477142857 0.0114285714 0.0310000000 0.7235238095\n",
       " [201] 0.3740476190 0.1576190476 0.2060952381 0.5191904762 0.2613333333\n",
       " [206] 0.6264013605 0.0287142857 0.9588571429 0.0915238095 0.7156666667\n",
       " [211] 0.0171428571 0.0000000000 0.0732857143 0.5182380952 0.1423333333\n",
       " [216] 0.7456190476 0.1544761905 0.0339047619 0.0091428571 0.0650952381\n",
       " [221] 0.0540952381 0.0091428571 0.3487142857 0.6580000000 0.0193809524\n",
       " [226] 0.1841428571 0.1122857143 0.8098571429 0.2129047619 0.0083809524\n",
       " [231] 0.8725714286 0.0310000000 0.3779047619 0.0561428571 0.0050000000\n",
       " [236] 0.0305238095 0.3211904762 0.1199047619 0.6004540816 0.6998095238\n",
       " [241] 0.0618571429 0.0095238095 0.0105714286 0.0430952381 0.0620340136\n",
       " [246] 0.0388571429 0.5071428571 0.6237142857 0.5100000000 0.0019047619\n",
       " [251] 0.0462380952 0.7914761905 0.7790952381 0.4380952381 0.0565238095\n",
       " [256] 0.5482380952 0.8283333333 0.1945714286 0.2058571429 0.1523333333\n",
       " [261] 0.5157142857 0.2415714286 0.0705238095 0.2841156463 0.6538571429\n",
       " [266] 0.3374761905 0.0201904762 0.1586190476 0.1109047619 0.0979523810\n",
       " [271] 0.3214285714 0.2122857143 0.2645714286 0.1706084656 0.0511904762\n",
       " [276] 0.2662380952 0.6474285714 0.0587142857 0.3011904762 0.1052380952\n",
       " [281] 0.3717074830 0.0566190476 0.2461904762 0.1199047619 0.6780000000\n",
       " [286] 0.3487619048 0.0734285714 0.3851666667 0.6414285714 0.5629523810\n",
       " [291] 0.3852857143 0.0872857143 0.3265238095 0.5540000000 0.2853809524\n",
       " [296] 0.8856666667 0.5486428571 0.2090000000 0.0412380952 0.0812380952\n",
       " [301] 0.8562857143 0.0225714286 0.0439047619 0.0000000000 0.0490476190\n",
       " [306] 0.0372857143 0.0788571429 0.1648095238 0.0354761905 0.0064285714\n",
       " [311] 0.0958095238 0.5145873016 0.6991547619 0.0326190476 0.0218095238\n",
       " [316] 0.7108571429 0.2441428571 0.1217142857 0.5106243386 0.5389523810\n",
       " [321] 0.0133333333 0.1911428571 0.0172380952 0.3314656085 0.2150952381\n",
       " [326] 0.5439523810 0.6310000000 0.5492380952 0.0415238095 0.3192857143\n",
       " [331] 0.4728095238 0.2364761905 0.5292380952 0.6479047619 0.8457671958\n",
       " [336] 0.2441428571 0.4964761905 0.7279523810 0.0120000000 0.1478571429\n",
       " [341] 0.1154285714 0.1095714286 0.0540476190 0.2754285714 0.3553333333\n",
       " [346] 0.0136190476 0.1478571429 0.1960000000 0.0373809524 0.0676190476\n",
       " [351] 0.0344761905 0.0174285714 0.1610952381 0.8123333333 0.1431428571\n",
       " [356] 0.3812380952 0.7920000000 0.1853333333 0.1050952381 0.3056258503\n",
       " [361] 0.0691428571 0.0235238095 0.0740476190 0.1611904762 0.4580000000\n",
       " [366] 0.3772380952 0.6458412698 0.0218571429 0.0412380952 0.0695714286\n",
       " [371] 0.7283809524 0.0640476190 0.0064285714 0.0625238095 0.1275238095\n",
       " [376] 0.7090476190 0.0662857143 0.0432857143 0.0021428571 0.2385714286\n",
       " [381] 0.2228095238 0.5473333333 0.0241428571 0.8174761905 0.1604761905\n",
       " [386] 0.0409523810 0.0085714286 0.3626666667 0.0647619048 0.4383809524\n",
       " [391] 0.0203333333 0.0693809524 0.8133333333 0.1209523810 0.0430952381\n",
       " [396] 0.3354761905 0.1057142857 0.0530000000 0.0066190476 0.0196190476\n",
       " [401] 0.1276666667 0.8018571429 0.1784761905 0.8942380952 0.0836666667\n",
       " [406] 0.4389047619 0.1100476190 0.2698571429 0.6849523810 0.0500000000\n",
       " [411] 0.2138730159 0.0328095238 0.5413333333 0.0242380952 0.5571428571\n",
       " [416] 0.4382380952 0.2028571429 0.8076190476 0.3500952381 0.6797142857\n",
       " [421] 0.3712857143 0.6718095238 0.0216666667 0.0465714286 0.1757142857\n",
       " [426] 0.3976190476 0.1386598639 0.5260476190 0.1557619048 0.4570000000\n",
       " [431] 0.4614285714 0.0723809524 0.3274761905 0.3050952381 0.4368571429\n",
       " [436] 0.2301428571 0.0022857143 0.2244761905 0.6939047619 0.3444285714\n",
       " [441] 0.0567142857 0.1390476190 0.0854761905 0.8801428571 0.1973809524\n",
       " [446] 0.5639523810 0.0028571429 0.0028571429 0.7171428571 0.0463809524\n",
       " [451] 0.5374285714 0.2610000000 0.0388571429 0.7923333333 0.1852857143\n",
       " [456] 0.0074285714 0.3864761905 0.4110952381 0.8450476190 0.5710476190\n",
       " [461] 0.0521904762 0.0850476190 0.1536666667 0.8104761905 0.0672380952\n",
       " [466] 0.3337777778 0.1055714286 0.0135238095 0.0434285714 0.5808571429\n",
       " [471] 0.3410476190 0.7138571429 0.1030952381 0.0604285714 0.5032380952\n",
       " [476] 0.0140000000 0.1627142857 0.7407619048 0.3320952381 0.0171428571\n",
       " [481] 0.0028571429 0.1034761905 0.8207619048 0.1341904762 0.0028571429\n",
       " [486] 0.1569523810 0.5404285714 0.0170952381 0.3430884354 0.0818095238\n",
       " [491] 0.5954285714 0.3602857143 0.6660952381 0.6998095238 0.1002380952\n",
       " [496] 0.0211428571 0.7564761905 0.0028571429 0.8127142857 0.1140000000\n",
       " [501] 0.1659523810 0.1364761905 0.0038095238 0.0181904762 0.1160000000\n",
       " [506] 0.0085714286 0.3265238095 0.6250595238 0.1198571429 0.0210000000\n",
       " [511] 0.4278571429 0.4143333333 0.0223809524 0.0226666667 0.0305714286\n",
       " [516] 0.3073703704 0.0508571429 0.0877142857 0.5249523810 0.4590476190\n",
       " [521] 0.0047619048 0.0303333333 0.4036190476 0.0057142857 0.2619523810\n",
       " [526] 0.3729047619 0.0593809524 0.8011904762 0.1766190476 0.0085714286\n",
       " [531] 0.2519047619 0.0074285714 0.5106031746 0.7654285714 0.2976666667\n",
       " [536] 0.0088095238 0.9504761905 0.6727142857 0.2734761905 0.2242857143\n",
       " [541] 0.1855714286 0.9163809524 0.0245238095 0.1000000000 0.0958095238\n",
       " [546] 0.0422857143 0.6261904762 0.7620000000 0.0266666667 0.5086190476\n",
       " [551] 0.0057142857 0.0341904762 0.0885714286 0.8171904762 0.5165714286\n",
       " [556] 0.6097619048 0.0881558442 0.0408571429 0.0534285714 0.3828095238\n",
       " [561] 0.0567142857 0.0720000000 0.0540952381 0.1403333333 0.2719115646\n",
       " [566] 0.0124285714 0.2723809524 0.0554761905 0.5116190476 0.3344285714\n",
       " [571] 0.0130000000 0.2221904762 0.2088571429 0.3286190476 0.2088571429\n",
       " [576] 0.4533741497 0.5348095238 0.1959523810 0.0801428571 0.0931428571\n",
       " [581] 0.0066666667 0.0080000000 0.4036190476 0.5206190476 0.2693809524\n",
       " [586] 0.4780952381 0.0175844156 0.1238571429 0.2360000000 0.0586666667\n",
       " [591] 0.5710000000 0.2153809524 0.0777619048 0.0000000000 0.0466666667\n",
       " [596] 0.6716666667 0.0275238095 0.0299523810 0.0368571429 0.5426666667\n",
       " [601] 0.0767619048 0.3106190476 0.6121904762 0.0524285714 0.0798571429\n",
       " [606] 0.2924761905 0.8690476190 0.1171428571 0.0777619048 0.1233333333\n",
       " [611] 0.2462380952 0.4671904762 0.6538095238 0.1610000000 0.0935714286\n",
       " [616] 0.5750952381 0.0360000000 0.3087142857 0.1774285714 0.9661481481\n",
       " [621] 0.1346666667 0.5350952381 0.9165238095 0.5964761905 0.2759523810\n",
       " [626] 0.2947619048 0.0554761905 0.0339047619 0.3359047619 0.0308571429\n",
       " [631] 0.3446190476 0.1134761905 0.6305238095 0.2380000000 0.2784285714\n",
       " [636] 0.0028571429 0.3704179894 0.1648571429 0.0195714286 0.2567619048\n",
       " [641] 0.3202448980 0.0954761905 0.0055714286 0.2368571429 0.1278095238\n",
       " [646] 0.0164285714 0.0946190476 0.0703809524 0.6458095238 0.4346190476\n",
       " [651] 0.1753809524 0.0544761905 0.4720000000 0.4375714286 0.3763809524\n",
       " [656] 0.1534761905 0.6859047619 0.6368571429 0.5757619048 0.2898095238\n",
       " [661] 0.2679523810 0.0232380952 0.0395238095 0.6000952381 0.2120476190\n",
       " [666] 0.1396190476 0.6136666667 0.0416666667 0.0000000000 0.7357142857\n",
       " [671] 0.3513809524 0.1143333333 0.0000000000 0.7542380952 0.4260476190\n",
       " [676] 0.4528095238 0.0110476190 0.6421904762 0.4939047619 0.3978571429\n",
       " [681] 0.0324285714 0.9282857143 0.0872380952 0.2775132275 0.0963333333\n",
       " [686] 0.6610476190 0.4032500000 0.4830476190 0.1514285714 0.1907619048\n",
       " [691] 0.0289047619 0.1120476190 0.5058231293 0.3845238095 0.0366796537\n",
       " [696] 0.4626394558 0.0467142857 0.0438095238 0.0238095238 0.1928571429\n",
       " [701] 0.0483809524 0.2797823129 0.7392857143 0.1843703704 0.6615238095\n",
       " [706] 0.6345238095 0.3696190476 0.0339523810 0.3425714286 0.0417142857\n",
       " [711] 0.4156598639 0.9534285714 0.0456666667 0.0057142857 0.1578435374\n",
       " [716] 0.4691904762 0.5211428571 0.6324761905 0.9139523810 0.0314285714\n",
       " [721] 0.1819047619 0.4416666667 0.7022857143 0.0378095238 0.0420000000\n",
       " [726] 0.0200000000 0.3879523810 0.4754285714 0.6023809524 0.0551428571\n",
       " [731] 0.4677619048 0.6523333333 0.2084285714 0.0815714286 0.0028571429\n",
       " [736] 0.0672380952 0.0209047619 0.5197619048 0.0913809524 0.0233809524\n",
       " [741] 0.0035714286 0.0980000000 0.7031904762 0.0992857143 0.9030000000\n",
       " [746] 0.1576190476 0.6247142857 0.7190000000 0.0092857143 0.2036666667\n",
       " [751] 0.0121904762 0.0327619048 0.1140000000 0.2697551020 0.0568571429\n",
       " [756] 0.7397142857 0.1630952381 0.0393809524 0.0123333333 0.0182857143\n",
       " [761] 0.2696190476 0.5477619048 0.0841428571 0.2393809524 0.9088095238\n",
       " [766] 0.0987142857 0.0034285714 0.0187619048 0.5457619048 0.2862857143\n",
       " [771] 0.0157142857 0.0345714286 0.5796190476 0.8853809524 0.5846666667\n",
       " [776] 0.1167619048 0.9030340136 0.3472857143 0.0279047619 0.4331428571\n",
       " [781] 0.1300952381 0.0034285714 0.5275714286 0.0000000000 0.0204761905\n",
       " [786] 0.4163703704 0.5122857143 0.3403809524 0.1404285714 0.4716666667\n",
       " [791] 0.0000000000 0.2770000000 0.6995238095 0.0248571429 0.1566190476\n",
       " [796] 0.0084285714 0.0034285714 0.0114285714 0.0345714286 0.3334285714\n",
       " [801] 0.2136666667 0.0085714286 0.2034285714 0.1784285714 0.2484761905\n",
       " [806] 0.0899047619 0.5545238095 0.0520476190 0.1035714286 0.6676190476\n",
       " [811] 0.4058095238 0.0800952381 0.0000000000 0.0839523810 0.3667445200\n",
       " [816] 0.5765238095 0.1672380952 0.2195238095 0.4775238095 0.0203809524\n",
       " [821] 0.2350476190 0.0269047619 0.6348095238 0.5270000000 0.0519047619\n",
       " [826] 0.0302857143 0.7535238095 0.0050000000 0.2138095238 0.4238571429\n",
       " [831] 0.0351428571 0.0294285714 0.7398095238 0.5370952381 0.8925714286\n",
       " [836] 0.5200952381 0.1582380952 0.4188571429 0.0576190476 0.6612857143\n",
       " [841] 0.2186190476 0.7945714286 0.0224761905 0.8159047619 0.6850952381\n",
       " [846] 0.6812380952 0.0286796537 0.1629047619 0.0384285714 0.3563333333\n",
       " [851] 0.0546666667 0.5806190476 0.6393809524 0.0163333333 0.8573809524\n",
       " [856] 0.2428095238 0.0376190476 0.0000000000 0.0000000000 0.6221904762\n",
       " [861] 0.0088571429 0.0962380952 0.1371428571 0.3305714286 0.5818571429\n",
       " [866] 0.0490000000 0.1148095238 0.5050476190 0.4840952381 0.0908095238\n",
       " [871] 0.0699523810 0.0179047619 0.0012857143 0.2396666667 0.4280952381\n",
       " [876] 0.4358843537 0.1357619048 0.3894285714 0.6238095238 0.8164285714\n",
       " [881] 0.9794285714 0.4035238095 0.6695714286 0.2108095238 0.0727142857\n",
       " [886] 0.0899523810 0.0253333333 0.8442380952 0.2774761905 0.5258095238\n",
       " [891] 0.6777619048 0.6811904762 0.2029523810 0.2334285714 0.2440476190\n",
       " [896] 0.1228571429 0.1851904762 0.2919047619 0.1960000000 0.0922721088\n",
       " [901] 0.1478571429 0.7024761905 0.0173809524 0.4277142857 0.6690476190\n",
       " [906] 0.0700000000 0.1661904762 0.0134285714 0.4862312925 0.4727619048\n",
       " [911] 0.0347619048 0.8184285714 0.7740476190 0.4501428571 0.0462380952\n",
       " [916] 0.0157619048 0.5314285714 0.5183809524 0.5977142857 0.1097142857\n",
       " [921] 0.0082857143 0.7761836735 0.3882857143 0.8248571429 0.0982857143\n",
       " [926] 0.0028571429 0.3072857143 0.1829047619 0.2689047619 0.6214285714\n",
       " [931] 0.9626666667 0.0693809524 0.1224285714 0.8100000000 0.2653333333\n",
       " [936] 0.0974285714 0.5218571429 0.0827619048 0.0747142857 0.2396666667\n",
       " [941] 0.3952380952 0.5881904762 0.4806666667 0.0242857143 0.2277142857\n",
       " [946] 0.0513809524 0.0182857143 0.9745238095 0.8680952381 0.7323809524\n",
       " [951] 0.2897619048 0.3042312925 0.0672857143 0.8725238095 0.1778571429\n",
       " [956] 0.3881904762 0.0262857143 0.0038095238 0.8224761905 0.0725238095\n",
       " [961] 0.0441904762 0.2485714286 0.0047619048 0.0646666667 0.0400952381\n",
       " [966] 0.4605238095 0.1584285714 0.0683333333 0.3823061224 0.6096190476\n",
       " [971] 0.0534285714 0.2668095238 0.7520952381 0.5067142857 0.2316666667\n",
       " [976] 0.0482857143 0.9434285714 0.3412857143 0.3009047619 0.7481904762\n",
       " [981] 0.9129523810 0.6721836735 0.4956190476 0.5397142857 0.6660204082\n",
       " [986] 0.5270952381 0.3176666667 0.4692448980 0.4723333333 0.0028571429\n",
       " [991] 0.0795714286 0.0309523810 0.0550000000 0.2654761905 0.0849047619\n",
       " [996] 0.8830476190 0.4299047619 0.0134285714 0.0017142857 0.6702380952\n",
       "[1001] 0.0296190476 0.2436190476 0.3641111111 0.3650476190 0.2830884354\n",
       "[1006] 0.8590952381 0.6761428571 0.0112380952 0.0019047619 0.0779047619\n",
       "[1011] 0.4183809524 0.2128095238 0.0934761905 0.5889455782 0.0851904762\n",
       "[1016] 0.7314761905 0.2887142857 0.3181322751 0.1985238095 0.6679047619\n",
       "[1021] 0.1842857143 0.0057142857 0.0257142857 0.0000000000 0.0092857143\n",
       "[1026] 0.1786190476 0.3172857143 0.3893741497 0.3520952381 0.0062857143\n",
       "[1031] 0.1221428571 0.5035986395 0.8641904762 0.4439047619 0.4619047619\n",
       "[1036] 0.0280000000 0.0925714286 0.5077142857 0.0493333333 0.1115714286\n",
       "[1041] 0.8738571429 0.5820000000 0.8647142857 0.0378095238 0.0157142857\n",
       "[1046] 0.0524285714 0.1978095238 0.1364285714 0.8155238095 0.4947619048\n",
       "[1051] 0.4230476190 0.0154285714 0.4070476190 0.0135714286 0.0905714286\n",
       "[1056] 0.4091428571 0.0121904762 0.0262857143 0.2821904762 0.0290952381\n",
       "[1061] 0.0000000000 0.0370952381 0.3990000000 0.2134149660 0.3312380952\n",
       "[1066] 0.2229047619 0.2952539683 0.0051428571 0.6473809524 0.8067619048\n",
       "[1071] 0.2837142857 0.6281904762 0.6046666667 0.2216190476 0.1195238095\n",
       "[1076] 0.0772380952 0.3296598639 0.0132857143 0.4211360544 0.6686190476\n",
       "[1081] 0.6450000000 0.0661904762 0.0291428571 0.6033809524 0.6207142857\n",
       "[1086] 0.0100000000 0.3297619048 0.3363809524 0.9794285714 0.2812857143\n",
       "[1091] 0.6003809524 0.7722857143 0.7971428571 0.0679523810 0.2189047619\n",
       "[1096] 0.2003333333 0.3442380952 0.6144285714 0.0587619048 0.9314285714\n",
       "[1101] 0.2050952381 0.0135714286 0.1447142857 0.5752857143 0.1668571429\n",
       "[1106] 0.2097482993 0.5230370370 0.1176190476 0.0724761905 0.6272063492\n",
       "[1111] 0.0019047619 0.1735714286 0.2504761905 0.3010476190 0.4629523810\n",
       "[1116] 0.5402636054 0.6512380952 0.0721428571 0.6776666667 0.0378571429\n",
       "[1121] 0.3122857143 0.2091904762 0.0603809524 0.7733809524 0.4750476190\n",
       "[1126] 0.1826825397 0.0162857143 0.0504285714 0.0315714286 0.0703809524\n",
       "[1131] 0.9112857143 0.3199047619 0.1484761905 0.1270476190 0.0028571429\n",
       "[1136] 0.1644285714 0.1765714286 0.0527619048 0.4422312925 0.0968571429\n",
       "[1141] 0.5620952381 0.0062857143 0.7189047619 0.1203333333 0.0085714286\n",
       "[1146] 0.6747142857 0.7526190476 0.2195238095 0.1581428571 0.0441904762\n",
       "[1151] 0.0465238095 0.5192380952 0.0471428571 0.3521428571 0.0792857143\n",
       "[1156] 0.0146666667 0.1126666667 0.6905493197 0.1010000000 0.8280476190\n",
       "[1161] 0.7970000000 0.1880476190 0.7176190476 0.3209047619 0.6630000000\n",
       "[1166] 0.6361481481 0.1056190476 0.6247142857 0.2805714286 0.1472380952\n",
       "[1171] 0.1898571429 0.5643809524 0.4066666667 0.2081904762 0.4576666667\n",
       "[1176] 0.0147619048 0.2729047619 0.2912380952 0.0905238095 0.3372380952\n",
       "[1181] 0.0840000000 0.5123928571 0.0095238095 0.6228027211 0.0122380952\n",
       "[1186] 0.3250476190 0.1362857143 0.0472857143 0.0047619048 0.1016190476\n",
       "[1191] 0.3133333333 0.6053809524 0.2419523810 0.0332857143 0.0122857143\n",
       "[1196] 0.1094285714 0.4503809524 0.0738571429 0.1190952381 0.2721428571\n",
       "[1201] 0.4573333333 0.0888503401 0.3021904762 0.0646666667 0.1303809524\n",
       "[1206] 0.0260476190 0.3310000000 0.0284285714 0.0868095238 0.0691904762\n",
       "[1211] 0.0459523810 0.5792857143 0.0391428571 0.0271904762 0.9438571429\n",
       "[1216] 0.6922040816 0.7792857143 0.1214761905 0.1317619048 0.1515714286\n",
       "[1221] 0.5060680272 0.0130476190 0.0355238095 0.5285238095 0.0259523810\n",
       "[1226] 0.0804285714 0.5611428571 0.4981904762 0.2556666667 0.0215238095\n",
       "[1231] 0.0504761905 0.6781904762 0.6819047619 0.0394761905 0.0945238095\n",
       "[1236] 0.1538571429 0.2241428571 0.8617619048 0.6985238095 0.3681428571\n",
       "[1241] 0.2260476190 0.0000000000 0.0131428571 0.6989047619 0.4266666667\n",
       "[1246] 0.0287619048 0.5812789116 0.0291428571 0.2200476190 0.2860952381\n",
       "[1251] 0.6881428571 0.9526190476 0.3096190476 0.2651428571 0.1650000000\n",
       "[1256] 0.9086666667 0.0225238095 0.0347142857 0.3717619048 0.0166666667\n",
       "[1261] 0.2887142857 0.5007619048 0.0203333333 0.0380952381 0.4887195767\n",
       "[1266] 0.0426190476 0.1812857143 0.4453809524 0.0730952381 0.4483809524\n",
       "[1271] 0.0331904762 0.0204285714 0.4903809524 0.0957619048 0.3841428571\n",
       "[1276] 0.8014285714 0.2939047619 0.4932857143 0.0222857143 0.0555714286\n",
       "[1281] 0.0192857143 0.5949523810 0.0130000000 0.0057142857 0.0750000000\n",
       "[1286] 0.0172857143 0.4941904762 0.0483333333 0.7778095238 0.1188571429\n",
       "[1291] 0.2035714286 0.8540000000 0.4897619048 0.6785238095 0.0278571429\n",
       "[1296] 0.8237142857 0.7683333333 0.2008095238 0.0120000000 0.3862244898\n",
       "[1301] 0.5653809524 0.0028571429 0.2750952381 0.3934761905 0.3059523810\n",
       "[1306] 0.7439047619 0.0519047619 0.0356666667 0.7211428571 0.0000000000\n",
       "[1311] 0.1279047619 0.1089523810 0.1493809524 0.0057142857 0.3866666667\n",
       "[1316] 0.4721292517 0.0047619048 0.1668571429 0.5379523810 0.6726190476\n",
       "[1321] 0.8037142857 0.1924285714 0.4304285714 0.2946190476 0.0057142857\n",
       "[1326] 0.5016190476 0.1701428571 0.0108571429 0.2843333333 0.0700000000\n",
       "[1331] 0.0257142857 0.1779047619 0.6085714286 0.3703809524 0.1719047619\n",
       "[1336] 0.7960952381 0.4132380952 0.0391428571 0.0045714286 0.1495238095\n",
       "[1341] 0.1310000000 0.3166190476 0.0050000000 0.2023333333 0.5085238095\n",
       "[1346] 0.2287142857 0.0028571429 0.3706560847 0.0304761905 0.1722857143\n",
       "[1351] 0.2047142857 0.0191428571 0.0638571429 0.3149523810 0.0394285714\n",
       "[1356] 0.7708911565 0.5390952381 0.4195714286 0.2317142857 0.3806190476\n",
       "[1361] 0.0000000000 0.5760000000 0.4170476190 0.3966190476 0.4134761905\n",
       "[1366] 0.2817074830 0.0028571429 0.3456666667 0.2293333333 0.1042380952\n",
       "[1371] 0.3069523810 0.2321428571 0.6690680272 0.2337619048 0.0374761905\n",
       "[1376] 0.0327142857 0.2439047619 0.0028571429 0.9321904762 0.2358571429\n",
       "[1381] 0.5641904762 0.3197142857 0.4445714286 0.5186190476 0.1384761905\n",
       "[1386] 0.3562380952 0.3636349206 0.3425714286 0.5885714286 0.3883809524\n",
       "[1391] 0.1810000000 0.0239047619 0.0149047619 0.0545238095 0.1796666667\n",
       "[1396] 0.4275714286 0.0766666667 0.3812380952 0.0219047619 0.1112857143\n",
       "[1401] 0.0412857143 0.1748095238 0.5742380952 0.0255238095 0.5452857143\n",
       "[1406] 0.9123809524 0.0385714286 0.4672857143 0.1641292517 0.0540000000\n",
       "[1411] 0.1589047619 0.6948571429 0.0014285714 0.7819523810 0.2634761905\n",
       "[1416] 0.0309523810 0.6017619048 0.7486666667 0.0636190476 0.0035714286\n",
       "[1421] 0.4166969010 0.3086190476 0.7099523810 0.3635238095 0.3672857143\n",
       "[1426] 0.4960000000 0.5576190476 0.3818095238 0.0135714286 0.7158095238\n",
       "[1431] 0.0130952381 0.4603809524 0.0426666667 0.7339523810 0.0657142857\n",
       "[1436] 0.1976666667 0.1709523810 0.0038095238 0.2202244898 0.4281428571\n",
       "[1441] 0.0698095238 0.0149047619 0.7310476190 0.1476190476 0.2450476190\n",
       "[1446] 0.6107619048 0.0408095238 0.3432925170 0.5462380952 0.0597619048\n",
       "[1451] 0.0470952381 0.3003333333 0.3646190476 0.2400952381 0.6389047619\n",
       "[1456] 0.3756666667 0.2560476190 0.0045714286 0.4003333333 0.3749047619\n",
       "[1461] 0.0066666667 0.6098095238 0.4059523810 0.7721904762 0.1676666667\n",
       "[1466] 0.8700476190 0.0993333333 0.0071428571 0.0034285714 0.8099047619\n",
       "[1471] 0.4689047619 0.0000000000 0.0380952381 0.0091428571 0.0375238095\n",
       "[1476] 0.1712380952 0.4523333333 0.2945714286 0.1128571429 0.9051428571\n",
       "[1481] 0.0421428571 0.2160476190 0.9298095238 0.0017142857 0.3250476190\n",
       "[1486] 0.4910000000 0.2844013605 0.2813333333 0.2766530612 0.4335238095\n",
       "[1491] 0.7180952381 0.7267619048 0.4522751323 0.3104761905 0.1844761905\n",
       "[1496] 0.1295714286 0.0019047619 0.0083809524 0.0488571429 0.7015714286\n",
       "[1501] 0.2622857143 0.2424761905 0.0877142857 0.4781428571 0.0736190476\n",
       "[1506] 0.0558095238 0.7523333333 0.7276666667 0.7210000000 0.4739047619\n",
       "[1511] 0.3710952381 0.0005714286 0.1742857143 0.1184761905 0.8605238095\n",
       "[1516] 0.0000000000 0.0988095238 0.3320317460 0.5264761905 0.0235714286\n",
       "[1521] 0.0035714286 0.2717619048 0.4076190476 0.8295238095 0.4084285714\n",
       "[1526] 0.0323809524 0.2702653061 0.3660000000 0.9075714286 0.0439047619\n",
       "[1531] 0.3114285714 0.7099047619 0.3591904762 0.1699047619 0.7625238095\n",
       "[1536] 0.0147619048 0.1880952381 0.8523333333 0.1321904762 0.0175714286\n",
       "[1541] 0.4156394558 0.1551428571 0.0414761905 0.6249047619 0.1627619048\n",
       "[1546] 0.2686190476 0.0107142857 0.2997619048 0.0019047619 0.0381904762\n",
       "[1551] 0.0530952381 0.1534285714 0.0379523810 0.0318571429 0.0722380952\n",
       "[1556] 0.0796666667 0.0401904762 0.0103809524 0.0798095238 0.0388095238\n",
       "[1561] 0.0101904762 0.0005714286 0.3674285714 0.9260952381 0.2885714286\n",
       "[1566] 0.0554761905 0.0161428571 0.5049523810 0.2860476190 0.2917619048\n",
       "[1571] 0.6153333333 0.7253095238 0.0867142857 0.4141428571 0.0497142857\n",
       "[1576] 0.0105714286 0.6874285714 0.2735238095 0.4685238095 0.2381428571\n",
       "[1581] 0.2307346939 0.2103333333 0.0282857143 0.4958571429 0.0521428571\n",
       "[1586] 0.0628095238 0.2544444444 0.4225646259 0.5233333333 0.1678571429\n",
       "[1591] 0.7952857143 0.4026190476 0.1666190476 0.1826190476 0.0729523810\n",
       "[1596] 0.3991428571 0.1134761905 0.0065714286 0.4445714286 0.3403809524\n",
       "[1601] 0.2209047619 0.0028571429 0.0562380952 0.5028571429 0.1897142857\n",
       "[1606] 0.8033333333 0.0363333333 0.6645238095 0.0931428571 0.9090680272\n",
       "[1611] 0.0080000000 0.0395714286 0.3365714286 0.8318571429 0.8422380952\n",
       "[1616] 0.0028571429 0.0280476190 0.0070000000 0.2287142857 0.4117142857\n",
       "[1621] 0.0014285714 0.9686666667 0.7413809524 0.7579047619 0.4102857143\n",
       "[1626] 0.6730952381 0.1555238095 0.7100476190 0.2965714286 0.0721428571\n",
       "[1631] 0.2176666667 0.4703333333 0.3085714286 0.1951428571 0.4385646259\n",
       "[1636] 0.4051428571 0.0028571429 0.0936666667 0.0028571429 0.5314761905\n",
       "[1641] 0.5730612245 0.1328095238 0.3567142857 0.0667142857 0.1993333333\n",
       "[1646] 0.3516666667 0.0710476190 0.3719931973 0.0696190476 0.0073333333\n",
       "[1651] 0.6323741497 0.0752857143 0.0213809524 0.0893333333 0.4858163265\n",
       "[1656] 0.2086870748 0.0103809524 0.3186190476 0.6618571429 0.0160000000\n",
       "[1661] 0.4728571429 0.1271904762 0.0238095238 0.3211904762 0.2521904762\n",
       "[1666] 0.1536666667 0.0108571429 0.3901428571 0.0610952381 0.2936666667\n",
       "[1671] 0.2404761905 0.0085714286 0.0652380952 0.3284897959 0.3865714286\n",
       "[1676] 0.9133862434 0.0273333333 0.2007142857 0.0085714286 0.5224761905\n",
       "[1681] 0.1939047619 0.3313333333 0.0877619048 0.4896190476 0.2689047619\n",
       "[1686] 0.0321904762 0.0424761905 0.2371904762 0.1621428571 0.0171428571\n",
       "[1691] 0.0449523810 0.4270952381 0.5495952381 0.1650000000 0.0114761905\n",
       "[1696] 0.0000000000 0.4816190476 0.0963809524 0.3663809524 0.1843809524\n",
       "[1701] 0.5322857143 0.1327142857 0.2520204082 0.1103809524 0.5908367347\n",
       "[1706] 0.1023333333 0.0573809524 0.6950476190 0.8438571429 0.1407619048\n",
       "[1711] 0.4266666667 0.1532380952 0.0205714286 0.0229523810 0.2882380952\n",
       "[1716] 0.2094761905 0.4689931973 0.3950000000 0.5025714286 0.0109523810\n",
       "[1721] 0.2338571429 0.7224285714 0.9062380952 0.0976190476 0.0846666667\n",
       "[1726] 0.0017142857 0.4598571429 0.0380952381 0.0248571429 0.7198571429\n",
       "[1731] 0.1722857143 0.2709047619 0.4109523810 0.8672857143 0.0474285714\n",
       "[1736] 0.0428571429 0.2524761905 0.0087619048 0.4366666667 0.6270952381\n",
       "[1741] 0.8020000000 0.3973809524 0.1705000000 0.0095714286 0.0893095238\n",
       "[1746] 0.0101904762 0.5593809524 0.4422857143 0.0645714286 0.4343809524\n",
       "[1751] 0.0474285714 0.7545238095 0.2821904762 0.8836666667 0.0085714286\n",
       "[1756] 0.6941428571 0.4259523810 0.2025714286 0.0233809524 0.2826666667\n",
       "[1761] 0.1057619048 0.0820952381 0.6868095238 0.1717142857 0.0000000000\n",
       "[1766] 0.2145714286 0.2807142857 0.0681428571 0.0329523810 0.6964693878\n",
       "[1771] 0.2708571429 0.2032857143 0.0121428571 0.3506666667 0.1876190476\n",
       "[1776] 0.5687619048 0.3960000000 0.0926190476 0.3610136054 0.3141904762\n",
       "[1781] 0.6453386243 0.0178571429 0.1732857143 0.1073809524 0.0066666667\n",
       "[1786] 0.0511428571 0.1220952381 0.1937142857 0.6847142857 0.0462380952\n",
       "[1791] 0.0932380952 0.0330952381 0.8151904762 0.0311428571 0.6615238095\n",
       "[1796] 0.0555238095 0.0527142857 0.1444761905 0.8058571429 0.5083333333\n",
       "[1801] 0.5113333333 0.8517619048 0.0014285714 0.3986666667 0.5985238095\n",
       "[1806] 0.1274285714 0.1885714286 0.7864285714 0.0047619048 0.2043809524\n",
       "[1811] 0.4517142857 0.0400000000 0.3173129252 0.0978571429 0.3951904762\n",
       "[1816] 0.6400634921 0.0128571429 0.7061904762 0.0417142857 0.7133809524\n",
       "[1821] 0.2632380952 0.4805714286 0.6636190476 0.0072380952 0.0201428571\n",
       "[1826] 0.0141428571 0.8288095238 0.8028095238 0.0204761905 0.0000000000\n",
       "[1831] 0.0160000000 0.0596666667 0.0272380952 0.0446666667 0.0371428571\n",
       "[1836] 0.7310952381 0.1563333333 0.0057142857 0.7771428571 0.7671428571\n",
       "[1841] 0.2186666667 0.0467619048 0.7538571429 0.0226190476 0.2270476190\n",
       "[1846] 0.0409523810 0.1803333333 0.0014285714 0.3382380952 0.5276666667\n",
       "[1851] 0.2345238095 0.3107619048 0.0060000000 0.5486190476 0.1526190476\n",
       "[1856] 0.0000000000 0.6064285714 0.0563809524 0.7198095238 0.6054761905\n",
       "[1861] 0.3210000000 0.1657619048 0.7754285714 0.1438571429 0.0890952381\n",
       "[1866] 0.3071904762 0.0317619048 0.3796190476 0.7136666667 0.1968571429\n",
       "[1871] 0.0320000000 0.4350595238 0.0265714286 0.3364285714 0.0073333333\n",
       "[1876] 0.4033333333 0.5503061224 0.5251428571 0.0394285714 0.0137142857\n",
       "[1881] 0.0454761905 0.9380952381 0.0355714286 0.4278571429 0.1175238095\n",
       "[1886] 0.0185714286 0.1737619048 0.0000000000 0.3640476190 0.0668095238\n",
       "[1891] 0.6835714286 0.4563333333 0.6005238095 0.0135714286 0.0017142857\n",
       "[1896] 0.3513928571 0.0131428571 0.6882500000 0.0060000000 0.9243333333\n",
       "[1901] 0.7873333333 0.7352380952 0.0461428571 0.2400000000 0.3265714286\n",
       "[1906] 0.1446190476 0.6191428571 0.0078571429 0.0000000000 0.9270476190\n",
       "[1911] 0.0876190476 0.7689047619 0.5317142857 0.0140952381 0.5267142857\n",
       "[1916] 0.0852857143 0.3610476190 0.4144761905 0.3017513228 0.1550000000\n",
       "[1921] 0.3385170068 0.4001428571 0.0252857143 0.0468095238 0.6044285714\n",
       "[1926] 0.5067142857 0.5110476190 0.1853809524 0.1610476190 0.5636190476\n",
       "[1931] 0.3622275132 0.7957619048 0.3991292517 0.2449523810 0.4246349206\n",
       "[1936] 0.1399047619 0.0142857143 0.7689047619 0.0069047619 0.0094285714\n",
       "[1941] 0.5041904762 0.5969523810 0.0040476190 0.0146190476 0.2294285714\n",
       "[1946] 0.0504285714 0.9288095238 0.1062380952 0.4336190476 0.0109523810\n",
       "[1951] 0.2142857143 0.1598095238 0.9164920635 0.7406394558 0.6799047619\n",
       "[1956] 0.0261904762 0.4503333333 0.0745714286 0.1624285714 0.3563333333\n",
       "[1961] 0.2575238095 0.7289047619 0.3612539683 0.0189047619 0.1627619048\n",
       "[1966] 0.0874761905 0.2041904762 0.0388571429 0.7168571429 0.9225714286\n",
       "[1971] 0.4393809524 0.4614761905 0.2643015873 0.1206190476 0.3429455782\n",
       "[1976] 0.0617142857 0.2885034014 0.2787619048 0.2628367347 0.2439047619\n",
       "[1981] 0.0800000000 0.8335238095 0.2176666667 0.0510476190 0.0807142857\n",
       "[1986] 0.8647142857 0.2962857143 0.2938095238 0.1681904762 0.3200238095\n",
       "[1991] 0.0669047619 0.5375714286 0.8912857143 0.4896666667 0.5550952381\n",
       "[1996] 0.0395238095 0.6187619048 0.0948095238 0.0710476190 0.6106190476\n",
       "[2001] 0.1497142857 0.2996666667 0.0040000000 0.1224285714 0.6408095238\n",
       "[2006] 0.2173333333 0.1640952381 0.2293809524 0.1645714286 0.0340000000\n",
       "[2011] 0.3429523810 0.2655714286 0.2690952381 0.4617619048 0.1810952381\n",
       "[2016] 0.3348571429 0.0060952381 0.7569047619 0.2160476190 0.3105714286\n",
       "[2021] 0.1002857143 0.4768095238 0.2597619048 0.6136190476 0.1952380952\n",
       "[2026] 0.2516666667 0.2575238095 0.6683809524 0.5093333333 0.3353492063\n",
       "[2031] 0.0157142857 0.0666190476 0.7557142857 0.1676190476 0.0000000000\n",
       "[2036] 0.0052857143 0.2622857143 0.0264285714 0.4870476190 0.5102380952\n",
       "[2041] 0.0095238095 0.0213809524 0.0028571429 0.0383333333 0.0834761905\n",
       "[2046] 0.1155714286 0.2384285714 0.0884761905 0.4470476190 0.0718095238\n",
       "[2051] 0.0383333333 0.3291666667 0.0306190476 0.1845238095 0.7377619048\n",
       "[2056] 0.3107619048 0.7825714286 0.2185714286 0.0222857143 0.0597142857\n",
       "[2061] 0.1645714286 0.0807142857 0.0187142857 0.3197142857 0.0135714286\n",
       "[2066] 0.7716190476 0.0030000000 0.0992380952 0.2935238095 0.0524761905\n",
       "[2071] 0.2550952381 0.0107142857 0.2149047619"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reduceddsubmit <- submitdata[,-removecols]\n",
    "predictions <- predict(RF12, reduceddsubmit,type = \"prob\" )$b; predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Format OK\"\n",
      "$submission\n",
      "[0.044,0.1919,0.3808,0.0376,0.6268,0.1811,0.4023,0.4962,0.0909,0,0.246,0.3773,0.0195,0.9128,0.163,0.0121,0.0132,0.1483,0.1487,0.0095,0.0749,0.377,0.9418,0.0048,0.5931,0.0772,0.4872,0.462,0.4333,0.2635,0.435,0.7113,0.0171,0.4223,0.6632,0.2353,0.6974,0.7086,0.9206,0.0226,0.1768,0.138,0.1328,0.8132,0.2217,0.8547,0.1919,0.0438,0.2917,0.1374,0.0139,0.0129,0.1447,0.4104,0.8504,0.2875,0.7568,0.5675,0.094,0.5144,0.8696,0.0057,0.0714,0.1812,0.0535,0.0584,0.0006,0.0171,0.626,0.1798,0.5747,0.0245,0.0407,0.6902,0.0747,0.2492,0.709,0.0419,0.485,0.0794,0.9332,0.08,0.0021,0.5281,0.2354,0.1627,0.001,0.0671,0.8447,0.2576,0.0163,0.0183,0.086,0.713,0.1028,0.41,0.1568,0.075,0.3473,0.4645,0.568,0.033,0.5982,0.3616,0.3432,0.3757,0.0126,0.2529,0.1035,0.0261,0.0245,0.8756,0.151,0.7659,0.2479,0.009,0.0138,0.3477,0.0344,0.0234,0.0103,0.0591,0.009,0.12,0.1737,0.0332,0.0326,0.2865,0.6593,0.0126,0.585,0.3826,0.3121,0.4244,0.4145,0.5882,0.0721,0.7065,0.1115,0.7342,0.3849,0.2206,0.289,0.3242,0.3239,0.1875,0.7195,0.1386,0.2697,0.0962,0.037,0.3898,0.2427,0.1495,0.0057,0.0927,0.0282,0.5624,0.3712,0.916,0.1726,0.1979,0.0319,0.03,0.0129,0.6241,0.0662,0.4937,0.0181,0.1188,0.7356,0.3249,0.3491,0.0395,0.4127,0.0107,0.2198,0.3001,0.5716,0.8909,0.7695,0.7083,0.4272,0.0129,0.862,0.024,0.0029,0.241,0.1974,0.2026,0,0.4473,0.6303,0.1186,0.0067,0,0.0477,0.0114,0.031,0.7235,0.374,0.1576,0.2061,0.5192,0.2613,0.6264,0.0287,0.9589,0.0915,0.7157,0.0171,0,0.0733,0.5182,0.1423,0.7456,0.1545,0.0339,0.0091,0.0651,0.0541,0.0091,0.3487,0.658,0.0194,0.1841,0.1123,0.8099,0.2129,0.0084,0.8726,0.031,0.3779,0.0561,0.005,0.0305,0.3212,0.1199,0.6005,0.6998,0.0619,0.0095,0.0106,0.0431,0.062,0.0389,0.5071,0.6237,0.51,0.0019,0.0462,0.7915,0.7791,0.4381,0.0565,0.5482,0.8283,0.1946,0.2059,0.1523,0.5157,0.2416,0.0705,0.2841,0.6539,0.3375,0.0202,0.1586,0.1109,0.098,0.3214,0.2123,0.2646,0.1706,0.0512,0.2662,0.6474,0.0587,0.3012,0.1052,0.3717,0.0566,0.2462,0.1199,0.678,0.3488,0.0734,0.3852,0.6414,0.563,0.3853,0.0873,0.3265,0.554,0.2854,0.8857,0.5486,0.209,0.0412,0.0812,0.8563,0.0226,0.0439,0,0.049,0.0373,0.0789,0.1648,0.0355,0.0064,0.0958,0.5146,0.6992,0.0326,0.0218,0.7109,0.2441,0.1217,0.5106,0.539,0.0133,0.1911,0.0172,0.3315,0.2151,0.544,0.631,0.5492,0.0415,0.3193,0.4728,0.2365,0.5292,0.6479,0.8458,0.2441,0.4965,0.728,0.012,0.1479,0.1154,0.1096,0.054,0.2754,0.3553,0.0136,0.1479,0.196,0.0374,0.0676,0.0345,0.0174,0.1611,0.8123,0.1431,0.3812,0.792,0.1853,0.1051,0.3056,0.0691,0.0235,0.074,0.1612,0.458,0.3772,0.6458,0.0219,0.0412,0.0696,0.7284,0.064,0.0064,0.0625,0.1275,0.709,0.0663,0.0433,0.0021,0.2386,0.2228,0.5473,0.0241,0.8175,0.1605,0.041,0.0086,0.3627,0.0648,0.4384,0.0203,0.0694,0.8133,0.121,0.0431,0.3355,0.1057,0.053,0.0066,0.0196,0.1277,0.8019,0.1785,0.8942,0.0837,0.4389,0.11,0.2699,0.685,0.05,0.2139,0.0328,0.5413,0.0242,0.5571,0.4382,0.2029,0.8076,0.3501,0.6797,0.3713,0.6718,0.0217,0.0466,0.1757,0.3976,0.1387,0.526,0.1558,0.457,0.4614,0.0724,0.3275,0.3051,0.4369,0.2301,0.0023,0.2245,0.6939,0.3444,0.0567,0.139,0.0855,0.8801,0.1974,0.564,0.0029,0.0029,0.7171,0.0464,0.5374,0.261,0.0389,0.7923,0.1853,0.0074,0.3865,0.4111,0.845,0.571,0.0522,0.085,0.1537,0.8105,0.0672,0.3338,0.1056,0.0135,0.0434,0.5809,0.341,0.7139,0.1031,0.0604,0.5032,0.014,0.1627,0.7408,0.3321,0.0171,0.0029,0.1035,0.8208,0.1342,0.0029,0.157,0.5404,0.0171,0.3431,0.0818,0.5954,0.3603,0.6661,0.6998,0.1002,0.0211,0.7565,0.0029,0.8127,0.114,0.166,0.1365,0.0038,0.0182,0.116,0.0086,0.3265,0.6251,0.1199,0.021,0.4279,0.4143,0.0224,0.0227,0.0306,0.3074,0.0509,0.0877,0.525,0.459,0.0048,0.0303,0.4036,0.0057,0.262,0.3729,0.0594,0.8012,0.1766,0.0086,0.2519,0.0074,0.5106,0.7654,0.2977,0.0088,0.9505,0.6727,0.2735,0.2243,0.1856,0.9164,0.0245,0.1,0.0958,0.0423,0.6262,0.762,0.0267,0.5086,0.0057,0.0342,0.0886,0.8172,0.5166,0.6098,0.0882,0.0409,0.0534,0.3828,0.0567,0.072,0.0541,0.1403,0.2719,0.0124,0.2724,0.0555,0.5116,0.3344,0.013,0.2222,0.2089,0.3286,0.2089,0.4534,0.5348,0.196,0.0801,0.0931,0.0067,0.008,0.4036,0.5206,0.2694,0.4781,0.0176,0.1239,0.236,0.0587,0.571,0.2154,0.0778,0,0.0467,0.6717,0.0275,0.03,0.0369,0.5427,0.0768,0.3106,0.6122,0.0524,0.0799,0.2925,0.869,0.1171,0.0778,0.1233,0.2462,0.4672,0.6538,0.161,0.0936,0.5751,0.036,0.3087,0.1774,0.9661,0.1347,0.5351,0.9165,0.5965,0.276,0.2948,0.0555,0.0339,0.3359,0.0309,0.3446,0.1135,0.6305,0.238,0.2784,0.0029,0.3704,0.1649,0.0196,0.2568,0.3202,0.0955,0.0056,0.2369,0.1278,0.0164,0.0946,0.0704,0.6458,0.4346,0.1754,0.0545,0.472,0.4376,0.3764,0.1535,0.6859,0.6369,0.5758,0.2898,0.268,0.0232,0.0395,0.6001,0.212,0.1396,0.6137,0.0417,0,0.7357,0.3514,0.1143,0,0.7542,0.426,0.4528,0.011,0.6422,0.4939,0.3979,0.0324,0.9283,0.0872,0.2775,0.0963,0.661,0.4032,0.483,0.1514,0.1908,0.0289,0.112,0.5058,0.3845,0.0367,0.4626,0.0467,0.0438,0.0238,0.1929,0.0484,0.2798,0.7393,0.1844,0.6615,0.6345,0.3696,0.034,0.3426,0.0417,0.4157,0.9534,0.0457,0.0057,0.1578,0.4692,0.5211,0.6325,0.914,0.0314,0.1819,0.4417,0.7023,0.0378,0.042,0.02,0.388,0.4754,0.6024,0.0551,0.4678,0.6523,0.2084,0.0816,0.0029,0.0672,0.0209,0.5198,0.0914,0.0234,0.0036,0.098,0.7032,0.0993,0.903,0.1576,0.6247,0.719,0.0093,0.2037,0.0122,0.0328,0.114,0.2698,0.0569,0.7397,0.1631,0.0394,0.0123,0.0183,0.2696,0.5478,0.0841,0.2394,0.9088,0.0987,0.0034,0.0188,0.5458,0.2863,0.0157,0.0346,0.5796,0.8854,0.5847,0.1168,0.903,0.3473,0.0279,0.4331,0.1301,0.0034,0.5276,0,0.0205,0.4164,0.5123,0.3404,0.1404,0.4717,0,0.277,0.6995,0.0249,0.1566,0.0084,0.0034,0.0114,0.0346,0.3334,0.2137,0.0086,0.2034,0.1784,0.2485,0.0899,0.5545,0.052,0.1036,0.6676,0.4058,0.0801,0,0.084,0.3667,0.5765,0.1672,0.2195,0.4775,0.0204,0.235,0.0269,0.6348,0.527,0.0519,0.0303,0.7535,0.005,0.2138,0.4239,0.0351,0.0294,0.7398,0.5371,0.8926,0.5201,0.1582,0.4189,0.0576,0.6613,0.2186,0.7946,0.0225,0.8159,0.6851,0.6812,0.0287,0.1629,0.0384,0.3563,0.0547,0.5806,0.6394,0.0163,0.8574,0.2428,0.0376,0,0,0.6222,0.0089,0.0962,0.1371,0.3306,0.5819,0.049,0.1148,0.505,0.4841,0.0908,0.07,0.0179,0.0013,0.2397,0.4281,0.4359,0.1358,0.3894,0.6238,0.8164,0.9794,0.4035,0.6696,0.2108,0.0727,0.09,0.0253,0.8442,0.2775,0.5258,0.6778,0.6812,0.203,0.2334,0.244,0.1229,0.1852,0.2919,0.196,0.0923,0.1479,0.7025,0.0174,0.4277,0.669,0.07,0.1662,0.0134,0.4862,0.4728,0.0348,0.8184,0.774,0.4501,0.0462,0.0158,0.5314,0.5184,0.5977,0.1097,0.0083,0.7762,0.3883,0.8249,0.0983,0.0029,0.3073,0.1829,0.2689,0.6214,0.9627,0.0694,0.1224,0.81,0.2653,0.0974,0.5219,0.0828,0.0747,0.2397,0.3952,0.5882,0.4807,0.0243,0.2277,0.0514,0.0183,0.9745,0.8681,0.7324,0.2898,0.3042,0.0673,0.8725,0.1779,0.3882,0.0263,0.0038,0.8225,0.0725,0.0442,0.2486,0.0048,0.0647,0.0401,0.4605,0.1584,0.0683,0.3823,0.6096,0.0534,0.2668,0.7521,0.5067,0.2317,0.0483,0.9434,0.3413,0.3009,0.7482,0.913,0.6722,0.4956,0.5397,0.666,0.5271,0.3177,0.4692,0.4723,0.0029,0.0796,0.031,0.055,0.2655,0.0849,0.883,0.4299,0.0134,0.0017,0.6702,0.0296,0.2436,0.3641,0.365,0.2831,0.8591,0.6761,0.0112,0.0019,0.0779,0.4184,0.2128,0.0935,0.5889,0.0852,0.7315,0.2887,0.3181,0.1985,0.6679,0.1843,0.0057,0.0257,0,0.0093,0.1786,0.3173,0.3894,0.3521,0.0063,0.1221,0.5036,0.8642,0.4439,0.4619,0.028,0.0926,0.5077,0.0493,0.1116,0.8739,0.582,0.8647,0.0378,0.0157,0.0524,0.1978,0.1364,0.8155,0.4948,0.423,0.0154,0.407,0.0136,0.0906,0.4091,0.0122,0.0263,0.2822,0.0291,0,0.0371,0.399,0.2134,0.3312,0.2229,0.2953,0.0051,0.6474,0.8068,0.2837,0.6282,0.6047,0.2216,0.1195,0.0772,0.3297,0.0133,0.4211,0.6686,0.645,0.0662,0.0291,0.6034,0.6207,0.01,0.3298,0.3364,0.9794,0.2813,0.6004,0.7723,0.7971,0.068,0.2189,0.2003,0.3442,0.6144,0.0588,0.9314,0.2051,0.0136,0.1447,0.5753,0.1669,0.2097,0.523,0.1176,0.0725,0.6272,0.0019,0.1736,0.2505,0.301,0.463,0.5403,0.6512,0.0721,0.6777,0.0379,0.3123,0.2092,0.0604,0.7734,0.475,0.1827,0.0163,0.0504,0.0316,0.0704,0.9113,0.3199,0.1485,0.127,0.0029,0.1644,0.1766,0.0528,0.4422,0.0969,0.5621,0.0063,0.7189,0.1203,0.0086,0.6747,0.7526,0.2195,0.1581,0.0442,0.0465,0.5192,0.0471,0.3521,0.0793,0.0147,0.1127,0.6905,0.101,0.828,0.797,0.188,0.7176,0.3209,0.663,0.6361,0.1056,0.6247,0.2806,0.1472,0.1899,0.5644,0.4067,0.2082,0.4577,0.0148,0.2729,0.2912,0.0905,0.3372,0.084,0.5124,0.0095,0.6228,0.0122,0.325,0.1363,0.0473,0.0048,0.1016,0.3133,0.6054,0.242,0.0333,0.0123,0.1094,0.4504,0.0739,0.1191,0.2721,0.4573,0.0889,0.3022,0.0647,0.1304,0.026,0.331,0.0284,0.0868,0.0692,0.046,0.5793,0.0391,0.0272,0.9439,0.6922,0.7793,0.1215,0.1318,0.1516,0.5061,0.013,0.0355,0.5285,0.026,0.0804,0.5611,0.4982,0.2557,0.0215,0.0505,0.6782,0.6819,0.0395,0.0945,0.1539,0.2241,0.8618,0.6985,0.3681,0.226,0,0.0131,0.6989,0.4267,0.0288,0.5813,0.0291,0.22,0.2861,0.6881,0.9526,0.3096,0.2651,0.165,0.9087,0.0225,0.0347,0.3718,0.0167,0.2887,0.5008,0.0203,0.0381,0.4887,0.0426,0.1813,0.4454,0.0731,0.4484,0.0332,0.0204,0.4904,0.0958,0.3841,0.8014,0.2939,0.4933,0.0223,0.0556,0.0193,0.595,0.013,0.0057,0.075,0.0173,0.4942,0.0483,0.7778,0.1189,0.2036,0.854,0.4898,0.6785,0.0279,0.8237,0.7683,0.2008,0.012,0.3862,0.5654,0.0029,0.2751,0.3935,0.306,0.7439,0.0519,0.0357,0.7211,0,0.1279,0.109,0.1494,0.0057,0.3867,0.4721,0.0048,0.1669,0.538,0.6726,0.8037,0.1924,0.4304,0.2946,0.0057,0.5016,0.1701,0.0109,0.2843,0.07,0.0257,0.1779,0.6086,0.3704,0.1719,0.7961,0.4132,0.0391,0.0046,0.1495,0.131,0.3166,0.005,0.2023,0.5085,0.2287,0.0029,0.3707,0.0305,0.1723,0.2047,0.0191,0.0639,0.315,0.0394,0.7709,0.5391,0.4196,0.2317,0.3806,0,0.576,0.417,0.3966,0.4135,0.2817,0.0029,0.3457,0.2293,0.1042,0.307,0.2321,0.6691,0.2338,0.0375,0.0327,0.2439,0.0029,0.9322,0.2359,0.5642,0.3197,0.4446,0.5186,0.1385,0.3562,0.3636,0.3426,0.5886,0.3884,0.181,0.0239,0.0149,0.0545,0.1797,0.4276,0.0767,0.3812,0.0219,0.1113,0.0413,0.1748,0.5742,0.0255,0.5453,0.9124,0.0386,0.4673,0.1641,0.054,0.1589,0.6949,0.0014,0.782,0.2635,0.031,0.6018,0.7487,0.0636,0.0036,0.4167,0.3086,0.71,0.3635,0.3673,0.496,0.5576,0.3818,0.0136,0.7158,0.0131,0.4604,0.0427,0.734,0.0657,0.1977,0.171,0.0038,0.2202,0.4281,0.0698,0.0149,0.731,0.1476,0.245,0.6108,0.0408,0.3433,0.5462,0.0598,0.0471,0.3003,0.3646,0.2401,0.6389,0.3757,0.256,0.0046,0.4003,0.3749,0.0067,0.6098,0.406,0.7722,0.1677,0.87,0.0993,0.0071,0.0034,0.8099,0.4689,0,0.0381,0.0091,0.0375,0.1712,0.4523,0.2946,0.1129,0.9051,0.0421,0.216,0.9298,0.0017,0.325,0.491,0.2844,0.2813,0.2767,0.4335,0.7181,0.7268,0.4523,0.3105,0.1845,0.1296,0.0019,0.0084,0.0489,0.7016,0.2623,0.2425,0.0877,0.4781,0.0736,0.0558,0.7523,0.7277,0.721,0.4739,0.3711,0.0006,0.1743,0.1185,0.8605,0,0.0988,0.332,0.5265,0.0236,0.0036,0.2718,0.4076,0.8295,0.4084,0.0324,0.2703,0.366,0.9076,0.0439,0.3114,0.7099,0.3592,0.1699,0.7625,0.0148,0.1881,0.8523,0.1322,0.0176,0.4156,0.1551,0.0415,0.6249,0.1628,0.2686,0.0107,0.2998,0.0019,0.0382,0.0531,0.1534,0.038,0.0319,0.0722,0.0797,0.0402,0.0104,0.0798,0.0388,0.0102,0.0006,0.3674,0.9261,0.2886,0.0555,0.0161,0.505,0.286,0.2918,0.6153,0.7253,0.0867,0.4141,0.0497,0.0106,0.6874,0.2735,0.4685,0.2381,0.2307,0.2103,0.0283,0.4959,0.0521,0.0628,0.2544,0.4226,0.5233,0.1679,0.7953,0.4026,0.1666,0.1826,0.073,0.3991,0.1135,0.0066,0.4446,0.3404,0.2209,0.0029,0.0562,0.5029,0.1897,0.8033,0.0363,0.6645,0.0931,0.9091,0.008,0.0396,0.3366,0.8319,0.8422,0.0029,0.028,0.007,0.2287,0.4117,0.0014,0.9687,0.7414,0.7579,0.4103,0.6731,0.1555,0.71,0.2966,0.0721,0.2177,0.4703,0.3086,0.1951,0.4386,0.4051,0.0029,0.0937,0.0029,0.5315,0.5731,0.1328,0.3567,0.0667,0.1993,0.3517,0.071,0.372,0.0696,0.0073,0.6324,0.0753,0.0214,0.0893,0.4858,0.2087,0.0104,0.3186,0.6619,0.016,0.4729,0.1272,0.0238,0.3212,0.2522,0.1537,0.0109,0.3901,0.0611,0.2937,0.2405,0.0086,0.0652,0.3285,0.3866,0.9134,0.0273,0.2007,0.0086,0.5225,0.1939,0.3313,0.0878,0.4896,0.2689,0.0322,0.0425,0.2372,0.1621,0.0171,0.045,0.4271,0.5496,0.165,0.0115,0,0.4816,0.0964,0.3664,0.1844,0.5323,0.1327,0.252,0.1104,0.5908,0.1023,0.0574,0.695,0.8439,0.1408,0.4267,0.1532,0.0206,0.023,0.2882,0.2095,0.469,0.395,0.5026,0.011,0.2339,0.7224,0.9062,0.0976,0.0847,0.0017,0.4599,0.0381,0.0249,0.7199,0.1723,0.2709,0.411,0.8673,0.0474,0.0429,0.2525,0.0088,0.4367,0.6271,0.802,0.3974,0.1705,0.0096,0.0893,0.0102,0.5594,0.4423,0.0646,0.4344,0.0474,0.7545,0.2822,0.8837,0.0086,0.6941,0.426,0.2026,0.0234,0.2827,0.1058,0.0821,0.6868,0.1717,0,0.2146,0.2807,0.0681,0.033,0.6965,0.2709,0.2033,0.0121,0.3507,0.1876,0.5688,0.396,0.0926,0.361,0.3142,0.6453,0.0179,0.1733,0.1074,0.0067,0.0511,0.1221,0.1937,0.6847,0.0462,0.0932,0.0331,0.8152,0.0311,0.6615,0.0555,0.0527,0.1445,0.8059,0.5083,0.5113,0.8518,0.0014,0.3987,0.5985,0.1274,0.1886,0.7864,0.0048,0.2044,0.4517,0.04,0.3173,0.0979,0.3952,0.6401,0.0129,0.7062,0.0417,0.7134,0.2632,0.4806,0.6636,0.0072,0.0201,0.0141,0.8288,0.8028,0.0205,0,0.016,0.0597,0.0272,0.0447,0.0371,0.7311,0.1563,0.0057,0.7771,0.7671,0.2187,0.0468,0.7539,0.0226,0.227,0.041,0.1803,0.0014,0.3382,0.5277,0.2345,0.3108,0.006,0.5486,0.1526,0,0.6064,0.0564,0.7198,0.6055,0.321,0.1658,0.7754,0.1439,0.0891,0.3072,0.0318,0.3796,0.7137,0.1969,0.032,0.4351,0.0266,0.3364,0.0073,0.4033,0.5503,0.5251,0.0394,0.0137,0.0455,0.9381,0.0356,0.4279,0.1175,0.0186,0.1738,0,0.364,0.0668,0.6836,0.4563,0.6005,0.0136,0.0017,0.3514,0.0131,0.6882,0.006,0.9243,0.7873,0.7352,0.0461,0.24,0.3266,0.1446,0.6191,0.0079,0,0.927,0.0876,0.7689,0.5317,0.0141,0.5267,0.0853,0.361,0.4145,0.3018,0.155,0.3385,0.4001,0.0253,0.0468,0.6044,0.5067,0.511,0.1854,0.161,0.5636,0.3622,0.7958,0.3991,0.245,0.4246,0.1399,0.0143,0.7689,0.0069,0.0094,0.5042,0.597,0.004,0.0146,0.2294,0.0504,0.9288,0.1062,0.4336,0.011,0.2143,0.1598,0.9165,0.7406,0.6799,0.0262,0.4503,0.0746,0.1624,0.3563,0.2575,0.7289,0.3613,0.0189,0.1628,0.0875,0.2042,0.0389,0.7169,0.9226,0.4394,0.4615,0.2643,0.1206,0.3429,0.0617,0.2885,0.2788,0.2628,0.2439,0.08,0.8335,0.2177,0.051,0.0807,0.8647,0.2963,0.2938,0.1682,0.32,0.0669,0.5376,0.8913,0.4897,0.5551,0.0395,0.6188,0.0948,0.071,0.6106,0.1497,0.2997,0.004,0.1224,0.6408,0.2173,0.1641,0.2294,0.1646,0.034,0.343,0.2656,0.2691,0.4618,0.1811,0.3349,0.0061,0.7569,0.216,0.3106,0.1003,0.4768,0.2598,0.6136,0.1952,0.2517,0.2575,0.6684,0.5093,0.3353,0.0157,0.0666,0.7557,0.1676,0,0.0053,0.2623,0.0264,0.487,0.5102,0.0095,0.0214,0.0029,0.0383,0.0835,0.1156,0.2384,0.0885,0.447,0.0718,0.0383,0.3292,0.0306,0.1845,0.7378,0.3108,0.7826,0.2186,0.0223,0.0597,0.1646,0.0807,0.0187,0.3197,0.0136,0.7716,0.003,0.0992,0.2935,0.0525,0.2551,0.0107,0.2149] \n",
      "\n",
      "[1] \"Successfully submitted. Below you can see the details of your submission\"\n",
      "$url\n",
      "[1] \"http://46.101.121.83/submission/517/\"\n",
      "\n",
      "$submission\n",
      "[1] \"[0.044, 0.1919, 0.3808, 0.0376, 0.6268, 0.1811, 0.4023, 0.4962, 0.0909, 0, 0.246, 0.3773, 0.0195, 0.9128, 0.163, 0.0121, 0.0132, 0.1483, 0.1487, 0.0095, 0.0749, 0.377, 0.9418, 0.0048, 0.5931, 0.0772, 0.4872, 0.462, 0.4333, 0.2635, 0.435, 0.7113, 0.0171, 0.4223, 0.6632, 0.2353, 0.6974, 0.7086, 0.9206, 0.0226, 0.1768, 0.138, 0.1328, 0.8132, 0.2217, 0.8547, 0.1919, 0.0438, 0.2917, 0.1374, 0.0139, 0.0129, 0.1447, 0.4104, 0.8504, 0.2875, 0.7568, 0.5675, 0.094, 0.5144, 0.8696, 0.0057, 0.0714, 0.1812, 0.0535, 0.0584, 0.0006, 0.0171, 0.626, 0.1798, 0.5747, 0.0245, 0.0407, 0.6902, 0.0747, 0.2492, 0.709, 0.0419, 0.485, 0.0794, 0.9332, 0.08, 0.0021, 0.5281, 0.2354, 0.1627, 0.001, 0.0671, 0.8447, 0.2576, 0.0163, 0.0183, 0.086, 0.713, 0.1028, 0.41, 0.1568, 0.075, 0.3473, 0.4645, 0.568, 0.033, 0.5982, 0.3616, 0.3432, 0.3757, 0.0126, 0.2529, 0.1035, 0.0261, 0.0245, 0.8756, 0.151, 0.7659, 0.2479, 0.009, 0.0138, 0.3477, 0.0344, 0.0234, 0.0103, 0.0591, 0.009, 0.12, 0.1737, 0.0332, 0.0326, 0.2865, 0.6593, 0.0126, 0.585, 0.3826, 0.3121, 0.4244, 0.4145, 0.5882, 0.0721, 0.7065, 0.1115, 0.7342, 0.3849, 0.2206, 0.289, 0.3242, 0.3239, 0.1875, 0.7195, 0.1386, 0.2697, 0.0962, 0.037, 0.3898, 0.2427, 0.1495, 0.0057, 0.0927, 0.0282, 0.5624, 0.3712, 0.916, 0.1726, 0.1979, 0.0319, 0.03, 0.0129, 0.6241, 0.0662, 0.4937, 0.0181, 0.1188, 0.7356, 0.3249, 0.3491, 0.0395, 0.4127, 0.0107, 0.2198, 0.3001, 0.5716, 0.8909, 0.7695, 0.7083, 0.4272, 0.0129, 0.862, 0.024, 0.0029, 0.241, 0.1974, 0.2026, 0, 0.4473, 0.6303, 0.1186, 0.0067, 0, 0.0477, 0.0114, 0.031, 0.7235, 0.374, 0.1576, 0.2061, 0.5192, 0.2613, 0.6264, 0.0287, 0.9589, 0.0915, 0.7157, 0.0171, 0, 0.0733, 0.5182, 0.1423, 0.7456, 0.1545, 0.0339, 0.0091, 0.0651, 0.0541, 0.0091, 0.3487, 0.658, 0.0194, 0.1841, 0.1123, 0.8099, 0.2129, 0.0084, 0.8726, 0.031, 0.3779, 0.0561, 0.005, 0.0305, 0.3212, 0.1199, 0.6005, 0.6998, 0.0619, 0.0095, 0.0106, 0.0431, 0.062, 0.0389, 0.5071, 0.6237, 0.51, 0.0019, 0.0462, 0.7915, 0.7791, 0.4381, 0.0565, 0.5482, 0.8283, 0.1946, 0.2059, 0.1523, 0.5157, 0.2416, 0.0705, 0.2841, 0.6539, 0.3375, 0.0202, 0.1586, 0.1109, 0.098, 0.3214, 0.2123, 0.2646, 0.1706, 0.0512, 0.2662, 0.6474, 0.0587, 0.3012, 0.1052, 0.3717, 0.0566, 0.2462, 0.1199, 0.678, 0.3488, 0.0734, 0.3852, 0.6414, 0.563, 0.3853, 0.0873, 0.3265, 0.554, 0.2854, 0.8857, 0.5486, 0.209, 0.0412, 0.0812, 0.8563, 0.0226, 0.0439, 0, 0.049, 0.0373, 0.0789, 0.1648, 0.0355, 0.0064, 0.0958, 0.5146, 0.6992, 0.0326, 0.0218, 0.7109, 0.2441, 0.1217, 0.5106, 0.539, 0.0133, 0.1911, 0.0172, 0.3315, 0.2151, 0.544, 0.631, 0.5492, 0.0415, 0.3193, 0.4728, 0.2365, 0.5292, 0.6479, 0.8458, 0.2441, 0.4965, 0.728, 0.012, 0.1479, 0.1154, 0.1096, 0.054, 0.2754, 0.3553, 0.0136, 0.1479, 0.196, 0.0374, 0.0676, 0.0345, 0.0174, 0.1611, 0.8123, 0.1431, 0.3812, 0.792, 0.1853, 0.1051, 0.3056, 0.0691, 0.0235, 0.074, 0.1612, 0.458, 0.3772, 0.6458, 0.0219, 0.0412, 0.0696, 0.7284, 0.064, 0.0064, 0.0625, 0.1275, 0.709, 0.0663, 0.0433, 0.0021, 0.2386, 0.2228, 0.5473, 0.0241, 0.8175, 0.1605, 0.041, 0.0086, 0.3627, 0.0648, 0.4384, 0.0203, 0.0694, 0.8133, 0.121, 0.0431, 0.3355, 0.1057, 0.053, 0.0066, 0.0196, 0.1277, 0.8019, 0.1785, 0.8942, 0.0837, 0.4389, 0.11, 0.2699, 0.685, 0.05, 0.2139, 0.0328, 0.5413, 0.0242, 0.5571, 0.4382, 0.2029, 0.8076, 0.3501, 0.6797, 0.3713, 0.6718, 0.0217, 0.0466, 0.1757, 0.3976, 0.1387, 0.526, 0.1558, 0.457, 0.4614, 0.0724, 0.3275, 0.3051, 0.4369, 0.2301, 0.0023, 0.2245, 0.6939, 0.3444, 0.0567, 0.139, 0.0855, 0.8801, 0.1974, 0.564, 0.0029, 0.0029, 0.7171, 0.0464, 0.5374, 0.261, 0.0389, 0.7923, 0.1853, 0.0074, 0.3865, 0.4111, 0.845, 0.571, 0.0522, 0.085, 0.1537, 0.8105, 0.0672, 0.3338, 0.1056, 0.0135, 0.0434, 0.5809, 0.341, 0.7139, 0.1031, 0.0604, 0.5032, 0.014, 0.1627, 0.7408, 0.3321, 0.0171, 0.0029, 0.1035, 0.8208, 0.1342, 0.0029, 0.157, 0.5404, 0.0171, 0.3431, 0.0818, 0.5954, 0.3603, 0.6661, 0.6998, 0.1002, 0.0211, 0.7565, 0.0029, 0.8127, 0.114, 0.166, 0.1365, 0.0038, 0.0182, 0.116, 0.0086, 0.3265, 0.6251, 0.1199, 0.021, 0.4279, 0.4143, 0.0224, 0.0227, 0.0306, 0.3074, 0.0509, 0.0877, 0.525, 0.459, 0.0048, 0.0303, 0.4036, 0.0057, 0.262, 0.3729, 0.0594, 0.8012, 0.1766, 0.0086, 0.2519, 0.0074, 0.5106, 0.7654, 0.2977, 0.0088, 0.9505, 0.6727, 0.2735, 0.2243, 0.1856, 0.9164, 0.0245, 0.1, 0.0958, 0.0423, 0.6262, 0.762, 0.0267, 0.5086, 0.0057, 0.0342, 0.0886, 0.8172, 0.5166, 0.6098, 0.0882, 0.0409, 0.0534, 0.3828, 0.0567, 0.072, 0.0541, 0.1403, 0.2719, 0.0124, 0.2724, 0.0555, 0.5116, 0.3344, 0.013, 0.2222, 0.2089, 0.3286, 0.2089, 0.4534, 0.5348, 0.196, 0.0801, 0.0931, 0.0067, 0.008, 0.4036, 0.5206, 0.2694, 0.4781, 0.0176, 0.1239, 0.236, 0.0587, 0.571, 0.2154, 0.0778, 0, 0.0467, 0.6717, 0.0275, 0.03, 0.0369, 0.5427, 0.0768, 0.3106, 0.6122, 0.0524, 0.0799, 0.2925, 0.869, 0.1171, 0.0778, 0.1233, 0.2462, 0.4672, 0.6538, 0.161, 0.0936, 0.5751, 0.036, 0.3087, 0.1774, 0.9661, 0.1347, 0.5351, 0.9165, 0.5965, 0.276, 0.2948, 0.0555, 0.0339, 0.3359, 0.0309, 0.3446, 0.1135, 0.6305, 0.238, 0.2784, 0.0029, 0.3704, 0.1649, 0.0196, 0.2568, 0.3202, 0.0955, 0.0056, 0.2369, 0.1278, 0.0164, 0.0946, 0.0704, 0.6458, 0.4346, 0.1754, 0.0545, 0.472, 0.4376, 0.3764, 0.1535, 0.6859, 0.6369, 0.5758, 0.2898, 0.268, 0.0232, 0.0395, 0.6001, 0.212, 0.1396, 0.6137, 0.0417, 0, 0.7357, 0.3514, 0.1143, 0, 0.7542, 0.426, 0.4528, 0.011, 0.6422, 0.4939, 0.3979, 0.0324, 0.9283, 0.0872, 0.2775, 0.0963, 0.661, 0.4032, 0.483, 0.1514, 0.1908, 0.0289, 0.112, 0.5058, 0.3845, 0.0367, 0.4626, 0.0467, 0.0438, 0.0238, 0.1929, 0.0484, 0.2798, 0.7393, 0.1844, 0.6615, 0.6345, 0.3696, 0.034, 0.3426, 0.0417, 0.4157, 0.9534, 0.0457, 0.0057, 0.1578, 0.4692, 0.5211, 0.6325, 0.914, 0.0314, 0.1819, 0.4417, 0.7023, 0.0378, 0.042, 0.02, 0.388, 0.4754, 0.6024, 0.0551, 0.4678, 0.6523, 0.2084, 0.0816, 0.0029, 0.0672, 0.0209, 0.5198, 0.0914, 0.0234, 0.0036, 0.098, 0.7032, 0.0993, 0.903, 0.1576, 0.6247, 0.719, 0.0093, 0.2037, 0.0122, 0.0328, 0.114, 0.2698, 0.0569, 0.7397, 0.1631, 0.0394, 0.0123, 0.0183, 0.2696, 0.5478, 0.0841, 0.2394, 0.9088, 0.0987, 0.0034, 0.0188, 0.5458, 0.2863, 0.0157, 0.0346, 0.5796, 0.8854, 0.5847, 0.1168, 0.903, 0.3473, 0.0279, 0.4331, 0.1301, 0.0034, 0.5276, 0, 0.0205, 0.4164, 0.5123, 0.3404, 0.1404, 0.4717, 0, 0.277, 0.6995, 0.0249, 0.1566, 0.0084, 0.0034, 0.0114, 0.0346, 0.3334, 0.2137, 0.0086, 0.2034, 0.1784, 0.2485, 0.0899, 0.5545, 0.052, 0.1036, 0.6676, 0.4058, 0.0801, 0, 0.084, 0.3667, 0.5765, 0.1672, 0.2195, 0.4775, 0.0204, 0.235, 0.0269, 0.6348, 0.527, 0.0519, 0.0303, 0.7535, 0.005, 0.2138, 0.4239, 0.0351, 0.0294, 0.7398, 0.5371, 0.8926, 0.5201, 0.1582, 0.4189, 0.0576, 0.6613, 0.2186, 0.7946, 0.0225, 0.8159, 0.6851, 0.6812, 0.0287, 0.1629, 0.0384, 0.3563, 0.0547, 0.5806, 0.6394, 0.0163, 0.8574, 0.2428, 0.0376, 0, 0, 0.6222, 0.0089, 0.0962, 0.1371, 0.3306, 0.5819, 0.049, 0.1148, 0.505, 0.4841, 0.0908, 0.07, 0.0179, 0.0013, 0.2397, 0.4281, 0.4359, 0.1358, 0.3894, 0.6238, 0.8164, 0.9794, 0.4035, 0.6696, 0.2108, 0.0727, 0.09, 0.0253, 0.8442, 0.2775, 0.5258, 0.6778, 0.6812, 0.203, 0.2334, 0.244, 0.1229, 0.1852, 0.2919, 0.196, 0.0923, 0.1479, 0.7025, 0.0174, 0.4277, 0.669, 0.07, 0.1662, 0.0134, 0.4862, 0.4728, 0.0348, 0.8184, 0.774, 0.4501, 0.0462, 0.0158, 0.5314, 0.5184, 0.5977, 0.1097, 0.0083, 0.7762, 0.3883, 0.8249, 0.0983, 0.0029, 0.3073, 0.1829, 0.2689, 0.6214, 0.9627, 0.0694, 0.1224, 0.81, 0.2653, 0.0974, 0.5219, 0.0828, 0.0747, 0.2397, 0.3952, 0.5882, 0.4807, 0.0243, 0.2277, 0.0514, 0.0183, 0.9745, 0.8681, 0.7324, 0.2898, 0.3042, 0.0673, 0.8725, 0.1779, 0.3882, 0.0263, 0.0038, 0.8225, 0.0725, 0.0442, 0.2486, 0.0048, 0.0647, 0.0401, 0.4605, 0.1584, 0.0683, 0.3823, 0.6096, 0.0534, 0.2668, 0.7521, 0.5067, 0.2317, 0.0483, 0.9434, 0.3413, 0.3009, 0.7482, 0.913, 0.6722, 0.4956, 0.5397, 0.666, 0.5271, 0.3177, 0.4692, 0.4723, 0.0029, 0.0796, 0.031, 0.055, 0.2655, 0.0849, 0.883, 0.4299, 0.0134, 0.0017, 0.6702, 0.0296, 0.2436, 0.3641, 0.365, 0.2831, 0.8591, 0.6761, 0.0112, 0.0019, 0.0779, 0.4184, 0.2128, 0.0935, 0.5889, 0.0852, 0.7315, 0.2887, 0.3181, 0.1985, 0.6679, 0.1843, 0.0057, 0.0257, 0, 0.0093, 0.1786, 0.3173, 0.3894, 0.3521, 0.0063, 0.1221, 0.5036, 0.8642, 0.4439, 0.4619, 0.028, 0.0926, 0.5077, 0.0493, 0.1116, 0.8739, 0.582, 0.8647, 0.0378, 0.0157, 0.0524, 0.1978, 0.1364, 0.8155, 0.4948, 0.423, 0.0154, 0.407, 0.0136, 0.0906, 0.4091, 0.0122, 0.0263, 0.2822, 0.0291, 0, 0.0371, 0.399, 0.2134, 0.3312, 0.2229, 0.2953, 0.0051, 0.6474, 0.8068, 0.2837, 0.6282, 0.6047, 0.2216, 0.1195, 0.0772, 0.3297, 0.0133, 0.4211, 0.6686, 0.645, 0.0662, 0.0291, 0.6034, 0.6207, 0.01, 0.3298, 0.3364, 0.9794, 0.2813, 0.6004, 0.7723, 0.7971, 0.068, 0.2189, 0.2003, 0.3442, 0.6144, 0.0588, 0.9314, 0.2051, 0.0136, 0.1447, 0.5753, 0.1669, 0.2097, 0.523, 0.1176, 0.0725, 0.6272, 0.0019, 0.1736, 0.2505, 0.301, 0.463, 0.5403, 0.6512, 0.0721, 0.6777, 0.0379, 0.3123, 0.2092, 0.0604, 0.7734, 0.475, 0.1827, 0.0163, 0.0504, 0.0316, 0.0704, 0.9113, 0.3199, 0.1485, 0.127, 0.0029, 0.1644, 0.1766, 0.0528, 0.4422, 0.0969, 0.5621, 0.0063, 0.7189, 0.1203, 0.0086, 0.6747, 0.7526, 0.2195, 0.1581, 0.0442, 0.0465, 0.5192, 0.0471, 0.3521, 0.0793, 0.0147, 0.1127, 0.6905, 0.101, 0.828, 0.797, 0.188, 0.7176, 0.3209, 0.663, 0.6361, 0.1056, 0.6247, 0.2806, 0.1472, 0.1899, 0.5644, 0.4067, 0.2082, 0.4577, 0.0148, 0.2729, 0.2912, 0.0905, 0.3372, 0.084, 0.5124, 0.0095, 0.6228, 0.0122, 0.325, 0.1363, 0.0473, 0.0048, 0.1016, 0.3133, 0.6054, 0.242, 0.0333, 0.0123, 0.1094, 0.4504, 0.0739, 0.1191, 0.2721, 0.4573, 0.0889, 0.3022, 0.0647, 0.1304, 0.026, 0.331, 0.0284, 0.0868, 0.0692, 0.046, 0.5793, 0.0391, 0.0272, 0.9439, 0.6922, 0.7793, 0.1215, 0.1318, 0.1516, 0.5061, 0.013, 0.0355, 0.5285, 0.026, 0.0804, 0.5611, 0.4982, 0.2557, 0.0215, 0.0505, 0.6782, 0.6819, 0.0395, 0.0945, 0.1539, 0.2241, 0.8618, 0.6985, 0.3681, 0.226, 0, 0.0131, 0.6989, 0.4267, 0.0288, 0.5813, 0.0291, 0.22, 0.2861, 0.6881, 0.9526, 0.3096, 0.2651, 0.165, 0.9087, 0.0225, 0.0347, 0.3718, 0.0167, 0.2887, 0.5008, 0.0203, 0.0381, 0.4887, 0.0426, 0.1813, 0.4454, 0.0731, 0.4484, 0.0332, 0.0204, 0.4904, 0.0958, 0.3841, 0.8014, 0.2939, 0.4933, 0.0223, 0.0556, 0.0193, 0.595, 0.013, 0.0057, 0.075, 0.0173, 0.4942, 0.0483, 0.7778, 0.1189, 0.2036, 0.854, 0.4898, 0.6785, 0.0279, 0.8237, 0.7683, 0.2008, 0.012, 0.3862, 0.5654, 0.0029, 0.2751, 0.3935, 0.306, 0.7439, 0.0519, 0.0357, 0.7211, 0, 0.1279, 0.109, 0.1494, 0.0057, 0.3867, 0.4721, 0.0048, 0.1669, 0.538, 0.6726, 0.8037, 0.1924, 0.4304, 0.2946, 0.0057, 0.5016, 0.1701, 0.0109, 0.2843, 0.07, 0.0257, 0.1779, 0.6086, 0.3704, 0.1719, 0.7961, 0.4132, 0.0391, 0.0046, 0.1495, 0.131, 0.3166, 0.005, 0.2023, 0.5085, 0.2287, 0.0029, 0.3707, 0.0305, 0.1723, 0.2047, 0.0191, 0.0639, 0.315, 0.0394, 0.7709, 0.5391, 0.4196, 0.2317, 0.3806, 0, 0.576, 0.417, 0.3966, 0.4135, 0.2817, 0.0029, 0.3457, 0.2293, 0.1042, 0.307, 0.2321, 0.6691, 0.2338, 0.0375, 0.0327, 0.2439, 0.0029, 0.9322, 0.2359, 0.5642, 0.3197, 0.4446, 0.5186, 0.1385, 0.3562, 0.3636, 0.3426, 0.5886, 0.3884, 0.181, 0.0239, 0.0149, 0.0545, 0.1797, 0.4276, 0.0767, 0.3812, 0.0219, 0.1113, 0.0413, 0.1748, 0.5742, 0.0255, 0.5453, 0.9124, 0.0386, 0.4673, 0.1641, 0.054, 0.1589, 0.6949, 0.0014, 0.782, 0.2635, 0.031, 0.6018, 0.7487, 0.0636, 0.0036, 0.4167, 0.3086, 0.71, 0.3635, 0.3673, 0.496, 0.5576, 0.3818, 0.0136, 0.7158, 0.0131, 0.4604, 0.0427, 0.734, 0.0657, 0.1977, 0.171, 0.0038, 0.2202, 0.4281, 0.0698, 0.0149, 0.731, 0.1476, 0.245, 0.6108, 0.0408, 0.3433, 0.5462, 0.0598, 0.0471, 0.3003, 0.3646, 0.2401, 0.6389, 0.3757, 0.256, 0.0046, 0.4003, 0.3749, 0.0067, 0.6098, 0.406, 0.7722, 0.1677, 0.87, 0.0993, 0.0071, 0.0034, 0.8099, 0.4689, 0, 0.0381, 0.0091, 0.0375, 0.1712, 0.4523, 0.2946, 0.1129, 0.9051, 0.0421, 0.216, 0.9298, 0.0017, 0.325, 0.491, 0.2844, 0.2813, 0.2767, 0.4335, 0.7181, 0.7268, 0.4523, 0.3105, 0.1845, 0.1296, 0.0019, 0.0084, 0.0489, 0.7016, 0.2623, 0.2425, 0.0877, 0.4781, 0.0736, 0.0558, 0.7523, 0.7277, 0.721, 0.4739, 0.3711, 0.0006, 0.1743, 0.1185, 0.8605, 0, 0.0988, 0.332, 0.5265, 0.0236, 0.0036, 0.2718, 0.4076, 0.8295, 0.4084, 0.0324, 0.2703, 0.366, 0.9076, 0.0439, 0.3114, 0.7099, 0.3592, 0.1699, 0.7625, 0.0148, 0.1881, 0.8523, 0.1322, 0.0176, 0.4156, 0.1551, 0.0415, 0.6249, 0.1628, 0.2686, 0.0107, 0.2998, 0.0019, 0.0382, 0.0531, 0.1534, 0.038, 0.0319, 0.0722, 0.0797, 0.0402, 0.0104, 0.0798, 0.0388, 0.0102, 0.0006, 0.3674, 0.9261, 0.2886, 0.0555, 0.0161, 0.505, 0.286, 0.2918, 0.6153, 0.7253, 0.0867, 0.4141, 0.0497, 0.0106, 0.6874, 0.2735, 0.4685, 0.2381, 0.2307, 0.2103, 0.0283, 0.4959, 0.0521, 0.0628, 0.2544, 0.4226, 0.5233, 0.1679, 0.7953, 0.4026, 0.1666, 0.1826, 0.073, 0.3991, 0.1135, 0.0066, 0.4446, 0.3404, 0.2209, 0.0029, 0.0562, 0.5029, 0.1897, 0.8033, 0.0363, 0.6645, 0.0931, 0.9091, 0.008, 0.0396, 0.3366, 0.8319, 0.8422, 0.0029, 0.028, 0.007, 0.2287, 0.4117, 0.0014, 0.9687, 0.7414, 0.7579, 0.4103, 0.6731, 0.1555, 0.71, 0.2966, 0.0721, 0.2177, 0.4703, 0.3086, 0.1951, 0.4386, 0.4051, 0.0029, 0.0937, 0.0029, 0.5315, 0.5731, 0.1328, 0.3567, 0.0667, 0.1993, 0.3517, 0.071, 0.372, 0.0696, 0.0073, 0.6324, 0.0753, 0.0214, 0.0893, 0.4858, 0.2087, 0.0104, 0.3186, 0.6619, 0.016, 0.4729, 0.1272, 0.0238, 0.3212, 0.2522, 0.1537, 0.0109, 0.3901, 0.0611, 0.2937, 0.2405, 0.0086, 0.0652, 0.3285, 0.3866, 0.9134, 0.0273, 0.2007, 0.0086, 0.5225, 0.1939, 0.3313, 0.0878, 0.4896, 0.2689, 0.0322, 0.0425, 0.2372, 0.1621, 0.0171, 0.045, 0.4271, 0.5496, 0.165, 0.0115, 0, 0.4816, 0.0964, 0.3664, 0.1844, 0.5323, 0.1327, 0.252, 0.1104, 0.5908, 0.1023, 0.0574, 0.695, 0.8439, 0.1408, 0.4267, 0.1532, 0.0206, 0.023, 0.2882, 0.2095, 0.469, 0.395, 0.5026, 0.011, 0.2339, 0.7224, 0.9062, 0.0976, 0.0847, 0.0017, 0.4599, 0.0381, 0.0249, 0.7199, 0.1723, 0.2709, 0.411, 0.8673, 0.0474, 0.0429, 0.2525, 0.0088, 0.4367, 0.6271, 0.802, 0.3974, 0.1705, 0.0096, 0.0893, 0.0102, 0.5594, 0.4423, 0.0646, 0.4344, 0.0474, 0.7545, 0.2822, 0.8837, 0.0086, 0.6941, 0.426, 0.2026, 0.0234, 0.2827, 0.1058, 0.0821, 0.6868, 0.1717, 0, 0.2146, 0.2807, 0.0681, 0.033, 0.6965, 0.2709, 0.2033, 0.0121, 0.3507, 0.1876, 0.5688, 0.396, 0.0926, 0.361, 0.3142, 0.6453, 0.0179, 0.1733, 0.1074, 0.0067, 0.0511, 0.1221, 0.1937, 0.6847, 0.0462, 0.0932, 0.0331, 0.8152, 0.0311, 0.6615, 0.0555, 0.0527, 0.1445, 0.8059, 0.5083, 0.5113, 0.8518, 0.0014, 0.3987, 0.5985, 0.1274, 0.1886, 0.7864, 0.0048, 0.2044, 0.4517, 0.04, 0.3173, 0.0979, 0.3952, 0.6401, 0.0129, 0.7062, 0.0417, 0.7134, 0.2632, 0.4806, 0.6636, 0.0072, 0.0201, 0.0141, 0.8288, 0.8028, 0.0205, 0, 0.016, 0.0597, 0.0272, 0.0447, 0.0371, 0.7311, 0.1563, 0.0057, 0.7771, 0.7671, 0.2187, 0.0468, 0.7539, 0.0226, 0.227, 0.041, 0.1803, 0.0014, 0.3382, 0.5277, 0.2345, 0.3108, 0.006, 0.5486, 0.1526, 0, 0.6064, 0.0564, 0.7198, 0.6055, 0.321, 0.1658, 0.7754, 0.1439, 0.0891, 0.3072, 0.0318, 0.3796, 0.7137, 0.1969, 0.032, 0.4351, 0.0266, 0.3364, 0.0073, 0.4033, 0.5503, 0.5251, 0.0394, 0.0137, 0.0455, 0.9381, 0.0356, 0.4279, 0.1175, 0.0186, 0.1738, 0, 0.364, 0.0668, 0.6836, 0.4563, 0.6005, 0.0136, 0.0017, 0.3514, 0.0131, 0.6882, 0.006, 0.9243, 0.7873, 0.7352, 0.0461, 0.24, 0.3266, 0.1446, 0.6191, 0.0079, 0, 0.927, 0.0876, 0.7689, 0.5317, 0.0141, 0.5267, 0.0853, 0.361, 0.4145, 0.3018, 0.155, 0.3385, 0.4001, 0.0253, 0.0468, 0.6044, 0.5067, 0.511, 0.1854, 0.161, 0.5636, 0.3622, 0.7958, 0.3991, 0.245, 0.4246, 0.1399, 0.0143, 0.7689, 0.0069, 0.0094, 0.5042, 0.597, 0.004, 0.0146, 0.2294, 0.0504, 0.9288, 0.1062, 0.4336, 0.011, 0.2143, 0.1598, 0.9165, 0.7406, 0.6799, 0.0262, 0.4503, 0.0746, 0.1624, 0.3563, 0.2575, 0.7289, 0.3613, 0.0189, 0.1628, 0.0875, 0.2042, 0.0389, 0.7169, 0.9226, 0.4394, 0.4615, 0.2643, 0.1206, 0.3429, 0.0617, 0.2885, 0.2788, 0.2628, 0.2439, 0.08, 0.8335, 0.2177, 0.051, 0.0807, 0.8647, 0.2963, 0.2938, 0.1682, 0.32, 0.0669, 0.5376, 0.8913, 0.4897, 0.5551, 0.0395, 0.6188, 0.0948, 0.071, 0.6106, 0.1497, 0.2997, 0.004, 0.1224, 0.6408, 0.2173, 0.1641, 0.2294, 0.1646, 0.034, 0.343, 0.2656, 0.2691, 0.4618, 0.1811, 0.3349, 0.0061, 0.7569, 0.216, 0.3106, 0.1003, 0.4768, 0.2598, 0.6136, 0.1952, 0.2517, 0.2575, 0.6684, 0.5093, 0.3353, 0.0157, 0.0666, 0.7557, 0.1676, 0, 0.0053, 0.2623, 0.0264, 0.487, 0.5102, 0.0095, 0.0214, 0.0029, 0.0383, 0.0835, 0.1156, 0.2384, 0.0885, 0.447, 0.0718, 0.0383, 0.3292, 0.0306, 0.1845, 0.7378, 0.3108, 0.7826, 0.2186, 0.0223, 0.0597, 0.1646, 0.0807, 0.0187, 0.3197, 0.0136, 0.7716, 0.003, 0.0992, 0.2935, 0.0525, 0.2551, 0.0107, 0.2149]\"\n",
      "\n",
      "$user\n",
      "$user$url\n",
      "[1] \"http://46.101.121.83/group/11/\"\n",
      "\n",
      "$user$username\n",
      "[1] \"Los Galacticos\"\n",
      "\n",
      "$user$best_score\n",
      "[1] 0.8775\n",
      "\n",
      "$user$students\n",
      "[1] \"2019702165;2019702174;2020702024\"\n",
      "\n",
      "\n",
      "$competition\n",
      "[1] \"IE582-Test Data\"\n",
      "\n",
      "$auc\n",
      "[1] 0.9207634\n",
      "\n",
      "$ber\n",
      "[1] 0.8167842\n",
      "\n",
      "$score\n",
      "[1] 0.8687738\n",
      "\n",
      "$date\n",
      "[1] \"2021-02-12T23:38:28.554428+03:00\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "send_submission(predictions, token, url=subm_url, submit_now= TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "imptable = data.table(varImp(RF12)$importance)\n",
    "imptable[,variable := 1:dim(imptable)[1] ]\n",
    "imptable <- imptable[order(Overall ,decreasing = TRUE),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Overall</th><th scope=col>variable</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>6.15339019</td><td>47        </td></tr>\n",
       "\t<tr><td>6.11382437</td><td>51        </td></tr>\n",
       "\t<tr><td>5.72927550</td><td>34        </td></tr>\n",
       "\t<tr><td>4.79350303</td><td>28        </td></tr>\n",
       "\t<tr><td>4.50524972</td><td>45        </td></tr>\n",
       "\t<tr><td>4.41258077</td><td>35        </td></tr>\n",
       "\t<tr><td>3.50020710</td><td>13        </td></tr>\n",
       "\t<tr><td>2.89943254</td><td>21        </td></tr>\n",
       "\t<tr><td>2.80804545</td><td>33        </td></tr>\n",
       "\t<tr><td>2.55859462</td><td>22        </td></tr>\n",
       "\t<tr><td>2.02576231</td><td>19        </td></tr>\n",
       "\t<tr><td>1.93695083</td><td>31        </td></tr>\n",
       "\t<tr><td>1.51329144</td><td>55        </td></tr>\n",
       "\t<tr><td>1.35365919</td><td>18        </td></tr>\n",
       "\t<tr><td>1.32269691</td><td>60        </td></tr>\n",
       "\t<tr><td>1.19626638</td><td>43        </td></tr>\n",
       "\t<tr><td>0.94714446</td><td>29        </td></tr>\n",
       "\t<tr><td>0.36653718</td><td>46        </td></tr>\n",
       "\t<tr><td>0.32662909</td><td>49        </td></tr>\n",
       "\t<tr><td>0.09556463</td><td>26        </td></tr>\n",
       "\t<tr><td>0.07712401</td><td>59        </td></tr>\n",
       "\t<tr><td>0.02723138</td><td>57        </td></tr>\n",
       "\t<tr><td>0.00000000</td><td>37        </td></tr>\n",
       "\t<tr><td>0.00000000</td><td>50        </td></tr>\n",
       "\t<tr><td>0.00000000</td><td>52        </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " Overall & variable\\\\\n",
       "\\hline\n",
       "\t 6.15339019 & 47        \\\\\n",
       "\t 6.11382437 & 51        \\\\\n",
       "\t 5.72927550 & 34        \\\\\n",
       "\t 4.79350303 & 28        \\\\\n",
       "\t 4.50524972 & 45        \\\\\n",
       "\t 4.41258077 & 35        \\\\\n",
       "\t 3.50020710 & 13        \\\\\n",
       "\t 2.89943254 & 21        \\\\\n",
       "\t 2.80804545 & 33        \\\\\n",
       "\t 2.55859462 & 22        \\\\\n",
       "\t 2.02576231 & 19        \\\\\n",
       "\t 1.93695083 & 31        \\\\\n",
       "\t 1.51329144 & 55        \\\\\n",
       "\t 1.35365919 & 18        \\\\\n",
       "\t 1.32269691 & 60        \\\\\n",
       "\t 1.19626638 & 43        \\\\\n",
       "\t 0.94714446 & 29        \\\\\n",
       "\t 0.36653718 & 46        \\\\\n",
       "\t 0.32662909 & 49        \\\\\n",
       "\t 0.09556463 & 26        \\\\\n",
       "\t 0.07712401 & 59        \\\\\n",
       "\t 0.02723138 & 57        \\\\\n",
       "\t 0.00000000 & 37        \\\\\n",
       "\t 0.00000000 & 50        \\\\\n",
       "\t 0.00000000 & 52        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Overall | variable |\n",
       "|---|---|\n",
       "| 6.15339019 | 47         |\n",
       "| 6.11382437 | 51         |\n",
       "| 5.72927550 | 34         |\n",
       "| 4.79350303 | 28         |\n",
       "| 4.50524972 | 45         |\n",
       "| 4.41258077 | 35         |\n",
       "| 3.50020710 | 13         |\n",
       "| 2.89943254 | 21         |\n",
       "| 2.80804545 | 33         |\n",
       "| 2.55859462 | 22         |\n",
       "| 2.02576231 | 19         |\n",
       "| 1.93695083 | 31         |\n",
       "| 1.51329144 | 55         |\n",
       "| 1.35365919 | 18         |\n",
       "| 1.32269691 | 60         |\n",
       "| 1.19626638 | 43         |\n",
       "| 0.94714446 | 29         |\n",
       "| 0.36653718 | 46         |\n",
       "| 0.32662909 | 49         |\n",
       "| 0.09556463 | 26         |\n",
       "| 0.07712401 | 59         |\n",
       "| 0.02723138 | 57         |\n",
       "| 0.00000000 | 37         |\n",
       "| 0.00000000 | 50         |\n",
       "| 0.00000000 | 52         |\n",
       "\n"
      ],
      "text/plain": [
       "   Overall    variable\n",
       "1  6.15339019 47      \n",
       "2  6.11382437 51      \n",
       "3  5.72927550 34      \n",
       "4  4.79350303 28      \n",
       "5  4.50524972 45      \n",
       "6  4.41258077 35      \n",
       "7  3.50020710 13      \n",
       "8  2.89943254 21      \n",
       "9  2.80804545 33      \n",
       "10 2.55859462 22      \n",
       "11 2.02576231 19      \n",
       "12 1.93695083 31      \n",
       "13 1.51329144 55      \n",
       "14 1.35365919 18      \n",
       "15 1.32269691 60      \n",
       "16 1.19626638 43      \n",
       "17 0.94714446 29      \n",
       "18 0.36653718 46      \n",
       "19 0.32662909 49      \n",
       "20 0.09556463 26      \n",
       "21 0.07712401 59      \n",
       "22 0.02723138 57      \n",
       "23 0.00000000 37      \n",
       "24 0.00000000 50      \n",
       "25 0.00000000 52      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tail(imptable,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Try 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### To Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4147</li>\n",
       "\t<li>60</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4147\n",
       "\\item 60\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4147\n",
       "2. 60\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 4147   60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alllng <- dim(alldata[,-length(alldata)])\n",
    "sbmlng <- dim(submitdata)\n",
    "\n",
    "bigdata <- rbind(alldata[,-length(alldata)],submitdata)\n",
    "bigscaled <- bigdata\n",
    "dim(bigscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols <- c(1,8,9,10,11,14,27,30,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for(c in cols){\n",
    "    bigscaled[,c] <- to.uniform(bigscaled[,c])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>x1</th><th scope=col>x2</th><th scope=col>x3</th><th scope=col>x4</th><th scope=col>x5</th><th scope=col>x6</th><th scope=col>x7</th><th scope=col>x8</th><th scope=col>x9</th><th scope=col>x10</th><th scope=col>...</th><th scope=col>x51</th><th scope=col>x52</th><th scope=col>x53</th><th scope=col>x54</th><th scope=col>x55</th><th scope=col>x56</th><th scope=col>x57</th><th scope=col>x58</th><th scope=col>x59</th><th scope=col>x60</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.2823728 </td><td>1         </td><td>1         </td><td>1         </td><td>18        </td><td> 3        </td><td> 1        </td><td>0.40414758</td><td>0.59175307</td><td>0.7598264 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.5408729 </td><td>0         </td><td>1         </td><td>1         </td><td>18        </td><td>13        </td><td> 3        </td><td>0.02218471</td><td>0.43163733</td><td>0.6520376 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.9404389 </td><td>0         </td><td>1         </td><td>1         </td><td> 1        </td><td> 3        </td><td>14        </td><td>0.74704606</td><td>0.86279238</td><td>0.6279238 </td><td>...       </td><td>1         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.4574391 </td><td>0         </td><td>1         </td><td>1         </td><td>14        </td><td> 9        </td><td> 3        </td><td>0.47070171</td><td>0.04244032</td><td>0.6233422 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.7716422 </td><td>1         </td><td>1         </td><td>0         </td><td> 2        </td><td>15        </td><td>12        </td><td>0.95249578</td><td>0.27803231</td><td>0.9438148 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.7716422 </td><td>0         </td><td>0         </td><td>1         </td><td> 5        </td><td> 5        </td><td>12        </td><td>0.26814565</td><td>0.71256330</td><td>0.7451170 </td><td>...       </td><td>0         </td><td>0         </td><td>1         </td><td>1         </td><td>0         </td><td>0         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       " x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x51 & x52 & x53 & x54 & x55 & x56 & x57 & x58 & x59 & x60\\\\\n",
       "\\hline\n",
       "\t 0.2823728  & 1          & 1          & 1          & 18         &  3         &  1         & 0.40414758 & 0.59175307 & 0.7598264  & ...        & 0          & 0          & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.5408729  & 0          & 1          & 1          & 18         & 13         &  3         & 0.02218471 & 0.43163733 & 0.6520376  & ...        & 0          & 0          & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.9404389  & 0          & 1          & 1          &  1         &  3         & 14         & 0.74704606 & 0.86279238 & 0.6279238  & ...        & 1          & 0          & 1          & 0          & 0          & 1          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.4574391  & 0          & 1          & 1          & 14         &  9         &  3         & 0.47070171 & 0.04244032 & 0.6233422  & ...        & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.7716422  & 1          & 1          & 0          &  2         & 15         & 12         & 0.95249578 & 0.27803231 & 0.9438148  & ...        & 0          & 0          & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.7716422  & 0          & 0          & 1          &  5         &  5         & 12         & 0.26814565 & 0.71256330 & 0.7451170  & ...        & 0          & 0          & 1          & 1          & 0          & 0          & 0          & 1          & 0          & 0         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 | x9 | x10 | ... | x51 | x52 | x53 | x54 | x55 | x56 | x57 | x58 | x59 | x60 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.2823728  | 1          | 1          | 1          | 18         |  3         |  1         | 0.40414758 | 0.59175307 | 0.7598264  | ...        | 0          | 0          | 0          | 0          | 0          | 1          | 0          | 0          | 0          | 0          |\n",
       "| 0.5408729  | 0          | 1          | 1          | 18         | 13         |  3         | 0.02218471 | 0.43163733 | 0.6520376  | ...        | 0          | 0          | 0          | 0          | 0          | 1          | 0          | 0          | 0          | 0          |\n",
       "| 0.9404389  | 0          | 1          | 1          |  1         |  3         | 14         | 0.74704606 | 0.86279238 | 0.6279238  | ...        | 1          | 0          | 1          | 0          | 0          | 1          | 0          | 0          | 0          | 0          |\n",
       "| 0.4574391  | 0          | 1          | 1          | 14         |  9         |  3         | 0.47070171 | 0.04244032 | 0.6233422  | ...        | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          |\n",
       "| 0.7716422  | 1          | 1          | 0          |  2         | 15         | 12         | 0.95249578 | 0.27803231 | 0.9438148  | ...        | 0          | 0          | 0          | 0          | 0          | 1          | 0          | 0          | 0          | 0          |\n",
       "| 0.7716422  | 0          | 0          | 1          |  5         |  5         | 12         | 0.26814565 | 0.71256330 | 0.7451170  | ...        | 0          | 0          | 1          | 1          | 0          | 0          | 0          | 1          | 0          | 0          |\n",
       "\n"
      ],
      "text/plain": [
       "  x1        x2 x3 x4 x5 x6 x7 x8         x9         x10       ... x51 x52 x53\n",
       "1 0.2823728 1  1  1  18  3  1 0.40414758 0.59175307 0.7598264 ... 0   0   0  \n",
       "2 0.5408729 0  1  1  18 13  3 0.02218471 0.43163733 0.6520376 ... 0   0   0  \n",
       "3 0.9404389 0  1  1   1  3 14 0.74704606 0.86279238 0.6279238 ... 1   0   1  \n",
       "4 0.4574391 0  1  1  14  9  3 0.47070171 0.04244032 0.6233422 ... 0   0   0  \n",
       "5 0.7716422 1  1  0   2 15 12 0.95249578 0.27803231 0.9438148 ... 0   0   0  \n",
       "6 0.7716422 0  0  1   5  5 12 0.26814565 0.71256330 0.7451170 ... 0   0   1  \n",
       "  x54 x55 x56 x57 x58 x59 x60\n",
       "1 0   0   1   0   0   0   0  \n",
       "2 0   0   1   0   0   0   0  \n",
       "3 0   0   1   0   0   0   0  \n",
       "4 0   0   0   0   0   0   0  \n",
       "5 0   0   1   0   0   0   0  \n",
       "6 1   0   0   0   1   0   0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(bigscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Column 36 & 42 to Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bigscaled[bigscaled$x42 >= 1,'x42'] = 1\n",
    "bigscaled[bigscaled$x36 >= 1,'x36'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# bigscaled[1:alllng[1],]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### As.Factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nonbinarycols <- cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t4147 obs. of  60 variables:\n",
      " $ x1 : num  0.282 0.541 0.94 0.457 0.772 ...\n",
      " $ x2 : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 2 2 2 1 ...\n",
      " $ x3 : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 1 1 2 2 2 ...\n",
      " $ x4 : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 1 2 2 2 2 1 ...\n",
      " $ x5 : Factor w/ 19 levels \"0\",\"1\",\"2\",\"3\",..: 19 19 2 15 3 6 17 14 1 9 ...\n",
      " $ x6 : Factor w/ 19 levels \"0\",\"1\",\"2\",\"3\",..: 4 14 4 10 16 6 2 5 1 19 ...\n",
      " $ x7 : Factor w/ 19 levels \"0\",\"1\",\"2\",\"3\",..: 2 4 15 4 13 13 3 18 3 19 ...\n",
      " $ x8 : num  0.4041 0.0222 0.747 0.4707 0.9525 ...\n",
      " $ x9 : num  0.5918 0.4316 0.8628 0.0424 0.278 ...\n",
      " $ x10: num  0.76 0.652 0.628 0.623 0.944 ...\n",
      " $ x11: num  0.609 0.646 0.51 0.276 0.785 ...\n",
      " $ x12: Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 1 1 1 1 2 ...\n",
      " $ x13: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x14: num  0.7 0.155 0.784 0.237 0.7 ...\n",
      " $ x15: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 1 2 1 2 2 2 ...\n",
      " $ x16: Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 2 1 ...\n",
      " $ x17: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x18: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x19: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x20: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x21: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x22: Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 1 1 1 1 ...\n",
      " $ x23: Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 1 2 1 1 1 ...\n",
      " $ x24: Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 2 2 1 ...\n",
      " $ x25: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x26: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n",
      " $ x27: num  0.462 0.579 0.793 0.968 0.897 ...\n",
      " $ x28: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x29: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x30: num  0.442 0.672 0.442 0.672 0.442 ...\n",
      " $ x31: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x32: num  0.455 0.158 0.517 0.878 0.204 ...\n",
      " $ x33: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x34: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 1 1 1 1 ...\n",
      " $ x35: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x36: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x37: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x38: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x39: Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n",
      " $ x40: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n",
      " $ x41: Factor w/ 2 levels \"0\",\"1\": 2 2 1 2 2 1 2 2 2 2 ...\n",
      " $ x42: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x43: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n",
      " $ x44: Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 2 2 1 2 2 ...\n",
      " $ x45: Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x46: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x47: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x48: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x49: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x50: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x51: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 1 1 1 1 1 ...\n",
      " $ x52: Factor w/ 1 level \"0\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x53: Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n",
      " $ x54: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 2 1 2 ...\n",
      " $ x55: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x56: Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 1 2 1 1 1 ...\n",
      " $ x57: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x58: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 2 1 2 ...\n",
      " $ x59: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x60: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n"
     ]
    }
   ],
   "source": [
    "for(c in colnames(bigscaled[,-nonbinarycols])  ){\n",
    "    bigscaled[,c] = as.factor(bigscaled[,c])\n",
    "}\n",
    "str(bigscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x5:x7 Dummies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>x1</th><th scope=col>x2</th><th scope=col>x3</th><th scope=col>x4</th><th scope=col>x5</th><th scope=col>x6</th><th scope=col>x7</th><th scope=col>x8</th><th scope=col>x9</th><th scope=col>x10</th><th scope=col>...</th><th scope=col>x7.9</th><th scope=col>x7.10</th><th scope=col>x7.11</th><th scope=col>x7.12</th><th scope=col>x7.13</th><th scope=col>x7.14</th><th scope=col>x7.15</th><th scope=col>x7.16</th><th scope=col>x7.17</th><th scope=col>x7.18</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.2823728 </td><td>1         </td><td>1         </td><td>1         </td><td>18        </td><td>3         </td><td>1         </td><td>0.40414758</td><td>0.59175307</td><td>0.7598264 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.5408729 </td><td>0         </td><td>1         </td><td>1         </td><td>18        </td><td>13        </td><td>3         </td><td>0.02218471</td><td>0.43163733</td><td>0.6520376 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.9404389 </td><td>0         </td><td>1         </td><td>1         </td><td>1         </td><td>3         </td><td>14        </td><td>0.74704606</td><td>0.86279238</td><td>0.6279238 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.4574391 </td><td>0         </td><td>1         </td><td>1         </td><td>14        </td><td>9         </td><td>3         </td><td>0.47070171</td><td>0.04244032</td><td>0.6233422 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.7716422 </td><td>1         </td><td>1         </td><td>0         </td><td>2         </td><td>15        </td><td>12        </td><td>0.95249578</td><td>0.27803231</td><td>0.9438148 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "\t<tr><td>0.7716422 </td><td>0         </td><td>0         </td><td>1         </td><td>5         </td><td>5         </td><td>12        </td><td>0.26814565</td><td>0.71256330</td><td>0.7451170 </td><td>...       </td><td>0         </td><td>0         </td><td>0         </td><td>1         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td><td>0         </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       " x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & x10 & ... & x7.9 & x7.10 & x7.11 & x7.12 & x7.13 & x7.14 & x7.15 & x7.16 & x7.17 & x7.18\\\\\n",
       "\\hline\n",
       "\t 0.2823728  & 1          & 1          & 1          & 18         & 3          & 1          & 0.40414758 & 0.59175307 & 0.7598264  & ...        & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.5408729  & 0          & 1          & 1          & 18         & 13         & 3          & 0.02218471 & 0.43163733 & 0.6520376  & ...        & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.9404389  & 0          & 1          & 1          & 1          & 3          & 14         & 0.74704606 & 0.86279238 & 0.6279238  & ...        & 0          & 0          & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.4574391  & 0          & 1          & 1          & 14         & 9          & 3          & 0.47070171 & 0.04244032 & 0.6233422  & ...        & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.7716422  & 1          & 1          & 0          & 2          & 15         & 12         & 0.95249578 & 0.27803231 & 0.9438148  & ...        & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 0          & 0          & 0         \\\\\n",
       "\t 0.7716422  & 0          & 0          & 1          & 5          & 5          & 12         & 0.26814565 & 0.71256330 & 0.7451170  & ...        & 0          & 0          & 0          & 1          & 0          & 0          & 0          & 0          & 0          & 0         \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| x1 | x2 | x3 | x4 | x5 | x6 | x7 | x8 | x9 | x10 | ... | x7.9 | x7.10 | x7.11 | x7.12 | x7.13 | x7.14 | x7.15 | x7.16 | x7.17 | x7.18 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 0.2823728  | 1          | 1          | 1          | 18         | 3          | 1          | 0.40414758 | 0.59175307 | 0.7598264  | ...        | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          |\n",
       "| 0.5408729  | 0          | 1          | 1          | 18         | 13         | 3          | 0.02218471 | 0.43163733 | 0.6520376  | ...        | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          |\n",
       "| 0.9404389  | 0          | 1          | 1          | 1          | 3          | 14         | 0.74704606 | 0.86279238 | 0.6279238  | ...        | 0          | 0          | 0          | 0          | 0          | 1          | 0          | 0          | 0          | 0          |\n",
       "| 0.4574391  | 0          | 1          | 1          | 14         | 9          | 3          | 0.47070171 | 0.04244032 | 0.6233422  | ...        | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          | 0          |\n",
       "| 0.7716422  | 1          | 1          | 0          | 2          | 15         | 12         | 0.95249578 | 0.27803231 | 0.9438148  | ...        | 0          | 0          | 0          | 1          | 0          | 0          | 0          | 0          | 0          | 0          |\n",
       "| 0.7716422  | 0          | 0          | 1          | 5          | 5          | 12         | 0.26814565 | 0.71256330 | 0.7451170  | ...        | 0          | 0          | 0          | 1          | 0          | 0          | 0          | 0          | 0          | 0          |\n",
       "\n"
      ],
      "text/plain": [
       "  x1        x2 x3 x4 x5 x6 x7 x8         x9         x10       ... x7.9 x7.10\n",
       "1 0.2823728 1  1  1  18 3  1  0.40414758 0.59175307 0.7598264 ... 0    0    \n",
       "2 0.5408729 0  1  1  18 13 3  0.02218471 0.43163733 0.6520376 ... 0    0    \n",
       "3 0.9404389 0  1  1  1  3  14 0.74704606 0.86279238 0.6279238 ... 0    0    \n",
       "4 0.4574391 0  1  1  14 9  3  0.47070171 0.04244032 0.6233422 ... 0    0    \n",
       "5 0.7716422 1  1  0  2  15 12 0.95249578 0.27803231 0.9438148 ... 0    0    \n",
       "6 0.7716422 0  0  1  5  5  12 0.26814565 0.71256330 0.7451170 ... 0    0    \n",
       "  x7.11 x7.12 x7.13 x7.14 x7.15 x7.16 x7.17 x7.18\n",
       "1 0     0     0     0     0     0     0     0    \n",
       "2 0     0     0     0     0     0     0     0    \n",
       "3 0     0     0     1     0     0     0     0    \n",
       "4 0     0     0     0     0     0     0     0    \n",
       "5 0     1     0     0     0     0     0     0    \n",
       "6 0     1     0     0     0     0     0     0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummied <- dummyVars(~x5+x6+x7,data = bigscaled)\n",
    "bigscaled <- cbind(bigscaled, predict(dummied,bigscaled))\n",
    "head(bigscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigscaled <- bigscaled[,-c(5,6,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'x5.0'"
      ],
      "text/latex": [
       "'x5.0'"
      ],
      "text/markdown": [
       "'x5.0'"
      ],
      "text/plain": [
       "[1] \"x5.0\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colnames(bigscaled)[58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t4147 obs. of  114 variables:\n",
      " $ x1   : num  0.282 0.541 0.94 0.457 0.772 ...\n",
      " $ x2   : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 2 2 2 1 ...\n",
      " $ x3   : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 1 1 2 2 2 ...\n",
      " $ x4   : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 1 2 2 2 2 1 ...\n",
      " $ x8   : num  0.4041 0.0222 0.747 0.4707 0.9525 ...\n",
      " $ x9   : num  0.5918 0.4316 0.8628 0.0424 0.278 ...\n",
      " $ x10  : num  0.76 0.652 0.628 0.623 0.944 ...\n",
      " $ x11  : num  0.609 0.646 0.51 0.276 0.785 ...\n",
      " $ x12  : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 1 1 1 1 2 ...\n",
      " $ x13  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x14  : num  0.7 0.155 0.784 0.237 0.7 ...\n",
      " $ x15  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 1 2 1 2 2 2 ...\n",
      " $ x16  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 2 1 ...\n",
      " $ x17  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x18  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x19  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x20  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x21  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x22  : Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 1 1 1 1 ...\n",
      " $ x23  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 1 2 1 1 1 ...\n",
      " $ x24  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 2 2 1 ...\n",
      " $ x25  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x26  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n",
      " $ x27  : num  0.462 0.579 0.793 0.968 0.897 ...\n",
      " $ x28  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x29  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x30  : num  0.442 0.672 0.442 0.672 0.442 ...\n",
      " $ x31  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x32  : num  0.455 0.158 0.517 0.878 0.204 ...\n",
      " $ x33  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x34  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 1 1 1 1 ...\n",
      " $ x35  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x36  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x37  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x38  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x39  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n",
      " $ x40  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n",
      " $ x41  : Factor w/ 2 levels \"0\",\"1\": 2 2 1 2 2 1 2 2 2 2 ...\n",
      " $ x42  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x43  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n",
      " $ x44  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 2 2 1 2 2 ...\n",
      " $ x45  : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x46  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x47  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x48  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x49  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x50  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x51  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 1 1 1 1 1 ...\n",
      " $ x52  : Factor w/ 1 level \"0\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x53  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n",
      " $ x54  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 2 1 2 ...\n",
      " $ x55  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x56  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 1 2 1 1 1 ...\n",
      " $ x57  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x58  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 2 1 2 ...\n",
      " $ x59  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x60  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.0 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n",
      " $ x5.1 : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 1 1 1 1 1 ...\n",
      " $ x5.2 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n",
      " $ x5.3 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.4 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.5 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x5.6 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.7 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.8 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n",
      " $ x5.9 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.10: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.11: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.12: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.13: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n",
      " $ x5.14: Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n",
      " $ x5.15: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.16: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x5.17: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.18: Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.0 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n",
      " $ x6.1 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x6.2 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.3 : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 1 1 1 ...\n",
      " $ x6.4 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n",
      " $ x6.5 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x6.6 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.7 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.8 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.9 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n",
      " $ x6.10: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.11: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.12: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.13: Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.14: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.15: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n",
      " $ x6.16: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.17: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.18: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n",
      " $ x7.0 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x7.1 : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x7.2 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 2 1 ...\n",
      " $ x7.3 : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 1 1 ...\n",
      "  [list output truncated]\n"
     ]
    }
   ],
   "source": [
    "for(c in 58:length(bigscaled)  ){\n",
    "    bigscaled[,c] = as.factor(bigscaled[,c])\n",
    "}\n",
    "str(bigscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>2074</li>\n",
       "\t<li>115</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2074\n",
       "\\item 115\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2074\n",
       "2. 115\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2074  115"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>2073</li>\n",
       "\t<li>114</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2073\n",
       "\\item 114\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2073\n",
       "2. 114\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2073  114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "allproc <- cbind(bigscaled[1:alllng[1] ,],alldata$y)\n",
    "submitproc <- bigscaled[ -(1:alllng[1]) ,]\n",
    "dim(allproc); dim(submitproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>x1</th><th scope=col>x2</th><th scope=col>x3</th><th scope=col>x4</th><th scope=col>x8</th><th scope=col>x9</th><th scope=col>x10</th><th scope=col>x11</th><th scope=col>x12</th><th scope=col>x13</th><th scope=col>...</th><th scope=col>x7.9</th><th scope=col>x7.10</th><th scope=col>x7.11</th><th scope=col>x7.12</th><th scope=col>x7.13</th><th scope=col>x7.14</th><th scope=col>x7.15</th><th scope=col>x7.16</th><th scope=col>x7.17</th><th scope=col>x7.18</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2075</th><td>0.1063419  </td><td>1          </td><td>1          </td><td>1          </td><td>0.05883771 </td><td>0.3457921  </td><td>0.009404389</td><td>0.03858211 </td><td>0          </td><td>0          </td><td>...        </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>2076</th><td>0.4574391  </td><td>1          </td><td>0          </td><td>1          </td><td>0.16180371 </td><td>0.3277068  </td><td>0.993006993</td><td>0.64962624 </td><td>0          </td><td>0          </td><td>...        </td><td>0          </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>2077</th><td>0.8256571  </td><td>1          </td><td>1          </td><td>1          </td><td>0.40414758 </td><td>0.4914396  </td><td>0.060284543</td><td>0.49095732 </td><td>0          </td><td>0          </td><td>...        </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>2078</th><td>0.6255124  </td><td>1          </td><td>1          </td><td>0          </td><td>0.79551483 </td><td>0.9713046  </td><td>0.882324572</td><td>0.98625512 </td><td>1          </td><td>0          </td><td>...        </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>2079</th><td>0.3684591  </td><td>1          </td><td>1          </td><td>1          </td><td>0.47070171 </td><td>0.9898722  </td><td>0.884977092</td><td>0.87822522 </td><td>0          </td><td>0          </td><td>...        </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td></tr>\n",
       "\t<tr><th scope=row>2080</th><td>0.8256571  </td><td>1          </td><td>0          </td><td>1          </td><td>0.40414758 </td><td>0.2794791  </td><td>0.622136484</td><td>0.83120328 </td><td>0          </td><td>0          </td><td>...        </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>0          </td><td>1          </td><td>0          </td><td>0          </td><td>0          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & x1 & x2 & x3 & x4 & x8 & x9 & x10 & x11 & x12 & x13 & ... & x7.9 & x7.10 & x7.11 & x7.12 & x7.13 & x7.14 & x7.15 & x7.16 & x7.17 & x7.18\\\\\n",
       "\\hline\n",
       "\t2075 & 0.1063419   & 1           & 1           & 1           & 0.05883771  & 0.3457921   & 0.009404389 & 0.03858211  & 0           & 0           & ...         & 0           & 0           & 0           & 0           & 0           & 0           & 0           & 1           & 0           & 0          \\\\\n",
       "\t2076 & 0.4574391   & 1           & 0           & 1           & 0.16180371  & 0.3277068   & 0.993006993 & 0.64962624  & 0           & 0           & ...         & 0           & 0           & 0           & 1           & 0           & 0           & 0           & 0           & 0           & 0          \\\\\n",
       "\t2077 & 0.8256571   & 1           & 1           & 1           & 0.40414758  & 0.4914396   & 0.060284543 & 0.49095732  & 0           & 0           & ...         & 0           & 1           & 0           & 0           & 0           & 0           & 0           & 0           & 0           & 0          \\\\\n",
       "\t2078 & 0.6255124   & 1           & 1           & 0           & 0.79551483  & 0.9713046   & 0.882324572 & 0.98625512  & 1           & 0           & ...         & 0           & 0           & 0           & 0           & 0           & 0           & 0           & 0           & 0           & 0          \\\\\n",
       "\t2079 & 0.3684591   & 1           & 1           & 1           & 0.47070171  & 0.9898722   & 0.884977092 & 0.87822522  & 0           & 0           & ...         & 0           & 0           & 0           & 0           & 0           & 0           & 0           & 0           & 0           & 0          \\\\\n",
       "\t2080 & 0.8256571   & 1           & 0           & 1           & 0.40414758  & 0.2794791   & 0.622136484 & 0.83120328  & 0           & 0           & ...         & 0           & 0           & 0           & 0           & 0           & 0           & 1           & 0           & 0           & 0          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | x1 | x2 | x3 | x4 | x8 | x9 | x10 | x11 | x12 | x13 | ... | x7.9 | x7.10 | x7.11 | x7.12 | x7.13 | x7.14 | x7.15 | x7.16 | x7.17 | x7.18 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 2075 | 0.1063419   | 1           | 1           | 1           | 0.05883771  | 0.3457921   | 0.009404389 | 0.03858211  | 0           | 0           | ...         | 0           | 0           | 0           | 0           | 0           | 0           | 0           | 1           | 0           | 0           |\n",
       "| 2076 | 0.4574391   | 1           | 0           | 1           | 0.16180371  | 0.3277068   | 0.993006993 | 0.64962624  | 0           | 0           | ...         | 0           | 0           | 0           | 1           | 0           | 0           | 0           | 0           | 0           | 0           |\n",
       "| 2077 | 0.8256571   | 1           | 1           | 1           | 0.40414758  | 0.4914396   | 0.060284543 | 0.49095732  | 0           | 0           | ...         | 0           | 1           | 0           | 0           | 0           | 0           | 0           | 0           | 0           | 0           |\n",
       "| 2078 | 0.6255124   | 1           | 1           | 0           | 0.79551483  | 0.9713046   | 0.882324572 | 0.98625512  | 1           | 0           | ...         | 0           | 0           | 0           | 0           | 0           | 0           | 0           | 0           | 0           | 0           |\n",
       "| 2079 | 0.3684591   | 1           | 1           | 1           | 0.47070171  | 0.9898722   | 0.884977092 | 0.87822522  | 0           | 0           | ...         | 0           | 0           | 0           | 0           | 0           | 0           | 0           | 0           | 0           | 0           |\n",
       "| 2080 | 0.8256571   | 1           | 0           | 1           | 0.40414758  | 0.2794791   | 0.622136484 | 0.83120328  | 0           | 0           | ...         | 0           | 0           | 0           | 0           | 0           | 0           | 1           | 0           | 0           | 0           |\n",
       "\n"
      ],
      "text/plain": [
       "     x1        x2 x3 x4 x8         x9        x10         x11        x12 x13 ...\n",
       "2075 0.1063419 1  1  1  0.05883771 0.3457921 0.009404389 0.03858211 0   0   ...\n",
       "2076 0.4574391 1  0  1  0.16180371 0.3277068 0.993006993 0.64962624 0   0   ...\n",
       "2077 0.8256571 1  1  1  0.40414758 0.4914396 0.060284543 0.49095732 0   0   ...\n",
       "2078 0.6255124 1  1  0  0.79551483 0.9713046 0.882324572 0.98625512 1   0   ...\n",
       "2079 0.3684591 1  1  1  0.47070171 0.9898722 0.884977092 0.87822522 0   0   ...\n",
       "2080 0.8256571 1  0  1  0.40414758 0.2794791 0.622136484 0.83120328 0   0   ...\n",
       "     x7.9 x7.10 x7.11 x7.12 x7.13 x7.14 x7.15 x7.16 x7.17 x7.18\n",
       "2075 0    0     0     0     0     0     0     1     0     0    \n",
       "2076 0    0     0     1     0     0     0     0     0     0    \n",
       "2077 0    1     0     0     0     0     0     0     0     0    \n",
       "2078 0    0     0     0     0     0     0     0     0     0    \n",
       "2079 0    0     0     0     0     0     0     0     0     0    \n",
       "2080 0    0     0     0     0     0     1     0     0     0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(submitproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(allproc)[ length(allproc)] <- \"y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 52 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "allproc <- allproc[,-c(47,49)]\n",
    "submitproc <- submitproc[,-c(47,49)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t2074 obs. of  113 variables:\n",
      " $ x1   : num  0.282 0.541 0.94 0.457 0.772 ...\n",
      " $ x2   : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 2 1 2 2 2 1 ...\n",
      " $ x3   : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 1 1 2 2 2 ...\n",
      " $ x4   : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 1 2 2 2 2 1 ...\n",
      " $ x8   : num  0.4041 0.0222 0.747 0.4707 0.9525 ...\n",
      " $ x9   : num  0.5918 0.4316 0.8628 0.0424 0.278 ...\n",
      " $ x10  : num  0.76 0.652 0.628 0.623 0.944 ...\n",
      " $ x11  : num  0.609 0.646 0.51 0.276 0.785 ...\n",
      " $ x12  : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 1 1 1 1 2 ...\n",
      " $ x13  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x14  : num  0.7 0.155 0.784 0.237 0.7 ...\n",
      " $ x15  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 1 2 1 2 2 2 ...\n",
      " $ x16  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 2 1 ...\n",
      " $ x17  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x18  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x19  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x20  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x21  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x22  : Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 1 1 1 1 ...\n",
      " $ x23  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 1 2 1 1 1 ...\n",
      " $ x24  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 2 2 1 ...\n",
      " $ x25  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x26  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n",
      " $ x27  : num  0.462 0.579 0.793 0.968 0.897 ...\n",
      " $ x28  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x29  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x30  : num  0.442 0.672 0.442 0.672 0.442 ...\n",
      " $ x31  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x32  : num  0.455 0.158 0.517 0.878 0.204 ...\n",
      " $ x33  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x34  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 2 1 1 1 1 1 ...\n",
      " $ x35  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x36  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x37  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x38  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x39  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n",
      " $ x40  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n",
      " $ x41  : Factor w/ 2 levels \"0\",\"1\": 2 2 1 2 2 1 2 2 2 2 ...\n",
      " $ x42  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x43  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n",
      " $ x44  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 2 2 1 2 2 ...\n",
      " $ x45  : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x46  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x47  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x48  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x49  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x51  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 1 1 1 1 1 ...\n",
      " $ x53  : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 2 1 1 1 1 ...\n",
      " $ x54  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 2 1 2 ...\n",
      " $ x55  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x56  : Factor w/ 2 levels \"0\",\"1\": 2 2 2 1 2 1 2 1 1 1 ...\n",
      " $ x57  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x58  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 2 1 2 ...\n",
      " $ x59  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x60  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.0 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n",
      " $ x5.1 : Factor w/ 2 levels \"0\",\"1\": 1 1 2 1 1 1 1 1 1 1 ...\n",
      " $ x5.2 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n",
      " $ x5.3 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.4 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.5 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x5.6 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.7 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.8 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n",
      " $ x5.9 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.10: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.11: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.12: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.13: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n",
      " $ x5.14: Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n",
      " $ x5.15: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.16: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x5.17: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x5.18: Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.0 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n",
      " $ x6.1 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 1 1 ...\n",
      " $ x6.2 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.3 : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 1 1 1 ...\n",
      " $ x6.4 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n",
      " $ x6.5 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 1 ...\n",
      " $ x6.6 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.7 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.8 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.9 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 2 1 1 1 1 1 1 ...\n",
      " $ x6.10: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.11: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.12: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.13: Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.14: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.15: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 2 1 1 1 1 1 ...\n",
      " $ x6.16: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.17: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x6.18: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n",
      " $ x7.0 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x7.1 : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x7.2 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 2 1 2 1 ...\n",
      " $ x7.3 : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 1 1 ...\n",
      " $ x7.4 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ x7.5 : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n",
      "  [list output truncated]\n"
     ]
    }
   ],
   "source": [
    "str(allproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitControl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 5, summaryFunction = twoClassSummary,\n",
    "                           verboseIter = TRUE, classProbs = TRUE, sampling = \"up\")\n",
    "tunegrid <- expand.grid(.mtry=c(3,5,10,15), .splitrule = \"gini\", .min.node.size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 15, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "2074 samples\n",
       " 112 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1867, 1867, 1867, 1866, 1868, 1866, ... \n",
       "Addtional sampling using up-sampling\n",
       "\n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  ROC        Sens       Spec     \n",
       "   3    0.8652345  0.8323363  0.6947373\n",
       "   5    0.8679994  0.8739956  0.6227608\n",
       "  10    0.8704624  0.8853715  0.6149333\n",
       "  15    0.8705841  0.8849763  0.6090039\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 15, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(821)\n",
    "RFproc <- train(y~., data = allproc, method=\"ranger\",  num.trees = 250, metric = \"ROC\",\n",
    "              trControl = fitControl, tuneGrid = tunegrid, importance = 'permutation')\n",
    "RFproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.0384</li>\n",
       "\t<li>0.206466666666667</li>\n",
       "\t<li>0.441133333333333</li>\n",
       "\t<li>0.0484</li>\n",
       "\t<li>0.568552380952381</li>\n",
       "\t<li>0.417514285714286</li>\n",
       "\t<li>0.4343</li>\n",
       "\t<li>0.52650303030303</li>\n",
       "\t<li>0.0675833333333333</li>\n",
       "\t<li>0.00533333333333333</li>\n",
       "\t<li>0.218466666666667</li>\n",
       "\t<li>0.420156140350877</li>\n",
       "\t<li>0.0290888888888889</li>\n",
       "\t<li>0.860369696969697</li>\n",
       "\t<li>0.1028</li>\n",
       "\t<li>0.0872</li>\n",
       "\t<li>0.0203333333333333</li>\n",
       "\t<li>0.139133333333333</li>\n",
       "\t<li>0.255266666666667</li>\n",
       "\t<li>0.0226</li>\n",
       "\t<li>0.0876</li>\n",
       "\t<li>0.397933333333333</li>\n",
       "\t<li>0.8608</li>\n",
       "\t<li>0.0088</li>\n",
       "\t<li>0.63707619047619</li>\n",
       "\t<li>0.0766666666666667</li>\n",
       "\t<li>0.48610303030303</li>\n",
       "\t<li>0.5024</li>\n",
       "\t<li>0.589111111111111</li>\n",
       "\t<li>0.2398</li>\n",
       "\t<li>0.513533333333333</li>\n",
       "\t<li>0.848342857142857</li>\n",
       "\t<li>0.0223333333333333</li>\n",
       "\t<li>0.470066666666667</li>\n",
       "\t<li>0.627866666666667</li>\n",
       "\t<li>0.2705</li>\n",
       "\t<li>0.6392</li>\n",
       "\t<li>0.61310303030303</li>\n",
       "\t<li>0.808066666666667</li>\n",
       "\t<li>0.021</li>\n",
       "\t<li>0.169933333333333</li>\n",
       "\t<li>0.225533333333333</li>\n",
       "\t<li>0.110066666666667</li>\n",
       "\t<li>0.6982</li>\n",
       "\t<li>0.184533333333333</li>\n",
       "\t<li>0.739533333333333</li>\n",
       "\t<li>0.194266666666667</li>\n",
       "\t<li>0.0194</li>\n",
       "\t<li>0.3604</li>\n",
       "\t<li>0.1258</li>\n",
       "\t<li>0.0442666666666667</li>\n",
       "\t<li>0.013</li>\n",
       "\t<li>0.183522222222222</li>\n",
       "\t<li>0.345933333333333</li>\n",
       "\t<li>0.737066666666667</li>\n",
       "\t<li>0.217266666666667</li>\n",
       "\t<li>0.696066666666667</li>\n",
       "\t<li>0.608633333333333</li>\n",
       "\t<li>0.300047619047619</li>\n",
       "\t<li>0.505533333333333</li>\n",
       "\t<li>0.762161904761905</li>\n",
       "\t<li>0.0184</li>\n",
       "\t<li>0.233866666666667</li>\n",
       "\t<li>0.174866666666667</li>\n",
       "\t<li>0.0677333333333334</li>\n",
       "\t<li>0.0642666666666667</li>\n",
       "\t<li>0.0122</li>\n",
       "\t<li>0.0202</li>\n",
       "\t<li>0.530133333333333</li>\n",
       "\t<li>0.161866666666667</li>\n",
       "\t<li>0.464266666666667</li>\n",
       "\t<li>0.0173333333333333</li>\n",
       "\t<li>0.0146666666666667</li>\n",
       "\t<li>0.592533333333333</li>\n",
       "\t<li>0.217114285714286</li>\n",
       "\t<li>0.206933333333333</li>\n",
       "\t<li>0.702133333333333</li>\n",
       "\t<li>0.0404</li>\n",
       "\t<li>0.472933333333333</li>\n",
       "\t<li>0.141466666666667</li>\n",
       "\t<li>0.828</li>\n",
       "\t<li>0.251333333333333</li>\n",
       "\t<li>0.005</li>\n",
       "\t<li>0.544066666666667</li>\n",
       "\t<li>0.309155555555556</li>\n",
       "\t<li>0.237233333333333</li>\n",
       "\t<li>0.0673333333333333</li>\n",
       "\t<li>0.0736666666666667</li>\n",
       "\t<li>0.666933333333333</li>\n",
       "\t<li>0.3192</li>\n",
       "\t<li>0.01</li>\n",
       "\t<li>0.0812666666666667</li>\n",
       "\t<li>0.117</li>\n",
       "\t<li>0.761942857142857</li>\n",
       "\t<li>0.134533333333333</li>\n",
       "\t<li>0.442133333333333</li>\n",
       "\t<li>0.144688888888889</li>\n",
       "\t<li>0.161780952380952</li>\n",
       "\t<li>0.406633333333333</li>\n",
       "\t<li>0.478266666666667</li>\n",
       "\t<li>0.617744418739156</li>\n",
       "\t<li>0.0512833333333333</li>\n",
       "\t<li>0.640133333333333</li>\n",
       "\t<li>0.359933333333333</li>\n",
       "\t<li>0.357688888888889</li>\n",
       "\t<li>0.4352</li>\n",
       "\t<li>0.019</li>\n",
       "\t<li>0.2228</li>\n",
       "\t<li>0.1364</li>\n",
       "\t<li>0.0460666666666667</li>\n",
       "\t<li>0.0682</li>\n",
       "\t<li>0.808504761904762</li>\n",
       "\t<li>0.139</li>\n",
       "\t<li>0.701942857142857</li>\n",
       "\t<li>0.167466666666667</li>\n",
       "\t<li>0.0136666666666667</li>\n",
       "\t<li>0.00866666666666667</li>\n",
       "\t<li>0.410533333333333</li>\n",
       "\t<li>0.0343333333333333</li>\n",
       "\t<li>0.0256666666666667</li>\n",
       "\t<li>0.0332888888888889</li>\n",
       "\t<li>0.0331333333333333</li>\n",
       "\t<li>0.028</li>\n",
       "\t<li>0.0968666666666667</li>\n",
       "\t<li>0.148266666666667</li>\n",
       "\t<li>0.0133333333333333</li>\n",
       "\t<li>0.0024</li>\n",
       "\t<li>0.340619047619048</li>\n",
       "\t<li>0.764142857142857</li>\n",
       "\t<li>0.0472666666666667</li>\n",
       "\t<li>0.6416</li>\n",
       "\t<li>0.469333333333333</li>\n",
       "\t<li>0.315666666666667</li>\n",
       "\t<li>0.4174</li>\n",
       "\t<li>0.515495238095238</li>\n",
       "\t<li>0.659466666666667</li>\n",
       "\t<li>0.0756</li>\n",
       "\t<li>0.767434920634921</li>\n",
       "\t<li>0.147866666666667</li>\n",
       "\t<li>0.697466666666667</li>\n",
       "\t<li>0.434333333333333</li>\n",
       "\t<li>0.233622222222222</li>\n",
       "\t<li>0.253</li>\n",
       "\t<li>0.476933333333333</li>\n",
       "\t<li>0.407333333333333</li>\n",
       "\t<li>0.149266666666667</li>\n",
       "\t<li>0.639822222222222</li>\n",
       "\t<li>0.192733333333333</li>\n",
       "\t<li>0.211888888888889</li>\n",
       "\t<li>0.1926</li>\n",
       "\t<li>0.0542222222222222</li>\n",
       "\t<li>0.387866666666667</li>\n",
       "\t<li>0.206</li>\n",
       "\t<li>0.1688</li>\n",
       "\t<li>0.0301333333333333</li>\n",
       "\t<li>0.1112</li>\n",
       "\t<li>0.0293333333333333</li>\n",
       "\t<li>0.528615384615385</li>\n",
       "\t<li>0.3578</li>\n",
       "\t<li>0.918266666666667</li>\n",
       "\t<li>0.1302</li>\n",
       "\t<li>0.1702</li>\n",
       "\t<li>0.0486</li>\n",
       "\t<li>0.116066666666667</li>\n",
       "\t<li>0.0128571428571429</li>\n",
       "\t<li>0.663588278388278</li>\n",
       "\t<li>0.0739333333333333</li>\n",
       "\t<li>0.531066666666667</li>\n",
       "\t<li>0.034</li>\n",
       "\t<li>0.295866666666667</li>\n",
       "\t<li>0.646533333333333</li>\n",
       "\t<li>0.302966666666667</li>\n",
       "\t<li>0.394733333333333</li>\n",
       "\t<li>0.0288</li>\n",
       "\t<li>0.418847619047619</li>\n",
       "\t<li>0.007</li>\n",
       "\t<li>0.234266666666667</li>\n",
       "\t<li>0.325066666666667</li>\n",
       "\t<li>0.668761904761905</li>\n",
       "\t<li>0.849933333333333</li>\n",
       "\t<li>0.836266666666667</li>\n",
       "\t<li>0.903</li>\n",
       "\t<li>0.491688888888889</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.860288888888889</li>\n",
       "\t<li>0.0342666666666667</li>\n",
       "\t<li>0.0068</li>\n",
       "\t<li>0.203533333333333</li>\n",
       "\t<li>0.252933333333333</li>\n",
       "\t<li>0.366</li>\n",
       "\t<li>0.006</li>\n",
       "\t<li>0.4553</li>\n",
       "\t<li>0.601504761904762</li>\n",
       "\t<li>0.1954</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.0076</li>\n",
       "\t<li>0.0856666666666667</li>\n",
       "\t<li>0.0419333333333333</li>\n",
       "\t<li>0.0350666666666667</li>\n",
       "\t<li>0.751809523809524</li>\n",
       "\t<li>0.486</li>\n",
       "\t<li>0.166133333333333</li>\n",
       "\t<li>0.170533333333333</li>\n",
       "\t<li>0.561133333333333</li>\n",
       "\t<li>0.284969841269841</li>\n",
       "\t<li>0.6282</li>\n",
       "\t<li>0.0330666666666667</li>\n",
       "\t<li>0.8897</li>\n",
       "\t<li>0.0762666666666667</li>\n",
       "\t<li>0.829066666666667</li>\n",
       "\t<li>0.0540666666666667</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0879333333333333</li>\n",
       "\t<li>0.515066666666667</li>\n",
       "\t<li>0.135716666666667</li>\n",
       "\t<li>0.7348</li>\n",
       "\t<li>0.134466666666667</li>\n",
       "\t<li>0.0577333333333333</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.133133333333333</li>\n",
       "\t<li>0.0398666666666667</li>\n",
       "\t<li>0.0181333333333333</li>\n",
       "\t<li>0.386155555555556</li>\n",
       "\t<li>0.6736</li>\n",
       "\t<li>0.0676666666666667</li>\n",
       "\t<li>0.172380952380952</li>\n",
       "\t<li>0.406447619047619</li>\n",
       "\t<li>0.625933333333333</li>\n",
       "\t<li>0.244466666666667</li>\n",
       "\t<li>0.002</li>\n",
       "\t<li>0.806209523809524</li>\n",
       "\t<li>0.0403333333333333</li>\n",
       "\t<li>0.432918045112782</li>\n",
       "\t<li>0.0575333333333333</li>\n",
       "\t<li>0.0176666666666667</li>\n",
       "\t<li>0.0554</li>\n",
       "\t<li>0.322955555555555</li>\n",
       "\t<li>0.148266666666667</li>\n",
       "\t<li>0.588833333333333</li>\n",
       "\t<li>0.7321</li>\n",
       "\t<li>0.0332</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0454666666666667</li>\n",
       "\t<li>0.128733333333333</li>\n",
       "\t<li>0.0615333333333333</li>\n",
       "\t<li>0.0259333333333333</li>\n",
       "\t<li>0.2938</li>\n",
       "\t<li>0.604466666666667</li>\n",
       "\t<li>0.293180952380952</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0542</li>\n",
       "\t<li>0.624761904761905</li>\n",
       "\t<li>0.748666666666667</li>\n",
       "\t<li>0.37767619047619</li>\n",
       "\t<li>0.0690666666666667</li>\n",
       "\t<li>0.490733333333333</li>\n",
       "\t<li>0.8684</li>\n",
       "\t<li>0.20385974025974</li>\n",
       "\t<li>0.1262</li>\n",
       "\t<li>0.1322</li>\n",
       "\t<li>0.446662745098039</li>\n",
       "\t<li>0.271530952380952</li>\n",
       "\t<li>0.0333333333333333</li>\n",
       "\t<li>0.343066666666667</li>\n",
       "\t<li>0.583704273504274</li>\n",
       "\t<li>0.358555555555555</li>\n",
       "\t<li>0.0388</li>\n",
       "\t<li>0.145866666666667</li>\n",
       "\t<li>0.0588</li>\n",
       "\t<li>0.130066666666667</li>\n",
       "\t<li>0.378488888888889</li>\n",
       "\t<li>0.136696103896104</li>\n",
       "\t<li>0.349577777777778</li>\n",
       "\t<li>0.201533333333333</li>\n",
       "\t<li>0.078</li>\n",
       "\t<li>0.2348</li>\n",
       "\t<li>0.628266666666667</li>\n",
       "\t<li>0.0378</li>\n",
       "\t<li>0.27520404040404</li>\n",
       "\t<li>0.1202</li>\n",
       "\t<li>0.382</li>\n",
       "\t<li>0.101733333333333</li>\n",
       "\t<li>0.3042</li>\n",
       "\t<li>0.105533333333333</li>\n",
       "\t<li>0.618019047619048</li>\n",
       "\t<li>0.337066666666667</li>\n",
       "\t<li>0.0877666666666667</li>\n",
       "\t<li>0.456895238095238</li>\n",
       "\t<li>0.695733333333334</li>\n",
       "\t<li>0.526</li>\n",
       "\t<li>0.424315384615385</li>\n",
       "\t<li>0.119133333333333</li>\n",
       "\t<li>0.375266666666667</li>\n",
       "\t<li>0.514166666666667</li>\n",
       "\t<li>0.348466666666667</li>\n",
       "\t<li>0.835866666666667</li>\n",
       "\t<li>0.7324</li>\n",
       "\t<li>0.148449816849817</li>\n",
       "\t<li>0.0544666666666667</li>\n",
       "\t<li>0.0476666666666667</li>\n",
       "\t<li>0.848677777777778</li>\n",
       "\t<li>0.0408</li>\n",
       "\t<li>0.0185333333333333</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0516</li>\n",
       "\t<li>0.0334</li>\n",
       "\t<li>0.102533333333333</li>\n",
       "\t<li>0.180333333333333</li>\n",
       "\t<li>0.0417</li>\n",
       "\t<li>0.0188</li>\n",
       "\t<li>0.119733333333333</li>\n",
       "\t<li>0.586066666666667</li>\n",
       "\t<li>0.670866666666667</li>\n",
       "\t<li>0.0564</li>\n",
       "\t<li>0.0304</li>\n",
       "\t<li>0.698671524966262</li>\n",
       "\t<li>0.261066666666667</li>\n",
       "\t<li>0.1222</li>\n",
       "\t<li>0.5628</li>\n",
       "\t<li>0.625066666666667</li>\n",
       "\t<li>0.02</li>\n",
       "\t<li>0.173066666666667</li>\n",
       "\t<li>0.0314</li>\n",
       "\t<li>0.308066666666667</li>\n",
       "\t<li>0.27</li>\n",
       "\t<li>0.579961904761905</li>\n",
       "\t<li>0.505066666666667</li>\n",
       "\t<li>0.462357142857143</li>\n",
       "\t<li>0.0121333333333333</li>\n",
       "\t<li>0.390695238095238</li>\n",
       "\t<li>0.464142857142857</li>\n",
       "\t<li>0.250733333333333</li>\n",
       "\t<li>0.448133333333333</li>\n",
       "\t<li>0.597842424242424</li>\n",
       "\t<li>0.792010256410256</li>\n",
       "\t<li>0.198466666666667</li>\n",
       "\t<li>0.543266666666667</li>\n",
       "\t<li>0.5032</li>\n",
       "\t<li>0.0381333333333333</li>\n",
       "\t<li>0.162</li>\n",
       "\t<li>0.0727333333333333</li>\n",
       "\t<li>0.0906666666666667</li>\n",
       "\t<li>0.0426666666666667</li>\n",
       "\t<li>0.24767619047619</li>\n",
       "\t<li>0.574945454545455</li>\n",
       "\t<li>0.0128</li>\n",
       "\t<li>0.252133333333333</li>\n",
       "\t<li>0.266866666666667</li>\n",
       "\t<li>0.0312</li>\n",
       "\t<li>0.0288</li>\n",
       "\t<li>0.0430222222222222</li>\n",
       "\t<li>0.015</li>\n",
       "\t<li>0.1694</li>\n",
       "\t<li>0.633933333333333</li>\n",
       "\t<li>0.123266666666667</li>\n",
       "\t<li>0.388351633986928</li>\n",
       "\t<li>0.634293506493506</li>\n",
       "\t<li>0.0818666666666667</li>\n",
       "\t<li>0.1352</li>\n",
       "\t<li>0.328244444444444</li>\n",
       "\t<li>0.0732</li>\n",
       "\t<li>0.0244666666666667</li>\n",
       "\t<li>0.136980952380952</li>\n",
       "\t<li>0.110466666666667</li>\n",
       "\t<li>0.417266666666667</li>\n",
       "\t<li>0.323866666666667</li>\n",
       "\t<li>0.670266666666667</li>\n",
       "\t<li>0.001</li>\n",
       "\t<li>0.0268</li>\n",
       "\t<li>0.0800222222222222</li>\n",
       "\t<li>0.711569696969697</li>\n",
       "\t<li>0.1312</li>\n",
       "\t<li>0.021</li>\n",
       "\t<li>0.0884</li>\n",
       "\t<li>0.113266666666667</li>\n",
       "\t<li>0.734333333333333</li>\n",
       "\t<li>0.175266666666667</li>\n",
       "\t<li>0.0734666666666667</li>\n",
       "\t<li>0.0116</li>\n",
       "\t<li>0.293333333333333</li>\n",
       "\t<li>0.277933333333333</li>\n",
       "\t<li>0.5704</li>\n",
       "\t<li>0.034</li>\n",
       "\t<li>0.779533333333333</li>\n",
       "\t<li>0.15865</li>\n",
       "\t<li>0.0683333333333333</li>\n",
       "\t<li>0.0176</li>\n",
       "\t<li>0.352333333333333</li>\n",
       "\t<li>0.0818</li>\n",
       "\t<li>0.504</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.121866666666667</li>\n",
       "\t<li>0.676104761904762</li>\n",
       "\t<li>0.118533333333333</li>\n",
       "\t<li>0.0912</li>\n",
       "\t<li>0.4195</li>\n",
       "\t<li>0.0548</li>\n",
       "\t<li>0.0612888888888889</li>\n",
       "\t<li>0.02</li>\n",
       "\t<li>0.0108</li>\n",
       "\t<li>0.133897435897436</li>\n",
       "\t<li>0.636533333333333</li>\n",
       "\t<li>0.200133333333333</li>\n",
       "\t<li>0.909266666666667</li>\n",
       "\t<li>0.0857333333333333</li>\n",
       "\t<li>0.500466666666667</li>\n",
       "\t<li>0.114697435897436</li>\n",
       "\t<li>0.263482051282051</li>\n",
       "\t<li>0.605866666666667</li>\n",
       "\t<li>0.0510666666666667</li>\n",
       "\t<li>0.233355555555556</li>\n",
       "\t<li>0.034</li>\n",
       "\t<li>0.410466666666667</li>\n",
       "\t<li>0.023</li>\n",
       "\t<li>0.5652</li>\n",
       "\t<li>0.526266666666667</li>\n",
       "\t<li>0.187066666666667</li>\n",
       "\t<li>0.630533333333333</li>\n",
       "\t<li>0.380333333333333</li>\n",
       "\t<li>0.628666666666667</li>\n",
       "\t<li>0.347482828282828</li>\n",
       "\t<li>0.6542</li>\n",
       "\t<li>0.0082</li>\n",
       "\t<li>0.0491333333333333</li>\n",
       "\t<li>0.204066666666667</li>\n",
       "\t<li>0.383828571428572</li>\n",
       "\t<li>0.1236</li>\n",
       "\t<li>0.563369696969697</li>\n",
       "\t<li>0.138181562881563</li>\n",
       "\t<li>0.5213</li>\n",
       "\t<li>0.413933333333333</li>\n",
       "\t<li>0.0808</li>\n",
       "\t<li>0.376266666666667</li>\n",
       "\t<li>0.369533333333333</li>\n",
       "\t<li>0.278047619047619</li>\n",
       "\t<li>0.2165</li>\n",
       "\t<li>0.0106</li>\n",
       "\t<li>0.2436</li>\n",
       "\t<li>0.767302564102564</li>\n",
       "\t<li>0.411433333333333</li>\n",
       "\t<li>0.1154</li>\n",
       "\t<li>0.120848484848485</li>\n",
       "\t<li>0.321133333333333</li>\n",
       "\t<li>0.825571428571429</li>\n",
       "\t<li>0.210459829059829</li>\n",
       "\t<li>0.34685</li>\n",
       "\t<li>0.0072</li>\n",
       "\t<li>0.0144</li>\n",
       "\t<li>0.643533333333333</li>\n",
       "\t<li>0.0453333333333333</li>\n",
       "\t<li>0.555733333333333</li>\n",
       "\t<li>0.264333333333333</li>\n",
       "\t<li>0.0601</li>\n",
       "\t<li>0.720628571428571</li>\n",
       "\t<li>0.226914285714286</li>\n",
       "\t<li>0.0024</li>\n",
       "\t<li>0.443866666666667</li>\n",
       "\t<li>0.386577777777778</li>\n",
       "\t<li>0.683733333333333</li>\n",
       "\t<li>0.692666666666667</li>\n",
       "\t<li>0.0965333333333333</li>\n",
       "\t<li>0.0906</li>\n",
       "\t<li>0.184161904761905</li>\n",
       "\t<li>0.825685714285714</li>\n",
       "\t<li>0.0323333333333333</li>\n",
       "\t<li>0.581733333333333</li>\n",
       "\t<li>0.170633333333333</li>\n",
       "\t<li>0.064</li>\n",
       "\t<li>0.0418666666666667</li>\n",
       "\t<li>0.688161904761905</li>\n",
       "\t<li>0.348422222222222</li>\n",
       "\t<li>0.690333333333333</li>\n",
       "\t<li>0.10452380952381</li>\n",
       "\t<li>0.0914</li>\n",
       "\t<li>0.556866666666667</li>\n",
       "\t<li>0.0188</li>\n",
       "\t<li>0.271466666666667</li>\n",
       "\t<li>0.539022222222222</li>\n",
       "\t<li>0.273266666666667</li>\n",
       "\t<li>0.0141333333333333</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.121447619047619</li>\n",
       "\t<li>0.771266666666667</li>\n",
       "\t<li>0.1372</li>\n",
       "\t<li>0.0132</li>\n",
       "\t<li>0.127381818181818</li>\n",
       "\t<li>0.593633333333333</li>\n",
       "\t<li>0.0202</li>\n",
       "\t<li>0.338142857142857</li>\n",
       "\t<li>0.0661333333333333</li>\n",
       "\t<li>0.598933333333333</li>\n",
       "\t<li>0.4218</li>\n",
       "\t<li>0.580360606060606</li>\n",
       "\t<li>0.681633333333333</li>\n",
       "\t<li>0.134822222222222</li>\n",
       "\t<li>0.0138666666666667</li>\n",
       "\t<li>0.726485714285714</li>\n",
       "\t<li>0.018</li>\n",
       "\t<li>0.799019047619047</li>\n",
       "\t<li>0.113333333333333</li>\n",
       "\t<li>0.152333333333333</li>\n",
       "\t<li>0.131057142857143</li>\n",
       "\t<li>0.0183333333333333</li>\n",
       "\t<li>0.0113333333333333</li>\n",
       "\t<li>0.159533333333333</li>\n",
       "\t<li>0.013</li>\n",
       "\t<li>0.4057</li>\n",
       "\t<li>0.615422222222222</li>\n",
       "\t<li>0.1036</li>\n",
       "\t<li>0.0498666666666667</li>\n",
       "\t<li>0.441333333333333</li>\n",
       "\t<li>0.455933333333333</li>\n",
       "\t<li>0.0197333333333333</li>\n",
       "\t<li>0.0132</li>\n",
       "\t<li>0.0444</li>\n",
       "\t<li>0.347133333333333</li>\n",
       "\t<li>0.0440285714285714</li>\n",
       "\t<li>0.0466</li>\n",
       "\t<li>0.539733333333333</li>\n",
       "\t<li>0.489533333333333</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.0312</li>\n",
       "\t<li>0.483742857142857</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.2564</li>\n",
       "\t<li>0.349733333333333</li>\n",
       "\t<li>0.0481333333333333</li>\n",
       "\t<li>0.561133333333333</li>\n",
       "\t<li>0.134</li>\n",
       "\t<li>0.0140666666666667</li>\n",
       "\t<li>0.257</li>\n",
       "\t<li>0.0258</li>\n",
       "\t<li>0.624533333333333</li>\n",
       "\t<li>0.587936363636364</li>\n",
       "\t<li>0.487933333333333</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.875275757575758</li>\n",
       "\t<li>0.6896</li>\n",
       "\t<li>0.2538</li>\n",
       "\t<li>0.2456</li>\n",
       "\t<li>0.131533333333333</li>\n",
       "\t<li>0.8014</li>\n",
       "\t<li>0.312203174603175</li>\n",
       "\t<li>0.165466666666667</li>\n",
       "\t<li>0.150733333333333</li>\n",
       "\t<li>0.06965</li>\n",
       "\t<li>0.66675</li>\n",
       "\t<li>0.776466666666667</li>\n",
       "\t<li>0.0113333333333333</li>\n",
       "\t<li>0.599</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.0614</li>\n",
       "\t<li>0.0778666666666667</li>\n",
       "\t<li>0.804355555555556</li>\n",
       "\t<li>0.348933333333333</li>\n",
       "\t<li>0.609533333333333</li>\n",
       "\t<li>0.083</li>\n",
       "\t<li>0.048</li>\n",
       "\t<li>0.0781333333333333</li>\n",
       "\t<li>0.329244444444444</li>\n",
       "\t<li>0.0318666666666667</li>\n",
       "\t<li>0.0939333333333333</li>\n",
       "\t<li>0.0586666666666667</li>\n",
       "\t<li>0.1614</li>\n",
       "\t<li>0.285088888888889</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.276266666666667</li>\n",
       "\t<li>0.0314</li>\n",
       "\t<li>0.656966666666667</li>\n",
       "\t<li>0.444538095238095</li>\n",
       "\t<li>0.0246666666666667</li>\n",
       "\t<li>0.250285714285714</li>\n",
       "\t<li>0.435933333333333</li>\n",
       "\t<li>0.311333333333333</li>\n",
       "\t<li>0.443466666666667</li>\n",
       "\t<li>0.498166666666667</li>\n",
       "\t<li>0.5006</li>\n",
       "\t<li>0.190866666666667</li>\n",
       "\t<li>0.104266666666667</li>\n",
       "\t<li>0.0278666666666667</li>\n",
       "\t<li>0.0162666666666667</li>\n",
       "\t<li>0.0068</li>\n",
       "\t<li>0.384666666666667</li>\n",
       "\t<li>0.430755555555556</li>\n",
       "\t<li>0.273</li>\n",
       "\t<li>0.433833333333333</li>\n",
       "\t<li>0.0382</li>\n",
       "\t<li>0.0416</li>\n",
       "\t<li>0.283066666666667</li>\n",
       "\t<li>0.041</li>\n",
       "\t<li>0.397533333333333</li>\n",
       "\t<li>0.2058</li>\n",
       "\t<li>0.0488</li>\n",
       "\t<li>0.00533333333333333</li>\n",
       "\t<li>0.0284</li>\n",
       "\t<li>0.577218300653595</li>\n",
       "\t<li>0.0288</li>\n",
       "\t<li>0.0497111111111111</li>\n",
       "\t<li>0.0276</li>\n",
       "\t<li>0.506448484848485</li>\n",
       "\t<li>0.043</li>\n",
       "\t<li>0.313866666666667</li>\n",
       "\t<li>0.634866666666667</li>\n",
       "\t<li>0.0640666666666667</li>\n",
       "\t<li>0.0881809523809524</li>\n",
       "\t<li>0.319923809523809</li>\n",
       "\t<li>0.772028571428572</li>\n",
       "\t<li>0.0893333333333333</li>\n",
       "\t<li>0.0627333333333333</li>\n",
       "\t<li>0.0899333333333333</li>\n",
       "\t<li>0.255775757575758</li>\n",
       "\t<li>0.461455555555556</li>\n",
       "\t<li>0.632056140350877</li>\n",
       "\t<li>0.1374</li>\n",
       "\t<li>0.0712</li>\n",
       "\t<li>0.4774</li>\n",
       "\t<li>0.0306</li>\n",
       "\t<li>0.325066666666667</li>\n",
       "\t<li>0.206066666666667</li>\n",
       "\t<li>0.918333333333333</li>\n",
       "\t<li>0.2567</li>\n",
       "\t<li>0.5268</li>\n",
       "\t<li>0.823995238095238</li>\n",
       "\t<li>0.539933333333333</li>\n",
       "\t<li>0.438244444444444</li>\n",
       "\t<li>0.273933333333333</li>\n",
       "\t<li>0.106666666666667</li>\n",
       "\t<li>0.0164</li>\n",
       "\t<li>0.460666666666667</li>\n",
       "\t<li>0.0959333333333333</li>\n",
       "\t<li>0.1478</li>\n",
       "\t<li>0.0886666666666667</li>\n",
       "\t<li>0.585866666666667</li>\n",
       "\t<li>0.3264</li>\n",
       "\t<li>0.507666666666667</li>\n",
       "\t<li>0.0242666666666667</li>\n",
       "\t<li>0.385169696969697</li>\n",
       "\t<li>0.190866666666667</li>\n",
       "\t<li>0.0229333333333333</li>\n",
       "\t<li>0.2196</li>\n",
       "\t<li>0.335733333333333</li>\n",
       "\t<li>0.114066666666667</li>\n",
       "\t<li>0.00986666666666667</li>\n",
       "\t<li>0.1424</li>\n",
       "\t<li>0.107466666666667</li>\n",
       "\t<li>0.0280666666666667</li>\n",
       "\t<li>0.119333333333333</li>\n",
       "\t<li>0.0361333333333333</li>\n",
       "\t<li>0.6014</li>\n",
       "\t<li>0.363236363636364</li>\n",
       "\t<li>0.16610303030303</li>\n",
       "\t<li>0.0593333333333333</li>\n",
       "\t<li>0.565169696969697</li>\n",
       "\t<li>0.370374891774892</li>\n",
       "\t<li>0.337822222222222</li>\n",
       "\t<li>0.178266666666667</li>\n",
       "\t<li>0.753295238095238</li>\n",
       "\t<li>0.60904662004662</li>\n",
       "\t<li>0.579828571428571</li>\n",
       "\t<li>0.2234</li>\n",
       "\t<li>0.116</li>\n",
       "\t<li>0.0258</li>\n",
       "\t<li>0.0555333333333333</li>\n",
       "\t<li>0.553085714285714</li>\n",
       "\t<li>0.242866666666667</li>\n",
       "\t<li>0.1392</li>\n",
       "\t<li>0.655604761904762</li>\n",
       "\t<li>0.0341333333333333</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.663666666666667</li>\n",
       "\t<li>0.423333333333333</li>\n",
       "\t<li>0.0750666666666667</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.731727272727273</li>\n",
       "\t<li>0.495666666666667</li>\n",
       "\t<li>0.489209523809524</li>\n",
       "\t<li>0.024</li>\n",
       "\t<li>0.4498</li>\n",
       "\t<li>0.532933333333333</li>\n",
       "\t<li>0.4976</li>\n",
       "\t<li>0.0434</li>\n",
       "\t<li>0.808</li>\n",
       "\t<li>0.098</li>\n",
       "\t<li>0.295466666666667</li>\n",
       "\t<li>0.0828222222222222</li>\n",
       "\t<li>0.691</li>\n",
       "\t<li>0.368177777777778</li>\n",
       "\t<li>0.415733333333333</li>\n",
       "\t<li>0.172666666666667</li>\n",
       "\t<li>0.2466</li>\n",
       "\t<li>0.264514285714286</li>\n",
       "\t<li>0.142</li>\n",
       "\t<li>0.613666666666667</li>\n",
       "\t<li>0.386</li>\n",
       "\t<li>0.0548</li>\n",
       "\t<li>0.303</li>\n",
       "\t<li>0.0562</li>\n",
       "\t<li>0.0496380952380952</li>\n",
       "\t<li>0.0142666666666667</li>\n",
       "\t<li>0.2236</li>\n",
       "\t<li>0.0474</li>\n",
       "\t<li>0.251322222222222</li>\n",
       "\t<li>0.757666666666667</li>\n",
       "\t<li>0.1774</li>\n",
       "\t<li>0.6092</li>\n",
       "\t<li>0.655433333333333</li>\n",
       "\t<li>0.425533333333333</li>\n",
       "\t<li>0.0386</li>\n",
       "\t<li>0.3898</li>\n",
       "\t<li>0.116466666666667</li>\n",
       "\t<li>0.443888888888889</li>\n",
       "\t<li>0.872533333333333</li>\n",
       "\t<li>0.0457333333333333</li>\n",
       "\t<li>0.0228666666666667</li>\n",
       "\t<li>0.2504</li>\n",
       "\t<li>0.497822222222222</li>\n",
       "\t<li>0.562542857142857</li>\n",
       "\t<li>0.467207326007326</li>\n",
       "\t<li>0.772333333333333</li>\n",
       "\t<li>0.031</li>\n",
       "\t<li>0.2836</li>\n",
       "\t<li>0.403466666666667</li>\n",
       "\t<li>0.6291</li>\n",
       "\t<li>0.0975333333333333</li>\n",
       "\t<li>0.0265333333333333</li>\n",
       "\t<li>0.0506666666666667</li>\n",
       "\t<li>0.406076190476191</li>\n",
       "\t<li>0.478645887445888</li>\n",
       "\t<li>0.4038</li>\n",
       "\t<li>0.041</li>\n",
       "\t<li>0.512807936507937</li>\n",
       "\t<li>0.646666666666667</li>\n",
       "\t<li>0.205996078431373</li>\n",
       "\t<li>0.122</li>\n",
       "\t<li>0.007</li>\n",
       "\t<li>0.0612</li>\n",
       "\t<li>0.0538666666666667</li>\n",
       "\t<li>0.636333333333333</li>\n",
       "\t<li>0.103</li>\n",
       "\t<li>0.0476</li>\n",
       "\t<li>0.003</li>\n",
       "\t<li>0.118266666666667</li>\n",
       "\t<li>0.72508051948052</li>\n",
       "\t<li>0.0708666666666667</li>\n",
       "\t<li>0.866933333333333</li>\n",
       "\t<li>0.150866666666667</li>\n",
       "\t<li>0.603666666666667</li>\n",
       "\t<li>0.701045887445887</li>\n",
       "\t<li>0.011</li>\n",
       "\t<li>0.174533333333333</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.0724666666666667</li>\n",
       "\t<li>0.1368</li>\n",
       "\t<li>0.203733333333333</li>\n",
       "\t<li>0.0724666666666667</li>\n",
       "\t<li>0.78272380952381</li>\n",
       "\t<li>0.163684711779449</li>\n",
       "\t<li>0.0552666666666667</li>\n",
       "\t<li>0.057</li>\n",
       "\t<li>0.0579333333333333</li>\n",
       "\t<li>0.299923809523809</li>\n",
       "\t<li>0.450488888888889</li>\n",
       "\t<li>0.0916</li>\n",
       "\t<li>0.297866666666667</li>\n",
       "\t<li>0.903228571428571</li>\n",
       "\t<li>0.1064</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.00733333333333333</li>\n",
       "\t<li>0.51248838612368</li>\n",
       "\t<li>0.3064</li>\n",
       "\t<li>0.0389333333333333</li>\n",
       "\t<li>0.0418666666666667</li>\n",
       "\t<li>0.573233333333333</li>\n",
       "\t<li>0.799533333333333</li>\n",
       "\t<li>0.597933333333333</li>\n",
       "\t<li>0.121466666666667</li>\n",
       "\t<li>0.8176</li>\n",
       "\t<li>0.461330402930403</li>\n",
       "\t<li>0.0531333333333333</li>\n",
       "\t<li>0.457366666666667</li>\n",
       "\t<li>0.1106</li>\n",
       "\t<li>0.001</li>\n",
       "\t<li>0.605761904761905</li>\n",
       "\t<li>0.032</li>\n",
       "\t<li>0.0516666666666667</li>\n",
       "\t<li>0.5368</li>\n",
       "\t<li>0.4388</li>\n",
       "\t<li>0.4436</li>\n",
       "\t<li>0.205</li>\n",
       "\t<li>0.4114</li>\n",
       "\t<li>0.00933333333333333</li>\n",
       "\t<li>0.301977777777778</li>\n",
       "\t<li>0.735366666666667</li>\n",
       "\t<li>0.016</li>\n",
       "\t<li>0.233666666666667</li>\n",
       "\t<li>0.0032</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.0048</li>\n",
       "\t<li>0.0384</li>\n",
       "\t<li>0.358733333333333</li>\n",
       "\t<li>0.218333333333333</li>\n",
       "\t<li>0.0106666666666667</li>\n",
       "\t<li>0.300504761904762</li>\n",
       "\t<li>0.141190476190476</li>\n",
       "\t<li>0.281314285714286</li>\n",
       "\t<li>0.0676</li>\n",
       "\t<li>0.454466666666667</li>\n",
       "\t<li>0.0514</li>\n",
       "\t<li>0.1074</li>\n",
       "\t<li>0.695466666666667</li>\n",
       "\t<li>0.395971428571429</li>\n",
       "\t<li>0.0938666666666667</li>\n",
       "\t<li>0.0126666666666667</li>\n",
       "\t<li>0.0736</li>\n",
       "\t<li>0.441365079365079</li>\n",
       "\t<li>0.661666666666667</li>\n",
       "\t<li>0.151</li>\n",
       "\t<li>0.2728</li>\n",
       "\t<li>0.459209523809524</li>\n",
       "\t<li>0.0498</li>\n",
       "\t<li>0.285633333333333</li>\n",
       "\t<li>0.0406</li>\n",
       "\t<li>0.514933333333333</li>\n",
       "\t<li>0.564566666666667</li>\n",
       "\t<li>0.0552666666666667</li>\n",
       "\t<li>0.0168666666666667</li>\n",
       "\t<li>0.701066666666667</li>\n",
       "\t<li>0.0198</li>\n",
       "\t<li>0.200466666666667</li>\n",
       "\t<li>0.4523</li>\n",
       "\t<li>0.0721333333333333</li>\n",
       "\t<li>0.0352</li>\n",
       "\t<li>0.732733333333333</li>\n",
       "\t<li>0.589622222222222</li>\n",
       "\t<li>0.654933333333333</li>\n",
       "\t<li>0.499866666666667</li>\n",
       "\t<li>0.159688888888889</li>\n",
       "\t<li>0.4317</li>\n",
       "\t<li>0.0599333333333333</li>\n",
       "\t<li>0.704733333333333</li>\n",
       "\t<li>0.221382051282051</li>\n",
       "\t<li>0.6252</li>\n",
       "\t<li>0.0284</li>\n",
       "\t<li>0.698333333333333</li>\n",
       "\t<li>0.727933333333333</li>\n",
       "\t<li>0.6536</li>\n",
       "\t<li>0.0173333333333333</li>\n",
       "\t<li>0.188733333333333</li>\n",
       "\t<li>0.081</li>\n",
       "\t<li>0.374766666666667</li>\n",
       "\t<li>0.0896666666666667</li>\n",
       "\t<li>0.247866666666667</li>\n",
       "\t<li>0.584533333333333</li>\n",
       "\t<li>0.0136</li>\n",
       "\t<li>0.872133333333333</li>\n",
       "\t<li>0.306466666666667</li>\n",
       "\t<li>0.0647333333333333</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0024</li>\n",
       "\t<li>0.646190476190476</li>\n",
       "\t<li>0.017</li>\n",
       "\t<li>0.0693333333333333</li>\n",
       "\t<li>0.199066666666667</li>\n",
       "\t<li>0.4084</li>\n",
       "\t<li>0.505333333333333</li>\n",
       "\t<li>0.0664222222222222</li>\n",
       "\t<li>0.107733333333333</li>\n",
       "\t<li>0.4294</li>\n",
       "\t<li>0.383466666666667</li>\n",
       "\t<li>0.0930666666666667</li>\n",
       "\t<li>0.1202</li>\n",
       "\t<li>0.002</li>\n",
       "\t<li>0.0228666666666667</li>\n",
       "\t<li>0.2854</li>\n",
       "\t<li>0.352333333333333</li>\n",
       "\t<li>0.3938</li>\n",
       "\t<li>0.172266666666667</li>\n",
       "\t<li>0.333233333333333</li>\n",
       "\t<li>0.551666666666667</li>\n",
       "\t<li>0.8372</li>\n",
       "\t<li>0.938933333333333</li>\n",
       "\t<li>0.370682051282051</li>\n",
       "\t<li>0.6364</li>\n",
       "\t<li>0.245933333333333</li>\n",
       "\t<li>0.0641555555555556</li>\n",
       "\t<li>0.116066666666667</li>\n",
       "\t<li>0.0705333333333333</li>\n",
       "\t<li>0.709066666666667</li>\n",
       "\t<li>0.284888888888889</li>\n",
       "\t<li>0.586828571428571</li>\n",
       "\t<li>0.7202</li>\n",
       "\t<li>0.693866666666667</li>\n",
       "\t<li>0.226133333333333</li>\n",
       "\t<li>0.233066666666667</li>\n",
       "\t<li>0.251933333333333</li>\n",
       "\t<li>0.1629</li>\n",
       "\t<li>0.1852</li>\n",
       "\t<li>0.370222222222222</li>\n",
       "\t<li>0.197933333333333</li>\n",
       "\t<li>0.173177777777778</li>\n",
       "\t<li>0.214066666666667</li>\n",
       "\t<li>0.66</li>\n",
       "\t<li>0.0219777777777778</li>\n",
       "\t<li>0.4735</li>\n",
       "\t<li>0.626419047619048</li>\n",
       "\t<li>0.204</li>\n",
       "\t<li>0.1664</li>\n",
       "\t<li>0.0172</li>\n",
       "\t<li>0.389066666666667</li>\n",
       "\t<li>0.548473992673992</li>\n",
       "\t<li>0.0391333333333333</li>\n",
       "\t<li>0.760057142857143</li>\n",
       "\t<li>0.814066666666667</li>\n",
       "\t<li>0.487933333333333</li>\n",
       "\t<li>0.0762666666666667</li>\n",
       "\t<li>0.016</li>\n",
       "\t<li>0.587466666666667</li>\n",
       "\t<li>0.560866666666667</li>\n",
       "\t<li>0.3756</li>\n",
       "\t<li>0.0688</li>\n",
       "\t<li>0.0256666666666667</li>\n",
       "\t<li>0.5748</li>\n",
       "\t<li>0.442533333333333</li>\n",
       "\t<li>0.724266666666667</li>\n",
       "\t<li>0.0578666666666667</li>\n",
       "\t<li>0.0048</li>\n",
       "\t<li>0.271638095238095</li>\n",
       "\t<li>0.2132</li>\n",
       "\t<li>0.323833333333333</li>\n",
       "\t<li>0.565780952380952</li>\n",
       "\t<li>0.9462</li>\n",
       "\t<li>0.0466666666666667</li>\n",
       "\t<li>0.123666666666667</li>\n",
       "\t<li>0.850247619047619</li>\n",
       "\t<li>0.271115151515152</li>\n",
       "\t<li>0.0545333333333333</li>\n",
       "\t<li>0.52965974025974</li>\n",
       "\t<li>0.0590666666666667</li>\n",
       "\t<li>0.0907904761904762</li>\n",
       "\t<li>0.280133333333333</li>\n",
       "\t<li>0.497692307692308</li>\n",
       "\t<li>0.546333333333333</li>\n",
       "\t<li>0.392133333333333</li>\n",
       "\t<li>0.00786666666666667</li>\n",
       "\t<li>0.301</li>\n",
       "\t<li>0.039</li>\n",
       "\t<li>0.028</li>\n",
       "\t<li>0.919933333333333</li>\n",
       "\t<li>0.794533333333333</li>\n",
       "\t<li>0.778266666666667</li>\n",
       "\t<li>0.285262745098039</li>\n",
       "\t<li>0.300666666666667</li>\n",
       "\t<li>0.1062</li>\n",
       "\t<li>0.825</li>\n",
       "\t<li>0.15387619047619</li>\n",
       "\t<li>0.3356</li>\n",
       "\t<li>0.011</li>\n",
       "\t<li>0.0432</li>\n",
       "\t<li>0.730666666666667</li>\n",
       "\t<li>0.0751333333333333</li>\n",
       "\t<li>0.026</li>\n",
       "\t<li>0.195266666666667</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0887833333333333</li>\n",
       "\t<li>0.0667333333333333</li>\n",
       "\t<li>0.47570303030303</li>\n",
       "\t<li>0.169866666666667</li>\n",
       "\t<li>0.06</li>\n",
       "\t<li>0.518</li>\n",
       "\t<li>0.749466666666667</li>\n",
       "\t<li>0.0626666666666667</li>\n",
       "\t<li>0.299266666666667</li>\n",
       "\t<li>0.643022222222222</li>\n",
       "\t<li>0.504666666666667</li>\n",
       "\t<li>0.26585974025974</li>\n",
       "\t<li>0.0640666666666667</li>\n",
       "\t<li>0.771028571428572</li>\n",
       "\t<li>0.370133333333333</li>\n",
       "\t<li>0.226057142857143</li>\n",
       "\t<li>0.704866666666667</li>\n",
       "\t<li>0.912266666666667</li>\n",
       "\t<li>0.602590476190476</li>\n",
       "\t<li>0.4874</li>\n",
       "\t<li>0.553409523809524</li>\n",
       "\t<li>0.628009523809524</li>\n",
       "\t<li>0.440129411764706</li>\n",
       "\t<li>0.348755555555556</li>\n",
       "\t<li>0.475066666666667</li>\n",
       "\t<li>0.472929411764706</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.1022</li>\n",
       "\t<li>0.0205333333333333</li>\n",
       "\t<li>0.0625333333333333</li>\n",
       "\t<li>0.262695238095238</li>\n",
       "\t<li>0.0899333333333333</li>\n",
       "\t<li>0.7598</li>\n",
       "\t<li>0.400888888888889</li>\n",
       "\t<li>0.0524</li>\n",
       "\t<li>0.0258</li>\n",
       "\t<li>0.570848717948718</li>\n",
       "\t<li>0.044</li>\n",
       "\t<li>0.30070303030303</li>\n",
       "\t<li>0.413133333333333</li>\n",
       "\t<li>0.3964</li>\n",
       "\t<li>0.349533333333333</li>\n",
       "\t<li>0.794466666666667</li>\n",
       "\t<li>0.7002</li>\n",
       "\t<li>0.0558666666666667</li>\n",
       "\t<li>0.0032</li>\n",
       "\t<li>0.0890666666666667</li>\n",
       "\t<li>0.42</li>\n",
       "\t<li>0.285466666666667</li>\n",
       "\t<li>0.0918222222222222</li>\n",
       "\t<li>0.523</li>\n",
       "\t<li>0.1118</li>\n",
       "\t<li>0.766466666666667</li>\n",
       "\t<li>0.285666666666667</li>\n",
       "\t<li>0.2864</li>\n",
       "\t<li>0.250933333333333</li>\n",
       "\t<li>0.754933333333333</li>\n",
       "\t<li>0.192283333333333</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.0444</li>\n",
       "\t<li>0.006</li>\n",
       "\t<li>0.001</li>\n",
       "\t<li>0.168685714285714</li>\n",
       "\t<li>0.367066666666667</li>\n",
       "\t<li>0.389066666666667</li>\n",
       "\t<li>0.1724</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.166433333333333</li>\n",
       "\t<li>0.628547008547009</li>\n",
       "\t<li>0.643</li>\n",
       "\t<li>0.489533333333333</li>\n",
       "\t<li>0.444104761904762</li>\n",
       "\t<li>0.0688666666666667</li>\n",
       "\t<li>0.128133333333333</li>\n",
       "\t<li>0.4426</li>\n",
       "\t<li>0.036</li>\n",
       "\t<li>0.0963333333333333</li>\n",
       "\t<li>0.794666666666667</li>\n",
       "\t<li>0.661587545787546</li>\n",
       "\t<li>0.834228571428572</li>\n",
       "\t<li>0.0522</li>\n",
       "\t<li>0.0462888888888889</li>\n",
       "\t<li>0.0827</li>\n",
       "\t<li>0.272847619047619</li>\n",
       "\t<li>0.1022</li>\n",
       "\t<li>0.861752380952381</li>\n",
       "\t<li>0.335266666666667</li>\n",
       "\t<li>0.405333333333333</li>\n",
       "\t<li>0.072</li>\n",
       "\t<li>0.432569696969697</li>\n",
       "\t<li>0.0171333333333333</li>\n",
       "\t<li>0.0614666666666667</li>\n",
       "\t<li>0.444593939393939</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.0309333333333333</li>\n",
       "\t<li>0.380933333333333</li>\n",
       "\t<li>0.0221333333333333</li>\n",
       "\t<li>0.0016</li>\n",
       "\t<li>0.0976666666666667</li>\n",
       "\t<li>0.40852380952381</li>\n",
       "\t<li>0.231711111111111</li>\n",
       "\t<li>0.413133333333333</li>\n",
       "\t<li>0.187733333333333</li>\n",
       "\t<li>0.453533333333333</li>\n",
       "\t<li>0.002</li>\n",
       "\t<li>0.6224</li>\n",
       "\t<li>0.759333333333333</li>\n",
       "\t<li>0.360155555555556</li>\n",
       "\t<li>0.624666666666667</li>\n",
       "\t<li>0.594329411764706</li>\n",
       "\t<li>0.176866666666667</li>\n",
       "\t<li>0.147066666666667</li>\n",
       "\t<li>0.137333333333333</li>\n",
       "\t<li>0.252866666666667</li>\n",
       "\t<li>0.0112</li>\n",
       "\t<li>0.448222222222222</li>\n",
       "\t<li>0.671428571428571</li>\n",
       "\t<li>0.624266666666667</li>\n",
       "\t<li>0.0368888888888889</li>\n",
       "\t<li>0.0546666666666667</li>\n",
       "\t<li>0.560533333333333</li>\n",
       "\t<li>0.617983333333333</li>\n",
       "\t<li>0.0192</li>\n",
       "\t<li>0.3876</li>\n",
       "\t<li>0.416466666666667</li>\n",
       "\t<li>0.926569696969697</li>\n",
       "\t<li>0.223266666666667</li>\n",
       "\t<li>0.706</li>\n",
       "\t<li>0.729228571428571</li>\n",
       "\t<li>0.828666666666667</li>\n",
       "\t<li>0.0381333333333333</li>\n",
       "\t<li>0.171733333333333</li>\n",
       "\t<li>0.1764</li>\n",
       "\t<li>0.400133333333333</li>\n",
       "\t<li>0.719561904761905</li>\n",
       "\t<li>0.0337333333333333</li>\n",
       "\t<li>0.920295238095238</li>\n",
       "\t<li>0.146266666666667</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.183733333333333</li>\n",
       "\t<li>0.652488888888889</li>\n",
       "\t<li>0.254822222222222</li>\n",
       "\t<li>0.228866666666667</li>\n",
       "\t<li>0.598</li>\n",
       "\t<li>0.1814</li>\n",
       "\t<li>0.0668</li>\n",
       "\t<li>0.576819047619048</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.1722</li>\n",
       "\t<li>0.182533333333333</li>\n",
       "\t<li>0.28651746031746</li>\n",
       "\t<li>0.510742857142857</li>\n",
       "\t<li>0.5052</li>\n",
       "\t<li>0.625285714285714</li>\n",
       "\t<li>0.0794</li>\n",
       "\t<li>0.774066666666667</li>\n",
       "\t<li>0.0228</li>\n",
       "\t<li>0.3284</li>\n",
       "\t<li>0.222448484848485</li>\n",
       "\t<li>0.0590222222222222</li>\n",
       "\t<li>0.914428571428572</li>\n",
       "\t<li>0.595644444444445</li>\n",
       "\t<li>0.250581818181818</li>\n",
       "\t<li>0.0064</li>\n",
       "\t<li>0.0136</li>\n",
       "\t<li>0.0418</li>\n",
       "\t<li>0.137133333333333</li>\n",
       "\t<li>0.778466666666667</li>\n",
       "\t<li>0.655733333333334</li>\n",
       "\t<li>0.133333333333333</li>\n",
       "\t<li>0.1562</li>\n",
       "\t<li>0.0078</li>\n",
       "\t<li>0.195866666666667</li>\n",
       "\t<li>0.176733333333333</li>\n",
       "\t<li>0.0567333333333333</li>\n",
       "\t<li>0.440333333333333</li>\n",
       "\t<li>0.022</li>\n",
       "\t<li>0.649622222222222</li>\n",
       "\t<li>0.0038</li>\n",
       "\t<li>0.696628571428571</li>\n",
       "\t<li>0.0758666666666667</li>\n",
       "\t<li>0.0074</li>\n",
       "\t<li>0.5579</li>\n",
       "\t<li>0.829057142857143</li>\n",
       "\t<li>0.303266666666667</li>\n",
       "\t<li>0.0862</li>\n",
       "\t<li>0.0621</li>\n",
       "\t<li>0.0519666666666667</li>\n",
       "\t<li>0.413433333333333</li>\n",
       "\t<li>0.0839333333333333</li>\n",
       "\t<li>0.387733333333333</li>\n",
       "\t<li>0.132</li>\n",
       "\t<li>0.0125333333333333</li>\n",
       "\t<li>0.153733333333333</li>\n",
       "\t<li>0.67390303030303</li>\n",
       "\t<li>0.1204</li>\n",
       "\t<li>0.745666666666667</li>\n",
       "\t<li>0.811133333333333</li>\n",
       "\t<li>0.250022222222222</li>\n",
       "\t<li>0.705933333333333</li>\n",
       "\t<li>0.307133333333333</li>\n",
       "\t<li>0.614614285714286</li>\n",
       "\t<li>0.693015384615385</li>\n",
       "\t<li>0.104066666666667</li>\n",
       "\t<li>0.5954</li>\n",
       "\t<li>0.342333333333333</li>\n",
       "\t<li>0.225533333333333</li>\n",
       "\t<li>0.136781818181818</li>\n",
       "\t<li>0.516022222222222</li>\n",
       "\t<li>0.525295238095238</li>\n",
       "\t<li>0.173066666666667</li>\n",
       "\t<li>0.3134</li>\n",
       "\t<li>0.0196666666666667</li>\n",
       "\t<li>0.349266666666667</li>\n",
       "\t<li>0.2724</li>\n",
       "\t<li>0.0774</li>\n",
       "\t<li>0.4402</li>\n",
       "\t<li>0.0454666666666667</li>\n",
       "\t<li>0.524</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.478</li>\n",
       "\t<li>0.0996</li>\n",
       "\t<li>0.304595238095238</li>\n",
       "\t<li>0.107533333333333</li>\n",
       "\t<li>0.0345666666666667</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.10212380952381</li>\n",
       "\t<li>0.311766666666667</li>\n",
       "\t<li>0.670957142857143</li>\n",
       "\t<li>0.271866666666667</li>\n",
       "\t<li>0.0418666666666667</li>\n",
       "\t<li>0.018</li>\n",
       "\t<li>0.106933333333333</li>\n",
       "\t<li>0.507866666666667</li>\n",
       "\t<li>0.11035</li>\n",
       "\t<li>0.149533333333333</li>\n",
       "\t<li>0.2718</li>\n",
       "\t<li>0.400766666666667</li>\n",
       "\t<li>0.119457142857143</li>\n",
       "\t<li>0.360066666666667</li>\n",
       "\t<li>0.0792</li>\n",
       "\t<li>0.156</li>\n",
       "\t<li>0.0392</li>\n",
       "\t<li>0.390866666666667</li>\n",
       "\t<li>0.0550666666666667</li>\n",
       "\t<li>0.0742666666666667</li>\n",
       "\t<li>0.1194</li>\n",
       "\t<li>0.0798666666666667</li>\n",
       "\t<li>0.596466666666667</li>\n",
       "\t<li>0.0429333333333333</li>\n",
       "\t<li>0.0092</li>\n",
       "\t<li>0.828761904761905</li>\n",
       "\t<li>0.56107619047619</li>\n",
       "\t<li>0.679266666666667</li>\n",
       "\t<li>0.154561904761905</li>\n",
       "\t<li>0.153333333333333</li>\n",
       "\t<li>0.154933333333333</li>\n",
       "\t<li>0.504533333333333</li>\n",
       "\t<li>0.00266666666666667</li>\n",
       "\t<li>0.056047619047619</li>\n",
       "\t<li>0.396115384615385</li>\n",
       "\t<li>0.0314666666666667</li>\n",
       "\t<li>0.042</li>\n",
       "\t<li>0.553933333333333</li>\n",
       "\t<li>0.5086</li>\n",
       "\t<li>0.264077777777778</li>\n",
       "\t<li>0.0722666666666667</li>\n",
       "\t<li>0.0762666666666667</li>\n",
       "\t<li>0.550266666666667</li>\n",
       "\t<li>0.624844444444445</li>\n",
       "\t<li>0.0496</li>\n",
       "\t<li>0.115266666666667</li>\n",
       "\t<li>0.225866666666667</li>\n",
       "\t<li>0.334519047619048</li>\n",
       "\t<li>0.796466666666667</li>\n",
       "\t<li>0.769295238095238</li>\n",
       "\t<li>0.486733333333333</li>\n",
       "\t<li>0.262533333333333</li>\n",
       "\t<li>0.0124666666666667</li>\n",
       "\t<li>0.016</li>\n",
       "\t<li>0.695504761904762</li>\n",
       "\t<li>0.428666666666667</li>\n",
       "\t<li>0.0428</li>\n",
       "\t<li>0.612333333333333</li>\n",
       "\t<li>0.0196666666666667</li>\n",
       "\t<li>0.199333333333333</li>\n",
       "\t<li>0.359133333333333</li>\n",
       "\t<li>0.494266666666667</li>\n",
       "\t<li>0.889266666666667</li>\n",
       "\t<li>0.3366</li>\n",
       "\t<li>0.3516</li>\n",
       "\t<li>0.1978</li>\n",
       "\t<li>0.8336</li>\n",
       "\t<li>0.0382666666666667</li>\n",
       "\t<li>0.0392222222222222</li>\n",
       "\t<li>0.436266666666667</li>\n",
       "\t<li>0.0542</li>\n",
       "\t<li>0.319441558441558</li>\n",
       "\t<li>0.556066666666667</li>\n",
       "\t<li>0.00866666666666667</li>\n",
       "\t<li>0.0388666666666667</li>\n",
       "\t<li>0.557866666666667</li>\n",
       "\t<li>0.0478666666666667</li>\n",
       "\t<li>0.2173</li>\n",
       "\t<li>0.495161904761905</li>\n",
       "\t<li>0.109866666666667</li>\n",
       "\t<li>0.515209523809524</li>\n",
       "\t<li>0.0404</li>\n",
       "\t<li>0.144333333333333</li>\n",
       "\t<li>0.521333333333333</li>\n",
       "\t<li>0.0892666666666666</li>\n",
       "\t<li>0.351333333333333</li>\n",
       "\t<li>0.484444444444444</li>\n",
       "\t<li>0.336733333333333</li>\n",
       "\t<li>0.403933333333333</li>\n",
       "\t<li>0.0296666666666667</li>\n",
       "\t<li>0.0755333333333333</li>\n",
       "\t<li>0.0419333333333333</li>\n",
       "\t<li>0.5966</li>\n",
       "\t<li>0.0289333333333333</li>\n",
       "\t<li>0.0402666666666667</li>\n",
       "\t<li>0.0476</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.503466666666667</li>\n",
       "\t<li>0.0322666666666667</li>\n",
       "\t<li>0.721088888888889</li>\n",
       "\t<li>0.127</li>\n",
       "\t<li>0.256133333333333</li>\n",
       "\t<li>0.827266666666667</li>\n",
       "\t<li>0.438266666666667</li>\n",
       "\t<li>0.688466666666667</li>\n",
       "\t<li>0.0679333333333333</li>\n",
       "\t<li>0.6014</li>\n",
       "\t<li>0.734095238095238</li>\n",
       "\t<li>0.235933333333333</li>\n",
       "\t<li>0.0326222222222222</li>\n",
       "\t<li>0.329733333333333</li>\n",
       "\t<li>0.537747186147186</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.371533333333333</li>\n",
       "\t<li>0.693828571428571</li>\n",
       "\t<li>0.2747</li>\n",
       "\t<li>0.63250303030303</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.0402</li>\n",
       "\t<li>0.749171428571429</li>\n",
       "\t<li>0.0072</li>\n",
       "\t<li>0.159180952380952</li>\n",
       "\t<li>0.1474</li>\n",
       "\t<li>0.173066666666667</li>\n",
       "\t<li>0.00985714285714286</li>\n",
       "\t<li>0.5274</li>\n",
       "\t<li>0.545333333333333</li>\n",
       "\t<li>0.0028</li>\n",
       "\t<li>0.1886</li>\n",
       "\t<li>0.582933333333333</li>\n",
       "\t<li>0.697180952380952</li>\n",
       "\t<li>0.716755555555555</li>\n",
       "\t<li>0.267933333333333</li>\n",
       "\t<li>0.362555555555555</li>\n",
       "\t<li>0.297444444444444</li>\n",
       "\t<li>0.0328</li>\n",
       "\t<li>0.4958</li>\n",
       "\t<li>0.2038</li>\n",
       "\t<li>0.01</li>\n",
       "\t<li>0.253533333333333</li>\n",
       "\t<li>0.066</li>\n",
       "\t<li>0.184247619047619</li>\n",
       "\t<li>0.210133333333333</li>\n",
       "\t<li>0.685266666666667</li>\n",
       "\t<li>0.405707936507937</li>\n",
       "\t<li>0.188933333333333</li>\n",
       "\t<li>0.5358</li>\n",
       "\t<li>0.367</li>\n",
       "\t<li>0.0260666666666667</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.121333333333333</li>\n",
       "\t<li>0.0815333333333333</li>\n",
       "\t<li>0.436923809523809</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.210066666666667</li>\n",
       "\t<li>0.5586</li>\n",
       "\t<li>0.372333333333333</li>\n",
       "\t<li>0.002</li>\n",
       "\t<li>0.349466666666667</li>\n",
       "\t<li>0.017</li>\n",
       "\t<li>0.143</li>\n",
       "\t<li>0.164</li>\n",
       "\t<li>0.0281333333333333</li>\n",
       "\t<li>0.0370666666666667</li>\n",
       "\t<li>0.3204</li>\n",
       "\t<li>0.0526</li>\n",
       "\t<li>0.712633333333333</li>\n",
       "\t<li>0.634276190476191</li>\n",
       "\t<li>0.356866666666667</li>\n",
       "\t<li>0.294866666666667</li>\n",
       "\t<li>0.292</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.6658</li>\n",
       "\t<li>0.453733333333333</li>\n",
       "\t<li>0.461790476190476</li>\n",
       "\t<li>0.475933333333333</li>\n",
       "\t<li>0.3352</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.312333333333333</li>\n",
       "\t<li>0.238066666666667</li>\n",
       "\t<li>0.0829333333333333</li>\n",
       "\t<li>0.355333333333333</li>\n",
       "\t<li>0.283533333333333</li>\n",
       "\t<li>0.723009523809524</li>\n",
       "\t<li>0.191133333333333</li>\n",
       "\t<li>0.0534666666666667</li>\n",
       "\t<li>0.0810666666666667</li>\n",
       "\t<li>0.264866666666667</li>\n",
       "\t<li>0.0088</li>\n",
       "\t<li>0.830419047619048</li>\n",
       "\t<li>0.293333333333333</li>\n",
       "\t<li>0.596066666666667</li>\n",
       "\t<li>0.3234</li>\n",
       "\t<li>0.495485714285714</li>\n",
       "\t<li>0.615</li>\n",
       "\t<li>0.167866666666667</li>\n",
       "\t<li>0.37</li>\n",
       "\t<li>0.474</li>\n",
       "\t<li>0.448133333333333</li>\n",
       "\t<li>0.670273015873016</li>\n",
       "\t<li>0.416422222222222</li>\n",
       "\t<li>0.1182</li>\n",
       "\t<li>0.0579333333333333</li>\n",
       "\t<li>0.0213333333333333</li>\n",
       "\t<li>0.029</li>\n",
       "\t<li>0.214266666666667</li>\n",
       "\t<li>0.404566666666667</li>\n",
       "\t<li>0.172866666666667</li>\n",
       "\t<li>0.345866666666667</li>\n",
       "\t<li>0.0685333333333333</li>\n",
       "\t<li>0.145316666666667</li>\n",
       "\t<li>0.0618</li>\n",
       "\t<li>0.231866666666667</li>\n",
       "\t<li>0.4736</li>\n",
       "\t<li>0.0248</li>\n",
       "\t<li>0.478388744588745</li>\n",
       "\t<li>0.82</li>\n",
       "\t<li>0.0708666666666667</li>\n",
       "\t<li>0.489466666666667</li>\n",
       "\t<li>0.146457142857143</li>\n",
       "\t<li>0.227647619047619</li>\n",
       "\t<li>0.207466666666667</li>\n",
       "\t<li>0.618433333333334</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.773333333333333</li>\n",
       "\t<li>0.2312</li>\n",
       "\t<li>0.0553333333333333</li>\n",
       "\t<li>0.574048351648352</li>\n",
       "\t<li>0.705933333333333</li>\n",
       "\t<li>0.0692222222222222</li>\n",
       "\t<li>0.0054</li>\n",
       "\t<li>0.289961904761905</li>\n",
       "\t<li>0.413</li>\n",
       "\t<li>0.748666666666667</li>\n",
       "\t<li>0.425133333333333</li>\n",
       "\t<li>0.3733</li>\n",
       "\t<li>0.539466666666667</li>\n",
       "\t<li>0.485798268398268</li>\n",
       "\t<li>0.3804</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.752333333333333</li>\n",
       "\t<li>0.0184</li>\n",
       "\t<li>0.425377896613191</li>\n",
       "\t<li>0.0148</li>\n",
       "\t<li>0.7104</li>\n",
       "\t<li>0.088</li>\n",
       "\t<li>0.3026</li>\n",
       "\t<li>0.347222222222222</li>\n",
       "\t<li>0.0109333333333333</li>\n",
       "\t<li>0.400666666666667</li>\n",
       "\t<li>0.418333333333333</li>\n",
       "\t<li>0.152133333333333</li>\n",
       "\t<li>0.0438666666666667</li>\n",
       "\t<li>0.819723809523809</li>\n",
       "\t<li>0.142888888888889</li>\n",
       "\t<li>0.474466666666667</li>\n",
       "\t<li>0.659342857142857</li>\n",
       "\t<li>0.0423333333333333</li>\n",
       "\t<li>0.220155555555556</li>\n",
       "\t<li>0.537133333333333</li>\n",
       "\t<li>0.0724666666666667</li>\n",
       "\t<li>0.0548</li>\n",
       "\t<li>0.253619047619048</li>\n",
       "\t<li>0.467555555555556</li>\n",
       "\t<li>0.216666666666667</li>\n",
       "\t<li>0.707533333333333</li>\n",
       "\t<li>0.3424</li>\n",
       "\t<li>0.297733333333333</li>\n",
       "\t<li>0.0238</li>\n",
       "\t<li>0.4306</li>\n",
       "\t<li>0.454533333333333</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.599447619047619</li>\n",
       "\t<li>0.380133333333333</li>\n",
       "\t<li>0.713369696969697</li>\n",
       "\t<li>0.0731333333333333</li>\n",
       "\t<li>0.795411111111111</li>\n",
       "\t<li>0.134133333333333</li>\n",
       "\t<li>0.0318</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.752666666666667</li>\n",
       "\t<li>0.482136363636364</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0959238095238095</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.0799</li>\n",
       "\t<li>0.208888888888889</li>\n",
       "\t<li>0.498036363636364</li>\n",
       "\t<li>0.400695238095238</li>\n",
       "\t<li>0.151</li>\n",
       "\t<li>0.824735714285714</li>\n",
       "\t<li>0.0474</li>\n",
       "\t<li>0.223133333333333</li>\n",
       "\t<li>0.868619047619048</li>\n",
       "\t<li>0.002</li>\n",
       "\t<li>0.275</li>\n",
       "\t<li>0.477333333333333</li>\n",
       "\t<li>0.408066666666667</li>\n",
       "\t<li>0.285047619047619</li>\n",
       "\t<li>0.260666666666667</li>\n",
       "\t<li>0.4912</li>\n",
       "\t<li>0.715148717948718</li>\n",
       "\t<li>0.786761904761905</li>\n",
       "\t<li>0.4124</li>\n",
       "\t<li>0.4268</li>\n",
       "\t<li>0.2124</li>\n",
       "\t<li>0.125781818181818</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0194</li>\n",
       "\t<li>0.319933333333333</li>\n",
       "\t<li>0.759723809523809</li>\n",
       "\t<li>0.332003174603175</li>\n",
       "\t<li>0.217515151515152</li>\n",
       "\t<li>0.0875333333333334</li>\n",
       "\t<li>0.466933333333333</li>\n",
       "\t<li>0.0436</li>\n",
       "\t<li>0.0289333333333333</li>\n",
       "\t<li>0.519133333333333</li>\n",
       "\t<li>0.8186</li>\n",
       "\t<li>0.746836363636364</li>\n",
       "\t<li>0.440266666666667</li>\n",
       "\t<li>0.321580952380952</li>\n",
       "\t<li>8e-04</li>\n",
       "\t<li>0.125266666666667</li>\n",
       "\t<li>0.0768</li>\n",
       "\t<li>0.852466666666667</li>\n",
       "\t<li>0.0107333333333333</li>\n",
       "\t<li>0.115533333333333</li>\n",
       "\t<li>0.366866666666667</li>\n",
       "\t<li>0.561233333333333</li>\n",
       "\t<li>0.0624</li>\n",
       "\t<li>0.0104</li>\n",
       "\t<li>0.297333333333333</li>\n",
       "\t<li>0.4828</li>\n",
       "\t<li>0.841123809523809</li>\n",
       "\t<li>0.4754</li>\n",
       "\t<li>0.0382</li>\n",
       "\t<li>0.3354</li>\n",
       "\t<li>0.399733333333333</li>\n",
       "\t<li>0.826319047619048</li>\n",
       "\t<li>0.0742</li>\n",
       "\t<li>0.288</li>\n",
       "\t<li>0.62690303030303</li>\n",
       "\t<li>0.278866666666667</li>\n",
       "\t<li>0.213</li>\n",
       "\t<li>0.570133333333333</li>\n",
       "\t<li>0.0272</li>\n",
       "\t<li>0.1694</li>\n",
       "\t<li>0.784</li>\n",
       "\t<li>0.144733333333333</li>\n",
       "\t<li>0.00333333333333333</li>\n",
       "\t<li>0.369866666666667</li>\n",
       "\t<li>0.125133333333333</li>\n",
       "\t<li>0.051</li>\n",
       "\t<li>0.5634</li>\n",
       "\t<li>0.177288888888889</li>\n",
       "\t<li>0.322933333333333</li>\n",
       "\t<li>0.0213333333333333</li>\n",
       "\t<li>0.3416</li>\n",
       "\t<li>8e-04</li>\n",
       "\t<li>0.0440888888888889</li>\n",
       "\t<li>0.0594666666666667</li>\n",
       "\t<li>0.111</li>\n",
       "\t<li>0.0652</li>\n",
       "\t<li>0.0268666666666667</li>\n",
       "\t<li>0.104266666666667</li>\n",
       "\t<li>0.120133333333333</li>\n",
       "\t<li>0.2918</li>\n",
       "\t<li>0.0204666666666667</li>\n",
       "\t<li>0.0808</li>\n",
       "\t<li>0.0813</li>\n",
       "\t<li>0.0133333333333333</li>\n",
       "\t<li>0.00793333333333333</li>\n",
       "\t<li>0.4382</li>\n",
       "\t<li>0.767866666666667</li>\n",
       "\t<li>0.369723809523809</li>\n",
       "\t<li>0.0972</li>\n",
       "\t<li>0.0618</li>\n",
       "\t<li>0.555555555555556</li>\n",
       "\t<li>0.259866666666667</li>\n",
       "\t<li>0.198866666666667</li>\n",
       "\t<li>0.646038095238095</li>\n",
       "\t<li>0.709961904761905</li>\n",
       "\t<li>0.122266666666667</li>\n",
       "\t<li>0.5438</li>\n",
       "\t<li>0.0821</li>\n",
       "\t<li>0.017</li>\n",
       "\t<li>0.748333333333334</li>\n",
       "\t<li>0.224127272727273</li>\n",
       "\t<li>0.492333333333333</li>\n",
       "\t<li>0.325993073593074</li>\n",
       "\t<li>0.277933333333333</li>\n",
       "\t<li>0.1946</li>\n",
       "\t<li>0.0443333333333333</li>\n",
       "\t<li>0.513866666666667</li>\n",
       "\t<li>0.0910666666666667</li>\n",
       "\t<li>0.0606</li>\n",
       "\t<li>0.2996</li>\n",
       "\t<li>0.5104</li>\n",
       "\t<li>0.449333333333333</li>\n",
       "\t<li>0.14545</li>\n",
       "\t<li>0.660952380952381</li>\n",
       "\t<li>0.346266666666667</li>\n",
       "\t<li>0.294390476190476</li>\n",
       "\t<li>0.106133333333333</li>\n",
       "\t<li>0.107</li>\n",
       "\t<li>0.434460606060606</li>\n",
       "\t<li>0.0903333333333333</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.471066666666667</li>\n",
       "\t<li>0.372733333333333</li>\n",
       "\t<li>0.256866666666667</li>\n",
       "\t<li>0.0552</li>\n",
       "\t<li>0.0738</li>\n",
       "\t<li>0.505035897435897</li>\n",
       "\t<li>0.305247619047619</li>\n",
       "\t<li>0.738333333333333</li>\n",
       "\t<li>0.0513333333333333</li>\n",
       "\t<li>0.553333333333333</li>\n",
       "\t<li>0.0659333333333333</li>\n",
       "\t<li>0.7698</li>\n",
       "\t<li>0.016</li>\n",
       "\t<li>0.0306666666666667</li>\n",
       "\t<li>0.36028354978355</li>\n",
       "\t<li>0.674695238095238</li>\n",
       "\t<li>0.822761904761905</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0575641025641026</li>\n",
       "\t<li>0.0621333333333333</li>\n",
       "\t<li>0.308916666666667</li>\n",
       "\t<li>0.42880303030303</li>\n",
       "\t<li>0.0234</li>\n",
       "\t<li>0.896333333333333</li>\n",
       "\t<li>0.812</li>\n",
       "\t<li>0.642666666666667</li>\n",
       "\t<li>0.352733333333333</li>\n",
       "\t<li>0.677</li>\n",
       "\t<li>0.1694</li>\n",
       "\t<li>0.610333333333333</li>\n",
       "\t<li>0.2728</li>\n",
       "\t<li>0.1418</li>\n",
       "\t<li>0.2014</li>\n",
       "\t<li>0.419066666666667</li>\n",
       "\t<li>0.197733333333333</li>\n",
       "\t<li>0.108627777777778</li>\n",
       "\t<li>0.358133333333333</li>\n",
       "\t<li>0.534</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.123855555555556</li>\n",
       "\t<li>0.0032</li>\n",
       "\t<li>0.539333333333333</li>\n",
       "\t<li>0.452959595959596</li>\n",
       "\t<li>0.0778</li>\n",
       "\t<li>0.343466666666667</li>\n",
       "\t<li>0.0579333333333333</li>\n",
       "\t<li>0.201133333333333</li>\n",
       "\t<li>0.3832</li>\n",
       "\t<li>0.0292</li>\n",
       "\t<li>0.344433333333333</li>\n",
       "\t<li>0.0809333333333334</li>\n",
       "\t<li>0.0162666666666667</li>\n",
       "\t<li>0.6366</li>\n",
       "\t<li>0.0328</li>\n",
       "\t<li>0.0573333333333333</li>\n",
       "\t<li>0.1062</li>\n",
       "\t<li>0.4754</li>\n",
       "\t<li>0.342266666666667</li>\n",
       "\t<li>0.00133333333333333</li>\n",
       "\t<li>0.430622222222222</li>\n",
       "\t<li>0.592933333333333</li>\n",
       "\t<li>0.0032</li>\n",
       "\t<li>0.473666666666667</li>\n",
       "\t<li>0.181266666666667</li>\n",
       "\t<li>0.0337333333333333</li>\n",
       "\t<li>0.332266666666667</li>\n",
       "\t<li>0.246766666666667</li>\n",
       "\t<li>0.191866666666667</li>\n",
       "\t<li>0.0116444444444444</li>\n",
       "\t<li>0.365933333333333</li>\n",
       "\t<li>0.1302</li>\n",
       "\t<li>0.278688888888889</li>\n",
       "\t<li>0.321666666666667</li>\n",
       "\t<li>0.0172</li>\n",
       "\t<li>0.0632666666666667</li>\n",
       "\t<li>0.343733333333333</li>\n",
       "\t<li>0.379435897435897</li>\n",
       "\t<li>0.862</li>\n",
       "\t<li>0.0313333333333333</li>\n",
       "\t<li>0.194733333333333</li>\n",
       "\t<li>0.0176</li>\n",
       "\t<li>0.517302564102564</li>\n",
       "\t<li>0.2246</li>\n",
       "\t<li>0.204333333333333</li>\n",
       "\t<li>0.494355555555556</li>\n",
       "\t<li>0.2418</li>\n",
       "\t<li>0.289390476190476</li>\n",
       "\t<li>0.0644974358974359</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.281466666666667</li>\n",
       "\t<li>0.210933333333333</li>\n",
       "\t<li>0.023</li>\n",
       "\t<li>0.034</li>\n",
       "\t<li>0.3638</li>\n",
       "\t<li>0.624244444444444</li>\n",
       "\t<li>0.285333333333333</li>\n",
       "\t<li>0.0084</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.4997</li>\n",
       "\t<li>0.119533333333333</li>\n",
       "\t<li>0.358533333333333</li>\n",
       "\t<li>0.2148</li>\n",
       "\t<li>0.692233333333333</li>\n",
       "\t<li>0.1018</li>\n",
       "\t<li>0.380466666666667</li>\n",
       "\t<li>0.112733333333333</li>\n",
       "\t<li>0.247</li>\n",
       "\t<li>0.111666666666667</li>\n",
       "\t<li>0.086</li>\n",
       "\t<li>0.648611111111111</li>\n",
       "\t<li>0.653466666666666</li>\n",
       "\t<li>0.1018</li>\n",
       "\t<li>0.418666666666667</li>\n",
       "\t<li>0.1836</li>\n",
       "\t<li>0.110733333333333</li>\n",
       "\t<li>0.0123333333333333</li>\n",
       "\t<li>0.245466666666667</li>\n",
       "\t<li>0.181933333333333</li>\n",
       "\t<li>0.576933333333333</li>\n",
       "\t<li>0.439922807017544</li>\n",
       "\t<li>0.552733333333333</li>\n",
       "\t<li>0.0386666666666667</li>\n",
       "\t<li>0.294409523809524</li>\n",
       "\t<li>0.744076923076923</li>\n",
       "\t<li>0.787533333333333</li>\n",
       "\t<li>0.117828571428571</li>\n",
       "\t<li>0.0647333333333333</li>\n",
       "\t<li>0.0785111111111111</li>\n",
       "\t<li>0.391165367965368</li>\n",
       "\t<li>0.1036</li>\n",
       "\t<li>0.0525333333333333</li>\n",
       "\t<li>0.6824</li>\n",
       "\t<li>0.102980952380952</li>\n",
       "\t<li>0.176666666666667</li>\n",
       "\t<li>0.4444</li>\n",
       "\t<li>0.888333333333333</li>\n",
       "\t<li>0.0742</li>\n",
       "\t<li>0.049</li>\n",
       "\t<li>0.413133333333333</li>\n",
       "\t<li>0.0273555555555556</li>\n",
       "\t<li>0.562666666666667</li>\n",
       "\t<li>0.596095238095238</li>\n",
       "\t<li>0.730533333333333</li>\n",
       "\t<li>0.417755555555555</li>\n",
       "\t<li>0.1578</li>\n",
       "\t<li>0.0202</li>\n",
       "\t<li>0.062</li>\n",
       "\t<li>0.00533333333333333</li>\n",
       "\t<li>0.553695238095238</li>\n",
       "\t<li>0.567133333333334</li>\n",
       "\t<li>0.0566</li>\n",
       "\t<li>0.382266666666667</li>\n",
       "\t<li>0.0423333333333333</li>\n",
       "\t<li>0.679175757575758</li>\n",
       "\t<li>0.362169696969697</li>\n",
       "\t<li>0.817614285714286</li>\n",
       "\t<li>0.0248</li>\n",
       "\t<li>0.809157142857143</li>\n",
       "\t<li>0.449380952380952</li>\n",
       "\t<li>0.321433333333333</li>\n",
       "\t<li>0.0350666666666667</li>\n",
       "\t<li>0.326109090909091</li>\n",
       "\t<li>0.101066666666667</li>\n",
       "\t<li>0.184666666666667</li>\n",
       "\t<li>0.605833333333333</li>\n",
       "\t<li>0.180066666666667</li>\n",
       "\t<li>0.00213333333333333</li>\n",
       "\t<li>0.2508</li>\n",
       "\t<li>0.23050303030303</li>\n",
       "\t<li>0.0544</li>\n",
       "\t<li>0.0499333333333333</li>\n",
       "\t<li>0.601533333333333</li>\n",
       "\t<li>0.3658</li>\n",
       "\t<li>0.1464</li>\n",
       "\t<li>0.00666666666666667</li>\n",
       "\t<li>0.3964</li>\n",
       "\t<li>0.263733333333333</li>\n",
       "\t<li>0.614533333333333</li>\n",
       "\t<li>0.401066666666667</li>\n",
       "\t<li>0.106</li>\n",
       "\t<li>0.5256</li>\n",
       "\t<li>0.328333333333333</li>\n",
       "\t<li>0.6816</li>\n",
       "\t<li>0.008</li>\n",
       "\t<li>0.199666666666667</li>\n",
       "\t<li>0.122322222222222</li>\n",
       "\t<li>0.0172</li>\n",
       "\t<li>0.0364</li>\n",
       "\t<li>0.161266666666667</li>\n",
       "\t<li>0.183</li>\n",
       "\t<li>0.726366666666667</li>\n",
       "\t<li>0.0518666666666667</li>\n",
       "\t<li>0.147133333333333</li>\n",
       "\t<li>0.0366666666666667</li>\n",
       "\t<li>0.6226</li>\n",
       "\t<li>0.0444888888888889</li>\n",
       "\t<li>0.5482</li>\n",
       "\t<li>0.116764102564103</li>\n",
       "\t<li>0.0766666666666667</li>\n",
       "\t<li>0.171933333333333</li>\n",
       "\t<li>0.7975</li>\n",
       "\t<li>0.560733333333333</li>\n",
       "\t<li>0.552533333333333</li>\n",
       "\t<li>0.595181818181818</li>\n",
       "\t<li>0.0329333333333333</li>\n",
       "\t<li>0.463231746031746</li>\n",
       "\t<li>0.526544444444444</li>\n",
       "\t<li>0.0649</li>\n",
       "\t<li>0.189133333333333</li>\n",
       "\t<li>0.802066666666667</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.1826</li>\n",
       "\t<li>0.4496</li>\n",
       "\t<li>0.0616</li>\n",
       "\t<li>0.3174</li>\n",
       "\t<li>0.117583333333333</li>\n",
       "\t<li>0.429733333333333</li>\n",
       "\t<li>0.748133333333333</li>\n",
       "\t<li>0.0422666666666667</li>\n",
       "\t<li>0.675266666666667</li>\n",
       "\t<li>0.0390666666666667</li>\n",
       "\t<li>0.718111111111111</li>\n",
       "\t<li>0.240466666666667</li>\n",
       "\t<li>0.484666666666667</li>\n",
       "\t<li>0.662090476190476</li>\n",
       "\t<li>0.0212888888888889</li>\n",
       "\t<li>0.0298</li>\n",
       "\t<li>0.0171333333333333</li>\n",
       "\t<li>0.813742857142857</li>\n",
       "\t<li>0.7118</li>\n",
       "\t<li>0.0650666666666667</li>\n",
       "\t<li>0.004</li>\n",
       "\t<li>0.0216</li>\n",
       "\t<li>0.0506</li>\n",
       "\t<li>0.0330666666666667</li>\n",
       "\t<li>0.0544</li>\n",
       "\t<li>0.088</li>\n",
       "\t<li>0.684933333333334</li>\n",
       "\t<li>0.1164</li>\n",
       "\t<li>0.012</li>\n",
       "\t<li>0.621771428571429</li>\n",
       "\t<li>0.5336</li>\n",
       "\t<li>0.278133333333333</li>\n",
       "\t<li>0.0402666666666667</li>\n",
       "\t<li>0.722488888888889</li>\n",
       "\t<li>0.0316666666666667</li>\n",
       "\t<li>0.2226</li>\n",
       "\t<li>0.0799333333333333</li>\n",
       "\t<li>0.156466666666667</li>\n",
       "\t<li>0.00773333333333333</li>\n",
       "\t<li>0.370466666666667</li>\n",
       "\t<li>0.583435897435898</li>\n",
       "\t<li>0.213533333333333</li>\n",
       "\t<li>0.366866666666667</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.561095238095238</li>\n",
       "\t<li>0.0978666666666667</li>\n",
       "\t<li>0.00133333333333333</li>\n",
       "\t<li>0.411251082251082</li>\n",
       "\t<li>0.0496</li>\n",
       "\t<li>0.626</li>\n",
       "\t<li>0.565902564102564</li>\n",
       "\t<li>0.337066666666667</li>\n",
       "\t<li>0.1694</li>\n",
       "\t<li>0.7704</li>\n",
       "\t<li>0.155</li>\n",
       "\t<li>0.117933333333333</li>\n",
       "\t<li>0.4018</li>\n",
       "\t<li>0.0769333333333333</li>\n",
       "\t<li>0.3683</li>\n",
       "\t<li>0.734733333333333</li>\n",
       "\t<li>0.274733333333333</li>\n",
       "\t<li>0.0583333333333333</li>\n",
       "\t<li>0.419</li>\n",
       "\t<li>0.0613333333333333</li>\n",
       "\t<li>0.333933333333333</li>\n",
       "\t<li>0.0032</li>\n",
       "\t<li>0.571533333333333</li>\n",
       "\t<li>0.506977777777778</li>\n",
       "\t<li>0.564933333333333</li>\n",
       "\t<li>0.0808222222222222</li>\n",
       "\t<li>0.0105333333333333</li>\n",
       "\t<li>0.0366</li>\n",
       "\t<li>0.807666666666667</li>\n",
       "\t<li>0.0358</li>\n",
       "\t<li>0.589</li>\n",
       "\t<li>0.113133333333333</li>\n",
       "\t<li>0.0211333333333333</li>\n",
       "\t<li>0.2186</li>\n",
       "\t<li>0.0182</li>\n",
       "\t<li>0.434933333333333</li>\n",
       "\t<li>0.0454</li>\n",
       "\t<li>0.725533333333333</li>\n",
       "\t<li>0.551733333333333</li>\n",
       "\t<li>0.6766</li>\n",
       "\t<li>0.041</li>\n",
       "\t<li>0.0293333333333333</li>\n",
       "\t<li>0.273466666666667</li>\n",
       "\t<li>0.00866666666666667</li>\n",
       "\t<li>0.6762</li>\n",
       "\t<li>0.0292</li>\n",
       "\t<li>0.742166666666667</li>\n",
       "\t<li>0.716266666666667</li>\n",
       "\t<li>0.620733333333333</li>\n",
       "\t<li>0.2784</li>\n",
       "\t<li>0.222333333333333</li>\n",
       "\t<li>0.393704761904762</li>\n",
       "\t<li>0.233933333333333</li>\n",
       "\t<li>0.471466666666667</li>\n",
       "\t<li>0.007</li>\n",
       "\t<li>0.0028</li>\n",
       "\t<li>0.898866666666667</li>\n",
       "\t<li>0.2916</li>\n",
       "\t<li>0.8208</li>\n",
       "\t<li>0.575085714285714</li>\n",
       "\t<li>0.0408</li>\n",
       "\t<li>0.573971428571429</li>\n",
       "\t<li>0.0859333333333333</li>\n",
       "\t<li>0.338233333333333</li>\n",
       "\t<li>0.502666666666667</li>\n",
       "\t<li>0.303</li>\n",
       "\t<li>0.102533333333333</li>\n",
       "\t<li>0.334266666666667</li>\n",
       "\t<li>0.370533333333333</li>\n",
       "\t<li>0.0374666666666667</li>\n",
       "\t<li>0.0555333333333333</li>\n",
       "\t<li>0.64150303030303</li>\n",
       "\t<li>0.522990476190476</li>\n",
       "\t<li>0.476066666666667</li>\n",
       "\t<li>0.1566</li>\n",
       "\t<li>0.1546</li>\n",
       "\t<li>0.529235897435897</li>\n",
       "\t<li>0.369866666666667</li>\n",
       "\t<li>0.714466666666667</li>\n",
       "\t<li>0.541733333333333</li>\n",
       "\t<li>0.191466666666667</li>\n",
       "\t<li>0.413133333333333</li>\n",
       "\t<li>0.160866666666667</li>\n",
       "\t<li>0.0352</li>\n",
       "\t<li>0.786136363636364</li>\n",
       "\t<li>0.001</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.556650793650794</li>\n",
       "\t<li>0.594028571428571</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.0590476190476191</li>\n",
       "\t<li>0.249133333333333</li>\n",
       "\t<li>0.09065</li>\n",
       "\t<li>0.855157142857143</li>\n",
       "\t<li>0.0999333333333333</li>\n",
       "\t<li>0.4867</li>\n",
       "\t<li>0.0416888888888889</li>\n",
       "\t<li>0.186933333333333</li>\n",
       "\t<li>0.120466666666667</li>\n",
       "\t<li>0.8998</li>\n",
       "\t<li>0.594361904761905</li>\n",
       "\t<li>0.745733333333333</li>\n",
       "\t<li>0.0442</li>\n",
       "\t<li>0.476</li>\n",
       "\t<li>0.128666666666667</li>\n",
       "\t<li>0.1396</li>\n",
       "\t<li>0.323933333333333</li>\n",
       "\t<li>0.267533333333333</li>\n",
       "\t<li>0.5198</li>\n",
       "\t<li>0.486066666666667</li>\n",
       "\t<li>0.0272</li>\n",
       "\t<li>0.225669841269841</li>\n",
       "\t<li>0.0854</li>\n",
       "\t<li>0.215733333333333</li>\n",
       "\t<li>0.0575333333333333</li>\n",
       "\t<li>0.675066666666667</li>\n",
       "\t<li>0.896133333333333</li>\n",
       "\t<li>0.531733333333333</li>\n",
       "\t<li>0.417733333333333</li>\n",
       "\t<li>0.3298</li>\n",
       "\t<li>0.0631333333333333</li>\n",
       "\t<li>0.3867</li>\n",
       "\t<li>0.0572</li>\n",
       "\t<li>0.2354</li>\n",
       "\t<li>0.273133333333333</li>\n",
       "\t<li>0.238927272727273</li>\n",
       "\t<li>0.215266666666667</li>\n",
       "\t<li>0.0942666666666667</li>\n",
       "\t<li>0.799733333333333</li>\n",
       "\t<li>0.243248484848485</li>\n",
       "\t<li>0.195380952380952</li>\n",
       "\t<li>0.0734666666666667</li>\n",
       "\t<li>0.798961904761905</li>\n",
       "\t<li>0.358133333333333</li>\n",
       "\t<li>0.308733333333333</li>\n",
       "\t<li>0.1464</li>\n",
       "\t<li>0.410769696969697</li>\n",
       "\t<li>0.062</li>\n",
       "\t<li>0.553933333333333</li>\n",
       "\t<li>0.632577896613191</li>\n",
       "\t<li>0.6972</li>\n",
       "\t<li>0.269714285714286</li>\n",
       "\t<li>0.0414</li>\n",
       "\t<li>0.654022222222222</li>\n",
       "\t<li>0.114733333333333</li>\n",
       "\t<li>0.0762666666666667</li>\n",
       "\t<li>0.6346</li>\n",
       "\t<li>0.217533333333333</li>\n",
       "\t<li>0.3662</li>\n",
       "\t<li>0.0168</li>\n",
       "\t<li>0.107</li>\n",
       "\t<li>0.612122807017544</li>\n",
       "\t<li>0.216</li>\n",
       "\t<li>0.197666666666667</li>\n",
       "\t<li>0.3418</li>\n",
       "\t<li>0.189333333333333</li>\n",
       "\t<li>0.0416666666666667</li>\n",
       "\t<li>0.431266666666667</li>\n",
       "\t<li>0.272</li>\n",
       "\t<li>0.2884</li>\n",
       "\t<li>0.488769696969697</li>\n",
       "\t<li>0.233533333333333</li>\n",
       "\t<li>0.367133333333333</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.743638095238095</li>\n",
       "\t<li>0.2864</li>\n",
       "\t<li>0.306733333333333</li>\n",
       "\t<li>0.113</li>\n",
       "\t<li>0.57150303030303</li>\n",
       "\t<li>0.375933333333333</li>\n",
       "\t<li>0.552133333333333</li>\n",
       "\t<li>0.237466666666667</li>\n",
       "\t<li>0.2733</li>\n",
       "\t<li>0.233066666666667</li>\n",
       "\t<li>0.719622222222222</li>\n",
       "\t<li>0.534333333333333</li>\n",
       "\t<li>0.3878</li>\n",
       "\t<li>0.0426666666666667</li>\n",
       "\t<li>0.0904</li>\n",
       "\t<li>0.8228</li>\n",
       "\t<li>0.1826</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0.00133333333333333</li>\n",
       "\t<li>0.309561904761905</li>\n",
       "\t<li>0.022</li>\n",
       "\t<li>0.5014</li>\n",
       "\t<li>0.433866666666667</li>\n",
       "\t<li>0.0202</li>\n",
       "\t<li>0.0277333333333333</li>\n",
       "\t<li>0.0186666666666667</li>\n",
       "\t<li>0.0926</li>\n",
       "\t<li>0.0683333333333333</li>\n",
       "\t<li>0.100892307692308</li>\n",
       "\t<li>0.2592</li>\n",
       "\t<li>0.0875555555555556</li>\n",
       "\t<li>0.457466666666667</li>\n",
       "\t<li>0.0424666666666667</li>\n",
       "\t<li>0.0392</li>\n",
       "\t<li>0.317266666666667</li>\n",
       "\t<li>0.0432</li>\n",
       "\t<li>0.139533333333333</li>\n",
       "\t<li>0.723866666666667</li>\n",
       "\t<li>0.2994</li>\n",
       "\t<li>0.582</li>\n",
       "\t<li>0.202966666666667</li>\n",
       "\t<li>0.0191333333333333</li>\n",
       "\t<li>0.04</li>\n",
       "\t<li>0.212</li>\n",
       "\t<li>0.135066666666667</li>\n",
       "\t<li>0.0194</li>\n",
       "\t<li>0.371366666666667</li>\n",
       "\t<li>0.0032</li>\n",
       "\t<li>0.624133333333333</li>\n",
       "\t<li>0.0082</li>\n",
       "\t<li>0.184171428571429</li>\n",
       "\t<li>0.324066666666667</li>\n",
       "\t<li>0.0549333333333333</li>\n",
       "\t<li>0.209444444444444</li>\n",
       "\t<li>0.0555333333333333</li>\n",
       "\t<li>0.1956</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.0384\n",
       "\\item 0.206466666666667\n",
       "\\item 0.441133333333333\n",
       "\\item 0.0484\n",
       "\\item 0.568552380952381\n",
       "\\item 0.417514285714286\n",
       "\\item 0.4343\n",
       "\\item 0.52650303030303\n",
       "\\item 0.0675833333333333\n",
       "\\item 0.00533333333333333\n",
       "\\item 0.218466666666667\n",
       "\\item 0.420156140350877\n",
       "\\item 0.0290888888888889\n",
       "\\item 0.860369696969697\n",
       "\\item 0.1028\n",
       "\\item 0.0872\n",
       "\\item 0.0203333333333333\n",
       "\\item 0.139133333333333\n",
       "\\item 0.255266666666667\n",
       "\\item 0.0226\n",
       "\\item 0.0876\n",
       "\\item 0.397933333333333\n",
       "\\item 0.8608\n",
       "\\item 0.0088\n",
       "\\item 0.63707619047619\n",
       "\\item 0.0766666666666667\n",
       "\\item 0.48610303030303\n",
       "\\item 0.5024\n",
       "\\item 0.589111111111111\n",
       "\\item 0.2398\n",
       "\\item 0.513533333333333\n",
       "\\item 0.848342857142857\n",
       "\\item 0.0223333333333333\n",
       "\\item 0.470066666666667\n",
       "\\item 0.627866666666667\n",
       "\\item 0.2705\n",
       "\\item 0.6392\n",
       "\\item 0.61310303030303\n",
       "\\item 0.808066666666667\n",
       "\\item 0.021\n",
       "\\item 0.169933333333333\n",
       "\\item 0.225533333333333\n",
       "\\item 0.110066666666667\n",
       "\\item 0.6982\n",
       "\\item 0.184533333333333\n",
       "\\item 0.739533333333333\n",
       "\\item 0.194266666666667\n",
       "\\item 0.0194\n",
       "\\item 0.3604\n",
       "\\item 0.1258\n",
       "\\item 0.0442666666666667\n",
       "\\item 0.013\n",
       "\\item 0.183522222222222\n",
       "\\item 0.345933333333333\n",
       "\\item 0.737066666666667\n",
       "\\item 0.217266666666667\n",
       "\\item 0.696066666666667\n",
       "\\item 0.608633333333333\n",
       "\\item 0.300047619047619\n",
       "\\item 0.505533333333333\n",
       "\\item 0.762161904761905\n",
       "\\item 0.0184\n",
       "\\item 0.233866666666667\n",
       "\\item 0.174866666666667\n",
       "\\item 0.0677333333333334\n",
       "\\item 0.0642666666666667\n",
       "\\item 0.0122\n",
       "\\item 0.0202\n",
       "\\item 0.530133333333333\n",
       "\\item 0.161866666666667\n",
       "\\item 0.464266666666667\n",
       "\\item 0.0173333333333333\n",
       "\\item 0.0146666666666667\n",
       "\\item 0.592533333333333\n",
       "\\item 0.217114285714286\n",
       "\\item 0.206933333333333\n",
       "\\item 0.702133333333333\n",
       "\\item 0.0404\n",
       "\\item 0.472933333333333\n",
       "\\item 0.141466666666667\n",
       "\\item 0.828\n",
       "\\item 0.251333333333333\n",
       "\\item 0.005\n",
       "\\item 0.544066666666667\n",
       "\\item 0.309155555555556\n",
       "\\item 0.237233333333333\n",
       "\\item 0.0673333333333333\n",
       "\\item 0.0736666666666667\n",
       "\\item 0.666933333333333\n",
       "\\item 0.3192\n",
       "\\item 0.01\n",
       "\\item 0.0812666666666667\n",
       "\\item 0.117\n",
       "\\item 0.761942857142857\n",
       "\\item 0.134533333333333\n",
       "\\item 0.442133333333333\n",
       "\\item 0.144688888888889\n",
       "\\item 0.161780952380952\n",
       "\\item 0.406633333333333\n",
       "\\item 0.478266666666667\n",
       "\\item 0.617744418739156\n",
       "\\item 0.0512833333333333\n",
       "\\item 0.640133333333333\n",
       "\\item 0.359933333333333\n",
       "\\item 0.357688888888889\n",
       "\\item 0.4352\n",
       "\\item 0.019\n",
       "\\item 0.2228\n",
       "\\item 0.1364\n",
       "\\item 0.0460666666666667\n",
       "\\item 0.0682\n",
       "\\item 0.808504761904762\n",
       "\\item 0.139\n",
       "\\item 0.701942857142857\n",
       "\\item 0.167466666666667\n",
       "\\item 0.0136666666666667\n",
       "\\item 0.00866666666666667\n",
       "\\item 0.410533333333333\n",
       "\\item 0.0343333333333333\n",
       "\\item 0.0256666666666667\n",
       "\\item 0.0332888888888889\n",
       "\\item 0.0331333333333333\n",
       "\\item 0.028\n",
       "\\item 0.0968666666666667\n",
       "\\item 0.148266666666667\n",
       "\\item 0.0133333333333333\n",
       "\\item 0.0024\n",
       "\\item 0.340619047619048\n",
       "\\item 0.764142857142857\n",
       "\\item 0.0472666666666667\n",
       "\\item 0.6416\n",
       "\\item 0.469333333333333\n",
       "\\item 0.315666666666667\n",
       "\\item 0.4174\n",
       "\\item 0.515495238095238\n",
       "\\item 0.659466666666667\n",
       "\\item 0.0756\n",
       "\\item 0.767434920634921\n",
       "\\item 0.147866666666667\n",
       "\\item 0.697466666666667\n",
       "\\item 0.434333333333333\n",
       "\\item 0.233622222222222\n",
       "\\item 0.253\n",
       "\\item 0.476933333333333\n",
       "\\item 0.407333333333333\n",
       "\\item 0.149266666666667\n",
       "\\item 0.639822222222222\n",
       "\\item 0.192733333333333\n",
       "\\item 0.211888888888889\n",
       "\\item 0.1926\n",
       "\\item 0.0542222222222222\n",
       "\\item 0.387866666666667\n",
       "\\item 0.206\n",
       "\\item 0.1688\n",
       "\\item 0.0301333333333333\n",
       "\\item 0.1112\n",
       "\\item 0.0293333333333333\n",
       "\\item 0.528615384615385\n",
       "\\item 0.3578\n",
       "\\item 0.918266666666667\n",
       "\\item 0.1302\n",
       "\\item 0.1702\n",
       "\\item 0.0486\n",
       "\\item 0.116066666666667\n",
       "\\item 0.0128571428571429\n",
       "\\item 0.663588278388278\n",
       "\\item 0.0739333333333333\n",
       "\\item 0.531066666666667\n",
       "\\item 0.034\n",
       "\\item 0.295866666666667\n",
       "\\item 0.646533333333333\n",
       "\\item 0.302966666666667\n",
       "\\item 0.394733333333333\n",
       "\\item 0.0288\n",
       "\\item 0.418847619047619\n",
       "\\item 0.007\n",
       "\\item 0.234266666666667\n",
       "\\item 0.325066666666667\n",
       "\\item 0.668761904761905\n",
       "\\item 0.849933333333333\n",
       "\\item 0.836266666666667\n",
       "\\item 0.903\n",
       "\\item 0.491688888888889\n",
       "\\item 0\n",
       "\\item 0.860288888888889\n",
       "\\item 0.0342666666666667\n",
       "\\item 0.0068\n",
       "\\item 0.203533333333333\n",
       "\\item 0.252933333333333\n",
       "\\item 0.366\n",
       "\\item 0.006\n",
       "\\item 0.4553\n",
       "\\item 0.601504761904762\n",
       "\\item 0.1954\n",
       "\\item 0.008\n",
       "\\item 0.0076\n",
       "\\item 0.0856666666666667\n",
       "\\item 0.0419333333333333\n",
       "\\item 0.0350666666666667\n",
       "\\item 0.751809523809524\n",
       "\\item 0.486\n",
       "\\item 0.166133333333333\n",
       "\\item 0.170533333333333\n",
       "\\item 0.561133333333333\n",
       "\\item 0.284969841269841\n",
       "\\item 0.6282\n",
       "\\item 0.0330666666666667\n",
       "\\item 0.8897\n",
       "\\item 0.0762666666666667\n",
       "\\item 0.829066666666667\n",
       "\\item 0.0540666666666667\n",
       "\\item 0\n",
       "\\item 0.0879333333333333\n",
       "\\item 0.515066666666667\n",
       "\\item 0.135716666666667\n",
       "\\item 0.7348\n",
       "\\item 0.134466666666667\n",
       "\\item 0.0577333333333333\n",
       "\\item 0.008\n",
       "\\item 0.133133333333333\n",
       "\\item 0.0398666666666667\n",
       "\\item 0.0181333333333333\n",
       "\\item 0.386155555555556\n",
       "\\item 0.6736\n",
       "\\item 0.0676666666666667\n",
       "\\item 0.172380952380952\n",
       "\\item 0.406447619047619\n",
       "\\item 0.625933333333333\n",
       "\\item 0.244466666666667\n",
       "\\item 0.002\n",
       "\\item 0.806209523809524\n",
       "\\item 0.0403333333333333\n",
       "\\item 0.432918045112782\n",
       "\\item 0.0575333333333333\n",
       "\\item 0.0176666666666667\n",
       "\\item 0.0554\n",
       "\\item 0.322955555555555\n",
       "\\item 0.148266666666667\n",
       "\\item 0.588833333333333\n",
       "\\item 0.7321\n",
       "\\item 0.0332\n",
       "\\item 0\n",
       "\\item 0.0454666666666667\n",
       "\\item 0.128733333333333\n",
       "\\item 0.0615333333333333\n",
       "\\item 0.0259333333333333\n",
       "\\item 0.2938\n",
       "\\item 0.604466666666667\n",
       "\\item 0.293180952380952\n",
       "\\item 0\n",
       "\\item 0.0542\n",
       "\\item 0.624761904761905\n",
       "\\item 0.748666666666667\n",
       "\\item 0.37767619047619\n",
       "\\item 0.0690666666666667\n",
       "\\item 0.490733333333333\n",
       "\\item 0.8684\n",
       "\\item 0.20385974025974\n",
       "\\item 0.1262\n",
       "\\item 0.1322\n",
       "\\item 0.446662745098039\n",
       "\\item 0.271530952380952\n",
       "\\item 0.0333333333333333\n",
       "\\item 0.343066666666667\n",
       "\\item 0.583704273504274\n",
       "\\item 0.358555555555555\n",
       "\\item 0.0388\n",
       "\\item 0.145866666666667\n",
       "\\item 0.0588\n",
       "\\item 0.130066666666667\n",
       "\\item 0.378488888888889\n",
       "\\item 0.136696103896104\n",
       "\\item 0.349577777777778\n",
       "\\item 0.201533333333333\n",
       "\\item 0.078\n",
       "\\item 0.2348\n",
       "\\item 0.628266666666667\n",
       "\\item 0.0378\n",
       "\\item 0.27520404040404\n",
       "\\item 0.1202\n",
       "\\item 0.382\n",
       "\\item 0.101733333333333\n",
       "\\item 0.3042\n",
       "\\item 0.105533333333333\n",
       "\\item 0.618019047619048\n",
       "\\item 0.337066666666667\n",
       "\\item 0.0877666666666667\n",
       "\\item 0.456895238095238\n",
       "\\item 0.695733333333334\n",
       "\\item 0.526\n",
       "\\item 0.424315384615385\n",
       "\\item 0.119133333333333\n",
       "\\item 0.375266666666667\n",
       "\\item 0.514166666666667\n",
       "\\item 0.348466666666667\n",
       "\\item 0.835866666666667\n",
       "\\item 0.7324\n",
       "\\item 0.148449816849817\n",
       "\\item 0.0544666666666667\n",
       "\\item 0.0476666666666667\n",
       "\\item 0.848677777777778\n",
       "\\item 0.0408\n",
       "\\item 0.0185333333333333\n",
       "\\item 0\n",
       "\\item 0.0516\n",
       "\\item 0.0334\n",
       "\\item 0.102533333333333\n",
       "\\item 0.180333333333333\n",
       "\\item 0.0417\n",
       "\\item 0.0188\n",
       "\\item 0.119733333333333\n",
       "\\item 0.586066666666667\n",
       "\\item 0.670866666666667\n",
       "\\item 0.0564\n",
       "\\item 0.0304\n",
       "\\item 0.698671524966262\n",
       "\\item 0.261066666666667\n",
       "\\item 0.1222\n",
       "\\item 0.5628\n",
       "\\item 0.625066666666667\n",
       "\\item 0.02\n",
       "\\item 0.173066666666667\n",
       "\\item 0.0314\n",
       "\\item 0.308066666666667\n",
       "\\item 0.27\n",
       "\\item 0.579961904761905\n",
       "\\item 0.505066666666667\n",
       "\\item 0.462357142857143\n",
       "\\item 0.0121333333333333\n",
       "\\item 0.390695238095238\n",
       "\\item 0.464142857142857\n",
       "\\item 0.250733333333333\n",
       "\\item 0.448133333333333\n",
       "\\item 0.597842424242424\n",
       "\\item 0.792010256410256\n",
       "\\item 0.198466666666667\n",
       "\\item 0.543266666666667\n",
       "\\item 0.5032\n",
       "\\item 0.0381333333333333\n",
       "\\item 0.162\n",
       "\\item 0.0727333333333333\n",
       "\\item 0.0906666666666667\n",
       "\\item 0.0426666666666667\n",
       "\\item 0.24767619047619\n",
       "\\item 0.574945454545455\n",
       "\\item 0.0128\n",
       "\\item 0.252133333333333\n",
       "\\item 0.266866666666667\n",
       "\\item 0.0312\n",
       "\\item 0.0288\n",
       "\\item 0.0430222222222222\n",
       "\\item 0.015\n",
       "\\item 0.1694\n",
       "\\item 0.633933333333333\n",
       "\\item 0.123266666666667\n",
       "\\item 0.388351633986928\n",
       "\\item 0.634293506493506\n",
       "\\item 0.0818666666666667\n",
       "\\item 0.1352\n",
       "\\item 0.328244444444444\n",
       "\\item 0.0732\n",
       "\\item 0.0244666666666667\n",
       "\\item 0.136980952380952\n",
       "\\item 0.110466666666667\n",
       "\\item 0.417266666666667\n",
       "\\item 0.323866666666667\n",
       "\\item 0.670266666666667\n",
       "\\item 0.001\n",
       "\\item 0.0268\n",
       "\\item 0.0800222222222222\n",
       "\\item 0.711569696969697\n",
       "\\item 0.1312\n",
       "\\item 0.021\n",
       "\\item 0.0884\n",
       "\\item 0.113266666666667\n",
       "\\item 0.734333333333333\n",
       "\\item 0.175266666666667\n",
       "\\item 0.0734666666666667\n",
       "\\item 0.0116\n",
       "\\item 0.293333333333333\n",
       "\\item 0.277933333333333\n",
       "\\item 0.5704\n",
       "\\item 0.034\n",
       "\\item 0.779533333333333\n",
       "\\item 0.15865\n",
       "\\item 0.0683333333333333\n",
       "\\item 0.0176\n",
       "\\item 0.352333333333333\n",
       "\\item 0.0818\n",
       "\\item 0.504\n",
       "\\item 0.004\n",
       "\\item 0.121866666666667\n",
       "\\item 0.676104761904762\n",
       "\\item 0.118533333333333\n",
       "\\item 0.0912\n",
       "\\item 0.4195\n",
       "\\item 0.0548\n",
       "\\item 0.0612888888888889\n",
       "\\item 0.02\n",
       "\\item 0.0108\n",
       "\\item 0.133897435897436\n",
       "\\item 0.636533333333333\n",
       "\\item 0.200133333333333\n",
       "\\item 0.909266666666667\n",
       "\\item 0.0857333333333333\n",
       "\\item 0.500466666666667\n",
       "\\item 0.114697435897436\n",
       "\\item 0.263482051282051\n",
       "\\item 0.605866666666667\n",
       "\\item 0.0510666666666667\n",
       "\\item 0.233355555555556\n",
       "\\item 0.034\n",
       "\\item 0.410466666666667\n",
       "\\item 0.023\n",
       "\\item 0.5652\n",
       "\\item 0.526266666666667\n",
       "\\item 0.187066666666667\n",
       "\\item 0.630533333333333\n",
       "\\item 0.380333333333333\n",
       "\\item 0.628666666666667\n",
       "\\item 0.347482828282828\n",
       "\\item 0.6542\n",
       "\\item 0.0082\n",
       "\\item 0.0491333333333333\n",
       "\\item 0.204066666666667\n",
       "\\item 0.383828571428572\n",
       "\\item 0.1236\n",
       "\\item 0.563369696969697\n",
       "\\item 0.138181562881563\n",
       "\\item 0.5213\n",
       "\\item 0.413933333333333\n",
       "\\item 0.0808\n",
       "\\item 0.376266666666667\n",
       "\\item 0.369533333333333\n",
       "\\item 0.278047619047619\n",
       "\\item 0.2165\n",
       "\\item 0.0106\n",
       "\\item 0.2436\n",
       "\\item 0.767302564102564\n",
       "\\item 0.411433333333333\n",
       "\\item 0.1154\n",
       "\\item 0.120848484848485\n",
       "\\item 0.321133333333333\n",
       "\\item 0.825571428571429\n",
       "\\item 0.210459829059829\n",
       "\\item 0.34685\n",
       "\\item 0.0072\n",
       "\\item 0.0144\n",
       "\\item 0.643533333333333\n",
       "\\item 0.0453333333333333\n",
       "\\item 0.555733333333333\n",
       "\\item 0.264333333333333\n",
       "\\item 0.0601\n",
       "\\item 0.720628571428571\n",
       "\\item 0.226914285714286\n",
       "\\item 0.0024\n",
       "\\item 0.443866666666667\n",
       "\\item 0.386577777777778\n",
       "\\item 0.683733333333333\n",
       "\\item 0.692666666666667\n",
       "\\item 0.0965333333333333\n",
       "\\item 0.0906\n",
       "\\item 0.184161904761905\n",
       "\\item 0.825685714285714\n",
       "\\item 0.0323333333333333\n",
       "\\item 0.581733333333333\n",
       "\\item 0.170633333333333\n",
       "\\item 0.064\n",
       "\\item 0.0418666666666667\n",
       "\\item 0.688161904761905\n",
       "\\item 0.348422222222222\n",
       "\\item 0.690333333333333\n",
       "\\item 0.10452380952381\n",
       "\\item 0.0914\n",
       "\\item 0.556866666666667\n",
       "\\item 0.0188\n",
       "\\item 0.271466666666667\n",
       "\\item 0.539022222222222\n",
       "\\item 0.273266666666667\n",
       "\\item 0.0141333333333333\n",
       "\\item 0\n",
       "\\item 0.121447619047619\n",
       "\\item 0.771266666666667\n",
       "\\item 0.1372\n",
       "\\item 0.0132\n",
       "\\item 0.127381818181818\n",
       "\\item 0.593633333333333\n",
       "\\item 0.0202\n",
       "\\item 0.338142857142857\n",
       "\\item 0.0661333333333333\n",
       "\\item 0.598933333333333\n",
       "\\item 0.4218\n",
       "\\item 0.580360606060606\n",
       "\\item 0.681633333333333\n",
       "\\item 0.134822222222222\n",
       "\\item 0.0138666666666667\n",
       "\\item 0.726485714285714\n",
       "\\item 0.018\n",
       "\\item 0.799019047619047\n",
       "\\item 0.113333333333333\n",
       "\\item 0.152333333333333\n",
       "\\item 0.131057142857143\n",
       "\\item 0.0183333333333333\n",
       "\\item 0.0113333333333333\n",
       "\\item 0.159533333333333\n",
       "\\item 0.013\n",
       "\\item 0.4057\n",
       "\\item 0.615422222222222\n",
       "\\item 0.1036\n",
       "\\item 0.0498666666666667\n",
       "\\item 0.441333333333333\n",
       "\\item 0.455933333333333\n",
       "\\item 0.0197333333333333\n",
       "\\item 0.0132\n",
       "\\item 0.0444\n",
       "\\item 0.347133333333333\n",
       "\\item 0.0440285714285714\n",
       "\\item 0.0466\n",
       "\\item 0.539733333333333\n",
       "\\item 0.489533333333333\n",
       "\\item 0.008\n",
       "\\item 0.0312\n",
       "\\item 0.483742857142857\n",
       "\\item 0.012\n",
       "\\item 0.2564\n",
       "\\item 0.349733333333333\n",
       "\\item 0.0481333333333333\n",
       "\\item 0.561133333333333\n",
       "\\item 0.134\n",
       "\\item 0.0140666666666667\n",
       "\\item 0.257\n",
       "\\item 0.0258\n",
       "\\item 0.624533333333333\n",
       "\\item 0.587936363636364\n",
       "\\item 0.487933333333333\n",
       "\\item 0.008\n",
       "\\item 0.875275757575758\n",
       "\\item 0.6896\n",
       "\\item 0.2538\n",
       "\\item 0.2456\n",
       "\\item 0.131533333333333\n",
       "\\item 0.8014\n",
       "\\item 0.312203174603175\n",
       "\\item 0.165466666666667\n",
       "\\item 0.150733333333333\n",
       "\\item 0.06965\n",
       "\\item 0.66675\n",
       "\\item 0.776466666666667\n",
       "\\item 0.0113333333333333\n",
       "\\item 0.599\n",
       "\\item 0.004\n",
       "\\item 0.0614\n",
       "\\item 0.0778666666666667\n",
       "\\item 0.804355555555556\n",
       "\\item 0.348933333333333\n",
       "\\item 0.609533333333333\n",
       "\\item 0.083\n",
       "\\item 0.048\n",
       "\\item 0.0781333333333333\n",
       "\\item 0.329244444444444\n",
       "\\item 0.0318666666666667\n",
       "\\item 0.0939333333333333\n",
       "\\item 0.0586666666666667\n",
       "\\item 0.1614\n",
       "\\item 0.285088888888889\n",
       "\\item 0.008\n",
       "\\item 0.276266666666667\n",
       "\\item 0.0314\n",
       "\\item 0.656966666666667\n",
       "\\item 0.444538095238095\n",
       "\\item 0.0246666666666667\n",
       "\\item 0.250285714285714\n",
       "\\item 0.435933333333333\n",
       "\\item 0.311333333333333\n",
       "\\item 0.443466666666667\n",
       "\\item 0.498166666666667\n",
       "\\item 0.5006\n",
       "\\item 0.190866666666667\n",
       "\\item 0.104266666666667\n",
       "\\item 0.0278666666666667\n",
       "\\item 0.0162666666666667\n",
       "\\item 0.0068\n",
       "\\item 0.384666666666667\n",
       "\\item 0.430755555555556\n",
       "\\item 0.273\n",
       "\\item 0.433833333333333\n",
       "\\item 0.0382\n",
       "\\item 0.0416\n",
       "\\item 0.283066666666667\n",
       "\\item 0.041\n",
       "\\item 0.397533333333333\n",
       "\\item 0.2058\n",
       "\\item 0.0488\n",
       "\\item 0.00533333333333333\n",
       "\\item 0.0284\n",
       "\\item 0.577218300653595\n",
       "\\item 0.0288\n",
       "\\item 0.0497111111111111\n",
       "\\item 0.0276\n",
       "\\item 0.506448484848485\n",
       "\\item 0.043\n",
       "\\item 0.313866666666667\n",
       "\\item 0.634866666666667\n",
       "\\item 0.0640666666666667\n",
       "\\item 0.0881809523809524\n",
       "\\item 0.319923809523809\n",
       "\\item 0.772028571428572\n",
       "\\item 0.0893333333333333\n",
       "\\item 0.0627333333333333\n",
       "\\item 0.0899333333333333\n",
       "\\item 0.255775757575758\n",
       "\\item 0.461455555555556\n",
       "\\item 0.632056140350877\n",
       "\\item 0.1374\n",
       "\\item 0.0712\n",
       "\\item 0.4774\n",
       "\\item 0.0306\n",
       "\\item 0.325066666666667\n",
       "\\item 0.206066666666667\n",
       "\\item 0.918333333333333\n",
       "\\item 0.2567\n",
       "\\item 0.5268\n",
       "\\item 0.823995238095238\n",
       "\\item 0.539933333333333\n",
       "\\item 0.438244444444444\n",
       "\\item 0.273933333333333\n",
       "\\item 0.106666666666667\n",
       "\\item 0.0164\n",
       "\\item 0.460666666666667\n",
       "\\item 0.0959333333333333\n",
       "\\item 0.1478\n",
       "\\item 0.0886666666666667\n",
       "\\item 0.585866666666667\n",
       "\\item 0.3264\n",
       "\\item 0.507666666666667\n",
       "\\item 0.0242666666666667\n",
       "\\item 0.385169696969697\n",
       "\\item 0.190866666666667\n",
       "\\item 0.0229333333333333\n",
       "\\item 0.2196\n",
       "\\item 0.335733333333333\n",
       "\\item 0.114066666666667\n",
       "\\item 0.00986666666666667\n",
       "\\item 0.1424\n",
       "\\item 0.107466666666667\n",
       "\\item 0.0280666666666667\n",
       "\\item 0.119333333333333\n",
       "\\item 0.0361333333333333\n",
       "\\item 0.6014\n",
       "\\item 0.363236363636364\n",
       "\\item 0.16610303030303\n",
       "\\item 0.0593333333333333\n",
       "\\item 0.565169696969697\n",
       "\\item 0.370374891774892\n",
       "\\item 0.337822222222222\n",
       "\\item 0.178266666666667\n",
       "\\item 0.753295238095238\n",
       "\\item 0.60904662004662\n",
       "\\item 0.579828571428571\n",
       "\\item 0.2234\n",
       "\\item 0.116\n",
       "\\item 0.0258\n",
       "\\item 0.0555333333333333\n",
       "\\item 0.553085714285714\n",
       "\\item 0.242866666666667\n",
       "\\item 0.1392\n",
       "\\item 0.655604761904762\n",
       "\\item 0.0341333333333333\n",
       "\\item 0\n",
       "\\item 0.663666666666667\n",
       "\\item 0.423333333333333\n",
       "\\item 0.0750666666666667\n",
       "\\item 0.004\n",
       "\\item 0.731727272727273\n",
       "\\item 0.495666666666667\n",
       "\\item 0.489209523809524\n",
       "\\item 0.024\n",
       "\\item 0.4498\n",
       "\\item 0.532933333333333\n",
       "\\item 0.4976\n",
       "\\item 0.0434\n",
       "\\item 0.808\n",
       "\\item 0.098\n",
       "\\item 0.295466666666667\n",
       "\\item 0.0828222222222222\n",
       "\\item 0.691\n",
       "\\item 0.368177777777778\n",
       "\\item 0.415733333333333\n",
       "\\item 0.172666666666667\n",
       "\\item 0.2466\n",
       "\\item 0.264514285714286\n",
       "\\item 0.142\n",
       "\\item 0.613666666666667\n",
       "\\item 0.386\n",
       "\\item 0.0548\n",
       "\\item 0.303\n",
       "\\item 0.0562\n",
       "\\item 0.0496380952380952\n",
       "\\item 0.0142666666666667\n",
       "\\item 0.2236\n",
       "\\item 0.0474\n",
       "\\item 0.251322222222222\n",
       "\\item 0.757666666666667\n",
       "\\item 0.1774\n",
       "\\item 0.6092\n",
       "\\item 0.655433333333333\n",
       "\\item 0.425533333333333\n",
       "\\item 0.0386\n",
       "\\item 0.3898\n",
       "\\item 0.116466666666667\n",
       "\\item 0.443888888888889\n",
       "\\item 0.872533333333333\n",
       "\\item 0.0457333333333333\n",
       "\\item 0.0228666666666667\n",
       "\\item 0.2504\n",
       "\\item 0.497822222222222\n",
       "\\item 0.562542857142857\n",
       "\\item 0.467207326007326\n",
       "\\item 0.772333333333333\n",
       "\\item 0.031\n",
       "\\item 0.2836\n",
       "\\item 0.403466666666667\n",
       "\\item 0.6291\n",
       "\\item 0.0975333333333333\n",
       "\\item 0.0265333333333333\n",
       "\\item 0.0506666666666667\n",
       "\\item 0.406076190476191\n",
       "\\item 0.478645887445888\n",
       "\\item 0.4038\n",
       "\\item 0.041\n",
       "\\item 0.512807936507937\n",
       "\\item 0.646666666666667\n",
       "\\item 0.205996078431373\n",
       "\\item 0.122\n",
       "\\item 0.007\n",
       "\\item 0.0612\n",
       "\\item 0.0538666666666667\n",
       "\\item 0.636333333333333\n",
       "\\item 0.103\n",
       "\\item 0.0476\n",
       "\\item 0.003\n",
       "\\item 0.118266666666667\n",
       "\\item 0.72508051948052\n",
       "\\item 0.0708666666666667\n",
       "\\item 0.866933333333333\n",
       "\\item 0.150866666666667\n",
       "\\item 0.603666666666667\n",
       "\\item 0.701045887445887\n",
       "\\item 0.011\n",
       "\\item 0.174533333333333\n",
       "\\item 0.012\n",
       "\\item 0.0724666666666667\n",
       "\\item 0.1368\n",
       "\\item 0.203733333333333\n",
       "\\item 0.0724666666666667\n",
       "\\item 0.78272380952381\n",
       "\\item 0.163684711779449\n",
       "\\item 0.0552666666666667\n",
       "\\item 0.057\n",
       "\\item 0.0579333333333333\n",
       "\\item 0.299923809523809\n",
       "\\item 0.450488888888889\n",
       "\\item 0.0916\n",
       "\\item 0.297866666666667\n",
       "\\item 0.903228571428571\n",
       "\\item 0.1064\n",
       "\\item 0\n",
       "\\item 0.00733333333333333\n",
       "\\item 0.51248838612368\n",
       "\\item 0.3064\n",
       "\\item 0.0389333333333333\n",
       "\\item 0.0418666666666667\n",
       "\\item 0.573233333333333\n",
       "\\item 0.799533333333333\n",
       "\\item 0.597933333333333\n",
       "\\item 0.121466666666667\n",
       "\\item 0.8176\n",
       "\\item 0.461330402930403\n",
       "\\item 0.0531333333333333\n",
       "\\item 0.457366666666667\n",
       "\\item 0.1106\n",
       "\\item 0.001\n",
       "\\item 0.605761904761905\n",
       "\\item 0.032\n",
       "\\item 0.0516666666666667\n",
       "\\item 0.5368\n",
       "\\item 0.4388\n",
       "\\item 0.4436\n",
       "\\item 0.205\n",
       "\\item 0.4114\n",
       "\\item 0.00933333333333333\n",
       "\\item 0.301977777777778\n",
       "\\item 0.735366666666667\n",
       "\\item 0.016\n",
       "\\item 0.233666666666667\n",
       "\\item 0.0032\n",
       "\\item 0.012\n",
       "\\item 0.0048\n",
       "\\item 0.0384\n",
       "\\item 0.358733333333333\n",
       "\\item 0.218333333333333\n",
       "\\item 0.0106666666666667\n",
       "\\item 0.300504761904762\n",
       "\\item 0.141190476190476\n",
       "\\item 0.281314285714286\n",
       "\\item 0.0676\n",
       "\\item 0.454466666666667\n",
       "\\item 0.0514\n",
       "\\item 0.1074\n",
       "\\item 0.695466666666667\n",
       "\\item 0.395971428571429\n",
       "\\item 0.0938666666666667\n",
       "\\item 0.0126666666666667\n",
       "\\item 0.0736\n",
       "\\item 0.441365079365079\n",
       "\\item 0.661666666666667\n",
       "\\item 0.151\n",
       "\\item 0.2728\n",
       "\\item 0.459209523809524\n",
       "\\item 0.0498\n",
       "\\item 0.285633333333333\n",
       "\\item 0.0406\n",
       "\\item 0.514933333333333\n",
       "\\item 0.564566666666667\n",
       "\\item 0.0552666666666667\n",
       "\\item 0.0168666666666667\n",
       "\\item 0.701066666666667\n",
       "\\item 0.0198\n",
       "\\item 0.200466666666667\n",
       "\\item 0.4523\n",
       "\\item 0.0721333333333333\n",
       "\\item 0.0352\n",
       "\\item 0.732733333333333\n",
       "\\item 0.589622222222222\n",
       "\\item 0.654933333333333\n",
       "\\item 0.499866666666667\n",
       "\\item 0.159688888888889\n",
       "\\item 0.4317\n",
       "\\item 0.0599333333333333\n",
       "\\item 0.704733333333333\n",
       "\\item 0.221382051282051\n",
       "\\item 0.6252\n",
       "\\item 0.0284\n",
       "\\item 0.698333333333333\n",
       "\\item 0.727933333333333\n",
       "\\item 0.6536\n",
       "\\item 0.0173333333333333\n",
       "\\item 0.188733333333333\n",
       "\\item 0.081\n",
       "\\item 0.374766666666667\n",
       "\\item 0.0896666666666667\n",
       "\\item 0.247866666666667\n",
       "\\item 0.584533333333333\n",
       "\\item 0.0136\n",
       "\\item 0.872133333333333\n",
       "\\item 0.306466666666667\n",
       "\\item 0.0647333333333333\n",
       "\\item 0\n",
       "\\item 0.0024\n",
       "\\item 0.646190476190476\n",
       "\\item 0.017\n",
       "\\item 0.0693333333333333\n",
       "\\item 0.199066666666667\n",
       "\\item 0.4084\n",
       "\\item 0.505333333333333\n",
       "\\item 0.0664222222222222\n",
       "\\item 0.107733333333333\n",
       "\\item 0.4294\n",
       "\\item 0.383466666666667\n",
       "\\item 0.0930666666666667\n",
       "\\item 0.1202\n",
       "\\item 0.002\n",
       "\\item 0.0228666666666667\n",
       "\\item 0.2854\n",
       "\\item 0.352333333333333\n",
       "\\item 0.3938\n",
       "\\item 0.172266666666667\n",
       "\\item 0.333233333333333\n",
       "\\item 0.551666666666667\n",
       "\\item 0.8372\n",
       "\\item 0.938933333333333\n",
       "\\item 0.370682051282051\n",
       "\\item 0.6364\n",
       "\\item 0.245933333333333\n",
       "\\item 0.0641555555555556\n",
       "\\item 0.116066666666667\n",
       "\\item 0.0705333333333333\n",
       "\\item 0.709066666666667\n",
       "\\item 0.284888888888889\n",
       "\\item 0.586828571428571\n",
       "\\item 0.7202\n",
       "\\item 0.693866666666667\n",
       "\\item 0.226133333333333\n",
       "\\item 0.233066666666667\n",
       "\\item 0.251933333333333\n",
       "\\item 0.1629\n",
       "\\item 0.1852\n",
       "\\item 0.370222222222222\n",
       "\\item 0.197933333333333\n",
       "\\item 0.173177777777778\n",
       "\\item 0.214066666666667\n",
       "\\item 0.66\n",
       "\\item 0.0219777777777778\n",
       "\\item 0.4735\n",
       "\\item 0.626419047619048\n",
       "\\item 0.204\n",
       "\\item 0.1664\n",
       "\\item 0.0172\n",
       "\\item 0.389066666666667\n",
       "\\item 0.548473992673992\n",
       "\\item 0.0391333333333333\n",
       "\\item 0.760057142857143\n",
       "\\item 0.814066666666667\n",
       "\\item 0.487933333333333\n",
       "\\item 0.0762666666666667\n",
       "\\item 0.016\n",
       "\\item 0.587466666666667\n",
       "\\item 0.560866666666667\n",
       "\\item 0.3756\n",
       "\\item 0.0688\n",
       "\\item 0.0256666666666667\n",
       "\\item 0.5748\n",
       "\\item 0.442533333333333\n",
       "\\item 0.724266666666667\n",
       "\\item 0.0578666666666667\n",
       "\\item 0.0048\n",
       "\\item 0.271638095238095\n",
       "\\item 0.2132\n",
       "\\item 0.323833333333333\n",
       "\\item 0.565780952380952\n",
       "\\item 0.9462\n",
       "\\item 0.0466666666666667\n",
       "\\item 0.123666666666667\n",
       "\\item 0.850247619047619\n",
       "\\item 0.271115151515152\n",
       "\\item 0.0545333333333333\n",
       "\\item 0.52965974025974\n",
       "\\item 0.0590666666666667\n",
       "\\item 0.0907904761904762\n",
       "\\item 0.280133333333333\n",
       "\\item 0.497692307692308\n",
       "\\item 0.546333333333333\n",
       "\\item 0.392133333333333\n",
       "\\item 0.00786666666666667\n",
       "\\item 0.301\n",
       "\\item 0.039\n",
       "\\item 0.028\n",
       "\\item 0.919933333333333\n",
       "\\item 0.794533333333333\n",
       "\\item 0.778266666666667\n",
       "\\item 0.285262745098039\n",
       "\\item 0.300666666666667\n",
       "\\item 0.1062\n",
       "\\item 0.825\n",
       "\\item 0.15387619047619\n",
       "\\item 0.3356\n",
       "\\item 0.011\n",
       "\\item 0.0432\n",
       "\\item 0.730666666666667\n",
       "\\item 0.0751333333333333\n",
       "\\item 0.026\n",
       "\\item 0.195266666666667\n",
       "\\item 0\n",
       "\\item 0.0887833333333333\n",
       "\\item 0.0667333333333333\n",
       "\\item 0.47570303030303\n",
       "\\item 0.169866666666667\n",
       "\\item 0.06\n",
       "\\item 0.518\n",
       "\\item 0.749466666666667\n",
       "\\item 0.0626666666666667\n",
       "\\item 0.299266666666667\n",
       "\\item 0.643022222222222\n",
       "\\item 0.504666666666667\n",
       "\\item 0.26585974025974\n",
       "\\item 0.0640666666666667\n",
       "\\item 0.771028571428572\n",
       "\\item 0.370133333333333\n",
       "\\item 0.226057142857143\n",
       "\\item 0.704866666666667\n",
       "\\item 0.912266666666667\n",
       "\\item 0.602590476190476\n",
       "\\item 0.4874\n",
       "\\item 0.553409523809524\n",
       "\\item 0.628009523809524\n",
       "\\item 0.440129411764706\n",
       "\\item 0.348755555555556\n",
       "\\item 0.475066666666667\n",
       "\\item 0.472929411764706\n",
       "\\item 0.004\n",
       "\\item 0.1022\n",
       "\\item 0.0205333333333333\n",
       "\\item 0.0625333333333333\n",
       "\\item 0.262695238095238\n",
       "\\item 0.0899333333333333\n",
       "\\item 0.7598\n",
       "\\item 0.400888888888889\n",
       "\\item 0.0524\n",
       "\\item 0.0258\n",
       "\\item 0.570848717948718\n",
       "\\item 0.044\n",
       "\\item 0.30070303030303\n",
       "\\item 0.413133333333333\n",
       "\\item 0.3964\n",
       "\\item 0.349533333333333\n",
       "\\item 0.794466666666667\n",
       "\\item 0.7002\n",
       "\\item 0.0558666666666667\n",
       "\\item 0.0032\n",
       "\\item 0.0890666666666667\n",
       "\\item 0.42\n",
       "\\item 0.285466666666667\n",
       "\\item 0.0918222222222222\n",
       "\\item 0.523\n",
       "\\item 0.1118\n",
       "\\item 0.766466666666667\n",
       "\\item 0.285666666666667\n",
       "\\item 0.2864\n",
       "\\item 0.250933333333333\n",
       "\\item 0.754933333333333\n",
       "\\item 0.192283333333333\n",
       "\\item 0.008\n",
       "\\item 0.0444\n",
       "\\item 0.006\n",
       "\\item 0.001\n",
       "\\item 0.168685714285714\n",
       "\\item 0.367066666666667\n",
       "\\item 0.389066666666667\n",
       "\\item 0.1724\n",
       "\\item 0\n",
       "\\item 0.166433333333333\n",
       "\\item 0.628547008547009\n",
       "\\item 0.643\n",
       "\\item 0.489533333333333\n",
       "\\item 0.444104761904762\n",
       "\\item 0.0688666666666667\n",
       "\\item 0.128133333333333\n",
       "\\item 0.4426\n",
       "\\item 0.036\n",
       "\\item 0.0963333333333333\n",
       "\\item 0.794666666666667\n",
       "\\item 0.661587545787546\n",
       "\\item 0.834228571428572\n",
       "\\item 0.0522\n",
       "\\item 0.0462888888888889\n",
       "\\item 0.0827\n",
       "\\item 0.272847619047619\n",
       "\\item 0.1022\n",
       "\\item 0.861752380952381\n",
       "\\item 0.335266666666667\n",
       "\\item 0.405333333333333\n",
       "\\item 0.072\n",
       "\\item 0.432569696969697\n",
       "\\item 0.0171333333333333\n",
       "\\item 0.0614666666666667\n",
       "\\item 0.444593939393939\n",
       "\\item 0.004\n",
       "\\item 0.0309333333333333\n",
       "\\item 0.380933333333333\n",
       "\\item 0.0221333333333333\n",
       "\\item 0.0016\n",
       "\\item 0.0976666666666667\n",
       "\\item 0.40852380952381\n",
       "\\item 0.231711111111111\n",
       "\\item 0.413133333333333\n",
       "\\item 0.187733333333333\n",
       "\\item 0.453533333333333\n",
       "\\item 0.002\n",
       "\\item 0.6224\n",
       "\\item 0.759333333333333\n",
       "\\item 0.360155555555556\n",
       "\\item 0.624666666666667\n",
       "\\item 0.594329411764706\n",
       "\\item 0.176866666666667\n",
       "\\item 0.147066666666667\n",
       "\\item 0.137333333333333\n",
       "\\item 0.252866666666667\n",
       "\\item 0.0112\n",
       "\\item 0.448222222222222\n",
       "\\item 0.671428571428571\n",
       "\\item 0.624266666666667\n",
       "\\item 0.0368888888888889\n",
       "\\item 0.0546666666666667\n",
       "\\item 0.560533333333333\n",
       "\\item 0.617983333333333\n",
       "\\item 0.0192\n",
       "\\item 0.3876\n",
       "\\item 0.416466666666667\n",
       "\\item 0.926569696969697\n",
       "\\item 0.223266666666667\n",
       "\\item 0.706\n",
       "\\item 0.729228571428571\n",
       "\\item 0.828666666666667\n",
       "\\item 0.0381333333333333\n",
       "\\item 0.171733333333333\n",
       "\\item 0.1764\n",
       "\\item 0.400133333333333\n",
       "\\item 0.719561904761905\n",
       "\\item 0.0337333333333333\n",
       "\\item 0.920295238095238\n",
       "\\item 0.146266666666667\n",
       "\\item 0.012\n",
       "\\item 0.183733333333333\n",
       "\\item 0.652488888888889\n",
       "\\item 0.254822222222222\n",
       "\\item 0.228866666666667\n",
       "\\item 0.598\n",
       "\\item 0.1814\n",
       "\\item 0.0668\n",
       "\\item 0.576819047619048\n",
       "\\item 0.008\n",
       "\\item 0.1722\n",
       "\\item 0.182533333333333\n",
       "\\item 0.28651746031746\n",
       "\\item 0.510742857142857\n",
       "\\item 0.5052\n",
       "\\item 0.625285714285714\n",
       "\\item 0.0794\n",
       "\\item 0.774066666666667\n",
       "\\item 0.0228\n",
       "\\item 0.3284\n",
       "\\item 0.222448484848485\n",
       "\\item 0.0590222222222222\n",
       "\\item 0.914428571428572\n",
       "\\item 0.595644444444445\n",
       "\\item 0.250581818181818\n",
       "\\item 0.0064\n",
       "\\item 0.0136\n",
       "\\item 0.0418\n",
       "\\item 0.137133333333333\n",
       "\\item 0.778466666666667\n",
       "\\item 0.655733333333334\n",
       "\\item 0.133333333333333\n",
       "\\item 0.1562\n",
       "\\item 0.0078\n",
       "\\item 0.195866666666667\n",
       "\\item 0.176733333333333\n",
       "\\item 0.0567333333333333\n",
       "\\item 0.440333333333333\n",
       "\\item 0.022\n",
       "\\item 0.649622222222222\n",
       "\\item 0.0038\n",
       "\\item 0.696628571428571\n",
       "\\item 0.0758666666666667\n",
       "\\item 0.0074\n",
       "\\item 0.5579\n",
       "\\item 0.829057142857143\n",
       "\\item 0.303266666666667\n",
       "\\item 0.0862\n",
       "\\item 0.0621\n",
       "\\item 0.0519666666666667\n",
       "\\item 0.413433333333333\n",
       "\\item 0.0839333333333333\n",
       "\\item 0.387733333333333\n",
       "\\item 0.132\n",
       "\\item 0.0125333333333333\n",
       "\\item 0.153733333333333\n",
       "\\item 0.67390303030303\n",
       "\\item 0.1204\n",
       "\\item 0.745666666666667\n",
       "\\item 0.811133333333333\n",
       "\\item 0.250022222222222\n",
       "\\item 0.705933333333333\n",
       "\\item 0.307133333333333\n",
       "\\item 0.614614285714286\n",
       "\\item 0.693015384615385\n",
       "\\item 0.104066666666667\n",
       "\\item 0.5954\n",
       "\\item 0.342333333333333\n",
       "\\item 0.225533333333333\n",
       "\\item 0.136781818181818\n",
       "\\item 0.516022222222222\n",
       "\\item 0.525295238095238\n",
       "\\item 0.173066666666667\n",
       "\\item 0.3134\n",
       "\\item 0.0196666666666667\n",
       "\\item 0.349266666666667\n",
       "\\item 0.2724\n",
       "\\item 0.0774\n",
       "\\item 0.4402\n",
       "\\item 0.0454666666666667\n",
       "\\item 0.524\n",
       "\\item 0.008\n",
       "\\item 0.478\n",
       "\\item 0.0996\n",
       "\\item 0.304595238095238\n",
       "\\item 0.107533333333333\n",
       "\\item 0.0345666666666667\n",
       "\\item 0.008\n",
       "\\item 0.10212380952381\n",
       "\\item 0.311766666666667\n",
       "\\item 0.670957142857143\n",
       "\\item 0.271866666666667\n",
       "\\item 0.0418666666666667\n",
       "\\item 0.018\n",
       "\\item 0.106933333333333\n",
       "\\item 0.507866666666667\n",
       "\\item 0.11035\n",
       "\\item 0.149533333333333\n",
       "\\item 0.2718\n",
       "\\item 0.400766666666667\n",
       "\\item 0.119457142857143\n",
       "\\item 0.360066666666667\n",
       "\\item 0.0792\n",
       "\\item 0.156\n",
       "\\item 0.0392\n",
       "\\item 0.390866666666667\n",
       "\\item 0.0550666666666667\n",
       "\\item 0.0742666666666667\n",
       "\\item 0.1194\n",
       "\\item 0.0798666666666667\n",
       "\\item 0.596466666666667\n",
       "\\item 0.0429333333333333\n",
       "\\item 0.0092\n",
       "\\item 0.828761904761905\n",
       "\\item 0.56107619047619\n",
       "\\item 0.679266666666667\n",
       "\\item 0.154561904761905\n",
       "\\item 0.153333333333333\n",
       "\\item 0.154933333333333\n",
       "\\item 0.504533333333333\n",
       "\\item 0.00266666666666667\n",
       "\\item 0.056047619047619\n",
       "\\item 0.396115384615385\n",
       "\\item 0.0314666666666667\n",
       "\\item 0.042\n",
       "\\item 0.553933333333333\n",
       "\\item 0.5086\n",
       "\\item 0.264077777777778\n",
       "\\item 0.0722666666666667\n",
       "\\item 0.0762666666666667\n",
       "\\item 0.550266666666667\n",
       "\\item 0.624844444444445\n",
       "\\item 0.0496\n",
       "\\item 0.115266666666667\n",
       "\\item 0.225866666666667\n",
       "\\item 0.334519047619048\n",
       "\\item 0.796466666666667\n",
       "\\item 0.769295238095238\n",
       "\\item 0.486733333333333\n",
       "\\item 0.262533333333333\n",
       "\\item 0.0124666666666667\n",
       "\\item 0.016\n",
       "\\item 0.695504761904762\n",
       "\\item 0.428666666666667\n",
       "\\item 0.0428\n",
       "\\item 0.612333333333333\n",
       "\\item 0.0196666666666667\n",
       "\\item 0.199333333333333\n",
       "\\item 0.359133333333333\n",
       "\\item 0.494266666666667\n",
       "\\item 0.889266666666667\n",
       "\\item 0.3366\n",
       "\\item 0.3516\n",
       "\\item 0.1978\n",
       "\\item 0.8336\n",
       "\\item 0.0382666666666667\n",
       "\\item 0.0392222222222222\n",
       "\\item 0.436266666666667\n",
       "\\item 0.0542\n",
       "\\item 0.319441558441558\n",
       "\\item 0.556066666666667\n",
       "\\item 0.00866666666666667\n",
       "\\item 0.0388666666666667\n",
       "\\item 0.557866666666667\n",
       "\\item 0.0478666666666667\n",
       "\\item 0.2173\n",
       "\\item 0.495161904761905\n",
       "\\item 0.109866666666667\n",
       "\\item 0.515209523809524\n",
       "\\item 0.0404\n",
       "\\item 0.144333333333333\n",
       "\\item 0.521333333333333\n",
       "\\item 0.0892666666666666\n",
       "\\item 0.351333333333333\n",
       "\\item 0.484444444444444\n",
       "\\item 0.336733333333333\n",
       "\\item 0.403933333333333\n",
       "\\item 0.0296666666666667\n",
       "\\item 0.0755333333333333\n",
       "\\item 0.0419333333333333\n",
       "\\item 0.5966\n",
       "\\item 0.0289333333333333\n",
       "\\item 0.0402666666666667\n",
       "\\item 0.0476\n",
       "\\item 0.004\n",
       "\\item 0.503466666666667\n",
       "\\item 0.0322666666666667\n",
       "\\item 0.721088888888889\n",
       "\\item 0.127\n",
       "\\item 0.256133333333333\n",
       "\\item 0.827266666666667\n",
       "\\item 0.438266666666667\n",
       "\\item 0.688466666666667\n",
       "\\item 0.0679333333333333\n",
       "\\item 0.6014\n",
       "\\item 0.734095238095238\n",
       "\\item 0.235933333333333\n",
       "\\item 0.0326222222222222\n",
       "\\item 0.329733333333333\n",
       "\\item 0.537747186147186\n",
       "\\item 0\n",
       "\\item 0.371533333333333\n",
       "\\item 0.693828571428571\n",
       "\\item 0.2747\n",
       "\\item 0.63250303030303\n",
       "\\item 0.012\n",
       "\\item 0.0402\n",
       "\\item 0.749171428571429\n",
       "\\item 0.0072\n",
       "\\item 0.159180952380952\n",
       "\\item 0.1474\n",
       "\\item 0.173066666666667\n",
       "\\item 0.00985714285714286\n",
       "\\item 0.5274\n",
       "\\item 0.545333333333333\n",
       "\\item 0.0028\n",
       "\\item 0.1886\n",
       "\\item 0.582933333333333\n",
       "\\item 0.697180952380952\n",
       "\\item 0.716755555555555\n",
       "\\item 0.267933333333333\n",
       "\\item 0.362555555555555\n",
       "\\item 0.297444444444444\n",
       "\\item 0.0328\n",
       "\\item 0.4958\n",
       "\\item 0.2038\n",
       "\\item 0.01\n",
       "\\item 0.253533333333333\n",
       "\\item 0.066\n",
       "\\item 0.184247619047619\n",
       "\\item 0.210133333333333\n",
       "\\item 0.685266666666667\n",
       "\\item 0.405707936507937\n",
       "\\item 0.188933333333333\n",
       "\\item 0.5358\n",
       "\\item 0.367\n",
       "\\item 0.0260666666666667\n",
       "\\item 0.004\n",
       "\\item 0.121333333333333\n",
       "\\item 0.0815333333333333\n",
       "\\item 0.436923809523809\n",
       "\\item 0.004\n",
       "\\item 0.210066666666667\n",
       "\\item 0.5586\n",
       "\\item 0.372333333333333\n",
       "\\item 0.002\n",
       "\\item 0.349466666666667\n",
       "\\item 0.017\n",
       "\\item 0.143\n",
       "\\item 0.164\n",
       "\\item 0.0281333333333333\n",
       "\\item 0.0370666666666667\n",
       "\\item 0.3204\n",
       "\\item 0.0526\n",
       "\\item 0.712633333333333\n",
       "\\item 0.634276190476191\n",
       "\\item 0.356866666666667\n",
       "\\item 0.294866666666667\n",
       "\\item 0.292\n",
       "\\item 0.008\n",
       "\\item 0.6658\n",
       "\\item 0.453733333333333\n",
       "\\item 0.461790476190476\n",
       "\\item 0.475933333333333\n",
       "\\item 0.3352\n",
       "\\item 0.008\n",
       "\\item 0.312333333333333\n",
       "\\item 0.238066666666667\n",
       "\\item 0.0829333333333333\n",
       "\\item 0.355333333333333\n",
       "\\item 0.283533333333333\n",
       "\\item 0.723009523809524\n",
       "\\item 0.191133333333333\n",
       "\\item 0.0534666666666667\n",
       "\\item 0.0810666666666667\n",
       "\\item 0.264866666666667\n",
       "\\item 0.0088\n",
       "\\item 0.830419047619048\n",
       "\\item 0.293333333333333\n",
       "\\item 0.596066666666667\n",
       "\\item 0.3234\n",
       "\\item 0.495485714285714\n",
       "\\item 0.615\n",
       "\\item 0.167866666666667\n",
       "\\item 0.37\n",
       "\\item 0.474\n",
       "\\item 0.448133333333333\n",
       "\\item 0.670273015873016\n",
       "\\item 0.416422222222222\n",
       "\\item 0.1182\n",
       "\\item 0.0579333333333333\n",
       "\\item 0.0213333333333333\n",
       "\\item 0.029\n",
       "\\item 0.214266666666667\n",
       "\\item 0.404566666666667\n",
       "\\item 0.172866666666667\n",
       "\\item 0.345866666666667\n",
       "\\item 0.0685333333333333\n",
       "\\item 0.145316666666667\n",
       "\\item 0.0618\n",
       "\\item 0.231866666666667\n",
       "\\item 0.4736\n",
       "\\item 0.0248\n",
       "\\item 0.478388744588745\n",
       "\\item 0.82\n",
       "\\item 0.0708666666666667\n",
       "\\item 0.489466666666667\n",
       "\\item 0.146457142857143\n",
       "\\item 0.227647619047619\n",
       "\\item 0.207466666666667\n",
       "\\item 0.618433333333334\n",
       "\\item 0\n",
       "\\item 0.773333333333333\n",
       "\\item 0.2312\n",
       "\\item 0.0553333333333333\n",
       "\\item 0.574048351648352\n",
       "\\item 0.705933333333333\n",
       "\\item 0.0692222222222222\n",
       "\\item 0.0054\n",
       "\\item 0.289961904761905\n",
       "\\item 0.413\n",
       "\\item 0.748666666666667\n",
       "\\item 0.425133333333333\n",
       "\\item 0.3733\n",
       "\\item 0.539466666666667\n",
       "\\item 0.485798268398268\n",
       "\\item 0.3804\n",
       "\\item 0.012\n",
       "\\item 0.752333333333333\n",
       "\\item 0.0184\n",
       "\\item 0.425377896613191\n",
       "\\item 0.0148\n",
       "\\item 0.7104\n",
       "\\item 0.088\n",
       "\\item 0.3026\n",
       "\\item 0.347222222222222\n",
       "\\item 0.0109333333333333\n",
       "\\item 0.400666666666667\n",
       "\\item 0.418333333333333\n",
       "\\item 0.152133333333333\n",
       "\\item 0.0438666666666667\n",
       "\\item 0.819723809523809\n",
       "\\item 0.142888888888889\n",
       "\\item 0.474466666666667\n",
       "\\item 0.659342857142857\n",
       "\\item 0.0423333333333333\n",
       "\\item 0.220155555555556\n",
       "\\item 0.537133333333333\n",
       "\\item 0.0724666666666667\n",
       "\\item 0.0548\n",
       "\\item 0.253619047619048\n",
       "\\item 0.467555555555556\n",
       "\\item 0.216666666666667\n",
       "\\item 0.707533333333333\n",
       "\\item 0.3424\n",
       "\\item 0.297733333333333\n",
       "\\item 0.0238\n",
       "\\item 0.4306\n",
       "\\item 0.454533333333333\n",
       "\\item 0.004\n",
       "\\item 0.599447619047619\n",
       "\\item 0.380133333333333\n",
       "\\item 0.713369696969697\n",
       "\\item 0.0731333333333333\n",
       "\\item 0.795411111111111\n",
       "\\item 0.134133333333333\n",
       "\\item 0.0318\n",
       "\\item 0\n",
       "\\item 0.752666666666667\n",
       "\\item 0.482136363636364\n",
       "\\item 0\n",
       "\\item 0.0959238095238095\n",
       "\\item 0.004\n",
       "\\item 0.0799\n",
       "\\item 0.208888888888889\n",
       "\\item 0.498036363636364\n",
       "\\item 0.400695238095238\n",
       "\\item 0.151\n",
       "\\item 0.824735714285714\n",
       "\\item 0.0474\n",
       "\\item 0.223133333333333\n",
       "\\item 0.868619047619048\n",
       "\\item 0.002\n",
       "\\item 0.275\n",
       "\\item 0.477333333333333\n",
       "\\item 0.408066666666667\n",
       "\\item 0.285047619047619\n",
       "\\item 0.260666666666667\n",
       "\\item 0.4912\n",
       "\\item 0.715148717948718\n",
       "\\item 0.786761904761905\n",
       "\\item 0.4124\n",
       "\\item 0.4268\n",
       "\\item 0.2124\n",
       "\\item 0.125781818181818\n",
       "\\item 0\n",
       "\\item 0.0194\n",
       "\\item 0.319933333333333\n",
       "\\item 0.759723809523809\n",
       "\\item 0.332003174603175\n",
       "\\item 0.217515151515152\n",
       "\\item 0.0875333333333334\n",
       "\\item 0.466933333333333\n",
       "\\item 0.0436\n",
       "\\item 0.0289333333333333\n",
       "\\item 0.519133333333333\n",
       "\\item 0.8186\n",
       "\\item 0.746836363636364\n",
       "\\item 0.440266666666667\n",
       "\\item 0.321580952380952\n",
       "\\item 8e-04\n",
       "\\item 0.125266666666667\n",
       "\\item 0.0768\n",
       "\\item 0.852466666666667\n",
       "\\item 0.0107333333333333\n",
       "\\item 0.115533333333333\n",
       "\\item 0.366866666666667\n",
       "\\item 0.561233333333333\n",
       "\\item 0.0624\n",
       "\\item 0.0104\n",
       "\\item 0.297333333333333\n",
       "\\item 0.4828\n",
       "\\item 0.841123809523809\n",
       "\\item 0.4754\n",
       "\\item 0.0382\n",
       "\\item 0.3354\n",
       "\\item 0.399733333333333\n",
       "\\item 0.826319047619048\n",
       "\\item 0.0742\n",
       "\\item 0.288\n",
       "\\item 0.62690303030303\n",
       "\\item 0.278866666666667\n",
       "\\item 0.213\n",
       "\\item 0.570133333333333\n",
       "\\item 0.0272\n",
       "\\item 0.1694\n",
       "\\item 0.784\n",
       "\\item 0.144733333333333\n",
       "\\item 0.00333333333333333\n",
       "\\item 0.369866666666667\n",
       "\\item 0.125133333333333\n",
       "\\item 0.051\n",
       "\\item 0.5634\n",
       "\\item 0.177288888888889\n",
       "\\item 0.322933333333333\n",
       "\\item 0.0213333333333333\n",
       "\\item 0.3416\n",
       "\\item 8e-04\n",
       "\\item 0.0440888888888889\n",
       "\\item 0.0594666666666667\n",
       "\\item 0.111\n",
       "\\item 0.0652\n",
       "\\item 0.0268666666666667\n",
       "\\item 0.104266666666667\n",
       "\\item 0.120133333333333\n",
       "\\item 0.2918\n",
       "\\item 0.0204666666666667\n",
       "\\item 0.0808\n",
       "\\item 0.0813\n",
       "\\item 0.0133333333333333\n",
       "\\item 0.00793333333333333\n",
       "\\item 0.4382\n",
       "\\item 0.767866666666667\n",
       "\\item 0.369723809523809\n",
       "\\item 0.0972\n",
       "\\item 0.0618\n",
       "\\item 0.555555555555556\n",
       "\\item 0.259866666666667\n",
       "\\item 0.198866666666667\n",
       "\\item 0.646038095238095\n",
       "\\item 0.709961904761905\n",
       "\\item 0.122266666666667\n",
       "\\item 0.5438\n",
       "\\item 0.0821\n",
       "\\item 0.017\n",
       "\\item 0.748333333333334\n",
       "\\item 0.224127272727273\n",
       "\\item 0.492333333333333\n",
       "\\item 0.325993073593074\n",
       "\\item 0.277933333333333\n",
       "\\item 0.1946\n",
       "\\item 0.0443333333333333\n",
       "\\item 0.513866666666667\n",
       "\\item 0.0910666666666667\n",
       "\\item 0.0606\n",
       "\\item 0.2996\n",
       "\\item 0.5104\n",
       "\\item 0.449333333333333\n",
       "\\item 0.14545\n",
       "\\item 0.660952380952381\n",
       "\\item 0.346266666666667\n",
       "\\item 0.294390476190476\n",
       "\\item 0.106133333333333\n",
       "\\item 0.107\n",
       "\\item 0.434460606060606\n",
       "\\item 0.0903333333333333\n",
       "\\item 0\n",
       "\\item 0.471066666666667\n",
       "\\item 0.372733333333333\n",
       "\\item 0.256866666666667\n",
       "\\item 0.0552\n",
       "\\item 0.0738\n",
       "\\item 0.505035897435897\n",
       "\\item 0.305247619047619\n",
       "\\item 0.738333333333333\n",
       "\\item 0.0513333333333333\n",
       "\\item 0.553333333333333\n",
       "\\item 0.0659333333333333\n",
       "\\item 0.7698\n",
       "\\item 0.016\n",
       "\\item 0.0306666666666667\n",
       "\\item 0.36028354978355\n",
       "\\item 0.674695238095238\n",
       "\\item 0.822761904761905\n",
       "\\item 0\n",
       "\\item 0.0575641025641026\n",
       "\\item 0.0621333333333333\n",
       "\\item 0.308916666666667\n",
       "\\item 0.42880303030303\n",
       "\\item 0.0234\n",
       "\\item 0.896333333333333\n",
       "\\item 0.812\n",
       "\\item 0.642666666666667\n",
       "\\item 0.352733333333333\n",
       "\\item 0.677\n",
       "\\item 0.1694\n",
       "\\item 0.610333333333333\n",
       "\\item 0.2728\n",
       "\\item 0.1418\n",
       "\\item 0.2014\n",
       "\\item 0.419066666666667\n",
       "\\item 0.197733333333333\n",
       "\\item 0.108627777777778\n",
       "\\item 0.358133333333333\n",
       "\\item 0.534\n",
       "\\item 0.004\n",
       "\\item 0.123855555555556\n",
       "\\item 0.0032\n",
       "\\item 0.539333333333333\n",
       "\\item 0.452959595959596\n",
       "\\item 0.0778\n",
       "\\item 0.343466666666667\n",
       "\\item 0.0579333333333333\n",
       "\\item 0.201133333333333\n",
       "\\item 0.3832\n",
       "\\item 0.0292\n",
       "\\item 0.344433333333333\n",
       "\\item 0.0809333333333334\n",
       "\\item 0.0162666666666667\n",
       "\\item 0.6366\n",
       "\\item 0.0328\n",
       "\\item 0.0573333333333333\n",
       "\\item 0.1062\n",
       "\\item 0.4754\n",
       "\\item 0.342266666666667\n",
       "\\item 0.00133333333333333\n",
       "\\item 0.430622222222222\n",
       "\\item 0.592933333333333\n",
       "\\item 0.0032\n",
       "\\item 0.473666666666667\n",
       "\\item 0.181266666666667\n",
       "\\item 0.0337333333333333\n",
       "\\item 0.332266666666667\n",
       "\\item 0.246766666666667\n",
       "\\item 0.191866666666667\n",
       "\\item 0.0116444444444444\n",
       "\\item 0.365933333333333\n",
       "\\item 0.1302\n",
       "\\item 0.278688888888889\n",
       "\\item 0.321666666666667\n",
       "\\item 0.0172\n",
       "\\item 0.0632666666666667\n",
       "\\item 0.343733333333333\n",
       "\\item 0.379435897435897\n",
       "\\item 0.862\n",
       "\\item 0.0313333333333333\n",
       "\\item 0.194733333333333\n",
       "\\item 0.0176\n",
       "\\item 0.517302564102564\n",
       "\\item 0.2246\n",
       "\\item 0.204333333333333\n",
       "\\item 0.494355555555556\n",
       "\\item 0.2418\n",
       "\\item 0.289390476190476\n",
       "\\item 0.0644974358974359\n",
       "\\item 0.008\n",
       "\\item 0.281466666666667\n",
       "\\item 0.210933333333333\n",
       "\\item 0.023\n",
       "\\item 0.034\n",
       "\\item 0.3638\n",
       "\\item 0.624244444444444\n",
       "\\item 0.285333333333333\n",
       "\\item 0.0084\n",
       "\\item 0.004\n",
       "\\item 0.4997\n",
       "\\item 0.119533333333333\n",
       "\\item 0.358533333333333\n",
       "\\item 0.2148\n",
       "\\item 0.692233333333333\n",
       "\\item 0.1018\n",
       "\\item 0.380466666666667\n",
       "\\item 0.112733333333333\n",
       "\\item 0.247\n",
       "\\item 0.111666666666667\n",
       "\\item 0.086\n",
       "\\item 0.648611111111111\n",
       "\\item 0.653466666666666\n",
       "\\item 0.1018\n",
       "\\item 0.418666666666667\n",
       "\\item 0.1836\n",
       "\\item 0.110733333333333\n",
       "\\item 0.0123333333333333\n",
       "\\item 0.245466666666667\n",
       "\\item 0.181933333333333\n",
       "\\item 0.576933333333333\n",
       "\\item 0.439922807017544\n",
       "\\item 0.552733333333333\n",
       "\\item 0.0386666666666667\n",
       "\\item 0.294409523809524\n",
       "\\item 0.744076923076923\n",
       "\\item 0.787533333333333\n",
       "\\item 0.117828571428571\n",
       "\\item 0.0647333333333333\n",
       "\\item 0.0785111111111111\n",
       "\\item 0.391165367965368\n",
       "\\item 0.1036\n",
       "\\item 0.0525333333333333\n",
       "\\item 0.6824\n",
       "\\item 0.102980952380952\n",
       "\\item 0.176666666666667\n",
       "\\item 0.4444\n",
       "\\item 0.888333333333333\n",
       "\\item 0.0742\n",
       "\\item 0.049\n",
       "\\item 0.413133333333333\n",
       "\\item 0.0273555555555556\n",
       "\\item 0.562666666666667\n",
       "\\item 0.596095238095238\n",
       "\\item 0.730533333333333\n",
       "\\item 0.417755555555555\n",
       "\\item 0.1578\n",
       "\\item 0.0202\n",
       "\\item 0.062\n",
       "\\item 0.00533333333333333\n",
       "\\item 0.553695238095238\n",
       "\\item 0.567133333333334\n",
       "\\item 0.0566\n",
       "\\item 0.382266666666667\n",
       "\\item 0.0423333333333333\n",
       "\\item 0.679175757575758\n",
       "\\item 0.362169696969697\n",
       "\\item 0.817614285714286\n",
       "\\item 0.0248\n",
       "\\item 0.809157142857143\n",
       "\\item 0.449380952380952\n",
       "\\item 0.321433333333333\n",
       "\\item 0.0350666666666667\n",
       "\\item 0.326109090909091\n",
       "\\item 0.101066666666667\n",
       "\\item 0.184666666666667\n",
       "\\item 0.605833333333333\n",
       "\\item 0.180066666666667\n",
       "\\item 0.00213333333333333\n",
       "\\item 0.2508\n",
       "\\item 0.23050303030303\n",
       "\\item 0.0544\n",
       "\\item 0.0499333333333333\n",
       "\\item 0.601533333333333\n",
       "\\item 0.3658\n",
       "\\item 0.1464\n",
       "\\item 0.00666666666666667\n",
       "\\item 0.3964\n",
       "\\item 0.263733333333333\n",
       "\\item 0.614533333333333\n",
       "\\item 0.401066666666667\n",
       "\\item 0.106\n",
       "\\item 0.5256\n",
       "\\item 0.328333333333333\n",
       "\\item 0.6816\n",
       "\\item 0.008\n",
       "\\item 0.199666666666667\n",
       "\\item 0.122322222222222\n",
       "\\item 0.0172\n",
       "\\item 0.0364\n",
       "\\item 0.161266666666667\n",
       "\\item 0.183\n",
       "\\item 0.726366666666667\n",
       "\\item 0.0518666666666667\n",
       "\\item 0.147133333333333\n",
       "\\item 0.0366666666666667\n",
       "\\item 0.6226\n",
       "\\item 0.0444888888888889\n",
       "\\item 0.5482\n",
       "\\item 0.116764102564103\n",
       "\\item 0.0766666666666667\n",
       "\\item 0.171933333333333\n",
       "\\item 0.7975\n",
       "\\item 0.560733333333333\n",
       "\\item 0.552533333333333\n",
       "\\item 0.595181818181818\n",
       "\\item 0.0329333333333333\n",
       "\\item 0.463231746031746\n",
       "\\item 0.526544444444444\n",
       "\\item 0.0649\n",
       "\\item 0.189133333333333\n",
       "\\item 0.802066666666667\n",
       "\\item 0.004\n",
       "\\item 0.1826\n",
       "\\item 0.4496\n",
       "\\item 0.0616\n",
       "\\item 0.3174\n",
       "\\item 0.117583333333333\n",
       "\\item 0.429733333333333\n",
       "\\item 0.748133333333333\n",
       "\\item 0.0422666666666667\n",
       "\\item 0.675266666666667\n",
       "\\item 0.0390666666666667\n",
       "\\item 0.718111111111111\n",
       "\\item 0.240466666666667\n",
       "\\item 0.484666666666667\n",
       "\\item 0.662090476190476\n",
       "\\item 0.0212888888888889\n",
       "\\item 0.0298\n",
       "\\item 0.0171333333333333\n",
       "\\item 0.813742857142857\n",
       "\\item 0.7118\n",
       "\\item 0.0650666666666667\n",
       "\\item 0.004\n",
       "\\item 0.0216\n",
       "\\item 0.0506\n",
       "\\item 0.0330666666666667\n",
       "\\item 0.0544\n",
       "\\item 0.088\n",
       "\\item 0.684933333333334\n",
       "\\item 0.1164\n",
       "\\item 0.012\n",
       "\\item 0.621771428571429\n",
       "\\item 0.5336\n",
       "\\item 0.278133333333333\n",
       "\\item 0.0402666666666667\n",
       "\\item 0.722488888888889\n",
       "\\item 0.0316666666666667\n",
       "\\item 0.2226\n",
       "\\item 0.0799333333333333\n",
       "\\item 0.156466666666667\n",
       "\\item 0.00773333333333333\n",
       "\\item 0.370466666666667\n",
       "\\item 0.583435897435898\n",
       "\\item 0.213533333333333\n",
       "\\item 0.366866666666667\n",
       "\\item 0\n",
       "\\item 0.561095238095238\n",
       "\\item 0.0978666666666667\n",
       "\\item 0.00133333333333333\n",
       "\\item 0.411251082251082\n",
       "\\item 0.0496\n",
       "\\item 0.626\n",
       "\\item 0.565902564102564\n",
       "\\item 0.337066666666667\n",
       "\\item 0.1694\n",
       "\\item 0.7704\n",
       "\\item 0.155\n",
       "\\item 0.117933333333333\n",
       "\\item 0.4018\n",
       "\\item 0.0769333333333333\n",
       "\\item 0.3683\n",
       "\\item 0.734733333333333\n",
       "\\item 0.274733333333333\n",
       "\\item 0.0583333333333333\n",
       "\\item 0.419\n",
       "\\item 0.0613333333333333\n",
       "\\item 0.333933333333333\n",
       "\\item 0.0032\n",
       "\\item 0.571533333333333\n",
       "\\item 0.506977777777778\n",
       "\\item 0.564933333333333\n",
       "\\item 0.0808222222222222\n",
       "\\item 0.0105333333333333\n",
       "\\item 0.0366\n",
       "\\item 0.807666666666667\n",
       "\\item 0.0358\n",
       "\\item 0.589\n",
       "\\item 0.113133333333333\n",
       "\\item 0.0211333333333333\n",
       "\\item 0.2186\n",
       "\\item 0.0182\n",
       "\\item 0.434933333333333\n",
       "\\item 0.0454\n",
       "\\item 0.725533333333333\n",
       "\\item 0.551733333333333\n",
       "\\item 0.6766\n",
       "\\item 0.041\n",
       "\\item 0.0293333333333333\n",
       "\\item 0.273466666666667\n",
       "\\item 0.00866666666666667\n",
       "\\item 0.6762\n",
       "\\item 0.0292\n",
       "\\item 0.742166666666667\n",
       "\\item 0.716266666666667\n",
       "\\item 0.620733333333333\n",
       "\\item 0.2784\n",
       "\\item 0.222333333333333\n",
       "\\item 0.393704761904762\n",
       "\\item 0.233933333333333\n",
       "\\item 0.471466666666667\n",
       "\\item 0.007\n",
       "\\item 0.0028\n",
       "\\item 0.898866666666667\n",
       "\\item 0.2916\n",
       "\\item 0.8208\n",
       "\\item 0.575085714285714\n",
       "\\item 0.0408\n",
       "\\item 0.573971428571429\n",
       "\\item 0.0859333333333333\n",
       "\\item 0.338233333333333\n",
       "\\item 0.502666666666667\n",
       "\\item 0.303\n",
       "\\item 0.102533333333333\n",
       "\\item 0.334266666666667\n",
       "\\item 0.370533333333333\n",
       "\\item 0.0374666666666667\n",
       "\\item 0.0555333333333333\n",
       "\\item 0.64150303030303\n",
       "\\item 0.522990476190476\n",
       "\\item 0.476066666666667\n",
       "\\item 0.1566\n",
       "\\item 0.1546\n",
       "\\item 0.529235897435897\n",
       "\\item 0.369866666666667\n",
       "\\item 0.714466666666667\n",
       "\\item 0.541733333333333\n",
       "\\item 0.191466666666667\n",
       "\\item 0.413133333333333\n",
       "\\item 0.160866666666667\n",
       "\\item 0.0352\n",
       "\\item 0.786136363636364\n",
       "\\item 0.001\n",
       "\\item 0\n",
       "\\item 0.556650793650794\n",
       "\\item 0.594028571428571\n",
       "\\item 0\n",
       "\\item 0.0590476190476191\n",
       "\\item 0.249133333333333\n",
       "\\item 0.09065\n",
       "\\item 0.855157142857143\n",
       "\\item 0.0999333333333333\n",
       "\\item 0.4867\n",
       "\\item 0.0416888888888889\n",
       "\\item 0.186933333333333\n",
       "\\item 0.120466666666667\n",
       "\\item 0.8998\n",
       "\\item 0.594361904761905\n",
       "\\item 0.745733333333333\n",
       "\\item 0.0442\n",
       "\\item 0.476\n",
       "\\item 0.128666666666667\n",
       "\\item 0.1396\n",
       "\\item 0.323933333333333\n",
       "\\item 0.267533333333333\n",
       "\\item 0.5198\n",
       "\\item 0.486066666666667\n",
       "\\item 0.0272\n",
       "\\item 0.225669841269841\n",
       "\\item 0.0854\n",
       "\\item 0.215733333333333\n",
       "\\item 0.0575333333333333\n",
       "\\item 0.675066666666667\n",
       "\\item 0.896133333333333\n",
       "\\item 0.531733333333333\n",
       "\\item 0.417733333333333\n",
       "\\item 0.3298\n",
       "\\item 0.0631333333333333\n",
       "\\item 0.3867\n",
       "\\item 0.0572\n",
       "\\item 0.2354\n",
       "\\item 0.273133333333333\n",
       "\\item 0.238927272727273\n",
       "\\item 0.215266666666667\n",
       "\\item 0.0942666666666667\n",
       "\\item 0.799733333333333\n",
       "\\item 0.243248484848485\n",
       "\\item 0.195380952380952\n",
       "\\item 0.0734666666666667\n",
       "\\item 0.798961904761905\n",
       "\\item 0.358133333333333\n",
       "\\item 0.308733333333333\n",
       "\\item 0.1464\n",
       "\\item 0.410769696969697\n",
       "\\item 0.062\n",
       "\\item 0.553933333333333\n",
       "\\item 0.632577896613191\n",
       "\\item 0.6972\n",
       "\\item 0.269714285714286\n",
       "\\item 0.0414\n",
       "\\item 0.654022222222222\n",
       "\\item 0.114733333333333\n",
       "\\item 0.0762666666666667\n",
       "\\item 0.6346\n",
       "\\item 0.217533333333333\n",
       "\\item 0.3662\n",
       "\\item 0.0168\n",
       "\\item 0.107\n",
       "\\item 0.612122807017544\n",
       "\\item 0.216\n",
       "\\item 0.197666666666667\n",
       "\\item 0.3418\n",
       "\\item 0.189333333333333\n",
       "\\item 0.0416666666666667\n",
       "\\item 0.431266666666667\n",
       "\\item 0.272\n",
       "\\item 0.2884\n",
       "\\item 0.488769696969697\n",
       "\\item 0.233533333333333\n",
       "\\item 0.367133333333333\n",
       "\\item 0\n",
       "\\item 0.743638095238095\n",
       "\\item 0.2864\n",
       "\\item 0.306733333333333\n",
       "\\item 0.113\n",
       "\\item 0.57150303030303\n",
       "\\item 0.375933333333333\n",
       "\\item 0.552133333333333\n",
       "\\item 0.237466666666667\n",
       "\\item 0.2733\n",
       "\\item 0.233066666666667\n",
       "\\item 0.719622222222222\n",
       "\\item 0.534333333333333\n",
       "\\item 0.3878\n",
       "\\item 0.0426666666666667\n",
       "\\item 0.0904\n",
       "\\item 0.8228\n",
       "\\item 0.1826\n",
       "\\item 0\n",
       "\\item 0.00133333333333333\n",
       "\\item 0.309561904761905\n",
       "\\item 0.022\n",
       "\\item 0.5014\n",
       "\\item 0.433866666666667\n",
       "\\item 0.0202\n",
       "\\item 0.0277333333333333\n",
       "\\item 0.0186666666666667\n",
       "\\item 0.0926\n",
       "\\item 0.0683333333333333\n",
       "\\item 0.100892307692308\n",
       "\\item 0.2592\n",
       "\\item 0.0875555555555556\n",
       "\\item 0.457466666666667\n",
       "\\item 0.0424666666666667\n",
       "\\item 0.0392\n",
       "\\item 0.317266666666667\n",
       "\\item 0.0432\n",
       "\\item 0.139533333333333\n",
       "\\item 0.723866666666667\n",
       "\\item 0.2994\n",
       "\\item 0.582\n",
       "\\item 0.202966666666667\n",
       "\\item 0.0191333333333333\n",
       "\\item 0.04\n",
       "\\item 0.212\n",
       "\\item 0.135066666666667\n",
       "\\item 0.0194\n",
       "\\item 0.371366666666667\n",
       "\\item 0.0032\n",
       "\\item 0.624133333333333\n",
       "\\item 0.0082\n",
       "\\item 0.184171428571429\n",
       "\\item 0.324066666666667\n",
       "\\item 0.0549333333333333\n",
       "\\item 0.209444444444444\n",
       "\\item 0.0555333333333333\n",
       "\\item 0.1956\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.0384\n",
       "2. 0.206466666666667\n",
       "3. 0.441133333333333\n",
       "4. 0.0484\n",
       "5. 0.568552380952381\n",
       "6. 0.417514285714286\n",
       "7. 0.4343\n",
       "8. 0.52650303030303\n",
       "9. 0.0675833333333333\n",
       "10. 0.00533333333333333\n",
       "11. 0.218466666666667\n",
       "12. 0.420156140350877\n",
       "13. 0.0290888888888889\n",
       "14. 0.860369696969697\n",
       "15. 0.1028\n",
       "16. 0.0872\n",
       "17. 0.0203333333333333\n",
       "18. 0.139133333333333\n",
       "19. 0.255266666666667\n",
       "20. 0.0226\n",
       "21. 0.0876\n",
       "22. 0.397933333333333\n",
       "23. 0.8608\n",
       "24. 0.0088\n",
       "25. 0.63707619047619\n",
       "26. 0.0766666666666667\n",
       "27. 0.48610303030303\n",
       "28. 0.5024\n",
       "29. 0.589111111111111\n",
       "30. 0.2398\n",
       "31. 0.513533333333333\n",
       "32. 0.848342857142857\n",
       "33. 0.0223333333333333\n",
       "34. 0.470066666666667\n",
       "35. 0.627866666666667\n",
       "36. 0.2705\n",
       "37. 0.6392\n",
       "38. 0.61310303030303\n",
       "39. 0.808066666666667\n",
       "40. 0.021\n",
       "41. 0.169933333333333\n",
       "42. 0.225533333333333\n",
       "43. 0.110066666666667\n",
       "44. 0.6982\n",
       "45. 0.184533333333333\n",
       "46. 0.739533333333333\n",
       "47. 0.194266666666667\n",
       "48. 0.0194\n",
       "49. 0.3604\n",
       "50. 0.1258\n",
       "51. 0.0442666666666667\n",
       "52. 0.013\n",
       "53. 0.183522222222222\n",
       "54. 0.345933333333333\n",
       "55. 0.737066666666667\n",
       "56. 0.217266666666667\n",
       "57. 0.696066666666667\n",
       "58. 0.608633333333333\n",
       "59. 0.300047619047619\n",
       "60. 0.505533333333333\n",
       "61. 0.762161904761905\n",
       "62. 0.0184\n",
       "63. 0.233866666666667\n",
       "64. 0.174866666666667\n",
       "65. 0.0677333333333334\n",
       "66. 0.0642666666666667\n",
       "67. 0.0122\n",
       "68. 0.0202\n",
       "69. 0.530133333333333\n",
       "70. 0.161866666666667\n",
       "71. 0.464266666666667\n",
       "72. 0.0173333333333333\n",
       "73. 0.0146666666666667\n",
       "74. 0.592533333333333\n",
       "75. 0.217114285714286\n",
       "76. 0.206933333333333\n",
       "77. 0.702133333333333\n",
       "78. 0.0404\n",
       "79. 0.472933333333333\n",
       "80. 0.141466666666667\n",
       "81. 0.828\n",
       "82. 0.251333333333333\n",
       "83. 0.005\n",
       "84. 0.544066666666667\n",
       "85. 0.309155555555556\n",
       "86. 0.237233333333333\n",
       "87. 0.0673333333333333\n",
       "88. 0.0736666666666667\n",
       "89. 0.666933333333333\n",
       "90. 0.3192\n",
       "91. 0.01\n",
       "92. 0.0812666666666667\n",
       "93. 0.117\n",
       "94. 0.761942857142857\n",
       "95. 0.134533333333333\n",
       "96. 0.442133333333333\n",
       "97. 0.144688888888889\n",
       "98. 0.161780952380952\n",
       "99. 0.406633333333333\n",
       "100. 0.478266666666667\n",
       "101. 0.617744418739156\n",
       "102. 0.0512833333333333\n",
       "103. 0.640133333333333\n",
       "104. 0.359933333333333\n",
       "105. 0.357688888888889\n",
       "106. 0.4352\n",
       "107. 0.019\n",
       "108. 0.2228\n",
       "109. 0.1364\n",
       "110. 0.0460666666666667\n",
       "111. 0.0682\n",
       "112. 0.808504761904762\n",
       "113. 0.139\n",
       "114. 0.701942857142857\n",
       "115. 0.167466666666667\n",
       "116. 0.0136666666666667\n",
       "117. 0.00866666666666667\n",
       "118. 0.410533333333333\n",
       "119. 0.0343333333333333\n",
       "120. 0.0256666666666667\n",
       "121. 0.0332888888888889\n",
       "122. 0.0331333333333333\n",
       "123. 0.028\n",
       "124. 0.0968666666666667\n",
       "125. 0.148266666666667\n",
       "126. 0.0133333333333333\n",
       "127. 0.0024\n",
       "128. 0.340619047619048\n",
       "129. 0.764142857142857\n",
       "130. 0.0472666666666667\n",
       "131. 0.6416\n",
       "132. 0.469333333333333\n",
       "133. 0.315666666666667\n",
       "134. 0.4174\n",
       "135. 0.515495238095238\n",
       "136. 0.659466666666667\n",
       "137. 0.0756\n",
       "138. 0.767434920634921\n",
       "139. 0.147866666666667\n",
       "140. 0.697466666666667\n",
       "141. 0.434333333333333\n",
       "142. 0.233622222222222\n",
       "143. 0.253\n",
       "144. 0.476933333333333\n",
       "145. 0.407333333333333\n",
       "146. 0.149266666666667\n",
       "147. 0.639822222222222\n",
       "148. 0.192733333333333\n",
       "149. 0.211888888888889\n",
       "150. 0.1926\n",
       "151. 0.0542222222222222\n",
       "152. 0.387866666666667\n",
       "153. 0.206\n",
       "154. 0.1688\n",
       "155. 0.0301333333333333\n",
       "156. 0.1112\n",
       "157. 0.0293333333333333\n",
       "158. 0.528615384615385\n",
       "159. 0.3578\n",
       "160. 0.918266666666667\n",
       "161. 0.1302\n",
       "162. 0.1702\n",
       "163. 0.0486\n",
       "164. 0.116066666666667\n",
       "165. 0.0128571428571429\n",
       "166. 0.663588278388278\n",
       "167. 0.0739333333333333\n",
       "168. 0.531066666666667\n",
       "169. 0.034\n",
       "170. 0.295866666666667\n",
       "171. 0.646533333333333\n",
       "172. 0.302966666666667\n",
       "173. 0.394733333333333\n",
       "174. 0.0288\n",
       "175. 0.418847619047619\n",
       "176. 0.007\n",
       "177. 0.234266666666667\n",
       "178. 0.325066666666667\n",
       "179. 0.668761904761905\n",
       "180. 0.849933333333333\n",
       "181. 0.836266666666667\n",
       "182. 0.903\n",
       "183. 0.491688888888889\n",
       "184. 0\n",
       "185. 0.860288888888889\n",
       "186. 0.0342666666666667\n",
       "187. 0.0068\n",
       "188. 0.203533333333333\n",
       "189. 0.252933333333333\n",
       "190. 0.366\n",
       "191. 0.006\n",
       "192. 0.4553\n",
       "193. 0.601504761904762\n",
       "194. 0.1954\n",
       "195. 0.008\n",
       "196. 0.0076\n",
       "197. 0.0856666666666667\n",
       "198. 0.0419333333333333\n",
       "199. 0.0350666666666667\n",
       "200. 0.751809523809524\n",
       "201. 0.486\n",
       "202. 0.166133333333333\n",
       "203. 0.170533333333333\n",
       "204. 0.561133333333333\n",
       "205. 0.284969841269841\n",
       "206. 0.6282\n",
       "207. 0.0330666666666667\n",
       "208. 0.8897\n",
       "209. 0.0762666666666667\n",
       "210. 0.829066666666667\n",
       "211. 0.0540666666666667\n",
       "212. 0\n",
       "213. 0.0879333333333333\n",
       "214. 0.515066666666667\n",
       "215. 0.135716666666667\n",
       "216. 0.7348\n",
       "217. 0.134466666666667\n",
       "218. 0.0577333333333333\n",
       "219. 0.008\n",
       "220. 0.133133333333333\n",
       "221. 0.0398666666666667\n",
       "222. 0.0181333333333333\n",
       "223. 0.386155555555556\n",
       "224. 0.6736\n",
       "225. 0.0676666666666667\n",
       "226. 0.172380952380952\n",
       "227. 0.406447619047619\n",
       "228. 0.625933333333333\n",
       "229. 0.244466666666667\n",
       "230. 0.002\n",
       "231. 0.806209523809524\n",
       "232. 0.0403333333333333\n",
       "233. 0.432918045112782\n",
       "234. 0.0575333333333333\n",
       "235. 0.0176666666666667\n",
       "236. 0.0554\n",
       "237. 0.322955555555555\n",
       "238. 0.148266666666667\n",
       "239. 0.588833333333333\n",
       "240. 0.7321\n",
       "241. 0.0332\n",
       "242. 0\n",
       "243. 0.0454666666666667\n",
       "244. 0.128733333333333\n",
       "245. 0.0615333333333333\n",
       "246. 0.0259333333333333\n",
       "247. 0.2938\n",
       "248. 0.604466666666667\n",
       "249. 0.293180952380952\n",
       "250. 0\n",
       "251. 0.0542\n",
       "252. 0.624761904761905\n",
       "253. 0.748666666666667\n",
       "254. 0.37767619047619\n",
       "255. 0.0690666666666667\n",
       "256. 0.490733333333333\n",
       "257. 0.8684\n",
       "258. 0.20385974025974\n",
       "259. 0.1262\n",
       "260. 0.1322\n",
       "261. 0.446662745098039\n",
       "262. 0.271530952380952\n",
       "263. 0.0333333333333333\n",
       "264. 0.343066666666667\n",
       "265. 0.583704273504274\n",
       "266. 0.358555555555555\n",
       "267. 0.0388\n",
       "268. 0.145866666666667\n",
       "269. 0.0588\n",
       "270. 0.130066666666667\n",
       "271. 0.378488888888889\n",
       "272. 0.136696103896104\n",
       "273. 0.349577777777778\n",
       "274. 0.201533333333333\n",
       "275. 0.078\n",
       "276. 0.2348\n",
       "277. 0.628266666666667\n",
       "278. 0.0378\n",
       "279. 0.27520404040404\n",
       "280. 0.1202\n",
       "281. 0.382\n",
       "282. 0.101733333333333\n",
       "283. 0.3042\n",
       "284. 0.105533333333333\n",
       "285. 0.618019047619048\n",
       "286. 0.337066666666667\n",
       "287. 0.0877666666666667\n",
       "288. 0.456895238095238\n",
       "289. 0.695733333333334\n",
       "290. 0.526\n",
       "291. 0.424315384615385\n",
       "292. 0.119133333333333\n",
       "293. 0.375266666666667\n",
       "294. 0.514166666666667\n",
       "295. 0.348466666666667\n",
       "296. 0.835866666666667\n",
       "297. 0.7324\n",
       "298. 0.148449816849817\n",
       "299. 0.0544666666666667\n",
       "300. 0.0476666666666667\n",
       "301. 0.848677777777778\n",
       "302. 0.0408\n",
       "303. 0.0185333333333333\n",
       "304. 0\n",
       "305. 0.0516\n",
       "306. 0.0334\n",
       "307. 0.102533333333333\n",
       "308. 0.180333333333333\n",
       "309. 0.0417\n",
       "310. 0.0188\n",
       "311. 0.119733333333333\n",
       "312. 0.586066666666667\n",
       "313. 0.670866666666667\n",
       "314. 0.0564\n",
       "315. 0.0304\n",
       "316. 0.698671524966262\n",
       "317. 0.261066666666667\n",
       "318. 0.1222\n",
       "319. 0.5628\n",
       "320. 0.625066666666667\n",
       "321. 0.02\n",
       "322. 0.173066666666667\n",
       "323. 0.0314\n",
       "324. 0.308066666666667\n",
       "325. 0.27\n",
       "326. 0.579961904761905\n",
       "327. 0.505066666666667\n",
       "328. 0.462357142857143\n",
       "329. 0.0121333333333333\n",
       "330. 0.390695238095238\n",
       "331. 0.464142857142857\n",
       "332. 0.250733333333333\n",
       "333. 0.448133333333333\n",
       "334. 0.597842424242424\n",
       "335. 0.792010256410256\n",
       "336. 0.198466666666667\n",
       "337. 0.543266666666667\n",
       "338. 0.5032\n",
       "339. 0.0381333333333333\n",
       "340. 0.162\n",
       "341. 0.0727333333333333\n",
       "342. 0.0906666666666667\n",
       "343. 0.0426666666666667\n",
       "344. 0.24767619047619\n",
       "345. 0.574945454545455\n",
       "346. 0.0128\n",
       "347. 0.252133333333333\n",
       "348. 0.266866666666667\n",
       "349. 0.0312\n",
       "350. 0.0288\n",
       "351. 0.0430222222222222\n",
       "352. 0.015\n",
       "353. 0.1694\n",
       "354. 0.633933333333333\n",
       "355. 0.123266666666667\n",
       "356. 0.388351633986928\n",
       "357. 0.634293506493506\n",
       "358. 0.0818666666666667\n",
       "359. 0.1352\n",
       "360. 0.328244444444444\n",
       "361. 0.0732\n",
       "362. 0.0244666666666667\n",
       "363. 0.136980952380952\n",
       "364. 0.110466666666667\n",
       "365. 0.417266666666667\n",
       "366. 0.323866666666667\n",
       "367. 0.670266666666667\n",
       "368. 0.001\n",
       "369. 0.0268\n",
       "370. 0.0800222222222222\n",
       "371. 0.711569696969697\n",
       "372. 0.1312\n",
       "373. 0.021\n",
       "374. 0.0884\n",
       "375. 0.113266666666667\n",
       "376. 0.734333333333333\n",
       "377. 0.175266666666667\n",
       "378. 0.0734666666666667\n",
       "379. 0.0116\n",
       "380. 0.293333333333333\n",
       "381. 0.277933333333333\n",
       "382. 0.5704\n",
       "383. 0.034\n",
       "384. 0.779533333333333\n",
       "385. 0.15865\n",
       "386. 0.0683333333333333\n",
       "387. 0.0176\n",
       "388. 0.352333333333333\n",
       "389. 0.0818\n",
       "390. 0.504\n",
       "391. 0.004\n",
       "392. 0.121866666666667\n",
       "393. 0.676104761904762\n",
       "394. 0.118533333333333\n",
       "395. 0.0912\n",
       "396. 0.4195\n",
       "397. 0.0548\n",
       "398. 0.0612888888888889\n",
       "399. 0.02\n",
       "400. 0.0108\n",
       "401. 0.133897435897436\n",
       "402. 0.636533333333333\n",
       "403. 0.200133333333333\n",
       "404. 0.909266666666667\n",
       "405. 0.0857333333333333\n",
       "406. 0.500466666666667\n",
       "407. 0.114697435897436\n",
       "408. 0.263482051282051\n",
       "409. 0.605866666666667\n",
       "410. 0.0510666666666667\n",
       "411. 0.233355555555556\n",
       "412. 0.034\n",
       "413. 0.410466666666667\n",
       "414. 0.023\n",
       "415. 0.5652\n",
       "416. 0.526266666666667\n",
       "417. 0.187066666666667\n",
       "418. 0.630533333333333\n",
       "419. 0.380333333333333\n",
       "420. 0.628666666666667\n",
       "421. 0.347482828282828\n",
       "422. 0.6542\n",
       "423. 0.0082\n",
       "424. 0.0491333333333333\n",
       "425. 0.204066666666667\n",
       "426. 0.383828571428572\n",
       "427. 0.1236\n",
       "428. 0.563369696969697\n",
       "429. 0.138181562881563\n",
       "430. 0.5213\n",
       "431. 0.413933333333333\n",
       "432. 0.0808\n",
       "433. 0.376266666666667\n",
       "434. 0.369533333333333\n",
       "435. 0.278047619047619\n",
       "436. 0.2165\n",
       "437. 0.0106\n",
       "438. 0.2436\n",
       "439. 0.767302564102564\n",
       "440. 0.411433333333333\n",
       "441. 0.1154\n",
       "442. 0.120848484848485\n",
       "443. 0.321133333333333\n",
       "444. 0.825571428571429\n",
       "445. 0.210459829059829\n",
       "446. 0.34685\n",
       "447. 0.0072\n",
       "448. 0.0144\n",
       "449. 0.643533333333333\n",
       "450. 0.0453333333333333\n",
       "451. 0.555733333333333\n",
       "452. 0.264333333333333\n",
       "453. 0.0601\n",
       "454. 0.720628571428571\n",
       "455. 0.226914285714286\n",
       "456. 0.0024\n",
       "457. 0.443866666666667\n",
       "458. 0.386577777777778\n",
       "459. 0.683733333333333\n",
       "460. 0.692666666666667\n",
       "461. 0.0965333333333333\n",
       "462. 0.0906\n",
       "463. 0.184161904761905\n",
       "464. 0.825685714285714\n",
       "465. 0.0323333333333333\n",
       "466. 0.581733333333333\n",
       "467. 0.170633333333333\n",
       "468. 0.064\n",
       "469. 0.0418666666666667\n",
       "470. 0.688161904761905\n",
       "471. 0.348422222222222\n",
       "472. 0.690333333333333\n",
       "473. 0.10452380952381\n",
       "474. 0.0914\n",
       "475. 0.556866666666667\n",
       "476. 0.0188\n",
       "477. 0.271466666666667\n",
       "478. 0.539022222222222\n",
       "479. 0.273266666666667\n",
       "480. 0.0141333333333333\n",
       "481. 0\n",
       "482. 0.121447619047619\n",
       "483. 0.771266666666667\n",
       "484. 0.1372\n",
       "485. 0.0132\n",
       "486. 0.127381818181818\n",
       "487. 0.593633333333333\n",
       "488. 0.0202\n",
       "489. 0.338142857142857\n",
       "490. 0.0661333333333333\n",
       "491. 0.598933333333333\n",
       "492. 0.4218\n",
       "493. 0.580360606060606\n",
       "494. 0.681633333333333\n",
       "495. 0.134822222222222\n",
       "496. 0.0138666666666667\n",
       "497. 0.726485714285714\n",
       "498. 0.018\n",
       "499. 0.799019047619047\n",
       "500. 0.113333333333333\n",
       "501. 0.152333333333333\n",
       "502. 0.131057142857143\n",
       "503. 0.0183333333333333\n",
       "504. 0.0113333333333333\n",
       "505. 0.159533333333333\n",
       "506. 0.013\n",
       "507. 0.4057\n",
       "508. 0.615422222222222\n",
       "509. 0.1036\n",
       "510. 0.0498666666666667\n",
       "511. 0.441333333333333\n",
       "512. 0.455933333333333\n",
       "513. 0.0197333333333333\n",
       "514. 0.0132\n",
       "515. 0.0444\n",
       "516. 0.347133333333333\n",
       "517. 0.0440285714285714\n",
       "518. 0.0466\n",
       "519. 0.539733333333333\n",
       "520. 0.489533333333333\n",
       "521. 0.008\n",
       "522. 0.0312\n",
       "523. 0.483742857142857\n",
       "524. 0.012\n",
       "525. 0.2564\n",
       "526. 0.349733333333333\n",
       "527. 0.0481333333333333\n",
       "528. 0.561133333333333\n",
       "529. 0.134\n",
       "530. 0.0140666666666667\n",
       "531. 0.257\n",
       "532. 0.0258\n",
       "533. 0.624533333333333\n",
       "534. 0.587936363636364\n",
       "535. 0.487933333333333\n",
       "536. 0.008\n",
       "537. 0.875275757575758\n",
       "538. 0.6896\n",
       "539. 0.2538\n",
       "540. 0.2456\n",
       "541. 0.131533333333333\n",
       "542. 0.8014\n",
       "543. 0.312203174603175\n",
       "544. 0.165466666666667\n",
       "545. 0.150733333333333\n",
       "546. 0.06965\n",
       "547. 0.66675\n",
       "548. 0.776466666666667\n",
       "549. 0.0113333333333333\n",
       "550. 0.599\n",
       "551. 0.004\n",
       "552. 0.0614\n",
       "553. 0.0778666666666667\n",
       "554. 0.804355555555556\n",
       "555. 0.348933333333333\n",
       "556. 0.609533333333333\n",
       "557. 0.083\n",
       "558. 0.048\n",
       "559. 0.0781333333333333\n",
       "560. 0.329244444444444\n",
       "561. 0.0318666666666667\n",
       "562. 0.0939333333333333\n",
       "563. 0.0586666666666667\n",
       "564. 0.1614\n",
       "565. 0.285088888888889\n",
       "566. 0.008\n",
       "567. 0.276266666666667\n",
       "568. 0.0314\n",
       "569. 0.656966666666667\n",
       "570. 0.444538095238095\n",
       "571. 0.0246666666666667\n",
       "572. 0.250285714285714\n",
       "573. 0.435933333333333\n",
       "574. 0.311333333333333\n",
       "575. 0.443466666666667\n",
       "576. 0.498166666666667\n",
       "577. 0.5006\n",
       "578. 0.190866666666667\n",
       "579. 0.104266666666667\n",
       "580. 0.0278666666666667\n",
       "581. 0.0162666666666667\n",
       "582. 0.0068\n",
       "583. 0.384666666666667\n",
       "584. 0.430755555555556\n",
       "585. 0.273\n",
       "586. 0.433833333333333\n",
       "587. 0.0382\n",
       "588. 0.0416\n",
       "589. 0.283066666666667\n",
       "590. 0.041\n",
       "591. 0.397533333333333\n",
       "592. 0.2058\n",
       "593. 0.0488\n",
       "594. 0.00533333333333333\n",
       "595. 0.0284\n",
       "596. 0.577218300653595\n",
       "597. 0.0288\n",
       "598. 0.0497111111111111\n",
       "599. 0.0276\n",
       "600. 0.506448484848485\n",
       "601. 0.043\n",
       "602. 0.313866666666667\n",
       "603. 0.634866666666667\n",
       "604. 0.0640666666666667\n",
       "605. 0.0881809523809524\n",
       "606. 0.319923809523809\n",
       "607. 0.772028571428572\n",
       "608. 0.0893333333333333\n",
       "609. 0.0627333333333333\n",
       "610. 0.0899333333333333\n",
       "611. 0.255775757575758\n",
       "612. 0.461455555555556\n",
       "613. 0.632056140350877\n",
       "614. 0.1374\n",
       "615. 0.0712\n",
       "616. 0.4774\n",
       "617. 0.0306\n",
       "618. 0.325066666666667\n",
       "619. 0.206066666666667\n",
       "620. 0.918333333333333\n",
       "621. 0.2567\n",
       "622. 0.5268\n",
       "623. 0.823995238095238\n",
       "624. 0.539933333333333\n",
       "625. 0.438244444444444\n",
       "626. 0.273933333333333\n",
       "627. 0.106666666666667\n",
       "628. 0.0164\n",
       "629. 0.460666666666667\n",
       "630. 0.0959333333333333\n",
       "631. 0.1478\n",
       "632. 0.0886666666666667\n",
       "633. 0.585866666666667\n",
       "634. 0.3264\n",
       "635. 0.507666666666667\n",
       "636. 0.0242666666666667\n",
       "637. 0.385169696969697\n",
       "638. 0.190866666666667\n",
       "639. 0.0229333333333333\n",
       "640. 0.2196\n",
       "641. 0.335733333333333\n",
       "642. 0.114066666666667\n",
       "643. 0.00986666666666667\n",
       "644. 0.1424\n",
       "645. 0.107466666666667\n",
       "646. 0.0280666666666667\n",
       "647. 0.119333333333333\n",
       "648. 0.0361333333333333\n",
       "649. 0.6014\n",
       "650. 0.363236363636364\n",
       "651. 0.16610303030303\n",
       "652. 0.0593333333333333\n",
       "653. 0.565169696969697\n",
       "654. 0.370374891774892\n",
       "655. 0.337822222222222\n",
       "656. 0.178266666666667\n",
       "657. 0.753295238095238\n",
       "658. 0.60904662004662\n",
       "659. 0.579828571428571\n",
       "660. 0.2234\n",
       "661. 0.116\n",
       "662. 0.0258\n",
       "663. 0.0555333333333333\n",
       "664. 0.553085714285714\n",
       "665. 0.242866666666667\n",
       "666. 0.1392\n",
       "667. 0.655604761904762\n",
       "668. 0.0341333333333333\n",
       "669. 0\n",
       "670. 0.663666666666667\n",
       "671. 0.423333333333333\n",
       "672. 0.0750666666666667\n",
       "673. 0.004\n",
       "674. 0.731727272727273\n",
       "675. 0.495666666666667\n",
       "676. 0.489209523809524\n",
       "677. 0.024\n",
       "678. 0.4498\n",
       "679. 0.532933333333333\n",
       "680. 0.4976\n",
       "681. 0.0434\n",
       "682. 0.808\n",
       "683. 0.098\n",
       "684. 0.295466666666667\n",
       "685. 0.0828222222222222\n",
       "686. 0.691\n",
       "687. 0.368177777777778\n",
       "688. 0.415733333333333\n",
       "689. 0.172666666666667\n",
       "690. 0.2466\n",
       "691. 0.264514285714286\n",
       "692. 0.142\n",
       "693. 0.613666666666667\n",
       "694. 0.386\n",
       "695. 0.0548\n",
       "696. 0.303\n",
       "697. 0.0562\n",
       "698. 0.0496380952380952\n",
       "699. 0.0142666666666667\n",
       "700. 0.2236\n",
       "701. 0.0474\n",
       "702. 0.251322222222222\n",
       "703. 0.757666666666667\n",
       "704. 0.1774\n",
       "705. 0.6092\n",
       "706. 0.655433333333333\n",
       "707. 0.425533333333333\n",
       "708. 0.0386\n",
       "709. 0.3898\n",
       "710. 0.116466666666667\n",
       "711. 0.443888888888889\n",
       "712. 0.872533333333333\n",
       "713. 0.0457333333333333\n",
       "714. 0.0228666666666667\n",
       "715. 0.2504\n",
       "716. 0.497822222222222\n",
       "717. 0.562542857142857\n",
       "718. 0.467207326007326\n",
       "719. 0.772333333333333\n",
       "720. 0.031\n",
       "721. 0.2836\n",
       "722. 0.403466666666667\n",
       "723. 0.6291\n",
       "724. 0.0975333333333333\n",
       "725. 0.0265333333333333\n",
       "726. 0.0506666666666667\n",
       "727. 0.406076190476191\n",
       "728. 0.478645887445888\n",
       "729. 0.4038\n",
       "730. 0.041\n",
       "731. 0.512807936507937\n",
       "732. 0.646666666666667\n",
       "733. 0.205996078431373\n",
       "734. 0.122\n",
       "735. 0.007\n",
       "736. 0.0612\n",
       "737. 0.0538666666666667\n",
       "738. 0.636333333333333\n",
       "739. 0.103\n",
       "740. 0.0476\n",
       "741. 0.003\n",
       "742. 0.118266666666667\n",
       "743. 0.72508051948052\n",
       "744. 0.0708666666666667\n",
       "745. 0.866933333333333\n",
       "746. 0.150866666666667\n",
       "747. 0.603666666666667\n",
       "748. 0.701045887445887\n",
       "749. 0.011\n",
       "750. 0.174533333333333\n",
       "751. 0.012\n",
       "752. 0.0724666666666667\n",
       "753. 0.1368\n",
       "754. 0.203733333333333\n",
       "755. 0.0724666666666667\n",
       "756. 0.78272380952381\n",
       "757. 0.163684711779449\n",
       "758. 0.0552666666666667\n",
       "759. 0.057\n",
       "760. 0.0579333333333333\n",
       "761. 0.299923809523809\n",
       "762. 0.450488888888889\n",
       "763. 0.0916\n",
       "764. 0.297866666666667\n",
       "765. 0.903228571428571\n",
       "766. 0.1064\n",
       "767. 0\n",
       "768. 0.00733333333333333\n",
       "769. 0.51248838612368\n",
       "770. 0.3064\n",
       "771. 0.0389333333333333\n",
       "772. 0.0418666666666667\n",
       "773. 0.573233333333333\n",
       "774. 0.799533333333333\n",
       "775. 0.597933333333333\n",
       "776. 0.121466666666667\n",
       "777. 0.8176\n",
       "778. 0.461330402930403\n",
       "779. 0.0531333333333333\n",
       "780. 0.457366666666667\n",
       "781. 0.1106\n",
       "782. 0.001\n",
       "783. 0.605761904761905\n",
       "784. 0.032\n",
       "785. 0.0516666666666667\n",
       "786. 0.5368\n",
       "787. 0.4388\n",
       "788. 0.4436\n",
       "789. 0.205\n",
       "790. 0.4114\n",
       "791. 0.00933333333333333\n",
       "792. 0.301977777777778\n",
       "793. 0.735366666666667\n",
       "794. 0.016\n",
       "795. 0.233666666666667\n",
       "796. 0.0032\n",
       "797. 0.012\n",
       "798. 0.0048\n",
       "799. 0.0384\n",
       "800. 0.358733333333333\n",
       "801. 0.218333333333333\n",
       "802. 0.0106666666666667\n",
       "803. 0.300504761904762\n",
       "804. 0.141190476190476\n",
       "805. 0.281314285714286\n",
       "806. 0.0676\n",
       "807. 0.454466666666667\n",
       "808. 0.0514\n",
       "809. 0.1074\n",
       "810. 0.695466666666667\n",
       "811. 0.395971428571429\n",
       "812. 0.0938666666666667\n",
       "813. 0.0126666666666667\n",
       "814. 0.0736\n",
       "815. 0.441365079365079\n",
       "816. 0.661666666666667\n",
       "817. 0.151\n",
       "818. 0.2728\n",
       "819. 0.459209523809524\n",
       "820. 0.0498\n",
       "821. 0.285633333333333\n",
       "822. 0.0406\n",
       "823. 0.514933333333333\n",
       "824. 0.564566666666667\n",
       "825. 0.0552666666666667\n",
       "826. 0.0168666666666667\n",
       "827. 0.701066666666667\n",
       "828. 0.0198\n",
       "829. 0.200466666666667\n",
       "830. 0.4523\n",
       "831. 0.0721333333333333\n",
       "832. 0.0352\n",
       "833. 0.732733333333333\n",
       "834. 0.589622222222222\n",
       "835. 0.654933333333333\n",
       "836. 0.499866666666667\n",
       "837. 0.159688888888889\n",
       "838. 0.4317\n",
       "839. 0.0599333333333333\n",
       "840. 0.704733333333333\n",
       "841. 0.221382051282051\n",
       "842. 0.6252\n",
       "843. 0.0284\n",
       "844. 0.698333333333333\n",
       "845. 0.727933333333333\n",
       "846. 0.6536\n",
       "847. 0.0173333333333333\n",
       "848. 0.188733333333333\n",
       "849. 0.081\n",
       "850. 0.374766666666667\n",
       "851. 0.0896666666666667\n",
       "852. 0.247866666666667\n",
       "853. 0.584533333333333\n",
       "854. 0.0136\n",
       "855. 0.872133333333333\n",
       "856. 0.306466666666667\n",
       "857. 0.0647333333333333\n",
       "858. 0\n",
       "859. 0.0024\n",
       "860. 0.646190476190476\n",
       "861. 0.017\n",
       "862. 0.0693333333333333\n",
       "863. 0.199066666666667\n",
       "864. 0.4084\n",
       "865. 0.505333333333333\n",
       "866. 0.0664222222222222\n",
       "867. 0.107733333333333\n",
       "868. 0.4294\n",
       "869. 0.383466666666667\n",
       "870. 0.0930666666666667\n",
       "871. 0.1202\n",
       "872. 0.002\n",
       "873. 0.0228666666666667\n",
       "874. 0.2854\n",
       "875. 0.352333333333333\n",
       "876. 0.3938\n",
       "877. 0.172266666666667\n",
       "878. 0.333233333333333\n",
       "879. 0.551666666666667\n",
       "880. 0.8372\n",
       "881. 0.938933333333333\n",
       "882. 0.370682051282051\n",
       "883. 0.6364\n",
       "884. 0.245933333333333\n",
       "885. 0.0641555555555556\n",
       "886. 0.116066666666667\n",
       "887. 0.0705333333333333\n",
       "888. 0.709066666666667\n",
       "889. 0.284888888888889\n",
       "890. 0.586828571428571\n",
       "891. 0.7202\n",
       "892. 0.693866666666667\n",
       "893. 0.226133333333333\n",
       "894. 0.233066666666667\n",
       "895. 0.251933333333333\n",
       "896. 0.1629\n",
       "897. 0.1852\n",
       "898. 0.370222222222222\n",
       "899. 0.197933333333333\n",
       "900. 0.173177777777778\n",
       "901. 0.214066666666667\n",
       "902. 0.66\n",
       "903. 0.0219777777777778\n",
       "904. 0.4735\n",
       "905. 0.626419047619048\n",
       "906. 0.204\n",
       "907. 0.1664\n",
       "908. 0.0172\n",
       "909. 0.389066666666667\n",
       "910. 0.548473992673992\n",
       "911. 0.0391333333333333\n",
       "912. 0.760057142857143\n",
       "913. 0.814066666666667\n",
       "914. 0.487933333333333\n",
       "915. 0.0762666666666667\n",
       "916. 0.016\n",
       "917. 0.587466666666667\n",
       "918. 0.560866666666667\n",
       "919. 0.3756\n",
       "920. 0.0688\n",
       "921. 0.0256666666666667\n",
       "922. 0.5748\n",
       "923. 0.442533333333333\n",
       "924. 0.724266666666667\n",
       "925. 0.0578666666666667\n",
       "926. 0.0048\n",
       "927. 0.271638095238095\n",
       "928. 0.2132\n",
       "929. 0.323833333333333\n",
       "930. 0.565780952380952\n",
       "931. 0.9462\n",
       "932. 0.0466666666666667\n",
       "933. 0.123666666666667\n",
       "934. 0.850247619047619\n",
       "935. 0.271115151515152\n",
       "936. 0.0545333333333333\n",
       "937. 0.52965974025974\n",
       "938. 0.0590666666666667\n",
       "939. 0.0907904761904762\n",
       "940. 0.280133333333333\n",
       "941. 0.497692307692308\n",
       "942. 0.546333333333333\n",
       "943. 0.392133333333333\n",
       "944. 0.00786666666666667\n",
       "945. 0.301\n",
       "946. 0.039\n",
       "947. 0.028\n",
       "948. 0.919933333333333\n",
       "949. 0.794533333333333\n",
       "950. 0.778266666666667\n",
       "951. 0.285262745098039\n",
       "952. 0.300666666666667\n",
       "953. 0.1062\n",
       "954. 0.825\n",
       "955. 0.15387619047619\n",
       "956. 0.3356\n",
       "957. 0.011\n",
       "958. 0.0432\n",
       "959. 0.730666666666667\n",
       "960. 0.0751333333333333\n",
       "961. 0.026\n",
       "962. 0.195266666666667\n",
       "963. 0\n",
       "964. 0.0887833333333333\n",
       "965. 0.0667333333333333\n",
       "966. 0.47570303030303\n",
       "967. 0.169866666666667\n",
       "968. 0.06\n",
       "969. 0.518\n",
       "970. 0.749466666666667\n",
       "971. 0.0626666666666667\n",
       "972. 0.299266666666667\n",
       "973. 0.643022222222222\n",
       "974. 0.504666666666667\n",
       "975. 0.26585974025974\n",
       "976. 0.0640666666666667\n",
       "977. 0.771028571428572\n",
       "978. 0.370133333333333\n",
       "979. 0.226057142857143\n",
       "980. 0.704866666666667\n",
       "981. 0.912266666666667\n",
       "982. 0.602590476190476\n",
       "983. 0.4874\n",
       "984. 0.553409523809524\n",
       "985. 0.628009523809524\n",
       "986. 0.440129411764706\n",
       "987. 0.348755555555556\n",
       "988. 0.475066666666667\n",
       "989. 0.472929411764706\n",
       "990. 0.004\n",
       "991. 0.1022\n",
       "992. 0.0205333333333333\n",
       "993. 0.0625333333333333\n",
       "994. 0.262695238095238\n",
       "995. 0.0899333333333333\n",
       "996. 0.7598\n",
       "997. 0.400888888888889\n",
       "998. 0.0524\n",
       "999. 0.0258\n",
       "1000. 0.570848717948718\n",
       "1001. 0.044\n",
       "1002. 0.30070303030303\n",
       "1003. 0.413133333333333\n",
       "1004. 0.3964\n",
       "1005. 0.349533333333333\n",
       "1006. 0.794466666666667\n",
       "1007. 0.7002\n",
       "1008. 0.0558666666666667\n",
       "1009. 0.0032\n",
       "1010. 0.0890666666666667\n",
       "1011. 0.42\n",
       "1012. 0.285466666666667\n",
       "1013. 0.0918222222222222\n",
       "1014. 0.523\n",
       "1015. 0.1118\n",
       "1016. 0.766466666666667\n",
       "1017. 0.285666666666667\n",
       "1018. 0.2864\n",
       "1019. 0.250933333333333\n",
       "1020. 0.754933333333333\n",
       "1021. 0.192283333333333\n",
       "1022. 0.008\n",
       "1023. 0.0444\n",
       "1024. 0.006\n",
       "1025. 0.001\n",
       "1026. 0.168685714285714\n",
       "1027. 0.367066666666667\n",
       "1028. 0.389066666666667\n",
       "1029. 0.1724\n",
       "1030. 0\n",
       "1031. 0.166433333333333\n",
       "1032. 0.628547008547009\n",
       "1033. 0.643\n",
       "1034. 0.489533333333333\n",
       "1035. 0.444104761904762\n",
       "1036. 0.0688666666666667\n",
       "1037. 0.128133333333333\n",
       "1038. 0.4426\n",
       "1039. 0.036\n",
       "1040. 0.0963333333333333\n",
       "1041. 0.794666666666667\n",
       "1042. 0.661587545787546\n",
       "1043. 0.834228571428572\n",
       "1044. 0.0522\n",
       "1045. 0.0462888888888889\n",
       "1046. 0.0827\n",
       "1047. 0.272847619047619\n",
       "1048. 0.1022\n",
       "1049. 0.861752380952381\n",
       "1050. 0.335266666666667\n",
       "1051. 0.405333333333333\n",
       "1052. 0.072\n",
       "1053. 0.432569696969697\n",
       "1054. 0.0171333333333333\n",
       "1055. 0.0614666666666667\n",
       "1056. 0.444593939393939\n",
       "1057. 0.004\n",
       "1058. 0.0309333333333333\n",
       "1059. 0.380933333333333\n",
       "1060. 0.0221333333333333\n",
       "1061. 0.0016\n",
       "1062. 0.0976666666666667\n",
       "1063. 0.40852380952381\n",
       "1064. 0.231711111111111\n",
       "1065. 0.413133333333333\n",
       "1066. 0.187733333333333\n",
       "1067. 0.453533333333333\n",
       "1068. 0.002\n",
       "1069. 0.6224\n",
       "1070. 0.759333333333333\n",
       "1071. 0.360155555555556\n",
       "1072. 0.624666666666667\n",
       "1073. 0.594329411764706\n",
       "1074. 0.176866666666667\n",
       "1075. 0.147066666666667\n",
       "1076. 0.137333333333333\n",
       "1077. 0.252866666666667\n",
       "1078. 0.0112\n",
       "1079. 0.448222222222222\n",
       "1080. 0.671428571428571\n",
       "1081. 0.624266666666667\n",
       "1082. 0.0368888888888889\n",
       "1083. 0.0546666666666667\n",
       "1084. 0.560533333333333\n",
       "1085. 0.617983333333333\n",
       "1086. 0.0192\n",
       "1087. 0.3876\n",
       "1088. 0.416466666666667\n",
       "1089. 0.926569696969697\n",
       "1090. 0.223266666666667\n",
       "1091. 0.706\n",
       "1092. 0.729228571428571\n",
       "1093. 0.828666666666667\n",
       "1094. 0.0381333333333333\n",
       "1095. 0.171733333333333\n",
       "1096. 0.1764\n",
       "1097. 0.400133333333333\n",
       "1098. 0.719561904761905\n",
       "1099. 0.0337333333333333\n",
       "1100. 0.920295238095238\n",
       "1101. 0.146266666666667\n",
       "1102. 0.012\n",
       "1103. 0.183733333333333\n",
       "1104. 0.652488888888889\n",
       "1105. 0.254822222222222\n",
       "1106. 0.228866666666667\n",
       "1107. 0.598\n",
       "1108. 0.1814\n",
       "1109. 0.0668\n",
       "1110. 0.576819047619048\n",
       "1111. 0.008\n",
       "1112. 0.1722\n",
       "1113. 0.182533333333333\n",
       "1114. 0.28651746031746\n",
       "1115. 0.510742857142857\n",
       "1116. 0.5052\n",
       "1117. 0.625285714285714\n",
       "1118. 0.0794\n",
       "1119. 0.774066666666667\n",
       "1120. 0.0228\n",
       "1121. 0.3284\n",
       "1122. 0.222448484848485\n",
       "1123. 0.0590222222222222\n",
       "1124. 0.914428571428572\n",
       "1125. 0.595644444444445\n",
       "1126. 0.250581818181818\n",
       "1127. 0.0064\n",
       "1128. 0.0136\n",
       "1129. 0.0418\n",
       "1130. 0.137133333333333\n",
       "1131. 0.778466666666667\n",
       "1132. 0.655733333333334\n",
       "1133. 0.133333333333333\n",
       "1134. 0.1562\n",
       "1135. 0.0078\n",
       "1136. 0.195866666666667\n",
       "1137. 0.176733333333333\n",
       "1138. 0.0567333333333333\n",
       "1139. 0.440333333333333\n",
       "1140. 0.022\n",
       "1141. 0.649622222222222\n",
       "1142. 0.0038\n",
       "1143. 0.696628571428571\n",
       "1144. 0.0758666666666667\n",
       "1145. 0.0074\n",
       "1146. 0.5579\n",
       "1147. 0.829057142857143\n",
       "1148. 0.303266666666667\n",
       "1149. 0.0862\n",
       "1150. 0.0621\n",
       "1151. 0.0519666666666667\n",
       "1152. 0.413433333333333\n",
       "1153. 0.0839333333333333\n",
       "1154. 0.387733333333333\n",
       "1155. 0.132\n",
       "1156. 0.0125333333333333\n",
       "1157. 0.153733333333333\n",
       "1158. 0.67390303030303\n",
       "1159. 0.1204\n",
       "1160. 0.745666666666667\n",
       "1161. 0.811133333333333\n",
       "1162. 0.250022222222222\n",
       "1163. 0.705933333333333\n",
       "1164. 0.307133333333333\n",
       "1165. 0.614614285714286\n",
       "1166. 0.693015384615385\n",
       "1167. 0.104066666666667\n",
       "1168. 0.5954\n",
       "1169. 0.342333333333333\n",
       "1170. 0.225533333333333\n",
       "1171. 0.136781818181818\n",
       "1172. 0.516022222222222\n",
       "1173. 0.525295238095238\n",
       "1174. 0.173066666666667\n",
       "1175. 0.3134\n",
       "1176. 0.0196666666666667\n",
       "1177. 0.349266666666667\n",
       "1178. 0.2724\n",
       "1179. 0.0774\n",
       "1180. 0.4402\n",
       "1181. 0.0454666666666667\n",
       "1182. 0.524\n",
       "1183. 0.008\n",
       "1184. 0.478\n",
       "1185. 0.0996\n",
       "1186. 0.304595238095238\n",
       "1187. 0.107533333333333\n",
       "1188. 0.0345666666666667\n",
       "1189. 0.008\n",
       "1190. 0.10212380952381\n",
       "1191. 0.311766666666667\n",
       "1192. 0.670957142857143\n",
       "1193. 0.271866666666667\n",
       "1194. 0.0418666666666667\n",
       "1195. 0.018\n",
       "1196. 0.106933333333333\n",
       "1197. 0.507866666666667\n",
       "1198. 0.11035\n",
       "1199. 0.149533333333333\n",
       "1200. 0.2718\n",
       "1201. 0.400766666666667\n",
       "1202. 0.119457142857143\n",
       "1203. 0.360066666666667\n",
       "1204. 0.0792\n",
       "1205. 0.156\n",
       "1206. 0.0392\n",
       "1207. 0.390866666666667\n",
       "1208. 0.0550666666666667\n",
       "1209. 0.0742666666666667\n",
       "1210. 0.1194\n",
       "1211. 0.0798666666666667\n",
       "1212. 0.596466666666667\n",
       "1213. 0.0429333333333333\n",
       "1214. 0.0092\n",
       "1215. 0.828761904761905\n",
       "1216. 0.56107619047619\n",
       "1217. 0.679266666666667\n",
       "1218. 0.154561904761905\n",
       "1219. 0.153333333333333\n",
       "1220. 0.154933333333333\n",
       "1221. 0.504533333333333\n",
       "1222. 0.00266666666666667\n",
       "1223. 0.056047619047619\n",
       "1224. 0.396115384615385\n",
       "1225. 0.0314666666666667\n",
       "1226. 0.042\n",
       "1227. 0.553933333333333\n",
       "1228. 0.5086\n",
       "1229. 0.264077777777778\n",
       "1230. 0.0722666666666667\n",
       "1231. 0.0762666666666667\n",
       "1232. 0.550266666666667\n",
       "1233. 0.624844444444445\n",
       "1234. 0.0496\n",
       "1235. 0.115266666666667\n",
       "1236. 0.225866666666667\n",
       "1237. 0.334519047619048\n",
       "1238. 0.796466666666667\n",
       "1239. 0.769295238095238\n",
       "1240. 0.486733333333333\n",
       "1241. 0.262533333333333\n",
       "1242. 0.0124666666666667\n",
       "1243. 0.016\n",
       "1244. 0.695504761904762\n",
       "1245. 0.428666666666667\n",
       "1246. 0.0428\n",
       "1247. 0.612333333333333\n",
       "1248. 0.0196666666666667\n",
       "1249. 0.199333333333333\n",
       "1250. 0.359133333333333\n",
       "1251. 0.494266666666667\n",
       "1252. 0.889266666666667\n",
       "1253. 0.3366\n",
       "1254. 0.3516\n",
       "1255. 0.1978\n",
       "1256. 0.8336\n",
       "1257. 0.0382666666666667\n",
       "1258. 0.0392222222222222\n",
       "1259. 0.436266666666667\n",
       "1260. 0.0542\n",
       "1261. 0.319441558441558\n",
       "1262. 0.556066666666667\n",
       "1263. 0.00866666666666667\n",
       "1264. 0.0388666666666667\n",
       "1265. 0.557866666666667\n",
       "1266. 0.0478666666666667\n",
       "1267. 0.2173\n",
       "1268. 0.495161904761905\n",
       "1269. 0.109866666666667\n",
       "1270. 0.515209523809524\n",
       "1271. 0.0404\n",
       "1272. 0.144333333333333\n",
       "1273. 0.521333333333333\n",
       "1274. 0.0892666666666666\n",
       "1275. 0.351333333333333\n",
       "1276. 0.484444444444444\n",
       "1277. 0.336733333333333\n",
       "1278. 0.403933333333333\n",
       "1279. 0.0296666666666667\n",
       "1280. 0.0755333333333333\n",
       "1281. 0.0419333333333333\n",
       "1282. 0.5966\n",
       "1283. 0.0289333333333333\n",
       "1284. 0.0402666666666667\n",
       "1285. 0.0476\n",
       "1286. 0.004\n",
       "1287. 0.503466666666667\n",
       "1288. 0.0322666666666667\n",
       "1289. 0.721088888888889\n",
       "1290. 0.127\n",
       "1291. 0.256133333333333\n",
       "1292. 0.827266666666667\n",
       "1293. 0.438266666666667\n",
       "1294. 0.688466666666667\n",
       "1295. 0.0679333333333333\n",
       "1296. 0.6014\n",
       "1297. 0.734095238095238\n",
       "1298. 0.235933333333333\n",
       "1299. 0.0326222222222222\n",
       "1300. 0.329733333333333\n",
       "1301. 0.537747186147186\n",
       "1302. 0\n",
       "1303. 0.371533333333333\n",
       "1304. 0.693828571428571\n",
       "1305. 0.2747\n",
       "1306. 0.63250303030303\n",
       "1307. 0.012\n",
       "1308. 0.0402\n",
       "1309. 0.749171428571429\n",
       "1310. 0.0072\n",
       "1311. 0.159180952380952\n",
       "1312. 0.1474\n",
       "1313. 0.173066666666667\n",
       "1314. 0.00985714285714286\n",
       "1315. 0.5274\n",
       "1316. 0.545333333333333\n",
       "1317. 0.0028\n",
       "1318. 0.1886\n",
       "1319. 0.582933333333333\n",
       "1320. 0.697180952380952\n",
       "1321. 0.716755555555555\n",
       "1322. 0.267933333333333\n",
       "1323. 0.362555555555555\n",
       "1324. 0.297444444444444\n",
       "1325. 0.0328\n",
       "1326. 0.4958\n",
       "1327. 0.2038\n",
       "1328. 0.01\n",
       "1329. 0.253533333333333\n",
       "1330. 0.066\n",
       "1331. 0.184247619047619\n",
       "1332. 0.210133333333333\n",
       "1333. 0.685266666666667\n",
       "1334. 0.405707936507937\n",
       "1335. 0.188933333333333\n",
       "1336. 0.5358\n",
       "1337. 0.367\n",
       "1338. 0.0260666666666667\n",
       "1339. 0.004\n",
       "1340. 0.121333333333333\n",
       "1341. 0.0815333333333333\n",
       "1342. 0.436923809523809\n",
       "1343. 0.004\n",
       "1344. 0.210066666666667\n",
       "1345. 0.5586\n",
       "1346. 0.372333333333333\n",
       "1347. 0.002\n",
       "1348. 0.349466666666667\n",
       "1349. 0.017\n",
       "1350. 0.143\n",
       "1351. 0.164\n",
       "1352. 0.0281333333333333\n",
       "1353. 0.0370666666666667\n",
       "1354. 0.3204\n",
       "1355. 0.0526\n",
       "1356. 0.712633333333333\n",
       "1357. 0.634276190476191\n",
       "1358. 0.356866666666667\n",
       "1359. 0.294866666666667\n",
       "1360. 0.292\n",
       "1361. 0.008\n",
       "1362. 0.6658\n",
       "1363. 0.453733333333333\n",
       "1364. 0.461790476190476\n",
       "1365. 0.475933333333333\n",
       "1366. 0.3352\n",
       "1367. 0.008\n",
       "1368. 0.312333333333333\n",
       "1369. 0.238066666666667\n",
       "1370. 0.0829333333333333\n",
       "1371. 0.355333333333333\n",
       "1372. 0.283533333333333\n",
       "1373. 0.723009523809524\n",
       "1374. 0.191133333333333\n",
       "1375. 0.0534666666666667\n",
       "1376. 0.0810666666666667\n",
       "1377. 0.264866666666667\n",
       "1378. 0.0088\n",
       "1379. 0.830419047619048\n",
       "1380. 0.293333333333333\n",
       "1381. 0.596066666666667\n",
       "1382. 0.3234\n",
       "1383. 0.495485714285714\n",
       "1384. 0.615\n",
       "1385. 0.167866666666667\n",
       "1386. 0.37\n",
       "1387. 0.474\n",
       "1388. 0.448133333333333\n",
       "1389. 0.670273015873016\n",
       "1390. 0.416422222222222\n",
       "1391. 0.1182\n",
       "1392. 0.0579333333333333\n",
       "1393. 0.0213333333333333\n",
       "1394. 0.029\n",
       "1395. 0.214266666666667\n",
       "1396. 0.404566666666667\n",
       "1397. 0.172866666666667\n",
       "1398. 0.345866666666667\n",
       "1399. 0.0685333333333333\n",
       "1400. 0.145316666666667\n",
       "1401. 0.0618\n",
       "1402. 0.231866666666667\n",
       "1403. 0.4736\n",
       "1404. 0.0248\n",
       "1405. 0.478388744588745\n",
       "1406. 0.82\n",
       "1407. 0.0708666666666667\n",
       "1408. 0.489466666666667\n",
       "1409. 0.146457142857143\n",
       "1410. 0.227647619047619\n",
       "1411. 0.207466666666667\n",
       "1412. 0.618433333333334\n",
       "1413. 0\n",
       "1414. 0.773333333333333\n",
       "1415. 0.2312\n",
       "1416. 0.0553333333333333\n",
       "1417. 0.574048351648352\n",
       "1418. 0.705933333333333\n",
       "1419. 0.0692222222222222\n",
       "1420. 0.0054\n",
       "1421. 0.289961904761905\n",
       "1422. 0.413\n",
       "1423. 0.748666666666667\n",
       "1424. 0.425133333333333\n",
       "1425. 0.3733\n",
       "1426. 0.539466666666667\n",
       "1427. 0.485798268398268\n",
       "1428. 0.3804\n",
       "1429. 0.012\n",
       "1430. 0.752333333333333\n",
       "1431. 0.0184\n",
       "1432. 0.425377896613191\n",
       "1433. 0.0148\n",
       "1434. 0.7104\n",
       "1435. 0.088\n",
       "1436. 0.3026\n",
       "1437. 0.347222222222222\n",
       "1438. 0.0109333333333333\n",
       "1439. 0.400666666666667\n",
       "1440. 0.418333333333333\n",
       "1441. 0.152133333333333\n",
       "1442. 0.0438666666666667\n",
       "1443. 0.819723809523809\n",
       "1444. 0.142888888888889\n",
       "1445. 0.474466666666667\n",
       "1446. 0.659342857142857\n",
       "1447. 0.0423333333333333\n",
       "1448. 0.220155555555556\n",
       "1449. 0.537133333333333\n",
       "1450. 0.0724666666666667\n",
       "1451. 0.0548\n",
       "1452. 0.253619047619048\n",
       "1453. 0.467555555555556\n",
       "1454. 0.216666666666667\n",
       "1455. 0.707533333333333\n",
       "1456. 0.3424\n",
       "1457. 0.297733333333333\n",
       "1458. 0.0238\n",
       "1459. 0.4306\n",
       "1460. 0.454533333333333\n",
       "1461. 0.004\n",
       "1462. 0.599447619047619\n",
       "1463. 0.380133333333333\n",
       "1464. 0.713369696969697\n",
       "1465. 0.0731333333333333\n",
       "1466. 0.795411111111111\n",
       "1467. 0.134133333333333\n",
       "1468. 0.0318\n",
       "1469. 0\n",
       "1470. 0.752666666666667\n",
       "1471. 0.482136363636364\n",
       "1472. 0\n",
       "1473. 0.0959238095238095\n",
       "1474. 0.004\n",
       "1475. 0.0799\n",
       "1476. 0.208888888888889\n",
       "1477. 0.498036363636364\n",
       "1478. 0.400695238095238\n",
       "1479. 0.151\n",
       "1480. 0.824735714285714\n",
       "1481. 0.0474\n",
       "1482. 0.223133333333333\n",
       "1483. 0.868619047619048\n",
       "1484. 0.002\n",
       "1485. 0.275\n",
       "1486. 0.477333333333333\n",
       "1487. 0.408066666666667\n",
       "1488. 0.285047619047619\n",
       "1489. 0.260666666666667\n",
       "1490. 0.4912\n",
       "1491. 0.715148717948718\n",
       "1492. 0.786761904761905\n",
       "1493. 0.4124\n",
       "1494. 0.4268\n",
       "1495. 0.2124\n",
       "1496. 0.125781818181818\n",
       "1497. 0\n",
       "1498. 0.0194\n",
       "1499. 0.319933333333333\n",
       "1500. 0.759723809523809\n",
       "1501. 0.332003174603175\n",
       "1502. 0.217515151515152\n",
       "1503. 0.0875333333333334\n",
       "1504. 0.466933333333333\n",
       "1505. 0.0436\n",
       "1506. 0.0289333333333333\n",
       "1507. 0.519133333333333\n",
       "1508. 0.8186\n",
       "1509. 0.746836363636364\n",
       "1510. 0.440266666666667\n",
       "1511. 0.321580952380952\n",
       "1512. 8e-04\n",
       "1513. 0.125266666666667\n",
       "1514. 0.0768\n",
       "1515. 0.852466666666667\n",
       "1516. 0.0107333333333333\n",
       "1517. 0.115533333333333\n",
       "1518. 0.366866666666667\n",
       "1519. 0.561233333333333\n",
       "1520. 0.0624\n",
       "1521. 0.0104\n",
       "1522. 0.297333333333333\n",
       "1523. 0.4828\n",
       "1524. 0.841123809523809\n",
       "1525. 0.4754\n",
       "1526. 0.0382\n",
       "1527. 0.3354\n",
       "1528. 0.399733333333333\n",
       "1529. 0.826319047619048\n",
       "1530. 0.0742\n",
       "1531. 0.288\n",
       "1532. 0.62690303030303\n",
       "1533. 0.278866666666667\n",
       "1534. 0.213\n",
       "1535. 0.570133333333333\n",
       "1536. 0.0272\n",
       "1537. 0.1694\n",
       "1538. 0.784\n",
       "1539. 0.144733333333333\n",
       "1540. 0.00333333333333333\n",
       "1541. 0.369866666666667\n",
       "1542. 0.125133333333333\n",
       "1543. 0.051\n",
       "1544. 0.5634\n",
       "1545. 0.177288888888889\n",
       "1546. 0.322933333333333\n",
       "1547. 0.0213333333333333\n",
       "1548. 0.3416\n",
       "1549. 8e-04\n",
       "1550. 0.0440888888888889\n",
       "1551. 0.0594666666666667\n",
       "1552. 0.111\n",
       "1553. 0.0652\n",
       "1554. 0.0268666666666667\n",
       "1555. 0.104266666666667\n",
       "1556. 0.120133333333333\n",
       "1557. 0.2918\n",
       "1558. 0.0204666666666667\n",
       "1559. 0.0808\n",
       "1560. 0.0813\n",
       "1561. 0.0133333333333333\n",
       "1562. 0.00793333333333333\n",
       "1563. 0.4382\n",
       "1564. 0.767866666666667\n",
       "1565. 0.369723809523809\n",
       "1566. 0.0972\n",
       "1567. 0.0618\n",
       "1568. 0.555555555555556\n",
       "1569. 0.259866666666667\n",
       "1570. 0.198866666666667\n",
       "1571. 0.646038095238095\n",
       "1572. 0.709961904761905\n",
       "1573. 0.122266666666667\n",
       "1574. 0.5438\n",
       "1575. 0.0821\n",
       "1576. 0.017\n",
       "1577. 0.748333333333334\n",
       "1578. 0.224127272727273\n",
       "1579. 0.492333333333333\n",
       "1580. 0.325993073593074\n",
       "1581. 0.277933333333333\n",
       "1582. 0.1946\n",
       "1583. 0.0443333333333333\n",
       "1584. 0.513866666666667\n",
       "1585. 0.0910666666666667\n",
       "1586. 0.0606\n",
       "1587. 0.2996\n",
       "1588. 0.5104\n",
       "1589. 0.449333333333333\n",
       "1590. 0.14545\n",
       "1591. 0.660952380952381\n",
       "1592. 0.346266666666667\n",
       "1593. 0.294390476190476\n",
       "1594. 0.106133333333333\n",
       "1595. 0.107\n",
       "1596. 0.434460606060606\n",
       "1597. 0.0903333333333333\n",
       "1598. 0\n",
       "1599. 0.471066666666667\n",
       "1600. 0.372733333333333\n",
       "1601. 0.256866666666667\n",
       "1602. 0.0552\n",
       "1603. 0.0738\n",
       "1604. 0.505035897435897\n",
       "1605. 0.305247619047619\n",
       "1606. 0.738333333333333\n",
       "1607. 0.0513333333333333\n",
       "1608. 0.553333333333333\n",
       "1609. 0.0659333333333333\n",
       "1610. 0.7698\n",
       "1611. 0.016\n",
       "1612. 0.0306666666666667\n",
       "1613. 0.36028354978355\n",
       "1614. 0.674695238095238\n",
       "1615. 0.822761904761905\n",
       "1616. 0\n",
       "1617. 0.0575641025641026\n",
       "1618. 0.0621333333333333\n",
       "1619. 0.308916666666667\n",
       "1620. 0.42880303030303\n",
       "1621. 0.0234\n",
       "1622. 0.896333333333333\n",
       "1623. 0.812\n",
       "1624. 0.642666666666667\n",
       "1625. 0.352733333333333\n",
       "1626. 0.677\n",
       "1627. 0.1694\n",
       "1628. 0.610333333333333\n",
       "1629. 0.2728\n",
       "1630. 0.1418\n",
       "1631. 0.2014\n",
       "1632. 0.419066666666667\n",
       "1633. 0.197733333333333\n",
       "1634. 0.108627777777778\n",
       "1635. 0.358133333333333\n",
       "1636. 0.534\n",
       "1637. 0.004\n",
       "1638. 0.123855555555556\n",
       "1639. 0.0032\n",
       "1640. 0.539333333333333\n",
       "1641. 0.452959595959596\n",
       "1642. 0.0778\n",
       "1643. 0.343466666666667\n",
       "1644. 0.0579333333333333\n",
       "1645. 0.201133333333333\n",
       "1646. 0.3832\n",
       "1647. 0.0292\n",
       "1648. 0.344433333333333\n",
       "1649. 0.0809333333333334\n",
       "1650. 0.0162666666666667\n",
       "1651. 0.6366\n",
       "1652. 0.0328\n",
       "1653. 0.0573333333333333\n",
       "1654. 0.1062\n",
       "1655. 0.4754\n",
       "1656. 0.342266666666667\n",
       "1657. 0.00133333333333333\n",
       "1658. 0.430622222222222\n",
       "1659. 0.592933333333333\n",
       "1660. 0.0032\n",
       "1661. 0.473666666666667\n",
       "1662. 0.181266666666667\n",
       "1663. 0.0337333333333333\n",
       "1664. 0.332266666666667\n",
       "1665. 0.246766666666667\n",
       "1666. 0.191866666666667\n",
       "1667. 0.0116444444444444\n",
       "1668. 0.365933333333333\n",
       "1669. 0.1302\n",
       "1670. 0.278688888888889\n",
       "1671. 0.321666666666667\n",
       "1672. 0.0172\n",
       "1673. 0.0632666666666667\n",
       "1674. 0.343733333333333\n",
       "1675. 0.379435897435897\n",
       "1676. 0.862\n",
       "1677. 0.0313333333333333\n",
       "1678. 0.194733333333333\n",
       "1679. 0.0176\n",
       "1680. 0.517302564102564\n",
       "1681. 0.2246\n",
       "1682. 0.204333333333333\n",
       "1683. 0.494355555555556\n",
       "1684. 0.2418\n",
       "1685. 0.289390476190476\n",
       "1686. 0.0644974358974359\n",
       "1687. 0.008\n",
       "1688. 0.281466666666667\n",
       "1689. 0.210933333333333\n",
       "1690. 0.023\n",
       "1691. 0.034\n",
       "1692. 0.3638\n",
       "1693. 0.624244444444444\n",
       "1694. 0.285333333333333\n",
       "1695. 0.0084\n",
       "1696. 0.004\n",
       "1697. 0.4997\n",
       "1698. 0.119533333333333\n",
       "1699. 0.358533333333333\n",
       "1700. 0.2148\n",
       "1701. 0.692233333333333\n",
       "1702. 0.1018\n",
       "1703. 0.380466666666667\n",
       "1704. 0.112733333333333\n",
       "1705. 0.247\n",
       "1706. 0.111666666666667\n",
       "1707. 0.086\n",
       "1708. 0.648611111111111\n",
       "1709. 0.653466666666666\n",
       "1710. 0.1018\n",
       "1711. 0.418666666666667\n",
       "1712. 0.1836\n",
       "1713. 0.110733333333333\n",
       "1714. 0.0123333333333333\n",
       "1715. 0.245466666666667\n",
       "1716. 0.181933333333333\n",
       "1717. 0.576933333333333\n",
       "1718. 0.439922807017544\n",
       "1719. 0.552733333333333\n",
       "1720. 0.0386666666666667\n",
       "1721. 0.294409523809524\n",
       "1722. 0.744076923076923\n",
       "1723. 0.787533333333333\n",
       "1724. 0.117828571428571\n",
       "1725. 0.0647333333333333\n",
       "1726. 0.0785111111111111\n",
       "1727. 0.391165367965368\n",
       "1728. 0.1036\n",
       "1729. 0.0525333333333333\n",
       "1730. 0.6824\n",
       "1731. 0.102980952380952\n",
       "1732. 0.176666666666667\n",
       "1733. 0.4444\n",
       "1734. 0.888333333333333\n",
       "1735. 0.0742\n",
       "1736. 0.049\n",
       "1737. 0.413133333333333\n",
       "1738. 0.0273555555555556\n",
       "1739. 0.562666666666667\n",
       "1740. 0.596095238095238\n",
       "1741. 0.730533333333333\n",
       "1742. 0.417755555555555\n",
       "1743. 0.1578\n",
       "1744. 0.0202\n",
       "1745. 0.062\n",
       "1746. 0.00533333333333333\n",
       "1747. 0.553695238095238\n",
       "1748. 0.567133333333334\n",
       "1749. 0.0566\n",
       "1750. 0.382266666666667\n",
       "1751. 0.0423333333333333\n",
       "1752. 0.679175757575758\n",
       "1753. 0.362169696969697\n",
       "1754. 0.817614285714286\n",
       "1755. 0.0248\n",
       "1756. 0.809157142857143\n",
       "1757. 0.449380952380952\n",
       "1758. 0.321433333333333\n",
       "1759. 0.0350666666666667\n",
       "1760. 0.326109090909091\n",
       "1761. 0.101066666666667\n",
       "1762. 0.184666666666667\n",
       "1763. 0.605833333333333\n",
       "1764. 0.180066666666667\n",
       "1765. 0.00213333333333333\n",
       "1766. 0.2508\n",
       "1767. 0.23050303030303\n",
       "1768. 0.0544\n",
       "1769. 0.0499333333333333\n",
       "1770. 0.601533333333333\n",
       "1771. 0.3658\n",
       "1772. 0.1464\n",
       "1773. 0.00666666666666667\n",
       "1774. 0.3964\n",
       "1775. 0.263733333333333\n",
       "1776. 0.614533333333333\n",
       "1777. 0.401066666666667\n",
       "1778. 0.106\n",
       "1779. 0.5256\n",
       "1780. 0.328333333333333\n",
       "1781. 0.6816\n",
       "1782. 0.008\n",
       "1783. 0.199666666666667\n",
       "1784. 0.122322222222222\n",
       "1785. 0.0172\n",
       "1786. 0.0364\n",
       "1787. 0.161266666666667\n",
       "1788. 0.183\n",
       "1789. 0.726366666666667\n",
       "1790. 0.0518666666666667\n",
       "1791. 0.147133333333333\n",
       "1792. 0.0366666666666667\n",
       "1793. 0.6226\n",
       "1794. 0.0444888888888889\n",
       "1795. 0.5482\n",
       "1796. 0.116764102564103\n",
       "1797. 0.0766666666666667\n",
       "1798. 0.171933333333333\n",
       "1799. 0.7975\n",
       "1800. 0.560733333333333\n",
       "1801. 0.552533333333333\n",
       "1802. 0.595181818181818\n",
       "1803. 0.0329333333333333\n",
       "1804. 0.463231746031746\n",
       "1805. 0.526544444444444\n",
       "1806. 0.0649\n",
       "1807. 0.189133333333333\n",
       "1808. 0.802066666666667\n",
       "1809. 0.004\n",
       "1810. 0.1826\n",
       "1811. 0.4496\n",
       "1812. 0.0616\n",
       "1813. 0.3174\n",
       "1814. 0.117583333333333\n",
       "1815. 0.429733333333333\n",
       "1816. 0.748133333333333\n",
       "1817. 0.0422666666666667\n",
       "1818. 0.675266666666667\n",
       "1819. 0.0390666666666667\n",
       "1820. 0.718111111111111\n",
       "1821. 0.240466666666667\n",
       "1822. 0.484666666666667\n",
       "1823. 0.662090476190476\n",
       "1824. 0.0212888888888889\n",
       "1825. 0.0298\n",
       "1826. 0.0171333333333333\n",
       "1827. 0.813742857142857\n",
       "1828. 0.7118\n",
       "1829. 0.0650666666666667\n",
       "1830. 0.004\n",
       "1831. 0.0216\n",
       "1832. 0.0506\n",
       "1833. 0.0330666666666667\n",
       "1834. 0.0544\n",
       "1835. 0.088\n",
       "1836. 0.684933333333334\n",
       "1837. 0.1164\n",
       "1838. 0.012\n",
       "1839. 0.621771428571429\n",
       "1840. 0.5336\n",
       "1841. 0.278133333333333\n",
       "1842. 0.0402666666666667\n",
       "1843. 0.722488888888889\n",
       "1844. 0.0316666666666667\n",
       "1845. 0.2226\n",
       "1846. 0.0799333333333333\n",
       "1847. 0.156466666666667\n",
       "1848. 0.00773333333333333\n",
       "1849. 0.370466666666667\n",
       "1850. 0.583435897435898\n",
       "1851. 0.213533333333333\n",
       "1852. 0.366866666666667\n",
       "1853. 0\n",
       "1854. 0.561095238095238\n",
       "1855. 0.0978666666666667\n",
       "1856. 0.00133333333333333\n",
       "1857. 0.411251082251082\n",
       "1858. 0.0496\n",
       "1859. 0.626\n",
       "1860. 0.565902564102564\n",
       "1861. 0.337066666666667\n",
       "1862. 0.1694\n",
       "1863. 0.7704\n",
       "1864. 0.155\n",
       "1865. 0.117933333333333\n",
       "1866. 0.4018\n",
       "1867. 0.0769333333333333\n",
       "1868. 0.3683\n",
       "1869. 0.734733333333333\n",
       "1870. 0.274733333333333\n",
       "1871. 0.0583333333333333\n",
       "1872. 0.419\n",
       "1873. 0.0613333333333333\n",
       "1874. 0.333933333333333\n",
       "1875. 0.0032\n",
       "1876. 0.571533333333333\n",
       "1877. 0.506977777777778\n",
       "1878. 0.564933333333333\n",
       "1879. 0.0808222222222222\n",
       "1880. 0.0105333333333333\n",
       "1881. 0.0366\n",
       "1882. 0.807666666666667\n",
       "1883. 0.0358\n",
       "1884. 0.589\n",
       "1885. 0.113133333333333\n",
       "1886. 0.0211333333333333\n",
       "1887. 0.2186\n",
       "1888. 0.0182\n",
       "1889. 0.434933333333333\n",
       "1890. 0.0454\n",
       "1891. 0.725533333333333\n",
       "1892. 0.551733333333333\n",
       "1893. 0.6766\n",
       "1894. 0.041\n",
       "1895. 0.0293333333333333\n",
       "1896. 0.273466666666667\n",
       "1897. 0.00866666666666667\n",
       "1898. 0.6762\n",
       "1899. 0.0292\n",
       "1900. 0.742166666666667\n",
       "1901. 0.716266666666667\n",
       "1902. 0.620733333333333\n",
       "1903. 0.2784\n",
       "1904. 0.222333333333333\n",
       "1905. 0.393704761904762\n",
       "1906. 0.233933333333333\n",
       "1907. 0.471466666666667\n",
       "1908. 0.007\n",
       "1909. 0.0028\n",
       "1910. 0.898866666666667\n",
       "1911. 0.2916\n",
       "1912. 0.8208\n",
       "1913. 0.575085714285714\n",
       "1914. 0.0408\n",
       "1915. 0.573971428571429\n",
       "1916. 0.0859333333333333\n",
       "1917. 0.338233333333333\n",
       "1918. 0.502666666666667\n",
       "1919. 0.303\n",
       "1920. 0.102533333333333\n",
       "1921. 0.334266666666667\n",
       "1922. 0.370533333333333\n",
       "1923. 0.0374666666666667\n",
       "1924. 0.0555333333333333\n",
       "1925. 0.64150303030303\n",
       "1926. 0.522990476190476\n",
       "1927. 0.476066666666667\n",
       "1928. 0.1566\n",
       "1929. 0.1546\n",
       "1930. 0.529235897435897\n",
       "1931. 0.369866666666667\n",
       "1932. 0.714466666666667\n",
       "1933. 0.541733333333333\n",
       "1934. 0.191466666666667\n",
       "1935. 0.413133333333333\n",
       "1936. 0.160866666666667\n",
       "1937. 0.0352\n",
       "1938. 0.786136363636364\n",
       "1939. 0.001\n",
       "1940. 0\n",
       "1941. 0.556650793650794\n",
       "1942. 0.594028571428571\n",
       "1943. 0\n",
       "1944. 0.0590476190476191\n",
       "1945. 0.249133333333333\n",
       "1946. 0.09065\n",
       "1947. 0.855157142857143\n",
       "1948. 0.0999333333333333\n",
       "1949. 0.4867\n",
       "1950. 0.0416888888888889\n",
       "1951. 0.186933333333333\n",
       "1952. 0.120466666666667\n",
       "1953. 0.8998\n",
       "1954. 0.594361904761905\n",
       "1955. 0.745733333333333\n",
       "1956. 0.0442\n",
       "1957. 0.476\n",
       "1958. 0.128666666666667\n",
       "1959. 0.1396\n",
       "1960. 0.323933333333333\n",
       "1961. 0.267533333333333\n",
       "1962. 0.5198\n",
       "1963. 0.486066666666667\n",
       "1964. 0.0272\n",
       "1965. 0.225669841269841\n",
       "1966. 0.0854\n",
       "1967. 0.215733333333333\n",
       "1968. 0.0575333333333333\n",
       "1969. 0.675066666666667\n",
       "1970. 0.896133333333333\n",
       "1971. 0.531733333333333\n",
       "1972. 0.417733333333333\n",
       "1973. 0.3298\n",
       "1974. 0.0631333333333333\n",
       "1975. 0.3867\n",
       "1976. 0.0572\n",
       "1977. 0.2354\n",
       "1978. 0.273133333333333\n",
       "1979. 0.238927272727273\n",
       "1980. 0.215266666666667\n",
       "1981. 0.0942666666666667\n",
       "1982. 0.799733333333333\n",
       "1983. 0.243248484848485\n",
       "1984. 0.195380952380952\n",
       "1985. 0.0734666666666667\n",
       "1986. 0.798961904761905\n",
       "1987. 0.358133333333333\n",
       "1988. 0.308733333333333\n",
       "1989. 0.1464\n",
       "1990. 0.410769696969697\n",
       "1991. 0.062\n",
       "1992. 0.553933333333333\n",
       "1993. 0.632577896613191\n",
       "1994. 0.6972\n",
       "1995. 0.269714285714286\n",
       "1996. 0.0414\n",
       "1997. 0.654022222222222\n",
       "1998. 0.114733333333333\n",
       "1999. 0.0762666666666667\n",
       "2000. 0.6346\n",
       "2001. 0.217533333333333\n",
       "2002. 0.3662\n",
       "2003. 0.0168\n",
       "2004. 0.107\n",
       "2005. 0.612122807017544\n",
       "2006. 0.216\n",
       "2007. 0.197666666666667\n",
       "2008. 0.3418\n",
       "2009. 0.189333333333333\n",
       "2010. 0.0416666666666667\n",
       "2011. 0.431266666666667\n",
       "2012. 0.272\n",
       "2013. 0.2884\n",
       "2014. 0.488769696969697\n",
       "2015. 0.233533333333333\n",
       "2016. 0.367133333333333\n",
       "2017. 0\n",
       "2018. 0.743638095238095\n",
       "2019. 0.2864\n",
       "2020. 0.306733333333333\n",
       "2021. 0.113\n",
       "2022. 0.57150303030303\n",
       "2023. 0.375933333333333\n",
       "2024. 0.552133333333333\n",
       "2025. 0.237466666666667\n",
       "2026. 0.2733\n",
       "2027. 0.233066666666667\n",
       "2028. 0.719622222222222\n",
       "2029. 0.534333333333333\n",
       "2030. 0.3878\n",
       "2031. 0.0426666666666667\n",
       "2032. 0.0904\n",
       "2033. 0.8228\n",
       "2034. 0.1826\n",
       "2035. 0\n",
       "2036. 0.00133333333333333\n",
       "2037. 0.309561904761905\n",
       "2038. 0.022\n",
       "2039. 0.5014\n",
       "2040. 0.433866666666667\n",
       "2041. 0.0202\n",
       "2042. 0.0277333333333333\n",
       "2043. 0.0186666666666667\n",
       "2044. 0.0926\n",
       "2045. 0.0683333333333333\n",
       "2046. 0.100892307692308\n",
       "2047. 0.2592\n",
       "2048. 0.0875555555555556\n",
       "2049. 0.457466666666667\n",
       "2050. 0.0424666666666667\n",
       "2051. 0.0392\n",
       "2052. 0.317266666666667\n",
       "2053. 0.0432\n",
       "2054. 0.139533333333333\n",
       "2055. 0.723866666666667\n",
       "2056. 0.2994\n",
       "2057. 0.582\n",
       "2058. 0.202966666666667\n",
       "2059. 0.0191333333333333\n",
       "2060. 0.04\n",
       "2061. 0.212\n",
       "2062. 0.135066666666667\n",
       "2063. 0.0194\n",
       "2064. 0.371366666666667\n",
       "2065. 0.0032\n",
       "2066. 0.624133333333333\n",
       "2067. 0.0082\n",
       "2068. 0.184171428571429\n",
       "2069. 0.324066666666667\n",
       "2070. 0.0549333333333333\n",
       "2071. 0.209444444444444\n",
       "2072. 0.0555333333333333\n",
       "2073. 0.1956\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   [1] 0.038400000 0.206466667 0.441133333 0.048400000 0.568552381 0.417514286\n",
       "   [7] 0.434300000 0.526503030 0.067583333 0.005333333 0.218466667 0.420156140\n",
       "  [13] 0.029088889 0.860369697 0.102800000 0.087200000 0.020333333 0.139133333\n",
       "  [19] 0.255266667 0.022600000 0.087600000 0.397933333 0.860800000 0.008800000\n",
       "  [25] 0.637076190 0.076666667 0.486103030 0.502400000 0.589111111 0.239800000\n",
       "  [31] 0.513533333 0.848342857 0.022333333 0.470066667 0.627866667 0.270500000\n",
       "  [37] 0.639200000 0.613103030 0.808066667 0.021000000 0.169933333 0.225533333\n",
       "  [43] 0.110066667 0.698200000 0.184533333 0.739533333 0.194266667 0.019400000\n",
       "  [49] 0.360400000 0.125800000 0.044266667 0.013000000 0.183522222 0.345933333\n",
       "  [55] 0.737066667 0.217266667 0.696066667 0.608633333 0.300047619 0.505533333\n",
       "  [61] 0.762161905 0.018400000 0.233866667 0.174866667 0.067733333 0.064266667\n",
       "  [67] 0.012200000 0.020200000 0.530133333 0.161866667 0.464266667 0.017333333\n",
       "  [73] 0.014666667 0.592533333 0.217114286 0.206933333 0.702133333 0.040400000\n",
       "  [79] 0.472933333 0.141466667 0.828000000 0.251333333 0.005000000 0.544066667\n",
       "  [85] 0.309155556 0.237233333 0.067333333 0.073666667 0.666933333 0.319200000\n",
       "  [91] 0.010000000 0.081266667 0.117000000 0.761942857 0.134533333 0.442133333\n",
       "  [97] 0.144688889 0.161780952 0.406633333 0.478266667 0.617744419 0.051283333\n",
       " [103] 0.640133333 0.359933333 0.357688889 0.435200000 0.019000000 0.222800000\n",
       " [109] 0.136400000 0.046066667 0.068200000 0.808504762 0.139000000 0.701942857\n",
       " [115] 0.167466667 0.013666667 0.008666667 0.410533333 0.034333333 0.025666667\n",
       " [121] 0.033288889 0.033133333 0.028000000 0.096866667 0.148266667 0.013333333\n",
       " [127] 0.002400000 0.340619048 0.764142857 0.047266667 0.641600000 0.469333333\n",
       " [133] 0.315666667 0.417400000 0.515495238 0.659466667 0.075600000 0.767434921\n",
       " [139] 0.147866667 0.697466667 0.434333333 0.233622222 0.253000000 0.476933333\n",
       " [145] 0.407333333 0.149266667 0.639822222 0.192733333 0.211888889 0.192600000\n",
       " [151] 0.054222222 0.387866667 0.206000000 0.168800000 0.030133333 0.111200000\n",
       " [157] 0.029333333 0.528615385 0.357800000 0.918266667 0.130200000 0.170200000\n",
       " [163] 0.048600000 0.116066667 0.012857143 0.663588278 0.073933333 0.531066667\n",
       " [169] 0.034000000 0.295866667 0.646533333 0.302966667 0.394733333 0.028800000\n",
       " [175] 0.418847619 0.007000000 0.234266667 0.325066667 0.668761905 0.849933333\n",
       " [181] 0.836266667 0.903000000 0.491688889 0.000000000 0.860288889 0.034266667\n",
       " [187] 0.006800000 0.203533333 0.252933333 0.366000000 0.006000000 0.455300000\n",
       " [193] 0.601504762 0.195400000 0.008000000 0.007600000 0.085666667 0.041933333\n",
       " [199] 0.035066667 0.751809524 0.486000000 0.166133333 0.170533333 0.561133333\n",
       " [205] 0.284969841 0.628200000 0.033066667 0.889700000 0.076266667 0.829066667\n",
       " [211] 0.054066667 0.000000000 0.087933333 0.515066667 0.135716667 0.734800000\n",
       " [217] 0.134466667 0.057733333 0.008000000 0.133133333 0.039866667 0.018133333\n",
       " [223] 0.386155556 0.673600000 0.067666667 0.172380952 0.406447619 0.625933333\n",
       " [229] 0.244466667 0.002000000 0.806209524 0.040333333 0.432918045 0.057533333\n",
       " [235] 0.017666667 0.055400000 0.322955556 0.148266667 0.588833333 0.732100000\n",
       " [241] 0.033200000 0.000000000 0.045466667 0.128733333 0.061533333 0.025933333\n",
       " [247] 0.293800000 0.604466667 0.293180952 0.000000000 0.054200000 0.624761905\n",
       " [253] 0.748666667 0.377676190 0.069066667 0.490733333 0.868400000 0.203859740\n",
       " [259] 0.126200000 0.132200000 0.446662745 0.271530952 0.033333333 0.343066667\n",
       " [265] 0.583704274 0.358555556 0.038800000 0.145866667 0.058800000 0.130066667\n",
       " [271] 0.378488889 0.136696104 0.349577778 0.201533333 0.078000000 0.234800000\n",
       " [277] 0.628266667 0.037800000 0.275204040 0.120200000 0.382000000 0.101733333\n",
       " [283] 0.304200000 0.105533333 0.618019048 0.337066667 0.087766667 0.456895238\n",
       " [289] 0.695733333 0.526000000 0.424315385 0.119133333 0.375266667 0.514166667\n",
       " [295] 0.348466667 0.835866667 0.732400000 0.148449817 0.054466667 0.047666667\n",
       " [301] 0.848677778 0.040800000 0.018533333 0.000000000 0.051600000 0.033400000\n",
       " [307] 0.102533333 0.180333333 0.041700000 0.018800000 0.119733333 0.586066667\n",
       " [313] 0.670866667 0.056400000 0.030400000 0.698671525 0.261066667 0.122200000\n",
       " [319] 0.562800000 0.625066667 0.020000000 0.173066667 0.031400000 0.308066667\n",
       " [325] 0.270000000 0.579961905 0.505066667 0.462357143 0.012133333 0.390695238\n",
       " [331] 0.464142857 0.250733333 0.448133333 0.597842424 0.792010256 0.198466667\n",
       " [337] 0.543266667 0.503200000 0.038133333 0.162000000 0.072733333 0.090666667\n",
       " [343] 0.042666667 0.247676190 0.574945455 0.012800000 0.252133333 0.266866667\n",
       " [349] 0.031200000 0.028800000 0.043022222 0.015000000 0.169400000 0.633933333\n",
       " [355] 0.123266667 0.388351634 0.634293506 0.081866667 0.135200000 0.328244444\n",
       " [361] 0.073200000 0.024466667 0.136980952 0.110466667 0.417266667 0.323866667\n",
       " [367] 0.670266667 0.001000000 0.026800000 0.080022222 0.711569697 0.131200000\n",
       " [373] 0.021000000 0.088400000 0.113266667 0.734333333 0.175266667 0.073466667\n",
       " [379] 0.011600000 0.293333333 0.277933333 0.570400000 0.034000000 0.779533333\n",
       " [385] 0.158650000 0.068333333 0.017600000 0.352333333 0.081800000 0.504000000\n",
       " [391] 0.004000000 0.121866667 0.676104762 0.118533333 0.091200000 0.419500000\n",
       " [397] 0.054800000 0.061288889 0.020000000 0.010800000 0.133897436 0.636533333\n",
       " [403] 0.200133333 0.909266667 0.085733333 0.500466667 0.114697436 0.263482051\n",
       " [409] 0.605866667 0.051066667 0.233355556 0.034000000 0.410466667 0.023000000\n",
       " [415] 0.565200000 0.526266667 0.187066667 0.630533333 0.380333333 0.628666667\n",
       " [421] 0.347482828 0.654200000 0.008200000 0.049133333 0.204066667 0.383828571\n",
       " [427] 0.123600000 0.563369697 0.138181563 0.521300000 0.413933333 0.080800000\n",
       " [433] 0.376266667 0.369533333 0.278047619 0.216500000 0.010600000 0.243600000\n",
       " [439] 0.767302564 0.411433333 0.115400000 0.120848485 0.321133333 0.825571429\n",
       " [445] 0.210459829 0.346850000 0.007200000 0.014400000 0.643533333 0.045333333\n",
       " [451] 0.555733333 0.264333333 0.060100000 0.720628571 0.226914286 0.002400000\n",
       " [457] 0.443866667 0.386577778 0.683733333 0.692666667 0.096533333 0.090600000\n",
       " [463] 0.184161905 0.825685714 0.032333333 0.581733333 0.170633333 0.064000000\n",
       " [469] 0.041866667 0.688161905 0.348422222 0.690333333 0.104523810 0.091400000\n",
       " [475] 0.556866667 0.018800000 0.271466667 0.539022222 0.273266667 0.014133333\n",
       " [481] 0.000000000 0.121447619 0.771266667 0.137200000 0.013200000 0.127381818\n",
       " [487] 0.593633333 0.020200000 0.338142857 0.066133333 0.598933333 0.421800000\n",
       " [493] 0.580360606 0.681633333 0.134822222 0.013866667 0.726485714 0.018000000\n",
       " [499] 0.799019048 0.113333333 0.152333333 0.131057143 0.018333333 0.011333333\n",
       " [505] 0.159533333 0.013000000 0.405700000 0.615422222 0.103600000 0.049866667\n",
       " [511] 0.441333333 0.455933333 0.019733333 0.013200000 0.044400000 0.347133333\n",
       " [517] 0.044028571 0.046600000 0.539733333 0.489533333 0.008000000 0.031200000\n",
       " [523] 0.483742857 0.012000000 0.256400000 0.349733333 0.048133333 0.561133333\n",
       " [529] 0.134000000 0.014066667 0.257000000 0.025800000 0.624533333 0.587936364\n",
       " [535] 0.487933333 0.008000000 0.875275758 0.689600000 0.253800000 0.245600000\n",
       " [541] 0.131533333 0.801400000 0.312203175 0.165466667 0.150733333 0.069650000\n",
       " [547] 0.666750000 0.776466667 0.011333333 0.599000000 0.004000000 0.061400000\n",
       " [553] 0.077866667 0.804355556 0.348933333 0.609533333 0.083000000 0.048000000\n",
       " [559] 0.078133333 0.329244444 0.031866667 0.093933333 0.058666667 0.161400000\n",
       " [565] 0.285088889 0.008000000 0.276266667 0.031400000 0.656966667 0.444538095\n",
       " [571] 0.024666667 0.250285714 0.435933333 0.311333333 0.443466667 0.498166667\n",
       " [577] 0.500600000 0.190866667 0.104266667 0.027866667 0.016266667 0.006800000\n",
       " [583] 0.384666667 0.430755556 0.273000000 0.433833333 0.038200000 0.041600000\n",
       " [589] 0.283066667 0.041000000 0.397533333 0.205800000 0.048800000 0.005333333\n",
       " [595] 0.028400000 0.577218301 0.028800000 0.049711111 0.027600000 0.506448485\n",
       " [601] 0.043000000 0.313866667 0.634866667 0.064066667 0.088180952 0.319923810\n",
       " [607] 0.772028571 0.089333333 0.062733333 0.089933333 0.255775758 0.461455556\n",
       " [613] 0.632056140 0.137400000 0.071200000 0.477400000 0.030600000 0.325066667\n",
       " [619] 0.206066667 0.918333333 0.256700000 0.526800000 0.823995238 0.539933333\n",
       " [625] 0.438244444 0.273933333 0.106666667 0.016400000 0.460666667 0.095933333\n",
       " [631] 0.147800000 0.088666667 0.585866667 0.326400000 0.507666667 0.024266667\n",
       " [637] 0.385169697 0.190866667 0.022933333 0.219600000 0.335733333 0.114066667\n",
       " [643] 0.009866667 0.142400000 0.107466667 0.028066667 0.119333333 0.036133333\n",
       " [649] 0.601400000 0.363236364 0.166103030 0.059333333 0.565169697 0.370374892\n",
       " [655] 0.337822222 0.178266667 0.753295238 0.609046620 0.579828571 0.223400000\n",
       " [661] 0.116000000 0.025800000 0.055533333 0.553085714 0.242866667 0.139200000\n",
       " [667] 0.655604762 0.034133333 0.000000000 0.663666667 0.423333333 0.075066667\n",
       " [673] 0.004000000 0.731727273 0.495666667 0.489209524 0.024000000 0.449800000\n",
       " [679] 0.532933333 0.497600000 0.043400000 0.808000000 0.098000000 0.295466667\n",
       " [685] 0.082822222 0.691000000 0.368177778 0.415733333 0.172666667 0.246600000\n",
       " [691] 0.264514286 0.142000000 0.613666667 0.386000000 0.054800000 0.303000000\n",
       " [697] 0.056200000 0.049638095 0.014266667 0.223600000 0.047400000 0.251322222\n",
       " [703] 0.757666667 0.177400000 0.609200000 0.655433333 0.425533333 0.038600000\n",
       " [709] 0.389800000 0.116466667 0.443888889 0.872533333 0.045733333 0.022866667\n",
       " [715] 0.250400000 0.497822222 0.562542857 0.467207326 0.772333333 0.031000000\n",
       " [721] 0.283600000 0.403466667 0.629100000 0.097533333 0.026533333 0.050666667\n",
       " [727] 0.406076190 0.478645887 0.403800000 0.041000000 0.512807937 0.646666667\n",
       " [733] 0.205996078 0.122000000 0.007000000 0.061200000 0.053866667 0.636333333\n",
       " [739] 0.103000000 0.047600000 0.003000000 0.118266667 0.725080519 0.070866667\n",
       " [745] 0.866933333 0.150866667 0.603666667 0.701045887 0.011000000 0.174533333\n",
       " [751] 0.012000000 0.072466667 0.136800000 0.203733333 0.072466667 0.782723810\n",
       " [757] 0.163684712 0.055266667 0.057000000 0.057933333 0.299923810 0.450488889\n",
       " [763] 0.091600000 0.297866667 0.903228571 0.106400000 0.000000000 0.007333333\n",
       " [769] 0.512488386 0.306400000 0.038933333 0.041866667 0.573233333 0.799533333\n",
       " [775] 0.597933333 0.121466667 0.817600000 0.461330403 0.053133333 0.457366667\n",
       " [781] 0.110600000 0.001000000 0.605761905 0.032000000 0.051666667 0.536800000\n",
       " [787] 0.438800000 0.443600000 0.205000000 0.411400000 0.009333333 0.301977778\n",
       " [793] 0.735366667 0.016000000 0.233666667 0.003200000 0.012000000 0.004800000\n",
       " [799] 0.038400000 0.358733333 0.218333333 0.010666667 0.300504762 0.141190476\n",
       " [805] 0.281314286 0.067600000 0.454466667 0.051400000 0.107400000 0.695466667\n",
       " [811] 0.395971429 0.093866667 0.012666667 0.073600000 0.441365079 0.661666667\n",
       " [817] 0.151000000 0.272800000 0.459209524 0.049800000 0.285633333 0.040600000\n",
       " [823] 0.514933333 0.564566667 0.055266667 0.016866667 0.701066667 0.019800000\n",
       " [829] 0.200466667 0.452300000 0.072133333 0.035200000 0.732733333 0.589622222\n",
       " [835] 0.654933333 0.499866667 0.159688889 0.431700000 0.059933333 0.704733333\n",
       " [841] 0.221382051 0.625200000 0.028400000 0.698333333 0.727933333 0.653600000\n",
       " [847] 0.017333333 0.188733333 0.081000000 0.374766667 0.089666667 0.247866667\n",
       " [853] 0.584533333 0.013600000 0.872133333 0.306466667 0.064733333 0.000000000\n",
       " [859] 0.002400000 0.646190476 0.017000000 0.069333333 0.199066667 0.408400000\n",
       " [865] 0.505333333 0.066422222 0.107733333 0.429400000 0.383466667 0.093066667\n",
       " [871] 0.120200000 0.002000000 0.022866667 0.285400000 0.352333333 0.393800000\n",
       " [877] 0.172266667 0.333233333 0.551666667 0.837200000 0.938933333 0.370682051\n",
       " [883] 0.636400000 0.245933333 0.064155556 0.116066667 0.070533333 0.709066667\n",
       " [889] 0.284888889 0.586828571 0.720200000 0.693866667 0.226133333 0.233066667\n",
       " [895] 0.251933333 0.162900000 0.185200000 0.370222222 0.197933333 0.173177778\n",
       " [901] 0.214066667 0.660000000 0.021977778 0.473500000 0.626419048 0.204000000\n",
       " [907] 0.166400000 0.017200000 0.389066667 0.548473993 0.039133333 0.760057143\n",
       " [913] 0.814066667 0.487933333 0.076266667 0.016000000 0.587466667 0.560866667\n",
       " [919] 0.375600000 0.068800000 0.025666667 0.574800000 0.442533333 0.724266667\n",
       " [925] 0.057866667 0.004800000 0.271638095 0.213200000 0.323833333 0.565780952\n",
       " [931] 0.946200000 0.046666667 0.123666667 0.850247619 0.271115152 0.054533333\n",
       " [937] 0.529659740 0.059066667 0.090790476 0.280133333 0.497692308 0.546333333\n",
       " [943] 0.392133333 0.007866667 0.301000000 0.039000000 0.028000000 0.919933333\n",
       " [949] 0.794533333 0.778266667 0.285262745 0.300666667 0.106200000 0.825000000\n",
       " [955] 0.153876190 0.335600000 0.011000000 0.043200000 0.730666667 0.075133333\n",
       " [961] 0.026000000 0.195266667 0.000000000 0.088783333 0.066733333 0.475703030\n",
       " [967] 0.169866667 0.060000000 0.518000000 0.749466667 0.062666667 0.299266667\n",
       " [973] 0.643022222 0.504666667 0.265859740 0.064066667 0.771028571 0.370133333\n",
       " [979] 0.226057143 0.704866667 0.912266667 0.602590476 0.487400000 0.553409524\n",
       " [985] 0.628009524 0.440129412 0.348755556 0.475066667 0.472929412 0.004000000\n",
       " [991] 0.102200000 0.020533333 0.062533333 0.262695238 0.089933333 0.759800000\n",
       " [997] 0.400888889 0.052400000 0.025800000 0.570848718 0.044000000 0.300703030\n",
       "[1003] 0.413133333 0.396400000 0.349533333 0.794466667 0.700200000 0.055866667\n",
       "[1009] 0.003200000 0.089066667 0.420000000 0.285466667 0.091822222 0.523000000\n",
       "[1015] 0.111800000 0.766466667 0.285666667 0.286400000 0.250933333 0.754933333\n",
       "[1021] 0.192283333 0.008000000 0.044400000 0.006000000 0.001000000 0.168685714\n",
       "[1027] 0.367066667 0.389066667 0.172400000 0.000000000 0.166433333 0.628547009\n",
       "[1033] 0.643000000 0.489533333 0.444104762 0.068866667 0.128133333 0.442600000\n",
       "[1039] 0.036000000 0.096333333 0.794666667 0.661587546 0.834228571 0.052200000\n",
       "[1045] 0.046288889 0.082700000 0.272847619 0.102200000 0.861752381 0.335266667\n",
       "[1051] 0.405333333 0.072000000 0.432569697 0.017133333 0.061466667 0.444593939\n",
       "[1057] 0.004000000 0.030933333 0.380933333 0.022133333 0.001600000 0.097666667\n",
       "[1063] 0.408523810 0.231711111 0.413133333 0.187733333 0.453533333 0.002000000\n",
       "[1069] 0.622400000 0.759333333 0.360155556 0.624666667 0.594329412 0.176866667\n",
       "[1075] 0.147066667 0.137333333 0.252866667 0.011200000 0.448222222 0.671428571\n",
       "[1081] 0.624266667 0.036888889 0.054666667 0.560533333 0.617983333 0.019200000\n",
       "[1087] 0.387600000 0.416466667 0.926569697 0.223266667 0.706000000 0.729228571\n",
       "[1093] 0.828666667 0.038133333 0.171733333 0.176400000 0.400133333 0.719561905\n",
       "[1099] 0.033733333 0.920295238 0.146266667 0.012000000 0.183733333 0.652488889\n",
       "[1105] 0.254822222 0.228866667 0.598000000 0.181400000 0.066800000 0.576819048\n",
       "[1111] 0.008000000 0.172200000 0.182533333 0.286517460 0.510742857 0.505200000\n",
       "[1117] 0.625285714 0.079400000 0.774066667 0.022800000 0.328400000 0.222448485\n",
       "[1123] 0.059022222 0.914428571 0.595644444 0.250581818 0.006400000 0.013600000\n",
       "[1129] 0.041800000 0.137133333 0.778466667 0.655733333 0.133333333 0.156200000\n",
       "[1135] 0.007800000 0.195866667 0.176733333 0.056733333 0.440333333 0.022000000\n",
       "[1141] 0.649622222 0.003800000 0.696628571 0.075866667 0.007400000 0.557900000\n",
       "[1147] 0.829057143 0.303266667 0.086200000 0.062100000 0.051966667 0.413433333\n",
       "[1153] 0.083933333 0.387733333 0.132000000 0.012533333 0.153733333 0.673903030\n",
       "[1159] 0.120400000 0.745666667 0.811133333 0.250022222 0.705933333 0.307133333\n",
       "[1165] 0.614614286 0.693015385 0.104066667 0.595400000 0.342333333 0.225533333\n",
       "[1171] 0.136781818 0.516022222 0.525295238 0.173066667 0.313400000 0.019666667\n",
       "[1177] 0.349266667 0.272400000 0.077400000 0.440200000 0.045466667 0.524000000\n",
       "[1183] 0.008000000 0.478000000 0.099600000 0.304595238 0.107533333 0.034566667\n",
       "[1189] 0.008000000 0.102123810 0.311766667 0.670957143 0.271866667 0.041866667\n",
       "[1195] 0.018000000 0.106933333 0.507866667 0.110350000 0.149533333 0.271800000\n",
       "[1201] 0.400766667 0.119457143 0.360066667 0.079200000 0.156000000 0.039200000\n",
       "[1207] 0.390866667 0.055066667 0.074266667 0.119400000 0.079866667 0.596466667\n",
       "[1213] 0.042933333 0.009200000 0.828761905 0.561076190 0.679266667 0.154561905\n",
       "[1219] 0.153333333 0.154933333 0.504533333 0.002666667 0.056047619 0.396115385\n",
       "[1225] 0.031466667 0.042000000 0.553933333 0.508600000 0.264077778 0.072266667\n",
       "[1231] 0.076266667 0.550266667 0.624844444 0.049600000 0.115266667 0.225866667\n",
       "[1237] 0.334519048 0.796466667 0.769295238 0.486733333 0.262533333 0.012466667\n",
       "[1243] 0.016000000 0.695504762 0.428666667 0.042800000 0.612333333 0.019666667\n",
       "[1249] 0.199333333 0.359133333 0.494266667 0.889266667 0.336600000 0.351600000\n",
       "[1255] 0.197800000 0.833600000 0.038266667 0.039222222 0.436266667 0.054200000\n",
       "[1261] 0.319441558 0.556066667 0.008666667 0.038866667 0.557866667 0.047866667\n",
       "[1267] 0.217300000 0.495161905 0.109866667 0.515209524 0.040400000 0.144333333\n",
       "[1273] 0.521333333 0.089266667 0.351333333 0.484444444 0.336733333 0.403933333\n",
       "[1279] 0.029666667 0.075533333 0.041933333 0.596600000 0.028933333 0.040266667\n",
       "[1285] 0.047600000 0.004000000 0.503466667 0.032266667 0.721088889 0.127000000\n",
       "[1291] 0.256133333 0.827266667 0.438266667 0.688466667 0.067933333 0.601400000\n",
       "[1297] 0.734095238 0.235933333 0.032622222 0.329733333 0.537747186 0.000000000\n",
       "[1303] 0.371533333 0.693828571 0.274700000 0.632503030 0.012000000 0.040200000\n",
       "[1309] 0.749171429 0.007200000 0.159180952 0.147400000 0.173066667 0.009857143\n",
       "[1315] 0.527400000 0.545333333 0.002800000 0.188600000 0.582933333 0.697180952\n",
       "[1321] 0.716755556 0.267933333 0.362555556 0.297444444 0.032800000 0.495800000\n",
       "[1327] 0.203800000 0.010000000 0.253533333 0.066000000 0.184247619 0.210133333\n",
       "[1333] 0.685266667 0.405707937 0.188933333 0.535800000 0.367000000 0.026066667\n",
       "[1339] 0.004000000 0.121333333 0.081533333 0.436923810 0.004000000 0.210066667\n",
       "[1345] 0.558600000 0.372333333 0.002000000 0.349466667 0.017000000 0.143000000\n",
       "[1351] 0.164000000 0.028133333 0.037066667 0.320400000 0.052600000 0.712633333\n",
       "[1357] 0.634276190 0.356866667 0.294866667 0.292000000 0.008000000 0.665800000\n",
       "[1363] 0.453733333 0.461790476 0.475933333 0.335200000 0.008000000 0.312333333\n",
       "[1369] 0.238066667 0.082933333 0.355333333 0.283533333 0.723009524 0.191133333\n",
       "[1375] 0.053466667 0.081066667 0.264866667 0.008800000 0.830419048 0.293333333\n",
       "[1381] 0.596066667 0.323400000 0.495485714 0.615000000 0.167866667 0.370000000\n",
       "[1387] 0.474000000 0.448133333 0.670273016 0.416422222 0.118200000 0.057933333\n",
       "[1393] 0.021333333 0.029000000 0.214266667 0.404566667 0.172866667 0.345866667\n",
       "[1399] 0.068533333 0.145316667 0.061800000 0.231866667 0.473600000 0.024800000\n",
       "[1405] 0.478388745 0.820000000 0.070866667 0.489466667 0.146457143 0.227647619\n",
       "[1411] 0.207466667 0.618433333 0.000000000 0.773333333 0.231200000 0.055333333\n",
       "[1417] 0.574048352 0.705933333 0.069222222 0.005400000 0.289961905 0.413000000\n",
       "[1423] 0.748666667 0.425133333 0.373300000 0.539466667 0.485798268 0.380400000\n",
       "[1429] 0.012000000 0.752333333 0.018400000 0.425377897 0.014800000 0.710400000\n",
       "[1435] 0.088000000 0.302600000 0.347222222 0.010933333 0.400666667 0.418333333\n",
       "[1441] 0.152133333 0.043866667 0.819723810 0.142888889 0.474466667 0.659342857\n",
       "[1447] 0.042333333 0.220155556 0.537133333 0.072466667 0.054800000 0.253619048\n",
       "[1453] 0.467555556 0.216666667 0.707533333 0.342400000 0.297733333 0.023800000\n",
       "[1459] 0.430600000 0.454533333 0.004000000 0.599447619 0.380133333 0.713369697\n",
       "[1465] 0.073133333 0.795411111 0.134133333 0.031800000 0.000000000 0.752666667\n",
       "[1471] 0.482136364 0.000000000 0.095923810 0.004000000 0.079900000 0.208888889\n",
       "[1477] 0.498036364 0.400695238 0.151000000 0.824735714 0.047400000 0.223133333\n",
       "[1483] 0.868619048 0.002000000 0.275000000 0.477333333 0.408066667 0.285047619\n",
       "[1489] 0.260666667 0.491200000 0.715148718 0.786761905 0.412400000 0.426800000\n",
       "[1495] 0.212400000 0.125781818 0.000000000 0.019400000 0.319933333 0.759723810\n",
       "[1501] 0.332003175 0.217515152 0.087533333 0.466933333 0.043600000 0.028933333\n",
       "[1507] 0.519133333 0.818600000 0.746836364 0.440266667 0.321580952 0.000800000\n",
       "[1513] 0.125266667 0.076800000 0.852466667 0.010733333 0.115533333 0.366866667\n",
       "[1519] 0.561233333 0.062400000 0.010400000 0.297333333 0.482800000 0.841123810\n",
       "[1525] 0.475400000 0.038200000 0.335400000 0.399733333 0.826319048 0.074200000\n",
       "[1531] 0.288000000 0.626903030 0.278866667 0.213000000 0.570133333 0.027200000\n",
       "[1537] 0.169400000 0.784000000 0.144733333 0.003333333 0.369866667 0.125133333\n",
       "[1543] 0.051000000 0.563400000 0.177288889 0.322933333 0.021333333 0.341600000\n",
       "[1549] 0.000800000 0.044088889 0.059466667 0.111000000 0.065200000 0.026866667\n",
       "[1555] 0.104266667 0.120133333 0.291800000 0.020466667 0.080800000 0.081300000\n",
       "[1561] 0.013333333 0.007933333 0.438200000 0.767866667 0.369723810 0.097200000\n",
       "[1567] 0.061800000 0.555555556 0.259866667 0.198866667 0.646038095 0.709961905\n",
       "[1573] 0.122266667 0.543800000 0.082100000 0.017000000 0.748333333 0.224127273\n",
       "[1579] 0.492333333 0.325993074 0.277933333 0.194600000 0.044333333 0.513866667\n",
       "[1585] 0.091066667 0.060600000 0.299600000 0.510400000 0.449333333 0.145450000\n",
       "[1591] 0.660952381 0.346266667 0.294390476 0.106133333 0.107000000 0.434460606\n",
       "[1597] 0.090333333 0.000000000 0.471066667 0.372733333 0.256866667 0.055200000\n",
       "[1603] 0.073800000 0.505035897 0.305247619 0.738333333 0.051333333 0.553333333\n",
       "[1609] 0.065933333 0.769800000 0.016000000 0.030666667 0.360283550 0.674695238\n",
       "[1615] 0.822761905 0.000000000 0.057564103 0.062133333 0.308916667 0.428803030\n",
       "[1621] 0.023400000 0.896333333 0.812000000 0.642666667 0.352733333 0.677000000\n",
       "[1627] 0.169400000 0.610333333 0.272800000 0.141800000 0.201400000 0.419066667\n",
       "[1633] 0.197733333 0.108627778 0.358133333 0.534000000 0.004000000 0.123855556\n",
       "[1639] 0.003200000 0.539333333 0.452959596 0.077800000 0.343466667 0.057933333\n",
       "[1645] 0.201133333 0.383200000 0.029200000 0.344433333 0.080933333 0.016266667\n",
       "[1651] 0.636600000 0.032800000 0.057333333 0.106200000 0.475400000 0.342266667\n",
       "[1657] 0.001333333 0.430622222 0.592933333 0.003200000 0.473666667 0.181266667\n",
       "[1663] 0.033733333 0.332266667 0.246766667 0.191866667 0.011644444 0.365933333\n",
       "[1669] 0.130200000 0.278688889 0.321666667 0.017200000 0.063266667 0.343733333\n",
       "[1675] 0.379435897 0.862000000 0.031333333 0.194733333 0.017600000 0.517302564\n",
       "[1681] 0.224600000 0.204333333 0.494355556 0.241800000 0.289390476 0.064497436\n",
       "[1687] 0.008000000 0.281466667 0.210933333 0.023000000 0.034000000 0.363800000\n",
       "[1693] 0.624244444 0.285333333 0.008400000 0.004000000 0.499700000 0.119533333\n",
       "[1699] 0.358533333 0.214800000 0.692233333 0.101800000 0.380466667 0.112733333\n",
       "[1705] 0.247000000 0.111666667 0.086000000 0.648611111 0.653466667 0.101800000\n",
       "[1711] 0.418666667 0.183600000 0.110733333 0.012333333 0.245466667 0.181933333\n",
       "[1717] 0.576933333 0.439922807 0.552733333 0.038666667 0.294409524 0.744076923\n",
       "[1723] 0.787533333 0.117828571 0.064733333 0.078511111 0.391165368 0.103600000\n",
       "[1729] 0.052533333 0.682400000 0.102980952 0.176666667 0.444400000 0.888333333\n",
       "[1735] 0.074200000 0.049000000 0.413133333 0.027355556 0.562666667 0.596095238\n",
       "[1741] 0.730533333 0.417755556 0.157800000 0.020200000 0.062000000 0.005333333\n",
       "[1747] 0.553695238 0.567133333 0.056600000 0.382266667 0.042333333 0.679175758\n",
       "[1753] 0.362169697 0.817614286 0.024800000 0.809157143 0.449380952 0.321433333\n",
       "[1759] 0.035066667 0.326109091 0.101066667 0.184666667 0.605833333 0.180066667\n",
       "[1765] 0.002133333 0.250800000 0.230503030 0.054400000 0.049933333 0.601533333\n",
       "[1771] 0.365800000 0.146400000 0.006666667 0.396400000 0.263733333 0.614533333\n",
       "[1777] 0.401066667 0.106000000 0.525600000 0.328333333 0.681600000 0.008000000\n",
       "[1783] 0.199666667 0.122322222 0.017200000 0.036400000 0.161266667 0.183000000\n",
       "[1789] 0.726366667 0.051866667 0.147133333 0.036666667 0.622600000 0.044488889\n",
       "[1795] 0.548200000 0.116764103 0.076666667 0.171933333 0.797500000 0.560733333\n",
       "[1801] 0.552533333 0.595181818 0.032933333 0.463231746 0.526544444 0.064900000\n",
       "[1807] 0.189133333 0.802066667 0.004000000 0.182600000 0.449600000 0.061600000\n",
       "[1813] 0.317400000 0.117583333 0.429733333 0.748133333 0.042266667 0.675266667\n",
       "[1819] 0.039066667 0.718111111 0.240466667 0.484666667 0.662090476 0.021288889\n",
       "[1825] 0.029800000 0.017133333 0.813742857 0.711800000 0.065066667 0.004000000\n",
       "[1831] 0.021600000 0.050600000 0.033066667 0.054400000 0.088000000 0.684933333\n",
       "[1837] 0.116400000 0.012000000 0.621771429 0.533600000 0.278133333 0.040266667\n",
       "[1843] 0.722488889 0.031666667 0.222600000 0.079933333 0.156466667 0.007733333\n",
       "[1849] 0.370466667 0.583435897 0.213533333 0.366866667 0.000000000 0.561095238\n",
       "[1855] 0.097866667 0.001333333 0.411251082 0.049600000 0.626000000 0.565902564\n",
       "[1861] 0.337066667 0.169400000 0.770400000 0.155000000 0.117933333 0.401800000\n",
       "[1867] 0.076933333 0.368300000 0.734733333 0.274733333 0.058333333 0.419000000\n",
       "[1873] 0.061333333 0.333933333 0.003200000 0.571533333 0.506977778 0.564933333\n",
       "[1879] 0.080822222 0.010533333 0.036600000 0.807666667 0.035800000 0.589000000\n",
       "[1885] 0.113133333 0.021133333 0.218600000 0.018200000 0.434933333 0.045400000\n",
       "[1891] 0.725533333 0.551733333 0.676600000 0.041000000 0.029333333 0.273466667\n",
       "[1897] 0.008666667 0.676200000 0.029200000 0.742166667 0.716266667 0.620733333\n",
       "[1903] 0.278400000 0.222333333 0.393704762 0.233933333 0.471466667 0.007000000\n",
       "[1909] 0.002800000 0.898866667 0.291600000 0.820800000 0.575085714 0.040800000\n",
       "[1915] 0.573971429 0.085933333 0.338233333 0.502666667 0.303000000 0.102533333\n",
       "[1921] 0.334266667 0.370533333 0.037466667 0.055533333 0.641503030 0.522990476\n",
       "[1927] 0.476066667 0.156600000 0.154600000 0.529235897 0.369866667 0.714466667\n",
       "[1933] 0.541733333 0.191466667 0.413133333 0.160866667 0.035200000 0.786136364\n",
       "[1939] 0.001000000 0.000000000 0.556650794 0.594028571 0.000000000 0.059047619\n",
       "[1945] 0.249133333 0.090650000 0.855157143 0.099933333 0.486700000 0.041688889\n",
       "[1951] 0.186933333 0.120466667 0.899800000 0.594361905 0.745733333 0.044200000\n",
       "[1957] 0.476000000 0.128666667 0.139600000 0.323933333 0.267533333 0.519800000\n",
       "[1963] 0.486066667 0.027200000 0.225669841 0.085400000 0.215733333 0.057533333\n",
       "[1969] 0.675066667 0.896133333 0.531733333 0.417733333 0.329800000 0.063133333\n",
       "[1975] 0.386700000 0.057200000 0.235400000 0.273133333 0.238927273 0.215266667\n",
       "[1981] 0.094266667 0.799733333 0.243248485 0.195380952 0.073466667 0.798961905\n",
       "[1987] 0.358133333 0.308733333 0.146400000 0.410769697 0.062000000 0.553933333\n",
       "[1993] 0.632577897 0.697200000 0.269714286 0.041400000 0.654022222 0.114733333\n",
       "[1999] 0.076266667 0.634600000 0.217533333 0.366200000 0.016800000 0.107000000\n",
       "[2005] 0.612122807 0.216000000 0.197666667 0.341800000 0.189333333 0.041666667\n",
       "[2011] 0.431266667 0.272000000 0.288400000 0.488769697 0.233533333 0.367133333\n",
       "[2017] 0.000000000 0.743638095 0.286400000 0.306733333 0.113000000 0.571503030\n",
       "[2023] 0.375933333 0.552133333 0.237466667 0.273300000 0.233066667 0.719622222\n",
       "[2029] 0.534333333 0.387800000 0.042666667 0.090400000 0.822800000 0.182600000\n",
       "[2035] 0.000000000 0.001333333 0.309561905 0.022000000 0.501400000 0.433866667\n",
       "[2041] 0.020200000 0.027733333 0.018666667 0.092600000 0.068333333 0.100892308\n",
       "[2047] 0.259200000 0.087555556 0.457466667 0.042466667 0.039200000 0.317266667\n",
       "[2053] 0.043200000 0.139533333 0.723866667 0.299400000 0.582000000 0.202966667\n",
       "[2059] 0.019133333 0.040000000 0.212000000 0.135066667 0.019400000 0.371366667\n",
       "[2065] 0.003200000 0.624133333 0.008200000 0.184171429 0.324066667 0.054933333\n",
       "[2071] 0.209444444 0.055533333 0.195600000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions <- predict(RFproc, submitproc,type = \"prob\" )$b; predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Format OK\"\n",
      "$submission\n",
      "[0.0384,0.2065,0.4411,0.0484,0.5686,0.4175,0.4343,0.5265,0.0676,0.0053,0.2185,0.4202,0.0291,0.8604,0.1028,0.0872,0.0203,0.1391,0.2553,0.0226,0.0876,0.3979,0.8608,0.0088,0.6371,0.0767,0.4861,0.5024,0.5891,0.2398,0.5135,0.8483,0.0223,0.4701,0.6279,0.2705,0.6392,0.6131,0.8081,0.021,0.1699,0.2255,0.1101,0.6982,0.1845,0.7395,0.1943,0.0194,0.3604,0.1258,0.0443,0.013,0.1835,0.3459,0.7371,0.2173,0.6961,0.6086,0.3,0.5055,0.7622,0.0184,0.2339,0.1749,0.0677,0.0643,0.0122,0.0202,0.5301,0.1619,0.4643,0.0173,0.0147,0.5925,0.2171,0.2069,0.7021,0.0404,0.4729,0.1415,0.828,0.2513,0.005,0.5441,0.3092,0.2372,0.0673,0.0737,0.6669,0.3192,0.01,0.0813,0.117,0.7619,0.1345,0.4421,0.1447,0.1618,0.4066,0.4783,0.6177,0.0513,0.6401,0.3599,0.3577,0.4352,0.019,0.2228,0.1364,0.0461,0.0682,0.8085,0.139,0.7019,0.1675,0.0137,0.0087,0.4105,0.0343,0.0257,0.0333,0.0331,0.028,0.0969,0.1483,0.0133,0.0024,0.3406,0.7641,0.0473,0.6416,0.4693,0.3157,0.4174,0.5155,0.6595,0.0756,0.7674,0.1479,0.6975,0.4343,0.2336,0.253,0.4769,0.4073,0.1493,0.6398,0.1927,0.2119,0.1926,0.0542,0.3879,0.206,0.1688,0.0301,0.1112,0.0293,0.5286,0.3578,0.9183,0.1302,0.1702,0.0486,0.1161,0.0129,0.6636,0.0739,0.5311,0.034,0.2959,0.6465,0.303,0.3947,0.0288,0.4188,0.007,0.2343,0.3251,0.6688,0.8499,0.8363,0.903,0.4917,0,0.8603,0.0343,0.0068,0.2035,0.2529,0.366,0.006,0.4553,0.6015,0.1954,0.008,0.0076,0.0857,0.0419,0.0351,0.7518,0.486,0.1661,0.1705,0.5611,0.285,0.6282,0.0331,0.8897,0.0763,0.8291,0.0541,0,0.0879,0.5151,0.1357,0.7348,0.1345,0.0577,0.008,0.1331,0.0399,0.0181,0.3862,0.6736,0.0677,0.1724,0.4064,0.6259,0.2445,0.002,0.8062,0.0403,0.4329,0.0575,0.0177,0.0554,0.323,0.1483,0.5888,0.7321,0.0332,0,0.0455,0.1287,0.0615,0.0259,0.2938,0.6045,0.2932,0,0.0542,0.6248,0.7487,0.3777,0.0691,0.4907,0.8684,0.2039,0.1262,0.1322,0.4467,0.2715,0.0333,0.3431,0.5837,0.3586,0.0388,0.1459,0.0588,0.1301,0.3785,0.1367,0.3496,0.2015,0.078,0.2348,0.6283,0.0378,0.2752,0.1202,0.382,0.1017,0.3042,0.1055,0.618,0.3371,0.0878,0.4569,0.6957,0.526,0.4243,0.1191,0.3753,0.5142,0.3485,0.8359,0.7324,0.1484,0.0545,0.0477,0.8487,0.0408,0.0185,0,0.0516,0.0334,0.1025,0.1803,0.0417,0.0188,0.1197,0.5861,0.6709,0.0564,0.0304,0.6987,0.2611,0.1222,0.5628,0.6251,0.02,0.1731,0.0314,0.3081,0.27,0.58,0.5051,0.4624,0.0121,0.3907,0.4641,0.2507,0.4481,0.5978,0.792,0.1985,0.5433,0.5032,0.0381,0.162,0.0727,0.0907,0.0427,0.2477,0.5749,0.0128,0.2521,0.2669,0.0312,0.0288,0.043,0.015,0.1694,0.6339,0.1233,0.3884,0.6343,0.0819,0.1352,0.3282,0.0732,0.0245,0.137,0.1105,0.4173,0.3239,0.6703,0.001,0.0268,0.08,0.7116,0.1312,0.021,0.0884,0.1133,0.7343,0.1753,0.0735,0.0116,0.2933,0.2779,0.5704,0.034,0.7795,0.1587,0.0683,0.0176,0.3523,0.0818,0.504,0.004,0.1219,0.6761,0.1185,0.0912,0.4195,0.0548,0.0613,0.02,0.0108,0.1339,0.6365,0.2001,0.9093,0.0857,0.5005,0.1147,0.2635,0.6059,0.0511,0.2334,0.034,0.4105,0.023,0.5652,0.5263,0.1871,0.6305,0.3803,0.6287,0.3475,0.6542,0.0082,0.0491,0.2041,0.3838,0.1236,0.5634,0.1382,0.5213,0.4139,0.0808,0.3763,0.3695,0.278,0.2165,0.0106,0.2436,0.7673,0.4114,0.1154,0.1208,0.3211,0.8256,0.2105,0.3468,0.0072,0.0144,0.6435,0.0453,0.5557,0.2643,0.0601,0.7206,0.2269,0.0024,0.4439,0.3866,0.6837,0.6927,0.0965,0.0906,0.1842,0.8257,0.0323,0.5817,0.1706,0.064,0.0419,0.6882,0.3484,0.6903,0.1045,0.0914,0.5569,0.0188,0.2715,0.539,0.2733,0.0141,0,0.1214,0.7713,0.1372,0.0132,0.1274,0.5936,0.0202,0.3381,0.0661,0.5989,0.4218,0.5804,0.6816,0.1348,0.0139,0.7265,0.018,0.799,0.1133,0.1523,0.1311,0.0183,0.0113,0.1595,0.013,0.4057,0.6154,0.1036,0.0499,0.4413,0.4559,0.0197,0.0132,0.0444,0.3471,0.044,0.0466,0.5397,0.4895,0.008,0.0312,0.4837,0.012,0.2564,0.3497,0.0481,0.5611,0.134,0.0141,0.257,0.0258,0.6245,0.5879,0.4879,0.008,0.8753,0.6896,0.2538,0.2456,0.1315,0.8014,0.3122,0.1655,0.1507,0.0696,0.6667,0.7765,0.0113,0.599,0.004,0.0614,0.0779,0.8044,0.3489,0.6095,0.083,0.048,0.0781,0.3292,0.0319,0.0939,0.0587,0.1614,0.2851,0.008,0.2763,0.0314,0.657,0.4445,0.0247,0.2503,0.4359,0.3113,0.4435,0.4982,0.5006,0.1909,0.1043,0.0279,0.0163,0.0068,0.3847,0.4308,0.273,0.4338,0.0382,0.0416,0.2831,0.041,0.3975,0.2058,0.0488,0.0053,0.0284,0.5772,0.0288,0.0497,0.0276,0.5064,0.043,0.3139,0.6349,0.0641,0.0882,0.3199,0.772,0.0893,0.0627,0.0899,0.2558,0.4615,0.6321,0.1374,0.0712,0.4774,0.0306,0.3251,0.2061,0.9183,0.2567,0.5268,0.824,0.5399,0.4382,0.2739,0.1067,0.0164,0.4607,0.0959,0.1478,0.0887,0.5859,0.3264,0.5077,0.0243,0.3852,0.1909,0.0229,0.2196,0.3357,0.1141,0.0099,0.1424,0.1075,0.0281,0.1193,0.0361,0.6014,0.3632,0.1661,0.0593,0.5652,0.3704,0.3378,0.1783,0.7533,0.609,0.5798,0.2234,0.116,0.0258,0.0555,0.5531,0.2429,0.1392,0.6556,0.0341,0,0.6637,0.4233,0.0751,0.004,0.7317,0.4957,0.4892,0.024,0.4498,0.5329,0.4976,0.0434,0.808,0.098,0.2955,0.0828,0.691,0.3682,0.4157,0.1727,0.2466,0.2645,0.142,0.6137,0.386,0.0548,0.303,0.0562,0.0496,0.0143,0.2236,0.0474,0.2513,0.7577,0.1774,0.6092,0.6554,0.4255,0.0386,0.3898,0.1165,0.4439,0.8725,0.0457,0.0229,0.2504,0.4978,0.5625,0.4672,0.7723,0.031,0.2836,0.4035,0.6291,0.0975,0.0265,0.0507,0.4061,0.4786,0.4038,0.041,0.5128,0.6467,0.206,0.122,0.007,0.0612,0.0539,0.6363,0.103,0.0476,0.003,0.1183,0.7251,0.0709,0.8669,0.1509,0.6037,0.701,0.011,0.1745,0.012,0.0725,0.1368,0.2037,0.0725,0.7827,0.1637,0.0553,0.057,0.0579,0.2999,0.4505,0.0916,0.2979,0.9032,0.1064,0,0.0073,0.5125,0.3064,0.0389,0.0419,0.5732,0.7995,0.5979,0.1215,0.8176,0.4613,0.0531,0.4574,0.1106,0.001,0.6058,0.032,0.0517,0.5368,0.4388,0.4436,0.205,0.4114,0.0093,0.302,0.7354,0.016,0.2337,0.0032,0.012,0.0048,0.0384,0.3587,0.2183,0.0107,0.3005,0.1412,0.2813,0.0676,0.4545,0.0514,0.1074,0.6955,0.396,0.0939,0.0127,0.0736,0.4414,0.6617,0.151,0.2728,0.4592,0.0498,0.2856,0.0406,0.5149,0.5646,0.0553,0.0169,0.7011,0.0198,0.2005,0.4523,0.0721,0.0352,0.7327,0.5896,0.6549,0.4999,0.1597,0.4317,0.0599,0.7047,0.2214,0.6252,0.0284,0.6983,0.7279,0.6536,0.0173,0.1887,0.081,0.3748,0.0897,0.2479,0.5845,0.0136,0.8721,0.3065,0.0647,0,0.0024,0.6462,0.017,0.0693,0.1991,0.4084,0.5053,0.0664,0.1077,0.4294,0.3835,0.0931,0.1202,0.002,0.0229,0.2854,0.3523,0.3938,0.1723,0.3332,0.5517,0.8372,0.9389,0.3707,0.6364,0.2459,0.0642,0.1161,0.0705,0.7091,0.2849,0.5868,0.7202,0.6939,0.2261,0.2331,0.2519,0.1629,0.1852,0.3702,0.1979,0.1732,0.2141,0.66,0.022,0.4735,0.6264,0.204,0.1664,0.0172,0.3891,0.5485,0.0391,0.7601,0.8141,0.4879,0.0763,0.016,0.5875,0.5609,0.3756,0.0688,0.0257,0.5748,0.4425,0.7243,0.0579,0.0048,0.2716,0.2132,0.3238,0.5658,0.9462,0.0467,0.1237,0.8502,0.2711,0.0545,0.5297,0.0591,0.0908,0.2801,0.4977,0.5463,0.3921,0.0079,0.301,0.039,0.028,0.9199,0.7945,0.7783,0.2853,0.3007,0.1062,0.825,0.1539,0.3356,0.011,0.0432,0.7307,0.0751,0.026,0.1953,0,0.0888,0.0667,0.4757,0.1699,0.06,0.518,0.7495,0.0627,0.2993,0.643,0.5047,0.2659,0.0641,0.771,0.3701,0.2261,0.7049,0.9123,0.6026,0.4874,0.5534,0.628,0.4401,0.3488,0.4751,0.4729,0.004,0.1022,0.0205,0.0625,0.2627,0.0899,0.7598,0.4009,0.0524,0.0258,0.5708,0.044,0.3007,0.4131,0.3964,0.3495,0.7945,0.7002,0.0559,0.0032,0.0891,0.42,0.2855,0.0918,0.523,0.1118,0.7665,0.2857,0.2864,0.2509,0.7549,0.1923,0.008,0.0444,0.006,0.001,0.1687,0.3671,0.3891,0.1724,0,0.1664,0.6285,0.643,0.4895,0.4441,0.0689,0.1281,0.4426,0.036,0.0963,0.7947,0.6616,0.8342,0.0522,0.0463,0.0827,0.2728,0.1022,0.8618,0.3353,0.4053,0.072,0.4326,0.0171,0.0615,0.4446,0.004,0.0309,0.3809,0.0221,0.0016,0.0977,0.4085,0.2317,0.4131,0.1877,0.4535,0.002,0.6224,0.7593,0.3602,0.6247,0.5943,0.1769,0.1471,0.1373,0.2529,0.0112,0.4482,0.6714,0.6243,0.0369,0.0547,0.5605,0.618,0.0192,0.3876,0.4165,0.9266,0.2233,0.706,0.7292,0.8287,0.0381,0.1717,0.1764,0.4001,0.7196,0.0337,0.9203,0.1463,0.012,0.1837,0.6525,0.2548,0.2289,0.598,0.1814,0.0668,0.5768,0.008,0.1722,0.1825,0.2865,0.5107,0.5052,0.6253,0.0794,0.7741,0.0228,0.3284,0.2224,0.059,0.9144,0.5956,0.2506,0.0064,0.0136,0.0418,0.1371,0.7785,0.6557,0.1333,0.1562,0.0078,0.1959,0.1767,0.0567,0.4403,0.022,0.6496,0.0038,0.6966,0.0759,0.0074,0.5579,0.8291,0.3033,0.0862,0.0621,0.052,0.4134,0.0839,0.3877,0.132,0.0125,0.1537,0.6739,0.1204,0.7457,0.8111,0.25,0.7059,0.3071,0.6146,0.693,0.1041,0.5954,0.3423,0.2255,0.1368,0.516,0.5253,0.1731,0.3134,0.0197,0.3493,0.2724,0.0774,0.4402,0.0455,0.524,0.008,0.478,0.0996,0.3046,0.1075,0.0346,0.008,0.1021,0.3118,0.671,0.2719,0.0419,0.018,0.1069,0.5079,0.1104,0.1495,0.2718,0.4008,0.1195,0.3601,0.0792,0.156,0.0392,0.3909,0.0551,0.0743,0.1194,0.0799,0.5965,0.0429,0.0092,0.8288,0.5611,0.6793,0.1546,0.1533,0.1549,0.5045,0.0027,0.056,0.3961,0.0315,0.042,0.5539,0.5086,0.2641,0.0723,0.0763,0.5503,0.6248,0.0496,0.1153,0.2259,0.3345,0.7965,0.7693,0.4867,0.2625,0.0125,0.016,0.6955,0.4287,0.0428,0.6123,0.0197,0.1993,0.3591,0.4943,0.8893,0.3366,0.3516,0.1978,0.8336,0.0383,0.0392,0.4363,0.0542,0.3194,0.5561,0.0087,0.0389,0.5579,0.0479,0.2173,0.4952,0.1099,0.5152,0.0404,0.1443,0.5213,0.0893,0.3513,0.4844,0.3367,0.4039,0.0297,0.0755,0.0419,0.5966,0.0289,0.0403,0.0476,0.004,0.5035,0.0323,0.7211,0.127,0.2561,0.8273,0.4383,0.6885,0.0679,0.6014,0.7341,0.2359,0.0326,0.3297,0.5377,0,0.3715,0.6938,0.2747,0.6325,0.012,0.0402,0.7492,0.0072,0.1592,0.1474,0.1731,0.0099,0.5274,0.5453,0.0028,0.1886,0.5829,0.6972,0.7168,0.2679,0.3626,0.2974,0.0328,0.4958,0.2038,0.01,0.2535,0.066,0.1842,0.2101,0.6853,0.4057,0.1889,0.5358,0.367,0.0261,0.004,0.1213,0.0815,0.4369,0.004,0.2101,0.5586,0.3723,0.002,0.3495,0.017,0.143,0.164,0.0281,0.0371,0.3204,0.0526,0.7126,0.6343,0.3569,0.2949,0.292,0.008,0.6658,0.4537,0.4618,0.4759,0.3352,0.008,0.3123,0.2381,0.0829,0.3553,0.2835,0.723,0.1911,0.0535,0.0811,0.2649,0.0088,0.8304,0.2933,0.5961,0.3234,0.4955,0.615,0.1679,0.37,0.474,0.4481,0.6703,0.4164,0.1182,0.0579,0.0213,0.029,0.2143,0.4046,0.1729,0.3459,0.0685,0.1453,0.0618,0.2319,0.4736,0.0248,0.4784,0.82,0.0709,0.4895,0.1465,0.2276,0.2075,0.6184,0,0.7733,0.2312,0.0553,0.574,0.7059,0.0692,0.0054,0.29,0.413,0.7487,0.4251,0.3733,0.5395,0.4858,0.3804,0.012,0.7523,0.0184,0.4254,0.0148,0.7104,0.088,0.3026,0.3472,0.0109,0.4007,0.4183,0.1521,0.0439,0.8197,0.1429,0.4745,0.6593,0.0423,0.2202,0.5371,0.0725,0.0548,0.2536,0.4676,0.2167,0.7075,0.3424,0.2977,0.0238,0.4306,0.4545,0.004,0.5994,0.3801,0.7134,0.0731,0.7954,0.1341,0.0318,0,0.7527,0.4821,0,0.0959,0.004,0.0799,0.2089,0.498,0.4007,0.151,0.8247,0.0474,0.2231,0.8686,0.002,0.275,0.4773,0.4081,0.285,0.2607,0.4912,0.7151,0.7868,0.4124,0.4268,0.2124,0.1258,0,0.0194,0.3199,0.7597,0.332,0.2175,0.0875,0.4669,0.0436,0.0289,0.5191,0.8186,0.7468,0.4403,0.3216,0.0008,0.1253,0.0768,0.8525,0.0107,0.1155,0.3669,0.5612,0.0624,0.0104,0.2973,0.4828,0.8411,0.4754,0.0382,0.3354,0.3997,0.8263,0.0742,0.288,0.6269,0.2789,0.213,0.5701,0.0272,0.1694,0.784,0.1447,0.0033,0.3699,0.1251,0.051,0.5634,0.1773,0.3229,0.0213,0.3416,0.0008,0.0441,0.0595,0.111,0.0652,0.0269,0.1043,0.1201,0.2918,0.0205,0.0808,0.0813,0.0133,0.0079,0.4382,0.7679,0.3697,0.0972,0.0618,0.5556,0.2599,0.1989,0.646,0.71,0.1223,0.5438,0.0821,0.017,0.7483,0.2241,0.4923,0.326,0.2779,0.1946,0.0443,0.5139,0.0911,0.0606,0.2996,0.5104,0.4493,0.1454,0.661,0.3463,0.2944,0.1061,0.107,0.4345,0.0903,0,0.4711,0.3727,0.2569,0.0552,0.0738,0.505,0.3052,0.7383,0.0513,0.5533,0.0659,0.7698,0.016,0.0307,0.3603,0.6747,0.8228,0,0.0576,0.0621,0.3089,0.4288,0.0234,0.8963,0.812,0.6427,0.3527,0.677,0.1694,0.6103,0.2728,0.1418,0.2014,0.4191,0.1977,0.1086,0.3581,0.534,0.004,0.1239,0.0032,0.5393,0.453,0.0778,0.3435,0.0579,0.2011,0.3832,0.0292,0.3444,0.0809,0.0163,0.6366,0.0328,0.0573,0.1062,0.4754,0.3423,0.0013,0.4306,0.5929,0.0032,0.4737,0.1813,0.0337,0.3323,0.2468,0.1919,0.0116,0.3659,0.1302,0.2787,0.3217,0.0172,0.0633,0.3437,0.3794,0.862,0.0313,0.1947,0.0176,0.5173,0.2246,0.2043,0.4944,0.2418,0.2894,0.0645,0.008,0.2815,0.2109,0.023,0.034,0.3638,0.6242,0.2853,0.0084,0.004,0.4997,0.1195,0.3585,0.2148,0.6922,0.1018,0.3805,0.1127,0.247,0.1117,0.086,0.6486,0.6535,0.1018,0.4187,0.1836,0.1107,0.0123,0.2455,0.1819,0.5769,0.4399,0.5527,0.0387,0.2944,0.7441,0.7875,0.1178,0.0647,0.0785,0.3912,0.1036,0.0525,0.6824,0.103,0.1767,0.4444,0.8883,0.0742,0.049,0.4131,0.0274,0.5627,0.5961,0.7305,0.4178,0.1578,0.0202,0.062,0.0053,0.5537,0.5671,0.0566,0.3823,0.0423,0.6792,0.3622,0.8176,0.0248,0.8092,0.4494,0.3214,0.0351,0.3261,0.1011,0.1847,0.6058,0.1801,0.0021,0.2508,0.2305,0.0544,0.0499,0.6015,0.3658,0.1464,0.0067,0.3964,0.2637,0.6145,0.4011,0.106,0.5256,0.3283,0.6816,0.008,0.1997,0.1223,0.0172,0.0364,0.1613,0.183,0.7264,0.0519,0.1471,0.0367,0.6226,0.0445,0.5482,0.1168,0.0767,0.1719,0.7975,0.5607,0.5525,0.5952,0.0329,0.4632,0.5265,0.0649,0.1891,0.8021,0.004,0.1826,0.4496,0.0616,0.3174,0.1176,0.4297,0.7481,0.0423,0.6753,0.0391,0.7181,0.2405,0.4847,0.6621,0.0213,0.0298,0.0171,0.8137,0.7118,0.0651,0.004,0.0216,0.0506,0.0331,0.0544,0.088,0.6849,0.1164,0.012,0.6218,0.5336,0.2781,0.0403,0.7225,0.0317,0.2226,0.0799,0.1565,0.0077,0.3705,0.5834,0.2135,0.3669,0,0.5611,0.0979,0.0013,0.4113,0.0496,0.626,0.5659,0.3371,0.1694,0.7704,0.155,0.1179,0.4018,0.0769,0.3683,0.7347,0.2747,0.0583,0.419,0.0613,0.3339,0.0032,0.5715,0.507,0.5649,0.0808,0.0105,0.0366,0.8077,0.0358,0.589,0.1131,0.0211,0.2186,0.0182,0.4349,0.0454,0.7255,0.5517,0.6766,0.041,0.0293,0.2735,0.0087,0.6762,0.0292,0.7422,0.7163,0.6207,0.2784,0.2223,0.3937,0.2339,0.4715,0.007,0.0028,0.8989,0.2916,0.8208,0.5751,0.0408,0.574,0.0859,0.3382,0.5027,0.303,0.1025,0.3343,0.3705,0.0375,0.0555,0.6415,0.523,0.4761,0.1566,0.1546,0.5292,0.3699,0.7145,0.5417,0.1915,0.4131,0.1609,0.0352,0.7861,0.001,0,0.5567,0.594,0,0.059,0.2491,0.0906,0.8552,0.0999,0.4867,0.0417,0.1869,0.1205,0.8998,0.5944,0.7457,0.0442,0.476,0.1287,0.1396,0.3239,0.2675,0.5198,0.4861,0.0272,0.2257,0.0854,0.2157,0.0575,0.6751,0.8961,0.5317,0.4177,0.3298,0.0631,0.3867,0.0572,0.2354,0.2731,0.2389,0.2153,0.0943,0.7997,0.2432,0.1954,0.0735,0.799,0.3581,0.3087,0.1464,0.4108,0.062,0.5539,0.6326,0.6972,0.2697,0.0414,0.654,0.1147,0.0763,0.6346,0.2175,0.3662,0.0168,0.107,0.6121,0.216,0.1977,0.3418,0.1893,0.0417,0.4313,0.272,0.2884,0.4888,0.2335,0.3671,0,0.7436,0.2864,0.3067,0.113,0.5715,0.3759,0.5521,0.2375,0.2733,0.2331,0.7196,0.5343,0.3878,0.0427,0.0904,0.8228,0.1826,0,0.0013,0.3096,0.022,0.5014,0.4339,0.0202,0.0277,0.0187,0.0926,0.0683,0.1009,0.2592,0.0876,0.4575,0.0425,0.0392,0.3173,0.0432,0.1395,0.7239,0.2994,0.582,0.203,0.0191,0.04,0.212,0.1351,0.0194,0.3714,0.0032,0.6241,0.0082,0.1842,0.3241,0.0549,0.2094,0.0555,0.1956] \n",
      "\n",
      "[1] \"Successfully submitted. Below you can see the details of your submission\"\n",
      "$url\n",
      "[1] \"http://46.101.121.83/submission/558/\"\n",
      "\n",
      "$submission\n",
      "[1] \"[0.0384, 0.2065, 0.4411, 0.0484, 0.5686, 0.4175, 0.4343, 0.5265, 0.0676, 0.0053, 0.2185, 0.4202, 0.0291, 0.8604, 0.1028, 0.0872, 0.0203, 0.1391, 0.2553, 0.0226, 0.0876, 0.3979, 0.8608, 0.0088, 0.6371, 0.0767, 0.4861, 0.5024, 0.5891, 0.2398, 0.5135, 0.8483, 0.0223, 0.4701, 0.6279, 0.2705, 0.6392, 0.6131, 0.8081, 0.021, 0.1699, 0.2255, 0.1101, 0.6982, 0.1845, 0.7395, 0.1943, 0.0194, 0.3604, 0.1258, 0.0443, 0.013, 0.1835, 0.3459, 0.7371, 0.2173, 0.6961, 0.6086, 0.3, 0.5055, 0.7622, 0.0184, 0.2339, 0.1749, 0.0677, 0.0643, 0.0122, 0.0202, 0.5301, 0.1619, 0.4643, 0.0173, 0.0147, 0.5925, 0.2171, 0.2069, 0.7021, 0.0404, 0.4729, 0.1415, 0.828, 0.2513, 0.005, 0.5441, 0.3092, 0.2372, 0.0673, 0.0737, 0.6669, 0.3192, 0.01, 0.0813, 0.117, 0.7619, 0.1345, 0.4421, 0.1447, 0.1618, 0.4066, 0.4783, 0.6177, 0.0513, 0.6401, 0.3599, 0.3577, 0.4352, 0.019, 0.2228, 0.1364, 0.0461, 0.0682, 0.8085, 0.139, 0.7019, 0.1675, 0.0137, 0.0087, 0.4105, 0.0343, 0.0257, 0.0333, 0.0331, 0.028, 0.0969, 0.1483, 0.0133, 0.0024, 0.3406, 0.7641, 0.0473, 0.6416, 0.4693, 0.3157, 0.4174, 0.5155, 0.6595, 0.0756, 0.7674, 0.1479, 0.6975, 0.4343, 0.2336, 0.253, 0.4769, 0.4073, 0.1493, 0.6398, 0.1927, 0.2119, 0.1926, 0.0542, 0.3879, 0.206, 0.1688, 0.0301, 0.1112, 0.0293, 0.5286, 0.3578, 0.9183, 0.1302, 0.1702, 0.0486, 0.1161, 0.0129, 0.6636, 0.0739, 0.5311, 0.034, 0.2959, 0.6465, 0.303, 0.3947, 0.0288, 0.4188, 0.007, 0.2343, 0.3251, 0.6688, 0.8499, 0.8363, 0.903, 0.4917, 0, 0.8603, 0.0343, 0.0068, 0.2035, 0.2529, 0.366, 0.006, 0.4553, 0.6015, 0.1954, 0.008, 0.0076, 0.0857, 0.0419, 0.0351, 0.7518, 0.486, 0.1661, 0.1705, 0.5611, 0.285, 0.6282, 0.0331, 0.8897, 0.0763, 0.8291, 0.0541, 0, 0.0879, 0.5151, 0.1357, 0.7348, 0.1345, 0.0577, 0.008, 0.1331, 0.0399, 0.0181, 0.3862, 0.6736, 0.0677, 0.1724, 0.4064, 0.6259, 0.2445, 0.002, 0.8062, 0.0403, 0.4329, 0.0575, 0.0177, 0.0554, 0.323, 0.1483, 0.5888, 0.7321, 0.0332, 0, 0.0455, 0.1287, 0.0615, 0.0259, 0.2938, 0.6045, 0.2932, 0, 0.0542, 0.6248, 0.7487, 0.3777, 0.0691, 0.4907, 0.8684, 0.2039, 0.1262, 0.1322, 0.4467, 0.2715, 0.0333, 0.3431, 0.5837, 0.3586, 0.0388, 0.1459, 0.0588, 0.1301, 0.3785, 0.1367, 0.3496, 0.2015, 0.078, 0.2348, 0.6283, 0.0378, 0.2752, 0.1202, 0.382, 0.1017, 0.3042, 0.1055, 0.618, 0.3371, 0.0878, 0.4569, 0.6957, 0.526, 0.4243, 0.1191, 0.3753, 0.5142, 0.3485, 0.8359, 0.7324, 0.1484, 0.0545, 0.0477, 0.8487, 0.0408, 0.0185, 0, 0.0516, 0.0334, 0.1025, 0.1803, 0.0417, 0.0188, 0.1197, 0.5861, 0.6709, 0.0564, 0.0304, 0.6987, 0.2611, 0.1222, 0.5628, 0.6251, 0.02, 0.1731, 0.0314, 0.3081, 0.27, 0.58, 0.5051, 0.4624, 0.0121, 0.3907, 0.4641, 0.2507, 0.4481, 0.5978, 0.792, 0.1985, 0.5433, 0.5032, 0.0381, 0.162, 0.0727, 0.0907, 0.0427, 0.2477, 0.5749, 0.0128, 0.2521, 0.2669, 0.0312, 0.0288, 0.043, 0.015, 0.1694, 0.6339, 0.1233, 0.3884, 0.6343, 0.0819, 0.1352, 0.3282, 0.0732, 0.0245, 0.137, 0.1105, 0.4173, 0.3239, 0.6703, 0.001, 0.0268, 0.08, 0.7116, 0.1312, 0.021, 0.0884, 0.1133, 0.7343, 0.1753, 0.0735, 0.0116, 0.2933, 0.2779, 0.5704, 0.034, 0.7795, 0.1587, 0.0683, 0.0176, 0.3523, 0.0818, 0.504, 0.004, 0.1219, 0.6761, 0.1185, 0.0912, 0.4195, 0.0548, 0.0613, 0.02, 0.0108, 0.1339, 0.6365, 0.2001, 0.9093, 0.0857, 0.5005, 0.1147, 0.2635, 0.6059, 0.0511, 0.2334, 0.034, 0.4105, 0.023, 0.5652, 0.5263, 0.1871, 0.6305, 0.3803, 0.6287, 0.3475, 0.6542, 0.0082, 0.0491, 0.2041, 0.3838, 0.1236, 0.5634, 0.1382, 0.5213, 0.4139, 0.0808, 0.3763, 0.3695, 0.278, 0.2165, 0.0106, 0.2436, 0.7673, 0.4114, 0.1154, 0.1208, 0.3211, 0.8256, 0.2105, 0.3468, 0.0072, 0.0144, 0.6435, 0.0453, 0.5557, 0.2643, 0.0601, 0.7206, 0.2269, 0.0024, 0.4439, 0.3866, 0.6837, 0.6927, 0.0965, 0.0906, 0.1842, 0.8257, 0.0323, 0.5817, 0.1706, 0.064, 0.0419, 0.6882, 0.3484, 0.6903, 0.1045, 0.0914, 0.5569, 0.0188, 0.2715, 0.539, 0.2733, 0.0141, 0, 0.1214, 0.7713, 0.1372, 0.0132, 0.1274, 0.5936, 0.0202, 0.3381, 0.0661, 0.5989, 0.4218, 0.5804, 0.6816, 0.1348, 0.0139, 0.7265, 0.018, 0.799, 0.1133, 0.1523, 0.1311, 0.0183, 0.0113, 0.1595, 0.013, 0.4057, 0.6154, 0.1036, 0.0499, 0.4413, 0.4559, 0.0197, 0.0132, 0.0444, 0.3471, 0.044, 0.0466, 0.5397, 0.4895, 0.008, 0.0312, 0.4837, 0.012, 0.2564, 0.3497, 0.0481, 0.5611, 0.134, 0.0141, 0.257, 0.0258, 0.6245, 0.5879, 0.4879, 0.008, 0.8753, 0.6896, 0.2538, 0.2456, 0.1315, 0.8014, 0.3122, 0.1655, 0.1507, 0.0696, 0.6667, 0.7765, 0.0113, 0.599, 0.004, 0.0614, 0.0779, 0.8044, 0.3489, 0.6095, 0.083, 0.048, 0.0781, 0.3292, 0.0319, 0.0939, 0.0587, 0.1614, 0.2851, 0.008, 0.2763, 0.0314, 0.657, 0.4445, 0.0247, 0.2503, 0.4359, 0.3113, 0.4435, 0.4982, 0.5006, 0.1909, 0.1043, 0.0279, 0.0163, 0.0068, 0.3847, 0.4308, 0.273, 0.4338, 0.0382, 0.0416, 0.2831, 0.041, 0.3975, 0.2058, 0.0488, 0.0053, 0.0284, 0.5772, 0.0288, 0.0497, 0.0276, 0.5064, 0.043, 0.3139, 0.6349, 0.0641, 0.0882, 0.3199, 0.772, 0.0893, 0.0627, 0.0899, 0.2558, 0.4615, 0.6321, 0.1374, 0.0712, 0.4774, 0.0306, 0.3251, 0.2061, 0.9183, 0.2567, 0.5268, 0.824, 0.5399, 0.4382, 0.2739, 0.1067, 0.0164, 0.4607, 0.0959, 0.1478, 0.0887, 0.5859, 0.3264, 0.5077, 0.0243, 0.3852, 0.1909, 0.0229, 0.2196, 0.3357, 0.1141, 0.0099, 0.1424, 0.1075, 0.0281, 0.1193, 0.0361, 0.6014, 0.3632, 0.1661, 0.0593, 0.5652, 0.3704, 0.3378, 0.1783, 0.7533, 0.609, 0.5798, 0.2234, 0.116, 0.0258, 0.0555, 0.5531, 0.2429, 0.1392, 0.6556, 0.0341, 0, 0.6637, 0.4233, 0.0751, 0.004, 0.7317, 0.4957, 0.4892, 0.024, 0.4498, 0.5329, 0.4976, 0.0434, 0.808, 0.098, 0.2955, 0.0828, 0.691, 0.3682, 0.4157, 0.1727, 0.2466, 0.2645, 0.142, 0.6137, 0.386, 0.0548, 0.303, 0.0562, 0.0496, 0.0143, 0.2236, 0.0474, 0.2513, 0.7577, 0.1774, 0.6092, 0.6554, 0.4255, 0.0386, 0.3898, 0.1165, 0.4439, 0.8725, 0.0457, 0.0229, 0.2504, 0.4978, 0.5625, 0.4672, 0.7723, 0.031, 0.2836, 0.4035, 0.6291, 0.0975, 0.0265, 0.0507, 0.4061, 0.4786, 0.4038, 0.041, 0.5128, 0.6467, 0.206, 0.122, 0.007, 0.0612, 0.0539, 0.6363, 0.103, 0.0476, 0.003, 0.1183, 0.7251, 0.0709, 0.8669, 0.1509, 0.6037, 0.701, 0.011, 0.1745, 0.012, 0.0725, 0.1368, 0.2037, 0.0725, 0.7827, 0.1637, 0.0553, 0.057, 0.0579, 0.2999, 0.4505, 0.0916, 0.2979, 0.9032, 0.1064, 0, 0.0073, 0.5125, 0.3064, 0.0389, 0.0419, 0.5732, 0.7995, 0.5979, 0.1215, 0.8176, 0.4613, 0.0531, 0.4574, 0.1106, 0.001, 0.6058, 0.032, 0.0517, 0.5368, 0.4388, 0.4436, 0.205, 0.4114, 0.0093, 0.302, 0.7354, 0.016, 0.2337, 0.0032, 0.012, 0.0048, 0.0384, 0.3587, 0.2183, 0.0107, 0.3005, 0.1412, 0.2813, 0.0676, 0.4545, 0.0514, 0.1074, 0.6955, 0.396, 0.0939, 0.0127, 0.0736, 0.4414, 0.6617, 0.151, 0.2728, 0.4592, 0.0498, 0.2856, 0.0406, 0.5149, 0.5646, 0.0553, 0.0169, 0.7011, 0.0198, 0.2005, 0.4523, 0.0721, 0.0352, 0.7327, 0.5896, 0.6549, 0.4999, 0.1597, 0.4317, 0.0599, 0.7047, 0.2214, 0.6252, 0.0284, 0.6983, 0.7279, 0.6536, 0.0173, 0.1887, 0.081, 0.3748, 0.0897, 0.2479, 0.5845, 0.0136, 0.8721, 0.3065, 0.0647, 0, 0.0024, 0.6462, 0.017, 0.0693, 0.1991, 0.4084, 0.5053, 0.0664, 0.1077, 0.4294, 0.3835, 0.0931, 0.1202, 0.002, 0.0229, 0.2854, 0.3523, 0.3938, 0.1723, 0.3332, 0.5517, 0.8372, 0.9389, 0.3707, 0.6364, 0.2459, 0.0642, 0.1161, 0.0705, 0.7091, 0.2849, 0.5868, 0.7202, 0.6939, 0.2261, 0.2331, 0.2519, 0.1629, 0.1852, 0.3702, 0.1979, 0.1732, 0.2141, 0.66, 0.022, 0.4735, 0.6264, 0.204, 0.1664, 0.0172, 0.3891, 0.5485, 0.0391, 0.7601, 0.8141, 0.4879, 0.0763, 0.016, 0.5875, 0.5609, 0.3756, 0.0688, 0.0257, 0.5748, 0.4425, 0.7243, 0.0579, 0.0048, 0.2716, 0.2132, 0.3238, 0.5658, 0.9462, 0.0467, 0.1237, 0.8502, 0.2711, 0.0545, 0.5297, 0.0591, 0.0908, 0.2801, 0.4977, 0.5463, 0.3921, 0.0079, 0.301, 0.039, 0.028, 0.9199, 0.7945, 0.7783, 0.2853, 0.3007, 0.1062, 0.825, 0.1539, 0.3356, 0.011, 0.0432, 0.7307, 0.0751, 0.026, 0.1953, 0, 0.0888, 0.0667, 0.4757, 0.1699, 0.06, 0.518, 0.7495, 0.0627, 0.2993, 0.643, 0.5047, 0.2659, 0.0641, 0.771, 0.3701, 0.2261, 0.7049, 0.9123, 0.6026, 0.4874, 0.5534, 0.628, 0.4401, 0.3488, 0.4751, 0.4729, 0.004, 0.1022, 0.0205, 0.0625, 0.2627, 0.0899, 0.7598, 0.4009, 0.0524, 0.0258, 0.5708, 0.044, 0.3007, 0.4131, 0.3964, 0.3495, 0.7945, 0.7002, 0.0559, 0.0032, 0.0891, 0.42, 0.2855, 0.0918, 0.523, 0.1118, 0.7665, 0.2857, 0.2864, 0.2509, 0.7549, 0.1923, 0.008, 0.0444, 0.006, 0.001, 0.1687, 0.3671, 0.3891, 0.1724, 0, 0.1664, 0.6285, 0.643, 0.4895, 0.4441, 0.0689, 0.1281, 0.4426, 0.036, 0.0963, 0.7947, 0.6616, 0.8342, 0.0522, 0.0463, 0.0827, 0.2728, 0.1022, 0.8618, 0.3353, 0.4053, 0.072, 0.4326, 0.0171, 0.0615, 0.4446, 0.004, 0.0309, 0.3809, 0.0221, 0.0016, 0.0977, 0.4085, 0.2317, 0.4131, 0.1877, 0.4535, 0.002, 0.6224, 0.7593, 0.3602, 0.6247, 0.5943, 0.1769, 0.1471, 0.1373, 0.2529, 0.0112, 0.4482, 0.6714, 0.6243, 0.0369, 0.0547, 0.5605, 0.618, 0.0192, 0.3876, 0.4165, 0.9266, 0.2233, 0.706, 0.7292, 0.8287, 0.0381, 0.1717, 0.1764, 0.4001, 0.7196, 0.0337, 0.9203, 0.1463, 0.012, 0.1837, 0.6525, 0.2548, 0.2289, 0.598, 0.1814, 0.0668, 0.5768, 0.008, 0.1722, 0.1825, 0.2865, 0.5107, 0.5052, 0.6253, 0.0794, 0.7741, 0.0228, 0.3284, 0.2224, 0.059, 0.9144, 0.5956, 0.2506, 0.0064, 0.0136, 0.0418, 0.1371, 0.7785, 0.6557, 0.1333, 0.1562, 0.0078, 0.1959, 0.1767, 0.0567, 0.4403, 0.022, 0.6496, 0.0038, 0.6966, 0.0759, 0.0074, 0.5579, 0.8291, 0.3033, 0.0862, 0.0621, 0.052, 0.4134, 0.0839, 0.3877, 0.132, 0.0125, 0.1537, 0.6739, 0.1204, 0.7457, 0.8111, 0.25, 0.7059, 0.3071, 0.6146, 0.693, 0.1041, 0.5954, 0.3423, 0.2255, 0.1368, 0.516, 0.5253, 0.1731, 0.3134, 0.0197, 0.3493, 0.2724, 0.0774, 0.4402, 0.0455, 0.524, 0.008, 0.478, 0.0996, 0.3046, 0.1075, 0.0346, 0.008, 0.1021, 0.3118, 0.671, 0.2719, 0.0419, 0.018, 0.1069, 0.5079, 0.1104, 0.1495, 0.2718, 0.4008, 0.1195, 0.3601, 0.0792, 0.156, 0.0392, 0.3909, 0.0551, 0.0743, 0.1194, 0.0799, 0.5965, 0.0429, 0.0092, 0.8288, 0.5611, 0.6793, 0.1546, 0.1533, 0.1549, 0.5045, 0.0027, 0.056, 0.3961, 0.0315, 0.042, 0.5539, 0.5086, 0.2641, 0.0723, 0.0763, 0.5503, 0.6248, 0.0496, 0.1153, 0.2259, 0.3345, 0.7965, 0.7693, 0.4867, 0.2625, 0.0125, 0.016, 0.6955, 0.4287, 0.0428, 0.6123, 0.0197, 0.1993, 0.3591, 0.4943, 0.8893, 0.3366, 0.3516, 0.1978, 0.8336, 0.0383, 0.0392, 0.4363, 0.0542, 0.3194, 0.5561, 0.0087, 0.0389, 0.5579, 0.0479, 0.2173, 0.4952, 0.1099, 0.5152, 0.0404, 0.1443, 0.5213, 0.0893, 0.3513, 0.4844, 0.3367, 0.4039, 0.0297, 0.0755, 0.0419, 0.5966, 0.0289, 0.0403, 0.0476, 0.004, 0.5035, 0.0323, 0.7211, 0.127, 0.2561, 0.8273, 0.4383, 0.6885, 0.0679, 0.6014, 0.7341, 0.2359, 0.0326, 0.3297, 0.5377, 0, 0.3715, 0.6938, 0.2747, 0.6325, 0.012, 0.0402, 0.7492, 0.0072, 0.1592, 0.1474, 0.1731, 0.0099, 0.5274, 0.5453, 0.0028, 0.1886, 0.5829, 0.6972, 0.7168, 0.2679, 0.3626, 0.2974, 0.0328, 0.4958, 0.2038, 0.01, 0.2535, 0.066, 0.1842, 0.2101, 0.6853, 0.4057, 0.1889, 0.5358, 0.367, 0.0261, 0.004, 0.1213, 0.0815, 0.4369, 0.004, 0.2101, 0.5586, 0.3723, 0.002, 0.3495, 0.017, 0.143, 0.164, 0.0281, 0.0371, 0.3204, 0.0526, 0.7126, 0.6343, 0.3569, 0.2949, 0.292, 0.008, 0.6658, 0.4537, 0.4618, 0.4759, 0.3352, 0.008, 0.3123, 0.2381, 0.0829, 0.3553, 0.2835, 0.723, 0.1911, 0.0535, 0.0811, 0.2649, 0.0088, 0.8304, 0.2933, 0.5961, 0.3234, 0.4955, 0.615, 0.1679, 0.37, 0.474, 0.4481, 0.6703, 0.4164, 0.1182, 0.0579, 0.0213, 0.029, 0.2143, 0.4046, 0.1729, 0.3459, 0.0685, 0.1453, 0.0618, 0.2319, 0.4736, 0.0248, 0.4784, 0.82, 0.0709, 0.4895, 0.1465, 0.2276, 0.2075, 0.6184, 0, 0.7733, 0.2312, 0.0553, 0.574, 0.7059, 0.0692, 0.0054, 0.29, 0.413, 0.7487, 0.4251, 0.3733, 0.5395, 0.4858, 0.3804, 0.012, 0.7523, 0.0184, 0.4254, 0.0148, 0.7104, 0.088, 0.3026, 0.3472, 0.0109, 0.4007, 0.4183, 0.1521, 0.0439, 0.8197, 0.1429, 0.4745, 0.6593, 0.0423, 0.2202, 0.5371, 0.0725, 0.0548, 0.2536, 0.4676, 0.2167, 0.7075, 0.3424, 0.2977, 0.0238, 0.4306, 0.4545, 0.004, 0.5994, 0.3801, 0.7134, 0.0731, 0.7954, 0.1341, 0.0318, 0, 0.7527, 0.4821, 0, 0.0959, 0.004, 0.0799, 0.2089, 0.498, 0.4007, 0.151, 0.8247, 0.0474, 0.2231, 0.8686, 0.002, 0.275, 0.4773, 0.4081, 0.285, 0.2607, 0.4912, 0.7151, 0.7868, 0.4124, 0.4268, 0.2124, 0.1258, 0, 0.0194, 0.3199, 0.7597, 0.332, 0.2175, 0.0875, 0.4669, 0.0436, 0.0289, 0.5191, 0.8186, 0.7468, 0.4403, 0.3216, 0.0008, 0.1253, 0.0768, 0.8525, 0.0107, 0.1155, 0.3669, 0.5612, 0.0624, 0.0104, 0.2973, 0.4828, 0.8411, 0.4754, 0.0382, 0.3354, 0.3997, 0.8263, 0.0742, 0.288, 0.6269, 0.2789, 0.213, 0.5701, 0.0272, 0.1694, 0.784, 0.1447, 0.0033, 0.3699, 0.1251, 0.051, 0.5634, 0.1773, 0.3229, 0.0213, 0.3416, 0.0008, 0.0441, 0.0595, 0.111, 0.0652, 0.0269, 0.1043, 0.1201, 0.2918, 0.0205, 0.0808, 0.0813, 0.0133, 0.0079, 0.4382, 0.7679, 0.3697, 0.0972, 0.0618, 0.5556, 0.2599, 0.1989, 0.646, 0.71, 0.1223, 0.5438, 0.0821, 0.017, 0.7483, 0.2241, 0.4923, 0.326, 0.2779, 0.1946, 0.0443, 0.5139, 0.0911, 0.0606, 0.2996, 0.5104, 0.4493, 0.1454, 0.661, 0.3463, 0.2944, 0.1061, 0.107, 0.4345, 0.0903, 0, 0.4711, 0.3727, 0.2569, 0.0552, 0.0738, 0.505, 0.3052, 0.7383, 0.0513, 0.5533, 0.0659, 0.7698, 0.016, 0.0307, 0.3603, 0.6747, 0.8228, 0, 0.0576, 0.0621, 0.3089, 0.4288, 0.0234, 0.8963, 0.812, 0.6427, 0.3527, 0.677, 0.1694, 0.6103, 0.2728, 0.1418, 0.2014, 0.4191, 0.1977, 0.1086, 0.3581, 0.534, 0.004, 0.1239, 0.0032, 0.5393, 0.453, 0.0778, 0.3435, 0.0579, 0.2011, 0.3832, 0.0292, 0.3444, 0.0809, 0.0163, 0.6366, 0.0328, 0.0573, 0.1062, 0.4754, 0.3423, 0.0013, 0.4306, 0.5929, 0.0032, 0.4737, 0.1813, 0.0337, 0.3323, 0.2468, 0.1919, 0.0116, 0.3659, 0.1302, 0.2787, 0.3217, 0.0172, 0.0633, 0.3437, 0.3794, 0.862, 0.0313, 0.1947, 0.0176, 0.5173, 0.2246, 0.2043, 0.4944, 0.2418, 0.2894, 0.0645, 0.008, 0.2815, 0.2109, 0.023, 0.034, 0.3638, 0.6242, 0.2853, 0.0084, 0.004, 0.4997, 0.1195, 0.3585, 0.2148, 0.6922, 0.1018, 0.3805, 0.1127, 0.247, 0.1117, 0.086, 0.6486, 0.6535, 0.1018, 0.4187, 0.1836, 0.1107, 0.0123, 0.2455, 0.1819, 0.5769, 0.4399, 0.5527, 0.0387, 0.2944, 0.7441, 0.7875, 0.1178, 0.0647, 0.0785, 0.3912, 0.1036, 0.0525, 0.6824, 0.103, 0.1767, 0.4444, 0.8883, 0.0742, 0.049, 0.4131, 0.0274, 0.5627, 0.5961, 0.7305, 0.4178, 0.1578, 0.0202, 0.062, 0.0053, 0.5537, 0.5671, 0.0566, 0.3823, 0.0423, 0.6792, 0.3622, 0.8176, 0.0248, 0.8092, 0.4494, 0.3214, 0.0351, 0.3261, 0.1011, 0.1847, 0.6058, 0.1801, 0.0021, 0.2508, 0.2305, 0.0544, 0.0499, 0.6015, 0.3658, 0.1464, 0.0067, 0.3964, 0.2637, 0.6145, 0.4011, 0.106, 0.5256, 0.3283, 0.6816, 0.008, 0.1997, 0.1223, 0.0172, 0.0364, 0.1613, 0.183, 0.7264, 0.0519, 0.1471, 0.0367, 0.6226, 0.0445, 0.5482, 0.1168, 0.0767, 0.1719, 0.7975, 0.5607, 0.5525, 0.5952, 0.0329, 0.4632, 0.5265, 0.0649, 0.1891, 0.8021, 0.004, 0.1826, 0.4496, 0.0616, 0.3174, 0.1176, 0.4297, 0.7481, 0.0423, 0.6753, 0.0391, 0.7181, 0.2405, 0.4847, 0.6621, 0.0213, 0.0298, 0.0171, 0.8137, 0.7118, 0.0651, 0.004, 0.0216, 0.0506, 0.0331, 0.0544, 0.088, 0.6849, 0.1164, 0.012, 0.6218, 0.5336, 0.2781, 0.0403, 0.7225, 0.0317, 0.2226, 0.0799, 0.1565, 0.0077, 0.3705, 0.5834, 0.2135, 0.3669, 0, 0.5611, 0.0979, 0.0013, 0.4113, 0.0496, 0.626, 0.5659, 0.3371, 0.1694, 0.7704, 0.155, 0.1179, 0.4018, 0.0769, 0.3683, 0.7347, 0.2747, 0.0583, 0.419, 0.0613, 0.3339, 0.0032, 0.5715, 0.507, 0.5649, 0.0808, 0.0105, 0.0366, 0.8077, 0.0358, 0.589, 0.1131, 0.0211, 0.2186, 0.0182, 0.4349, 0.0454, 0.7255, 0.5517, 0.6766, 0.041, 0.0293, 0.2735, 0.0087, 0.6762, 0.0292, 0.7422, 0.7163, 0.6207, 0.2784, 0.2223, 0.3937, 0.2339, 0.4715, 0.007, 0.0028, 0.8989, 0.2916, 0.8208, 0.5751, 0.0408, 0.574, 0.0859, 0.3382, 0.5027, 0.303, 0.1025, 0.3343, 0.3705, 0.0375, 0.0555, 0.6415, 0.523, 0.4761, 0.1566, 0.1546, 0.5292, 0.3699, 0.7145, 0.5417, 0.1915, 0.4131, 0.1609, 0.0352, 0.7861, 0.001, 0, 0.5567, 0.594, 0, 0.059, 0.2491, 0.0906, 0.8552, 0.0999, 0.4867, 0.0417, 0.1869, 0.1205, 0.8998, 0.5944, 0.7457, 0.0442, 0.476, 0.1287, 0.1396, 0.3239, 0.2675, 0.5198, 0.4861, 0.0272, 0.2257, 0.0854, 0.2157, 0.0575, 0.6751, 0.8961, 0.5317, 0.4177, 0.3298, 0.0631, 0.3867, 0.0572, 0.2354, 0.2731, 0.2389, 0.2153, 0.0943, 0.7997, 0.2432, 0.1954, 0.0735, 0.799, 0.3581, 0.3087, 0.1464, 0.4108, 0.062, 0.5539, 0.6326, 0.6972, 0.2697, 0.0414, 0.654, 0.1147, 0.0763, 0.6346, 0.2175, 0.3662, 0.0168, 0.107, 0.6121, 0.216, 0.1977, 0.3418, 0.1893, 0.0417, 0.4313, 0.272, 0.2884, 0.4888, 0.2335, 0.3671, 0, 0.7436, 0.2864, 0.3067, 0.113, 0.5715, 0.3759, 0.5521, 0.2375, 0.2733, 0.2331, 0.7196, 0.5343, 0.3878, 0.0427, 0.0904, 0.8228, 0.1826, 0, 0.0013, 0.3096, 0.022, 0.5014, 0.4339, 0.0202, 0.0277, 0.0187, 0.0926, 0.0683, 0.1009, 0.2592, 0.0876, 0.4575, 0.0425, 0.0392, 0.3173, 0.0432, 0.1395, 0.7239, 0.2994, 0.582, 0.203, 0.0191, 0.04, 0.212, 0.1351, 0.0194, 0.3714, 0.0032, 0.6241, 0.0082, 0.1842, 0.3241, 0.0549, 0.2094, 0.0555, 0.1956]\"\n",
      "\n",
      "$user\n",
      "$user$url\n",
      "[1] \"http://46.101.121.83/group/11/\"\n",
      "\n",
      "$user$username\n",
      "[1] \"Los Galacticos\"\n",
      "\n",
      "$user$best_score\n",
      "[1] 0.8775\n",
      "\n",
      "$user$students\n",
      "[1] \"2019702165;2019702174;2020702024\"\n",
      "\n",
      "\n",
      "$competition\n",
      "[1] \"IE582-Test Data\"\n",
      "\n",
      "$auc\n",
      "[1] 0.9101978\n",
      "\n",
      "$ber\n",
      "[1] 0.81485\n",
      "\n",
      "$score\n",
      "[1] 0.8625239\n",
      "\n",
      "$date\n",
      "[1] \"2021-02-13T21:21:09.405624+03:00\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "send_submission(predictions, token, url=subm_url, submit_now= TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "imptable = data.table(varImp(RFproc)$importance)\n",
    "imptable[,variable := 1:dim(imptable)[1] ]\n",
    "imptable <- imptable[order(Overall ,decreasing = TRUE),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Overall</th><th scope=col>variable</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>100.000000</td><td>27        </td></tr>\n",
       "\t<tr><td> 89.411760</td><td>20        </td></tr>\n",
       "\t<tr><td> 83.171799</td><td>29        </td></tr>\n",
       "\t<tr><td> 54.456687</td><td>51        </td></tr>\n",
       "\t<tr><td> 53.133259</td><td>11        </td></tr>\n",
       "\t<tr><td> 36.751516</td><td>24        </td></tr>\n",
       "\t<tr><td> 29.408763</td><td> 6        </td></tr>\n",
       "\t<tr><td> 28.914562</td><td>45        </td></tr>\n",
       "\t<tr><td> 28.610669</td><td> 7        </td></tr>\n",
       "\t<tr><td> 28.601743</td><td> 5        </td></tr>\n",
       "\t<tr><td> 28.463362</td><td> 8        </td></tr>\n",
       "\t<tr><td> 26.594064</td><td>39        </td></tr>\n",
       "\t<tr><td> 25.223440</td><td>49        </td></tr>\n",
       "\t<tr><td> 20.142176</td><td> 1        </td></tr>\n",
       "\t<tr><td> 17.068469</td><td>35        </td></tr>\n",
       "\t<tr><td> 15.727722</td><td>41        </td></tr>\n",
       "\t<tr><td> 14.508706</td><td>14        </td></tr>\n",
       "\t<tr><td> 10.248858</td><td>17        </td></tr>\n",
       "\t<tr><td>  9.980433</td><td>21        </td></tr>\n",
       "\t<tr><td>  9.921187</td><td> 9        </td></tr>\n",
       "\t<tr><td>  8.481579</td><td>53        </td></tr>\n",
       "\t<tr><td>  7.745012</td><td>37        </td></tr>\n",
       "\t<tr><td>  7.513224</td><td> 3        </td></tr>\n",
       "\t<tr><td>  7.287701</td><td>38        </td></tr>\n",
       "\t<tr><td>  6.457845</td><td>13        </td></tr>\n",
       "\t<tr><td>  5.857344</td><td>36        </td></tr>\n",
       "\t<tr><td>  5.505843</td><td> 2        </td></tr>\n",
       "\t<tr><td>  5.334111</td><td> 4        </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " Overall & variable\\\\\n",
       "\\hline\n",
       "\t 100.000000 & 27        \\\\\n",
       "\t  89.411760 & 20        \\\\\n",
       "\t  83.171799 & 29        \\\\\n",
       "\t  54.456687 & 51        \\\\\n",
       "\t  53.133259 & 11        \\\\\n",
       "\t  36.751516 & 24        \\\\\n",
       "\t  29.408763 &  6        \\\\\n",
       "\t  28.914562 & 45        \\\\\n",
       "\t  28.610669 &  7        \\\\\n",
       "\t  28.601743 &  5        \\\\\n",
       "\t  28.463362 &  8        \\\\\n",
       "\t  26.594064 & 39        \\\\\n",
       "\t  25.223440 & 49        \\\\\n",
       "\t  20.142176 &  1        \\\\\n",
       "\t  17.068469 & 35        \\\\\n",
       "\t  15.727722 & 41        \\\\\n",
       "\t  14.508706 & 14        \\\\\n",
       "\t  10.248858 & 17        \\\\\n",
       "\t   9.980433 & 21        \\\\\n",
       "\t   9.921187 &  9        \\\\\n",
       "\t   8.481579 & 53        \\\\\n",
       "\t   7.745012 & 37        \\\\\n",
       "\t   7.513224 &  3        \\\\\n",
       "\t   7.287701 & 38        \\\\\n",
       "\t   6.457845 & 13        \\\\\n",
       "\t   5.857344 & 36        \\\\\n",
       "\t   5.505843 &  2        \\\\\n",
       "\t   5.334111 &  4        \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Overall | variable |\n",
       "|---|---|\n",
       "| 100.000000 | 27         |\n",
       "|  89.411760 | 20         |\n",
       "|  83.171799 | 29         |\n",
       "|  54.456687 | 51         |\n",
       "|  53.133259 | 11         |\n",
       "|  36.751516 | 24         |\n",
       "|  29.408763 |  6         |\n",
       "|  28.914562 | 45         |\n",
       "|  28.610669 |  7         |\n",
       "|  28.601743 |  5         |\n",
       "|  28.463362 |  8         |\n",
       "|  26.594064 | 39         |\n",
       "|  25.223440 | 49         |\n",
       "|  20.142176 |  1         |\n",
       "|  17.068469 | 35         |\n",
       "|  15.727722 | 41         |\n",
       "|  14.508706 | 14         |\n",
       "|  10.248858 | 17         |\n",
       "|   9.980433 | 21         |\n",
       "|   9.921187 |  9         |\n",
       "|   8.481579 | 53         |\n",
       "|   7.745012 | 37         |\n",
       "|   7.513224 |  3         |\n",
       "|   7.287701 | 38         |\n",
       "|   6.457845 | 13         |\n",
       "|   5.857344 | 36         |\n",
       "|   5.505843 |  2         |\n",
       "|   5.334111 |  4         |\n",
       "\n"
      ],
      "text/plain": [
       "   Overall    variable\n",
       "1  100.000000 27      \n",
       "2   89.411760 20      \n",
       "3   83.171799 29      \n",
       "4   54.456687 51      \n",
       "5   53.133259 11      \n",
       "6   36.751516 24      \n",
       "7   29.408763  6      \n",
       "8   28.914562 45      \n",
       "9   28.610669  7      \n",
       "10  28.601743  5      \n",
       "11  28.463362  8      \n",
       "12  26.594064 39      \n",
       "13  25.223440 49      \n",
       "14  20.142176  1      \n",
       "15  17.068469 35      \n",
       "16  15.727722 41      \n",
       "17  14.508706 14      \n",
       "18  10.248858 17      \n",
       "19   9.980433 21      \n",
       "20   9.921187  9      \n",
       "21   8.481579 53      \n",
       "22   7.745012 37      \n",
       "23   7.513224  3      \n",
       "24   7.287701 38      \n",
       "25   6.457845 13      \n",
       "26   5.857344 36      \n",
       "27   5.505843  2      \n",
       "28   5.334111  4      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(imptable,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAAgP9NTU1oaGh8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\njQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(varImp(RFproc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>27</li>\n",
       "\t<li>20</li>\n",
       "\t<li>29</li>\n",
       "\t<li>51</li>\n",
       "\t<li>11</li>\n",
       "\t<li>24</li>\n",
       "\t<li>6</li>\n",
       "\t<li>45</li>\n",
       "\t<li>7</li>\n",
       "\t<li>5</li>\n",
       "\t<li>8</li>\n",
       "\t<li>39</li>\n",
       "\t<li>49</li>\n",
       "\t<li>1</li>\n",
       "\t<li>35</li>\n",
       "\t<li>41</li>\n",
       "\t<li>14</li>\n",
       "\t<li>17</li>\n",
       "\t<li>21</li>\n",
       "\t<li>9</li>\n",
       "\t<li>53</li>\n",
       "\t<li>37</li>\n",
       "\t<li>3</li>\n",
       "\t<li>38</li>\n",
       "\t<li>13</li>\n",
       "\t<li>36</li>\n",
       "\t<li>2</li>\n",
       "\t<li>4</li>\n",
       "\t<li>113</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 27\n",
       "\\item 20\n",
       "\\item 29\n",
       "\\item 51\n",
       "\\item 11\n",
       "\\item 24\n",
       "\\item 6\n",
       "\\item 45\n",
       "\\item 7\n",
       "\\item 5\n",
       "\\item 8\n",
       "\\item 39\n",
       "\\item 49\n",
       "\\item 1\n",
       "\\item 35\n",
       "\\item 41\n",
       "\\item 14\n",
       "\\item 17\n",
       "\\item 21\n",
       "\\item 9\n",
       "\\item 53\n",
       "\\item 37\n",
       "\\item 3\n",
       "\\item 38\n",
       "\\item 13\n",
       "\\item 36\n",
       "\\item 2\n",
       "\\item 4\n",
       "\\item 113\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 27\n",
       "2. 20\n",
       "3. 29\n",
       "4. 51\n",
       "5. 11\n",
       "6. 24\n",
       "7. 6\n",
       "8. 45\n",
       "9. 7\n",
       "10. 5\n",
       "11. 8\n",
       "12. 39\n",
       "13. 49\n",
       "14. 1\n",
       "15. 35\n",
       "16. 41\n",
       "17. 14\n",
       "18. 17\n",
       "19. 21\n",
       "20. 9\n",
       "21. 53\n",
       "22. 37\n",
       "23. 3\n",
       "24. 38\n",
       "25. 13\n",
       "26. 36\n",
       "27. 2\n",
       "28. 4\n",
       "29. 113\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1]  27  20  29  51  11  24   6  45   7   5   8  39  49   1  35  41  14  17  21\n",
       "[20]   9  53  37   3  38  13  36   2   4 113"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols <- c(imptable[c(1:28),variable ] ,113);cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "allprocreduced <- allproc[,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep1: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep2: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep3: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep4: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold01.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold02.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold03.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold04.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold05.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold06.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold07.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold08.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold09.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 3, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry= 5, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=10, splitrule=gini, min.node.size=5 \n",
      "+ Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "- Fold10.Rep5: mtry=15, splitrule=gini, min.node.size=5 \n",
      "Aggregating results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting tuning parameters\n",
      "Fitting mtry = 3, splitrule = gini, min.node.size = 5 on full training set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "2074 samples\n",
       "  28 predictor\n",
       "   2 classes: 'a', 'b' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold, repeated 5 times) \n",
       "Summary of sample sizes: 1867, 1867, 1867, 1866, 1866, 1867, ... \n",
       "Addtional sampling using up-sampling\n",
       "\n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  ROC        Sens       Spec     \n",
       "   3    0.8697704  0.8496080  0.6757490\n",
       "   5    0.8695502  0.8853993  0.6203608\n",
       "  10    0.8637471  0.8820684  0.6081804\n",
       "  15    0.8578561  0.8742789  0.5999373\n",
       "\n",
       "Tuning parameter 'splitrule' was held constant at a value of gini\n",
       "\n",
       "Tuning parameter 'min.node.size' was held constant at a value of 5\n",
       "ROC was used to select the optimal model using the largest value.\n",
       "The final values used for the model were mtry = 3, splitrule = gini\n",
       " and min.node.size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(13)\n",
    "RFprocred <- train(y~., data = allprocreduced, method=\"ranger\",  num.trees = 250, metric = \"ROC\",\n",
    "              trControl = fitControl, tuneGrid = tunegrid, importance = 'permutation')\n",
    "RFprocred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions <- predict(RFprocred, submitproc,type = \"prob\" )$b; predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_submission(predictions, token, url=subm_url, submit_now= TRUE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
